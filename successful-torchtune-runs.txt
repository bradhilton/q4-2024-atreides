$ tune run --nproc_per_node 8 full_finetune_distributed --config llama3/8B_full compile=true
$ tune run --nproc_per_node 8 full_finetune_distributed --config mistral/7B_full batch_size=64
$ tune run --nproc_per_node 8 full_finetune_distributed --config gemma/2B_full batch_size=32
$ tune run --nproc_per_node 8 full_finetune_distributed --config phi3/mini_full batch_size=64
$ tune run --nproc_per_node 8 full_finetune_distributed --config gemma/7B_full batch_size=16
$ tune run --nproc_per_node 8 full_finetune_distributed --config gemma2/2B_full batch_size=32
$ tune run --nproc_per_node 8 full_finetune_distributed --config qwen2/7B_full batch_size=32
$ tune run --nproc_per_node 8 full_finetune_distributed --config gemma2/9B_full batch_size=8
$ tune run --nproc_per_node 8 full_finetune_distributed --config qwen2/0.5B_full batch_size=16
$ tune run --nproc_per_node 8 full_finetune_distributed --config qwen2/1.5B_full batch_size=8
$ tune run --nproc_per_node 8 full_finetune_distributed --config qwen2_5/0.5B_full batch_size=16
$ tune run --nproc_per_node 8 full_finetune_distributed --config qwen2_5/1.5B_full batch_size=8
$ tune run --nproc_per_node 8 full_finetune_distributed --config gemma2/27B_full batch_size=4
$ tune run --nproc_per_node 8 full_finetune_distributed --config qwen2_5/3B_full batch_size=32
$ tune run --nproc_per_node 8 lora_finetune_distributed --config gemma/2B_lora batch_size=32
$ tune run --nproc_per_node 8 full_finetune_distributed --config qwen2_5/7B_full batch_size=32
$ tune run --nproc_per_node 8 lora_finetune_distributed --config gemma/7B_lora batch_size=32
$ tune run --nproc_per_node 8 lora_finetune_distributed --config gemma2/2B_lora batch_size=32
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2/7B_lora batch_size=8
$ tune run --nproc_per_node 8 full_finetune_distributed --config llama3_1/8B_full batch_size=8
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2/0.5B_lora batch_size=32
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2/1.5B_lora batch_size=16
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2_5/0.5B_lora batch_size=16
$ tune run --nproc_per_node 8 full_finetune_distributed --config llama2/7B_full batch_size=8
$ tune run --nproc_per_node 8 lora_finetune_distributed --config gemma2/9B_lora batch_size=16
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2_5/1.5B_lora batch_size=8
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2_5/3B_lora batch_size=8
$ tune run --nproc_per_node 8 full_finetune_distributed --config llama2/13B_full batch_size=4
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2_5/7B_lora batch_size=4
$ tune run --nproc_per_node 8 full_finetune_distributed --config llama3_2/1B_full batch_size=16
$ tune run --nproc_per_node 8 lora_finetune_distributed --config gemma2/27B_lora batch_size=4
$ tune run --nproc_per_node 8 full_finetune_distributed --config llama3_2/3B_full batch_size=8
$ tune run --nproc_per_node 8 lora_finetune_distributed --config mistral/7B_lora batch_size=8
$ tune run --nproc_per_node 8 lora_finetune_distributed --config phi3/mini_lora batch_size=8
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2_5/32B_lora batch_size=2
$ tune run --nproc_per_node 8 lora_finetune_distributed --config llama2/7B_lora batch_size=4
$ tune run --nproc_per_node 8 lora_finetune_distributed --config llama2/13B_lora batch_size=4
$ tune run --nproc_per_node 8 lora_finetune_distributed --config qwen2_5/72B_lora batch_size=4
$ tune run --nproc_per_node 8 knowledge_distillation_distributed --config qwen2/knowledge_distillation_distributed batch_size=4
$ tune run --nproc_per_node 8 full_finetune_distributed --config llama3_2_vision/11B_full batch_size=4
$ tune run --nproc_per_node 8 qat_distributed --config llama2/7B_qat_full batch_size=16
$ tune run --nproc_per_node 8 lora_finetune_distributed --config llama2/70B_lora batch_size=8
$ tune run --nproc_per_node 8 qat_distributed --config llama3/8B_qat_full batch_size=16
$ tune run --nproc_per_node 8 lora_finetune_distributed --config llama2/7B_qlora batch_size=16