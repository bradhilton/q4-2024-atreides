{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.54it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.28it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.68it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.vllm import start_vllm_server\n",
    "\n",
    "model = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "shutdown_server, client = await start_vllm_server(\n",
    "    disable_log_requests=True,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "model_dir = subprocess.run(\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from torchtune.training.checkpointing import FullModelHFCheckpointer\n",
    "\n",
    "output_dir = \"./models/test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "    checkpoint_dir=model_dir,\n",
    "    checkpoint_files=glob.glob(f\"{model_dir}/*.safetensors\")\n",
    "    + glob.glob(f\"{model_dir}/*.pt\"),\n",
    "    output_dir=output_dir,\n",
    "    model_type=\"LLAMA3\",  # type: ignore\n",
    ")\n",
    "state_dict = checkpointer.load_checkpoint()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1031 22:57:29.403000 124506728208192 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.0643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1031 22:57:54.877000 124491669702208 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.1835\n",
      "Loss: 11.1322\n",
      "Loss: 6.4802\n",
      "Loss: 3.6116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "from dataclasses import dataclass\n",
    "import bitsandbytes.optim\n",
    "from torchtune.modules.loss import CEWithChunkedOutputLoss\n",
    "from typing import Iterable\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    tokens: torch.Tensor  # (batch_size, seq_len)\n",
    "    mask: torch.Tensor  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "\n",
    "def batches(batch_size: int, sequence_length: int) -> Iterable[Batch]:\n",
    "    while True:\n",
    "        yield Batch(\n",
    "            tokens=torch.randint(\n",
    "                0,\n",
    "                model.tok_embeddings.weight.shape[0],\n",
    "                size=(batch_size, sequence_length),\n",
    "            ).to(\"cuda\"),\n",
    "            # causal mask\n",
    "            mask=torch.tril(torch.ones(sequence_length, sequence_length))\n",
    "            .unsqueeze(0)\n",
    "            .to(torch.bfloat16)  # Convert mask to bf16\n",
    "            .to(\"cuda\"),\n",
    "        )\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "model = llama3_1_8b()\n",
    "model.load_state_dict(state_dict[\"model\"])\n",
    "model.to(torch.bfloat16).to(\"cuda\")\n",
    "model.compile()\n",
    "optimizer = bitsandbytes.optim.PagedAdamW8bit(model.parameters(), lr=2e-5)\n",
    "loss_fn = CEWithChunkedOutputLoss()\n",
    "loss_fn.compute_cross_entropy = torch.compile(loss_fn.compute_cross_entropy)\n",
    "\n",
    "\n",
    "for i, batch in enumerate(batches(1, 4096)):\n",
    "    logits = model.forward(batch.tokens, mask=batch.mask)\n",
    "    assert isinstance(logits, torch.Tensor)\n",
    "    # shift tokens right to create targets\n",
    "    targets = batch.tokens[:, 1:]\n",
    "    preds = logits[:, :-1, :]\n",
    "\n",
    "    loss = loss_fn(\n",
    "        preds.chunk(loss_fn.num_output_chunks, dim=1),\n",
    "        targets,\n",
    "    )\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2667\n",
      "Loss: 1.8741\n",
      "Loss: 1.4779\n",
      "Loss: 1.1809\n",
      "Loss: 1.0327\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(batches(1, 4096)):\n",
    "    logits = model.forward(batch.tokens, mask=batch.mask)\n",
    "    assert isinstance(logits, torch.Tensor)\n",
    "    # shift tokens right to create targets\n",
    "    targets = batch.tokens[:, 1:]\n",
    "    preds = logits[:, :-1, :]\n",
    "\n",
    "    loss = loss_fn(\n",
    "        preds.chunk(loss_fn.num_output_chunks, dim=1),\n",
    "        targets,\n",
    "    )\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if i > 3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
