{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 02:25:38 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import Trainer\n",
    "from lib.utils import return_exception\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "@return_exception\n",
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": random.sample(Clue.suspects, k=3),\n",
    "            \"weapon\": random.sample(Clue.weapons, k=3),\n",
    "            \"room\": random.sample(Clue.rooms, k=3),\n",
    "            # \"motive\": random.sample(Clue.motives, k=3),\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt = game.get_prompt()\n",
    "    follow_up = \"Fill out your answer like this:\\n\" + \"\\n\".join(\n",
    "        f\"{element.capitalize()}: <#{element.upper()}#>\" for element in game.elements\n",
    "    )\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{element}: {solution}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for element, solution in game.solution.items()\n",
    "            ]\n",
    "        ) / len(game.solution)\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_episodes():\n",
    "    while True:\n",
    "        yield sample_random_episode()\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=\"./models/rl\",\n",
    "    samples_per_episode=8,\n",
    "    branch_factor=2,\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=256,\n",
    "    patience_per_sample=0.25,\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=[sample_random_episode() for _ in range(256)],\n",
    "    torchrun_kwargs=dict(nnodes=1, nproc_per_node=torch.cuda.device_count()),\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=4,\n",
    "        epochs=2,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=5e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            clip_epsilon=0.3,\n",
    "            entropy_coef=0.04,\n",
    "            kl_coef=0.02,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=2,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "    vllm_kwargs=dict(\n",
    "        block_size=32,\n",
    "        disable_log_requests=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enforce_eager=True,\n",
    "        max_model_len=16384,\n",
    "        max_num_seqs=2048,\n",
    "        max_num_batched_tokens=16384 * 4,\n",
    "        # scheduling_policy=\"priority\",\n",
    "        # tensor_parallel_size=torch.cuda.device_count() // 8,\n",
    "    ),\n",
    "    vllm_max_concurrent_samples=2048,\n",
    "    vllm_min_time_between_requests=0.0,\n",
    "    vllm_num=8,\n",
    "    vllm_timeout=90 + 30 * torch.cuda.device_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df367c78bf9d40c98bbf4de74b44955e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (28.0 remaining samples x 0.25 patience per sample = 7.0 seconds)\n",
      "Generated 538,429 tokens\n",
      "Packed sequences in 0.22s ✓\n",
      "Prepared tensors in 4.77s ✓\n",
      "Created mask in 11.35s ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.max_mask_sequence_batch_size = 16\n",
    "# (eval_score, eval_exceptions), \n",
    "result, = await asyncio.gather(\n",
    "    # trainer.eval(\"val\", 0, return_exceptions=True),\n",
    "    trainer.explore(1, return_exceptions=True),\n",
    ")\n",
    "# print(f\"Eval score: {eval_score:.2%}\")\n",
    "print(\n",
    "    f\"Generated {sum(completion.num_token_logprobs() for episode in result.episodes for completion in episode.completion.descendants()):,} tokens\"\n",
    ")\n",
    "tensors = trainer.tensors(result.episodes)\n",
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed sequences in 0.23s ✓\n",
      "Prepared tensors in 4.05s ✓\n",
      "Created mask in 11.62s ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors = trainer.tensors(result.episodes)\n",
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2113536)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(tensors[\"advantages\"].shape).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([127, 16384, 16384])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "i = 0\n",
    "_tensors = result.tensors()\n",
    "key = \"input_pos\"\n",
    "\n",
    "show(\n",
    "    _tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        _tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(_tensors[key][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(_tensors[key][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16384, 16384])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tensors()[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m127\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m127\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;28mprint\u001b[39m(i, j)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(127):\n",
    "    for j in range(127):\n",
    "        if (tensors[\"mask\"][i] == result.tensors()[\"mask\"][j]).all():\n",
    "            print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = \"advantages\"\n",
    "torch.isclose(tensors[key], result.tensors()[key], rtol=1e-5, atol=1e-8, equal_nan=True).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all((tensors[\"weights\"] == result.tensors()[\"weights\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mexceptions[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:43\u001b[0m, in \u001b[0;36mExploreResult.done_callback\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: asyncio\u001b[38;5;241m.\u001b[39mTask[Episode]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pack_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_exception(exception)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:112\u001b[0m, in \u001b[0;36mExploreResult._pack_episode\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mancestors(including_self\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence[c] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:157\u001b[0m, in \u001b[0;36mExploreResult._write_sequence\u001b[0;34m(self, force_write_mask)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    154\u001b[0m     force_write_mask\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_mask_sequence_batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    156\u001b[0m ):\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m packed_tensors\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:187\u001b[0m, in \u001b[0;36mExploreResult._write_mask\u001b[0;34m(self, packed_tensors)\u001b[0m\n\u001b[1;32m    179\u001b[0m     ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m*\u001b[39m (max_ancestors \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids))\n\u001b[1;32m    180\u001b[0m     ancestor_ids[completion] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ids)\n\u001b[1;32m    181\u001b[0m packed_tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][start:stop] \u001b[38;5;241m=\u001b[39m get_mask(\n\u001b[1;32m    182\u001b[0m     ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequences_to_tensor(\n\u001b[1;32m    183\u001b[0m         sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences[start:stop],\n\u001b[1;32m    184\u001b[0m         pad_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m completion: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_tensors[completion][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    186\u001b[0m     ),\n\u001b[0;32m--> 187\u001b[0m     ancestor_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sequences_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                \u001b[49m\u001b[43mancestor_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    199\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:305\u001b[0m, in \u001b[0;36mExploreResult._sequences_to_tensor\u001b[0;34m(self, sequences, pad_value, map)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sequences_to_tensor\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    299\u001b[0m     sequences: \u001b[38;5;28mlist\u001b[39m[Counter[Completion]],\n\u001b[1;32m    300\u001b[0m     pad_value: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mmap\u001b[39m: Callable[[Completion], torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    302\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    304\u001b[0m         [\n\u001b[0;32m--> 305\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sequence_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpad_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m    311\u001b[0m         ]\n\u001b[1;32m    312\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:321\u001b[0m, in \u001b[0;36mExploreResult._sequence_to_tensor\u001b[0;34m(self, sequence, pad_value, map)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sequence_to_tensor\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     sequence: Counter[Completion],\n\u001b[1;32m    317\u001b[0m     pad_value: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mmap\u001b[39m: Callable[[Completion], torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    319\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m truncate_pad(\n\u001b[0;32m--> 321\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m sequence]),\n\u001b[1;32m    322\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length],\n\u001b[1;32m    323\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    324\u001b[0m         value\u001b[38;5;241m=\u001b[39mpad_value,\n\u001b[1;32m    325\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:190\u001b[0m, in \u001b[0;36mExploreResult._write_mask.<locals>.<lambda>\u001b[0;34m(completion)\u001b[0m\n\u001b[1;32m    179\u001b[0m     ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m*\u001b[39m (max_ancestors \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids))\n\u001b[1;32m    180\u001b[0m     ancestor_ids[completion] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ids)\n\u001b[1;32m    181\u001b[0m packed_tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][start:stop] \u001b[38;5;241m=\u001b[39m get_mask(\n\u001b[1;32m    182\u001b[0m     ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequences_to_tensor(\n\u001b[1;32m    183\u001b[0m         sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences[start:stop],\n\u001b[1;32m    184\u001b[0m         pad_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m completion: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_tensors[completion][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    186\u001b[0m     ),\n\u001b[1;32m    187\u001b[0m     ancestor_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequences_to_tensor(\n\u001b[1;32m    188\u001b[0m         sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences[start:stop],\n\u001b[1;32m    189\u001b[0m         pad_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m completion: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                \u001b[49m\u001b[43mancestor_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    198\u001b[0m     ),\n\u001b[1;32m    199\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "raise result.exceptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mexceptions\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result.exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478521.64705882355"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_033_717 / 4.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309918.3183183183"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_064_056 / 6.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192005.2093023256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_064_056 / 10.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258950.125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_071_601 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176306.46808510637"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_071_601 / 11.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249638.84848484848"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4_119_041 / 16.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl/0003 --disable-log-requests --max-num-seqs=512 --scheduling-policy=priority --tensor-parallel-size=8 --port=8000 --api-key=default\n",
      "INFO 11-24 02:30:57 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-24 02:30:57 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl/0003', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='priority', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7443e69927a0>)\n",
      "INFO 11-24 02:30:57 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/30ed2f7f-cd62-4b95-9c23-f484c435b110 for IPC Path.\n",
      "INFO 11-24 02:30:57 api_server.py:179] Started engine process with PID 54698\n",
      "INFO 11-24 02:31:02 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-24 02:31:02 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-24 02:31:11 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-24 02:31:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-24 02:31:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-24 02:31:12 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 240 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-24 02:31:12 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x78ac2a596390>, local_subscribe_port=59765, remote_subscribe_port=None)\n",
      "INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.20s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.08s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:45 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:45 model_runner.py:1067] Loading model weights took 1.8735 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.20s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.29s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "INFO 11-24 02:31:53 distributed_gpu_executor.py:57] # GPU blocks: 260871, # CPU blocks: 16384\n",
      "INFO 11-24 02:31:53 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 509.51x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "INFO 11-24 02:33:00 api_server.py:232] vLLM to use /tmp/tmpg5rmsg7p as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-24 02:33:00 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-24 02:33:00 launcher.py:19] Available routes are:\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [54578]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37536 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e516f643d0354e5fa0302c300abd6ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405bcd9d59fe43b79f2fb94869d969ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed sequences in 7.92s ✓\n",
      "Prepared tensors in 18.11s ✓\n",
      "Created mask in 29.57s ✓\n",
      "$ tune run --nnodes=1 --nproc-per-node=8 lib.recipes.rl.RLRecipe --config /home/ubuntu/atreides/experiments/models/rl/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1124 02:46:47.268000 125095847155520 torch/distributed/run.py:779] \n",
      "W1124 02:46:47.268000 125095847155520 torch/distributed/run.py:779] *****************************************\n",
      "W1124 02:46:47.268000 125095847155520 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1124 02:46:47.268000 125095847155520 torch/distributed/run.py:779] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl/0003\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl/0003/hf_model_0002_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl/0003/hf_model_0003_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl/0003/hf_model_0004_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl/0003/hf_model_0001_0.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl/tensors\n",
      "  num_sequences: 202\n",
      "  sequence_length: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 2\n",
      "gradient_accumulation_steps: 2\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.3\n",
      "  entropy_coef: 0.04\n",
      "  kl_coef: 0.02\n",
      "  normalize_advantages: false\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/models/rl/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/models/rl/logs/log_1732416425.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 20.46 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 2.91 GiB\n",
      "\tGPU peak memory reserved: 3.03 GiB\n",
      "\tGPU peak memory active: 2.91 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|3|Loss: 0.0023: 100%|██████████| 3/3 [02:56<00:00, 51.93s/it, entropy=0.1170, kl_div=0.0372, policy=0.0062]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 9.97 secs\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict...\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict took 21.00 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 32.12 GB saved to /home/ubuntu/atreides/experiments/models/rl/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 84.50 secs\n",
      "\n",
      "1|3|Loss: 0.0023: 100%|██████████| 3/3 [04:56<00:00, 98.68s/it, entropy=0.1170, kl_div=0.0372, policy=0.0062]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:37<01:15, 37.84s/it]\u001b[A\n",
      "2|4|Loss: -0.0010:  33%|███▎      | 1/3 [00:37<01:15, 37.84s/it]\u001b[A\n",
      "2|4|Loss: -0.0010:  33%|███▎      | 1/3 [00:37<01:15, 37.84s/it, entropy=0.1279, kl_div=0.0382, policy=0.0034]\u001b[A\n",
      "2|4|Loss: -0.0010:  67%|██████▋   | 2/3 [01:15<00:37, 37.65s/it, entropy=0.1279, kl_div=0.0382, policy=0.0034]\u001b[A\n",
      "2|5|Loss: -0.0018:  67%|██████▋   | 2/3 [01:15<00:37, 37.65s/it, entropy=0.1279, kl_div=0.0382, policy=0.0034]\u001b[A\n",
      "2|5|Loss: -0.0018:  67%|██████▋   | 2/3 [01:15<00:37, 37.65s/it, entropy=0.1210, kl_div=0.0351, policy=0.0024]\u001b[A\n",
      "2|5|Loss: -0.0018: 100%|██████████| 3/3 [01:53<00:00, 37.75s/it, entropy=0.1210, kl_div=0.0351, policy=0.0024]\u001b[A\n",
      "2|6|Loss: -0.0011: 100%|██████████| 3/3 [01:53<00:00, 37.75s/it, entropy=0.1210, kl_div=0.0351, policy=0.0024]\u001b[A\n",
      "2|6|Loss: -0.0011: 100%|██████████| 3/3 [01:53<00:00, 37.75s/it, entropy=0.1255, kl_div=0.0317, policy=0.0033]\u001b[AINFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 12.02 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0003_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0004_1.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 19.18 secs\n",
      "2|6|Loss: -0.0011: 100%|██████████| 3/3 [02:25<00:00, 48.62s/it, entropy=0.1255, kl_div=0.0317, policy=0.0033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 4 model files to /home/ubuntu/atreides/experiments/models/rl/0004\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl/0004 --disable-log-requests --max-num-seqs=512 --scheduling-policy=priority --tensor-parallel-size=8 --port=8000 --api-key=default\n",
      "INFO 11-24 02:55:10 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-24 02:55:10 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl/0004', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='priority', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7c3bcdd927a0>)\n",
      "INFO 11-24 02:55:10 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/52b9a20c-529a-4186-b7a7-cc696dd5ae19 for IPC Path.\n",
      "INFO 11-24 02:55:10 api_server.py:179] Started engine process with PID 64280\n",
      "INFO 11-24 02:55:15 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-24 02:55:15 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-24 02:55:24 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-24 02:55:24 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-24 02:55:24 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-24 02:55:24 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 240 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-24 02:55:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:55:46 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x798a7dc0ee40>, local_subscribe_port=44071, remote_subscribe_port=None)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.93s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.77s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.40s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.93s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.03s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:58 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "INFO 11-24 02:55:58 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "INFO 11-24 02:56:07 distributed_gpu_executor.py:57] # GPU blocks: 260871, # CPU blocks: 16384\n",
      "INFO 11-24 02:56:07 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 509.51x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "INFO 11-24 02:57:15 api_server.py:232] vLLM to use /tmp/tmpc4rmlj4w as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-24 02:57:15 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-24 02:57:15 launcher.py:19] Available routes are:\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [64179]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58728 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb47eeea50ec42cba9675af9ce87e2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --disable-log-requests --scheduling-policy=priority --tensor-parallel-size=2 --api-key=default\n",
      "INFO 11-23 18:31:25 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-23 18:31:25 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='priority', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7107af372840>)\n",
      "INFO 11-23 18:31:25 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/8ea271a9-8ace-46ef-ad84-78ace07a0439 for IPC Path.\n",
      "INFO 11-23 18:31:25 api_server.py:179] Started engine process with PID 24512\n",
      "INFO 11-23 18:31:29 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-23 18:31:29 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-23 18:31:32 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-23 18:31:32 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-23 18:31:32 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-23 18:31:32 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 26 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-23 18:31:32 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-23 18:31:36 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-23 18:31:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 11-23 18:31:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 11-23 18:31:36 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7288f6d732f0>, local_subscribe_port=53275, remote_subscribe_port=None)\n",
      "INFO 11-23 18:31:36 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-23 18:31:36 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.58it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.99it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.94it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 18:31:39 model_runner.py:1067] Loading model weights took 7.4829 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:39 model_runner.py:1067] Loading model weights took 7.4829 GB\n",
      "INFO 11-23 18:31:40 distributed_gpu_executor.py:57] # GPU blocks: 61653, # CPU blocks: 4096\n",
      "INFO 11-23 18:31:40 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 120.42x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:42 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:42 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-23 18:31:42 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-23 18:31:42 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:51 custom_all_reduce.py:233] Registering 2275 cuda graph addresses\n",
      "INFO 11-23 18:31:51 custom_all_reduce.py:233] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:51 model_runner.py:1523] Graph capturing finished in 10 secs.\n",
      "INFO 11-23 18:31:51 model_runner.py:1523] Graph capturing finished in 10 secs.\n",
      "INFO 11-23 18:31:52 api_server.py:232] vLLM to use /tmp/tmpjv4iurcw as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-23 18:31:52 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-23 18:31:52 launcher.py:19] Available routes are:\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [24430]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:38090 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181dad2fd5ad46578ee0150fde392c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357a2e98b4364cc483bfe12e719c8ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_score, episodes = await asyncio.gather(trainer.eval(\"val\", 0), trainer.explore(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed sequences in 0.01s ✓\n",
      "Prepared tensors in 0.35s ✓\n",
      "Created mask in 0.65s ✓\n"
     ]
    }
   ],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from torchtune.training import cleanup_before_training\n",
    "from torchtune.training.metric_logging import DiskLogger\n",
    "from typing import Any\n",
    "\n",
    "from lib.recipes.rl import ComponentConfig, RLConfig, RLRecipe\n",
    "from lib.rl.pack import PackedDataset, packed_tensors_to_dir\n",
    "from lib.rl.ppo import PPOLoss\n",
    "\n",
    "\n",
    "tensors, checkpoint_dir, checkpoint_files = await trainer.tune_resources(episodes)\n",
    "\n",
    "PLACEHOLDER: Any = None\n",
    "\n",
    "config = RLConfig(\n",
    "    # Dataset\n",
    "    dataset=ComponentConfig(\n",
    "        PackedDataset, **packed_tensors_to_dir(tensors, trainer.output_dir + \"/tensors\")\n",
    "    ),\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    # Model\n",
    "    model=ComponentConfig(llama3_1_8b),\n",
    "    num_output_chunks=4,\n",
    "    # Checkpointer\n",
    "    checkpointer=ComponentConfig(\n",
    "        \"torchtune.training.FullModelHFCheckpointer\",\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_files=checkpoint_files,\n",
    "        recipe_checkpoint=None,\n",
    "        output_dir=trainer.output_dir,\n",
    "        model_type=\"LLAMA3\",\n",
    "    ),\n",
    "    resume_from_checkpoint=False,\n",
    "    # Fine-tuning arguments\n",
    "    batch_size=4,\n",
    "    epochs=1,\n",
    "    optimizer=ComponentConfig(\n",
    "        \"torch.optim.AdamW\",\n",
    "        # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "        # \"bitsandbytes.optim.AdamW\",\n",
    "        # params=PLACEHOLDER,\n",
    "        lr=5e-6,\n",
    "        fused=True,\n",
    "    ),\n",
    "    loss=ComponentConfig(\n",
    "        PPOLoss,\n",
    "        # clip_epsilon=0.3,\n",
    "        # entropy_coef=0.0,\n",
    "        # kl_coef=0.0,\n",
    "        clip_epsilon=0.3,\n",
    "        entropy_coef=0.025,\n",
    "        kl_coef=0.025,\n",
    "        normalize_advantages=False,\n",
    "    ),\n",
    "    max_steps_per_epoch=None,\n",
    "    compile=False,\n",
    "    optimizer_in_bwd=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Training env\n",
    "    device=\"cuda\",\n",
    "    # Memory management\n",
    "    enable_activation_checkpointing=True,\n",
    "    enable_activation_offloading=False,\n",
    "    custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "    # Reduced precision\n",
    "    dtype=\"bf16\",\n",
    "    # Logging\n",
    "    metric_logger=ComponentConfig(\n",
    "        DiskLogger, log_dir=\"/home/ubuntu/atreides/experiments/logs\"\n",
    "    ),\n",
    "    log_every_n_steps=1,\n",
    "    log_peak_memory_stats=True,\n",
    ")\n",
    "\n",
    "# recipe = RLRecipe(config)\n",
    "# recipe.setup(config)\n",
    "# recipe.train()\n",
    "# recipe.cleanup()\n",
    "# del tensors, recipe\n",
    "# cleanup_before_training()\n",
    "# trainer.save(base_checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "dict_config = config.dict_config()\n",
    "OmegaConf.save(dict_config, trainer.output_dir + \"/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.recipes.rl.RLRecipe --config /home/ubuntu/atreides/experiments/models/rl/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import IO\n",
    "\n",
    "torchrun_kwargs = {\"nnodes\": 1, \"nproc_per_node\": 2}\n",
    "kwargs = {}\n",
    "env = {\"CUDA_LAUNCH_BLOCKING\": \"1\"}\n",
    "\n",
    "args = [\n",
    "    \"tune\",\n",
    "    \"run\",\n",
    "    *[\n",
    "        f\"--{key.replace('_', '-')}{f'={value}' if value is not True else ''}\"\n",
    "        for key, value in torchrun_kwargs.items()\n",
    "    ],\n",
    "    \"lib.recipes.rl.RLRecipe\",\n",
    "    \"--config\",\n",
    "    trainer.output_dir + \"/config.yaml\",\n",
    "    *[\n",
    "        f\"--{key.replace('_', '-')}{f'={value}' if value != True else ''}\"\n",
    "        for key, value in kwargs.items()\n",
    "    ],\n",
    "]\n",
    "print(f\"$ {' '.join(args)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1123 18:47:25.110000 137426734001024 torch/distributed/run.py:779] \n",
      "W1123 18:47:25.110000 137426734001024 torch/distributed/run.py:779] *****************************************\n",
      "W1123 18:47:25.110000 137426734001024 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1123 18:47:25.110000 137426734001024 torch/distributed/run.py:779] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl/tensors\n",
      "  num_sequences: 11\n",
      "  sequence_length: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.3\n",
      "  entropy_coef: 0.025\n",
      "  kl_coef: 0.025\n",
      "  normalize_advantages: false\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/logs/log_1732387649.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 4.42 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 8.47 GiB\n",
      "\tGPU peak memory reserved: 8.62 GiB\n",
      "\tGPU peak memory active: 8.47 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|1|Loss: 0.0149: 100%|██████████| 1/1 [00:41<00:00, 41.20s/it, entropy=0.4231, kl_div=0.0643, policy=0.0239]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 7.35 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 19.64 secs\n",
      "1|1|Loss: 0.0149: 100%|██████████| 1/1 [01:08<00:00, 68.56s/it, entropy=0.4231, kl_div=0.0643, policy=0.0239]\n"
     ]
    }
   ],
   "source": [
    "process = await asyncio.create_subprocess_exec(\n",
    "    *args,\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE,\n",
    "    env={\n",
    "        **os.environ,\n",
    "        **(env or {}),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "async def log_output(stream: asyncio.StreamReader, io: IO[str]) -> None:\n",
    "    while True:\n",
    "        line = await stream.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        decoded_line = line.decode()\n",
    "        io.write(decoded_line)\n",
    "        io.flush()\n",
    "\n",
    "\n",
    "tasks = []\n",
    "if process.stdout:\n",
    "    tasks.append(asyncio.create_task(log_output(process.stdout, sys.stdout)))\n",
    "if process.stderr:\n",
    "    tasks.append(asyncio.create_task(log_output(process.stderr, sys.stderr)))\n",
    "_ = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl/tensors\n",
      "  num_sequences: 10\n",
      "  sequence_length: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.3\n",
      "  entropy_coef: 0.025\n",
      "  kl_coef: 0.025\n",
      "  normalize_advantages: false\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/logs/log_1732386575.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 3.00 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.89 GiB\n",
      "\tGPU peak memory reserved: 15.99 GiB\n",
      "\tGPU peak memory active: 15.89 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|1|Loss: 0.0031:  20%|██        | 1/5 [00:29<01:58, 29.72s/it, entropy=0.5079, kl_div=0.0659, policy=0.0141]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No backend type associated with device type cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWORLD_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrecipe_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/experiments/lib/recipes/rl.py:1147\u001b[0m, in \u001b[0;36mrecipe_main\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m   1145\u001b[0m recipe \u001b[38;5;241m=\u001b[39m RLRecipe(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[1;32m   1146\u001b[0m recipe\u001b[38;5;241m.\u001b[39msetup(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m-> 1147\u001b[0m \u001b[43mrecipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m recipe\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m~/atreides/experiments/lib/recipes/rl.py:1019\u001b[0m, in \u001b[0;36mRLRecipe.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training\u001b[38;5;241m.\u001b[39mis_distributed():\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m running_result\u001b[38;5;241m.\u001b[39mnamed_tensors():\n\u001b[0;32m-> 1019\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# Manually scale the gradients from unnormalized loss by total # of tokens\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m training\u001b[38;5;241m.\u001b[39mscale_grads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m running_result\u001b[38;5;241m.\u001b[39mnum_tokens)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:79\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     81\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:2288\u001b[0m, in \u001b[0;36mall_reduce\u001b[0;34m(tensor, op, group, async_op)\u001b[0m\n\u001b[1;32m   2285\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2286\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2288\u001b[0m work \u001b[38;5;241m=\u001b[39m \u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_op:\n\u001b[1;32m   2291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m work\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No backend type associated with device type cpu"
     ]
    }
   ],
   "source": [
    "from lib.recipes.rl import recipe_main\n",
    "import os\n",
    "from torch import distributed as dist\n",
    "from torchtune.training import is_distributed\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "\n",
    "\n",
    "recipe_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "dict_config = config.dict_config()\n",
    "OmegaConf.save(dict_config, trainer.output_dir + \"/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'lib.rl.completion.Completion'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.rl.completion import Completion\n",
    "\n",
    "\n",
    "OmegaConf.create(OmegaConf.to_yaml(DictConfig(dict(name=f\"{Completion.__module__}.{Completion.__name__}\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import sys\n",
    "\n",
    "traceback.clear_frames(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model checkpoint files found to save in output directory /home/ubuntu/atreides/experiments/models/rl\n"
     ]
    }
   ],
   "source": [
    "trainer.save(base_checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m show(\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mtensors\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     19\u001b[0m         tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m&\u001b[39m (\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;241m~\u001b[39mtorch\u001b[38;5;241m.\u001b[39misnan(tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvantages\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mtorch\u001b[38;5;241m.\u001b[39misnan(tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvantages\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m         )\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensors' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "i = 1\n",
    "\n",
    "show(\n",
    "    tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    f'<div style=\"white-space: pre-wrap\">{list(episodes[2].completion.leaves())[0].html(30.0)}</div>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_pos_ids(\n",
    "    ids: torch.Tensor, parent_ids: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Creates an attention mask and position IDs for hierarchical attention based on node IDs and their parent IDs.\n",
    "\n",
    "    Args:\n",
    "        ids: A tensor of shape (batch_size, sequence_length) containing node IDs\n",
    "        parent_ids: A tensor of shape (batch_size, sequence_length) containing parent IDs for each node\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - mask: A boolean tensor of shape (batch_size, sequence_length, sequence_length) where True indicates\n",
    "          allowed attention connections. Each position can attend to itself and any of its ancestors\n",
    "          in the hierarchy, but only for previous positions (due to causal masking).\n",
    "        - pos_ids: A tensor of shape (batch_size, sequence_length, sequence_length) containing relative\n",
    "          position IDs for each allowed attention connection, with -1 for masked positions.\n",
    "    \"\"\"\n",
    "    mask = ids.unsqueeze(1) == ids.unsqueeze(2)\n",
    "    _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    while torch.any(mask != _mask):\n",
    "        parent_ids = parent_ids.gather(\n",
    "            1, torch.argmax((parent_ids.unsqueeze(2) == ids.unsqueeze(1)).int(), dim=2)\n",
    "        )\n",
    "        mask = _mask\n",
    "        _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    mask &= torch.tril(torch.ones_like(mask, dtype=torch.bool, device=ids.device))\n",
    "    # mask = torch.linalg.matrix_power(mask.float(), mask.size(1) - 1) > 0\n",
    "    pos_ids = (torch.where(mask, mask.cumsum(2), 0) - 1).max(1).values\n",
    "    return mask, pos_ids\n",
    "\n",
    "\n",
    "def test_mask_and_pos_ids(\n",
    "    ids: list[int],\n",
    "    parent_ids: list[int],\n",
    "    expected_mask: list[list[int]],\n",
    "    expected_pos_ids: list[int],\n",
    "):\n",
    "    mask, pos_ids = mask_and_pos_ids(\n",
    "        ids=torch.tensor([ids]), parent_ids=torch.tensor([parent_ids])\n",
    "    )\n",
    "    assert torch.all(mask.int() == torch.tensor([expected_mask])), f\"\\n{mask.int()[0]}\"\n",
    "    assert torch.all(\n",
    "        pos_ids == torch.tensor([expected_pos_ids])\n",
    "    ), f\"{pos_ids[0].tolist()}\"\n",
    "\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1],\n",
    "    parent_ids=[0, 1],\n",
    "    expected_mask=[[1, 0], [0, 1]],\n",
    "    expected_pos_ids=[0, 0],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 1],\n",
    "    parent_ids=[0, 0, 0],\n",
    "    expected_mask=[[1, 0, 0], [1, 1, 0], [1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 0, 1, 2],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 0, 1, 1],\n",
    "    parent_ids=[0, 0, 1, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 0, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 1, 0, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [0, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 1]],\n",
    "    expected_pos_ids=[0, 0, 1, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 2, 3, 3],\n",
    "    parent_ids=[0, 1, 0, 0, 1, 1],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 0, 1, 2, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3, 4, 4, 5, 5],\n",
    "    parent_ids=[0, 0, 1, 1, 2, 2, 3, 3],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 2, 2, 3, 4, 3, 4],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[2, 1, 0],\n",
    "    parent_ids=[2, 2, 0],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 0, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
