{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 22:01:11 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.trainer import Trainer\n",
    "import re\n",
    "\n",
    "\n",
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": Clue.suspects[:3],\n",
    "            \"weapon\": Clue.weapons[:3],\n",
    "            \"room\": Clue.rooms[:3],\n",
    "            # \"motive\": Clue.motives[:6],\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt = game.get_prompt()\n",
    "    follow_up = \"Fill out your answer like this:\\n\" + \"\\n\".join(\n",
    "        f\"{element.capitalize()}: <#{element.upper()}#>\" for element in game.elements\n",
    "    )\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        follow_up_completion = await completion.follow_up(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": follow_up},\n",
    "            ]\n",
    "        )\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{element}: {solution}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for element, solution in game.solution.items()\n",
    "            ]\n",
    "        ) / len(game.solution)\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_episodes():\n",
    "    while True:\n",
    "        yield sample_random_episode()\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    samples_per_episode=8,\n",
    "    branch_factor=2,\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=128,\n",
    "    val_episodes=(sample_random_episode() for _ in range(64)),\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_kwargs=dict(disable_log_requests=True),\n",
    "    vllm_max_concurrent_requests=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --disable-log-requests --api-key=default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 22:01:19 api_server.py:528] vLLM API server version dev\n",
      "INFO 11-20 22:01:19 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x73b67dd00cc0>)\n",
      "INFO 11-20 22:01:19 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/c618cd11-48cc-42f7-ab97-93f5b37fe137 for IPC Path.\n",
      "INFO 11-20 22:01:19 api_server.py:179] Started engine process with PID 14335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 22:01:29 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 11-20 22:01:30 model_runner.py:1060] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-20 22:01:31 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.54it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.68it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-20 22:01:34 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "INFO 11-20 22:01:35 gpu_executor.py:122] # GPU blocks: 9604, # CPU blocks: 2048\n",
      "INFO 11-20 22:01:35 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 18.76x\n",
      "INFO 11-20 22:01:38 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-20 22:01:38 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-20 22:01:50 model_runner.py:1530] Graph capturing finished in 12 secs.\n",
      "INFO 11-20 22:01:50 api_server.py:232] vLLM to use /tmp/tmpdm0swwnl as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-20 22:01:50 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-20 22:01:50 launcher.py:19] Available routes are:\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-20 22:01:50 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [14296]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42386 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c4799b21ff4da7bc5ec440dc23f18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/1 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:72\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:377\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 377\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mAsyncIterable)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:216\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:196\u001b[0m, in \u001b[0;36mAsyncConnectionPool.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m connection\u001b[38;5;241m.\u001b[39mhandle_async_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:99\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:76\u001b[0m, in \u001b[0;36mAsyncHTTPConnection.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:122\u001b[0m, in \u001b[0;36mAsyncHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    123\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py:30\u001b[0m, in \u001b[0;36mAutoBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_backend()\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\n\u001b[1;32m     31\u001b[0m     host,\n\u001b[1;32m     32\u001b[0m     port,\n\u001b[1;32m     33\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     34\u001b[0m     local_address\u001b[38;5;241m=\u001b[39mlocal_address,\n\u001b[1;32m     35\u001b[0m     socket_options\u001b[38;5;241m=\u001b[39msocket_options,\n\u001b[1;32m     36\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:115\u001b[0m, in \u001b[0;36mAnyIOBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    110\u001b[0m exc_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    113\u001b[0m     anyio\u001b[38;5;241m.\u001b[39mBrokenResourceError: ConnectError,\n\u001b[1;32m    114\u001b[0m }\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mfail_after(timeout):\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1571\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1571\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m   1572\u001b[0m         request,\n\u001b[1;32m   1573\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1575\u001b[0m     )\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpx/_client.py:1674\u001b[0m, in \u001b[0;36mAsyncClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1672\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m-> 1674\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m   1675\u001b[0m     request,\n\u001b[1;32m   1676\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1677\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1678\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m   1679\u001b[0m )\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpx/_client.py:1702\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1702\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m   1703\u001b[0m         request,\n\u001b[1;32m   1704\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1705\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpx/_client.py:1739\u001b[0m, in \u001b[0;36mAsyncClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[0;32m-> 1739\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpx/_client.py:1776\u001b[0m, in \u001b[0;36mAsyncClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1776\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m transport\u001b[38;5;241m.\u001b[39mhandle_async_request(request)\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, AsyncByteStream)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:376\u001b[0m, in \u001b[0;36mAsyncHTTPTransport.handle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    364\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    365\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    366\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    375\u001b[0m )\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    377\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_async_request(req)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:89\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39meval(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:166\u001b[0m, in \u001b[0;36mTrainer.eval\u001b[0;34m(self, split, pbar_position, return_exceptions)\u001b[0m\n\u001b[1;32m    164\u001b[0m pbar\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(episodes)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_episodes[split] \u001b[38;5;241m=\u001b[39m episodes\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    167\u001b[0m score \u001b[38;5;241m=\u001b[39m get_score()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_scores[split][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel] \u001b[38;5;241m=\u001b[39m get_score()\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:148\u001b[0m, in \u001b[0;36mTrainer.eval.<locals>.done_callback\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    146\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    150\u001b[0m     exceptions\u001b[38;5;241m.\u001b[39mappend(exception)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/episode.py:202\u001b[0m, in \u001b[0;36mEpisode.sample_completions_v2\u001b[0;34m(self, completion_sampler, branch_factor, max_parallel_splits, split_by, split_separators)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_completions(\n\u001b[1;32m    205\u001b[0m                 parent,\n\u001b[1;32m    206\u001b[0m                 model,\n\u001b[1;32m    207\u001b[0m                 completion_sampler,\n\u001b[1;32m    208\u001b[0m                 branch_factor,\n\u001b[1;32m    209\u001b[0m                 fork_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    210\u001b[0m                 split_separators\u001b[38;5;241m=\u001b[39msplit_separators \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mset\u001b[39m(),\n\u001b[1;32m    211\u001b[0m             )\n\u001b[1;32m    212\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m parent \u001b[38;5;129;01min\u001b[39;00m parents\n\u001b[1;32m    213\u001b[0m         )\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/episode.py:259\u001b[0m, in \u001b[0;36mEpisode._sample_completions\u001b[0;34m(self, parent, model, completion_sampler, branch_factor, fork_decay, split_separators)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m completion_sampler\u001b[38;5;241m.\u001b[39msample_completions(\n\u001b[1;32m    260\u001b[0m     parent,\n\u001b[1;32m    261\u001b[0m     strip\u001b[38;5;241m=\u001b[39msplit_separators,\n\u001b[1;32m    262\u001b[0m     n\u001b[38;5;241m=\u001b[39mn,\n\u001b[1;32m    263\u001b[0m )\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_children:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m completions:\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/completion_sampler.py:104\u001b[0m, in \u001b[0;36mCompletionSampler.sample_completions\u001b[0;34m(self, parent, continue_last_message_if_assistant, strip, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     untyped_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemaphore:\n\u001b[1;32m    102\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    103\u001b[0m         ChatCompletion,\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39muntyped_kwargs),\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    107\u001b[0m     Completion(\n\u001b[1;32m    108\u001b[0m         parent\u001b[38;5;241m=\u001b[39mparent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices\n\u001b[1;32m    118\u001b[0m ]\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:1633\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1632\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1635\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1636\u001b[0m             {\n\u001b[1;32m   1637\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1638\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1639\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1640\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1641\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1642\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1643\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1645\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1646\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1647\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1648\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1665\u001b[0m             },\n\u001b[1;32m   1666\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1667\u001b[0m         ),\n\u001b[1;32m   1668\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1669\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1670\u001b[0m         ),\n\u001b[1;32m   1671\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1672\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1673\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1674\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1838\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1826\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1834\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1835\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1836\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1837\u001b[0m     )\n\u001b[0;32m-> 1838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1532\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1533\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1534\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1535\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1536\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1537\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1538\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1618\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[0;32m-> 1618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1619\u001b[0m         input_options,\n\u001b[1;32m   1620\u001b[0m         cast_to,\n\u001b[1;32m   1621\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1622\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1623\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1624\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1625\u001b[0m     )\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1665\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1661\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1666\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1667\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1668\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1669\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1670\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1671\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1595\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1592\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1596\u001b[0m         input_options,\n\u001b[1;32m   1597\u001b[0m         cast_to,\n\u001b[1;32m   1598\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1599\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1600\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1601\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1602\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1665\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1661\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1666\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1667\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1668\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1669\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1670\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1671\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1605\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1596\u001b[0m             input_options,\n\u001b[1;32m   1597\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1601\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1602\u001b[0m         )\n\u001b[1;32m   1604\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1607\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mreason_phrase\n\u001b[1;32m   1609\u001b[0m )\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
     ]
    }
   ],
   "source": [
    "await trainer.eval(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edc977588e14cc5a97ed07a033bd007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 8234 tokens in the messages, Please reduce the length of the messages.\", 'type': 'BadRequestError', 'param': None, 'code': 400}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mexplore()\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:154\u001b[0m, in \u001b[0;36mTrainer.explore\u001b[0;34m(self, pbar_position)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_explore_episode(episode, pbar)))\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:168\u001b[0m, in \u001b[0;36mTrainer._explore_episode\u001b[0;34m(self, episode, pbar)\u001b[0m\n\u001b[1;32m    166\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(frac \u001b[38;5;241m-\u001b[39m _frac)\n\u001b[1;32m    167\u001b[0m frac \u001b[38;5;241m=\u001b[39m _frac\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m episode\u001b[38;5;241m.\u001b[39msample_completions_v2(\n\u001b[1;32m    169\u001b[0m     completion_sampler\u001b[38;5;241m=\u001b[39mcompletion_sampler,\n\u001b[1;32m    170\u001b[0m     branch_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch_factor,\n\u001b[1;32m    171\u001b[0m     max_parallel_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\n\u001b[1;32m    172\u001b[0m         math\u001b[38;5;241m.\u001b[39mceil(remaining_samples \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch_factor \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    173\u001b[0m     ),\n\u001b[1;32m    174\u001b[0m ):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frac \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    177\u001b[0m     c\u001b[38;5;241m.\u001b[39madvantage(cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m episode\u001b[38;5;241m.\u001b[39mcompletion\u001b[38;5;241m.\u001b[39mchildren\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    180\u001b[0m ):\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/episode.py:202\u001b[0m, in \u001b[0;36mEpisode.sample_completions_v2\u001b[0;34m(self, completion_sampler, branch_factor, max_parallel_splits, split_by, split_separators)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_completions(\n\u001b[1;32m    205\u001b[0m                 parent,\n\u001b[1;32m    206\u001b[0m                 model,\n\u001b[1;32m    207\u001b[0m                 completion_sampler,\n\u001b[1;32m    208\u001b[0m                 branch_factor,\n\u001b[1;32m    209\u001b[0m                 fork_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    210\u001b[0m                 split_separators\u001b[38;5;241m=\u001b[39msplit_separators \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mset\u001b[39m(),\n\u001b[1;32m    211\u001b[0m             )\n\u001b[1;32m    212\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m parent \u001b[38;5;129;01min\u001b[39;00m parents\n\u001b[1;32m    213\u001b[0m         )\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/episode.py:275\u001b[0m, in \u001b[0;36mEpisode._sample_completions\u001b[0;34m(self, parent, model, completion_sampler, branch_factor, fork_decay, split_separators)\u001b[0m\n\u001b[1;32m    268\u001b[0m on_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_sample(\n\u001b[1;32m    269\u001b[0m     [\n\u001b[1;32m    270\u001b[0m         EpisodeCompletion(completion, completion_sampler)\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m completions\n\u001b[1;32m    272\u001b[0m     ]\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(on_sample, Coroutine):\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m on_sample\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m, in \u001b[0;36msample_random_episode.<locals>.on_sample\u001b[0;34m(completions)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_sample\u001b[39m(completions: \u001b[38;5;28mlist\u001b[39m[EpisodeCompletion]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;241m*\u001b[39m[reward_completion(completion) \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     68\u001b[0m     ):\n\u001b[1;32m     69\u001b[0m         completion\u001b[38;5;241m.\u001b[39mcommit()\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36msample_random_episode.<locals>.reward_completion\u001b[0;34m(completion)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreward_completion\u001b[39m(completion: EpisodeCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EpisodeCompletion:\n\u001b[0;32m---> 44\u001b[0m     follow_up_completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mfollow_up(\n\u001b[1;32m     45\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     46\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: follow_up},\n\u001b[1;32m     47\u001b[0m         ]\n\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     answer \u001b[38;5;241m=\u001b[39m follow_up_completion\u001b[38;5;241m.\u001b[39mlast_assistant_message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(answer, \u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/episode.py:51\u001b[0m, in \u001b[0;36mEpisodeCompletion.follow_up\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfollow_up\u001b[39m(\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m, messages: \u001b[38;5;28mlist\u001b[39m[ChatCompletionMessageParam]\n\u001b[1;32m     50\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisodeCompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler\u001b[38;5;241m.\u001b[39msample_completions(\n\u001b[1;32m     52\u001b[0m         Completion(parent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_completion, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EpisodeCompletion(_completion\u001b[38;5;241m=\u001b[39mcompletions[\u001b[38;5;241m0\u001b[39m], _sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/completion_sampler.py:104\u001b[0m, in \u001b[0;36mCompletionSampler.sample_completions\u001b[0;34m(self, parent, continue_last_message_if_assistant, strip, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     untyped_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemaphore:\n\u001b[1;32m    102\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    103\u001b[0m         ChatCompletion,\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39muntyped_kwargs),\n\u001b[1;32m    105\u001b[0m     )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    107\u001b[0m     Completion(\n\u001b[1;32m    108\u001b[0m         parent\u001b[38;5;241m=\u001b[39mparent,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices\n\u001b[1;32m    118\u001b[0m ]\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:1633\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1632\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1635\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1636\u001b[0m             {\n\u001b[1;32m   1637\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1638\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1639\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   1640\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1641\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1642\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1643\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1645\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   1646\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1647\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   1648\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   1649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   1657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1665\u001b[0m             },\n\u001b[1;32m   1666\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1667\u001b[0m         ),\n\u001b[1;32m   1668\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1669\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1670\u001b[0m         ),\n\u001b[1;32m   1671\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1672\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1673\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1674\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1838\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1826\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1834\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1835\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1836\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1837\u001b[0m     )\n\u001b[0;32m-> 1838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1532\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1533\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1534\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1535\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1536\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1537\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1538\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/openai/_base_client.py:1633\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1632\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1636\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1637\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1642\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 8192 tokens. However, you requested 8234 tokens in the messages, Please reduce the length of the messages.\", 'type': 'BadRequestError', 'param': None, 'code': 400}"
     ]
    }
   ],
   "source": [
    "await trainer.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49479166666666663, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes: list[Episode] = trainer.eval_episodes[\"val\"]  # type: ignore\n",
    "divisor = max(\n",
    "    sum(\n",
    "        1\n",
    "        for episode in episodes\n",
    "        if any(child.model == trainer.model for child in episode.completion.children)\n",
    "    ),\n",
    "    1,\n",
    ")\n",
    "score = sum(\n",
    "    episode.completion.value(model=trainer.model) for episode in episodes\n",
    ") / divisor\n",
    "score, divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(list(episode.completion.descendants())) for episode in episodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant', 'content': '\\n<|im_end|>'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(episodes[0].completion.children)[0].children)[0].message_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3333333333333333,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 1.3333333333333333,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 1.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333333,\n",
       " 0.0,\n",
       " 1.6666666666666665,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333333,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333333,\n",
       " 1.3333333333333333,\n",
       " 0.0,\n",
       " 1.3333333333333333,\n",
       " 0.0,\n",
       " 1.3333333333333333,\n",
       " 0.0,\n",
       " 1.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.6666666666666666,\n",
       " 0.0,\n",
       " 0.3333333333333333,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 1.3333333333333333,\n",
       " 1.3333333333333333]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[episode.completion.value(model=trainer.model) for episode in episodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(\n",
    "    sum(child.model == trainer.model for child in episode.completion.children)\n",
    "    for episode in episodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0),\n",
       " Episode(samples=1, weight=1.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NousResearch/Hermes-2-Theta-Llama-3-8B': 0.8854166666666666}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_scores[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_mask(ids: torch.Tensor, parent_ids: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a causal attention mask based on token group IDs and their parent relationships.\n",
    "\n",
    "    Each token can attend to:\n",
    "    1. All preceding tokens in its own group (tokens with same ID)\n",
    "    2. All tokens in any ancestor group (following parent_ids chain up)\n",
    "    The mask enforces causality - tokens cannot attend to future positions.\n",
    "\n",
    "    Args:\n",
    "        ids: 1D tensor of token group IDs. Shape: [sequence_length]\n",
    "        parent_ids: 1D tensor of parent group IDs. Shape: [sequence_length]\n",
    "            Any integer value is valid - if a parent ID is not found in ids,\n",
    "            that group is treated as a root node.\n",
    "            Each ID must have a corresponding parent ID at the same index.\n",
    "\n",
    "    Returns:\n",
    "        2D boolean tensor of shape [sequence_length, sequence_length] where\n",
    "        mask[i,j] = True means position i can attend to position j.\n",
    "        False means position i cannot attend to position j.\n",
    "\n",
    "    Example:\n",
    "        ids = torch.tensor([0, 0, 1, 1])\n",
    "        parent_ids = torch.tensor([100, 100, 0, 0])\n",
    "        # Group 1 tokens can attend to group 0 tokens since 0 is their parent\n",
    "        # Group 0's parent (100) is not in ids so it's treated as root\n",
    "    \"\"\"\n",
    "    # Create mask of shape [seq_len, seq_len] initialized to False\n",
    "    seq_len = len(ids)\n",
    "    mask = torch.zeros((seq_len, seq_len), dtype=torch.bool)\n",
    "\n",
    "    # For each target position\n",
    "    for i in range(seq_len):\n",
    "        # Get current id and parent id\n",
    "        curr_id = ids[i]\n",
    "        curr_parent = parent_ids[i]\n",
    "\n",
    "        # For each source position up to and including target\n",
    "        for j in range(i + 1):\n",
    "            # Token can attend to itself and earlier tokens in same group\n",
    "            if ids[j] == curr_id:\n",
    "                mask[i, j] = True\n",
    "\n",
    "            # Token can attend to tokens in ancestor groups\n",
    "            ancestor_id = curr_parent\n",
    "            while ancestor_id in ids:  # Only follow chain while ancestor exists in ids\n",
    "                if ids[j] == ancestor_id:\n",
    "                    mask[i, j] = True\n",
    "                # Move up to next ancestor\n",
    "                # Find first occurrence of current ancestor to get its parent\n",
    "                ancestor_idx = torch.where(ids == ancestor_id)[0][0]\n",
    "                ancestor_id = parent_ids[ancestor_idx]\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_fast_mask(ids: torch.Tensor, parent_ids: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Faster implementation of get_mask using vectorized operations\"\"\"\n",
    "    seq_len = len(ids)\n",
    "\n",
    "    # Get unique group IDs and map them to indices\n",
    "    unique_ids = torch.unique(ids)\n",
    "    group_id_list = unique_ids.tolist()  # Converts tensor to list of Python numbers\n",
    "    group_id_to_index = {group_id: idx for idx, group_id in enumerate(group_id_list)}\n",
    "    num_groups = len(unique_ids)\n",
    "\n",
    "    # Build group_id_to_parent_id mapping\n",
    "    group_parents = {}\n",
    "    for group_id in group_id_list:\n",
    "        indices = (ids == group_id).nonzero(as_tuple=True)[0]\n",
    "        idx = indices[0].item()  # Select the first occurrence\n",
    "        parent_id = parent_ids[idx].item()\n",
    "        group_parents[group_id] = parent_id\n",
    "\n",
    "    # For each group, compute its ancestors\n",
    "    group_ancestors = {}\n",
    "    for group_id in group_id_list:\n",
    "        ancestors = set()\n",
    "        parent_id = group_parents.get(group_id, None)\n",
    "        while parent_id in group_id_list and parent_id not in ancestors:\n",
    "            ancestors.add(parent_id)\n",
    "            parent_id = group_parents.get(parent_id, None)\n",
    "        group_ancestors[group_id] = ancestors\n",
    "\n",
    "    # Create allowed_groups per group index\n",
    "    allowed_groups = torch.zeros((num_groups, num_groups), dtype=torch.bool)\n",
    "    for i, group_id in enumerate(group_id_list):\n",
    "        # Each group can attend to itself\n",
    "        allowed_groups[i, i] = True\n",
    "        # And its ancestors\n",
    "        for ancestor_id in group_ancestors[group_id]:\n",
    "            ancestor_idx = group_id_to_index[ancestor_id]\n",
    "            allowed_groups[i, ancestor_idx] = True\n",
    "\n",
    "    # Map positions to group indices\n",
    "    group_indices = torch.tensor(\n",
    "        [group_id_to_index[group_id.item()] for group_id in ids], dtype=torch.long\n",
    "    )\n",
    "\n",
    "    # Get allowed groups per position\n",
    "    allowed_groups_per_position = allowed_groups[\n",
    "        group_indices\n",
    "    ]  # Shape: [seq_len, num_groups]\n",
    "\n",
    "    # Create group indices matrix for source positions\n",
    "    group_indices_source = group_indices.unsqueeze(0).expand(\n",
    "        seq_len, seq_len\n",
    "    )  # Shape: [seq_len, seq_len]\n",
    "\n",
    "    # Compute mask by checking if source group is allowed for target position\n",
    "    mask = allowed_groups_per_position.gather(1, group_indices_source)\n",
    "\n",
    "    # Enforce causality (tokens cannot attend to future positions)\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))\n",
    "    mask = mask & causal_mask\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "ids = torch.tensor([0, 0, 0, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5] * 2)\n",
    "parent_ids = torch.tensor([-1, -1, -1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2] * 2)\n",
    "mask = get_mask(ids, parent_ids)\n",
    "fast_mask = get_fast_mask(ids, parent_ids)\n",
    "assert torch.all(mask == fast_mask), \"Fast implementation does not match original\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(fast_mask, cmap=\"binary\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Attention Mask\")\n",
    "plt.xlabel(\"Source Position\")\n",
    "plt.ylabel(\"Target Position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faster_mask(ids: torch.Tensor, parent_ids: torch.Tensor) -> torch.Tensor:\n",
    "    mask = ids.unsqueeze(0) == ids.unsqueeze(1)\n",
    "    _mask = mask | (ids.unsqueeze(0) == parent_ids.unsqueeze(1))\n",
    "    parent_ids = parent_ids[ids]\n",
    "    while torch.any(mask != _mask):\n",
    "        mask = _mask\n",
    "        _mask = mask | (ids.unsqueeze(0) == parent_ids.unsqueeze(1))\n",
    "        parent_ids = parent_ids[parent_ids]\n",
    "    mask &= torch.tril(torch.ones_like(mask))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.4 μs ± 2.08 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "get_faster_mask(ids, parent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 5, 6, 7],\n",
       "        [0, 1, 0, 0, 1, 2, 2, 3, 5]])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id: int, parent: \"Node | None\" = None, size: int = 1):\n",
    "        self.id = id\n",
    "        self.parent = parent\n",
    "        self.size = size\n",
    "\n",
    "\n",
    "nodes = [Node(id=0, size=random.randint(1, 1)), Node(id=1, size=random.randint(1, 1))]\n",
    "for i in range(2, 8):\n",
    "    parent = random.choice(nodes)\n",
    "    size = random.randint(1, 2)\n",
    "    node = Node(id=i, parent=parent, size=size)\n",
    "    nodes.append(node)\n",
    "ids = torch.tensor([node.id for node in nodes for _ in range(node.size)])\n",
    "parent_ids = torch.tensor(\n",
    "    [\n",
    "        node.parent.id if node.parent else node.id\n",
    "        for node in nodes\n",
    "        for _ in range(node.size)\n",
    "    ]\n",
    ")\n",
    "# mask = get_mask(ids, parent_ids)\n",
    "# faster_mask = get_faster_mask(ids, parent_ids)\n",
    "torch.stack([ids, parent_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "self must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[406], line 57\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(\n\u001b[1;32m     46\u001b[0m         pos_ids \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([expected_pos_ids])\n\u001b[1;32m     47\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m test_mask_and_pos_ids(\n\u001b[1;32m     51\u001b[0m     ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     52\u001b[0m     parent_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     53\u001b[0m     expected_mask\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m     54\u001b[0m     expected_pos_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     55\u001b[0m )\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtest_mask_and_pos_ids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpected_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpected_pos_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m test_mask_and_pos_ids(\n\u001b[1;32m     65\u001b[0m     ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     66\u001b[0m     parent_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     67\u001b[0m     expected_mask\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m     68\u001b[0m     expected_pos_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     71\u001b[0m test_mask_and_pos_ids(\n\u001b[1;32m     72\u001b[0m     ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     73\u001b[0m     parent_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     74\u001b[0m     expected_mask\u001b[38;5;241m=\u001b[39m[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]],\n\u001b[1;32m     75\u001b[0m     expected_pos_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     76\u001b[0m )\n",
      "Cell \u001b[0;32mIn[406], line 41\u001b[0m, in \u001b[0;36mtest_mask_and_pos_ids\u001b[0;34m(ids, parent_ids, expected_mask, expected_pos_ids)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_mask_and_pos_ids\u001b[39m(\n\u001b[1;32m     36\u001b[0m     ids: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m     37\u001b[0m     parent_ids: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m     38\u001b[0m     expected_mask: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]],\n\u001b[1;32m     39\u001b[0m     expected_pos_ids: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m     40\u001b[0m ):\n\u001b[0;32m---> 41\u001b[0m     mask, pos_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmask_and_pos_ids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mparent_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(mask\u001b[38;5;241m.\u001b[39mint() \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([expected_mask])), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmask\u001b[38;5;241m.\u001b[39mint()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(\n\u001b[1;32m     46\u001b[0m         pos_ids \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([expected_pos_ids])\n\u001b[1;32m     47\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[406], line 26\u001b[0m, in \u001b[0;36mmask_and_pos_ids\u001b[0;34m(ids, parent_ids)\u001b[0m\n\u001b[1;32m     24\u001b[0m     mask \u001b[38;5;241m=\u001b[39m _mask\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     _mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# mask = ids.unsqueeze(1) == ids.unsqueeze(2)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# mask |= ids.unsqueeze(1) == parent_ids.unsqueeze(2)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m mask \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(torch\u001b[38;5;241m.\u001b[39mones_like(mask, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39mids\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: self must be a matrix"
     ]
    }
   ],
   "source": [
    "def mask_and_pos_ids(\n",
    "    ids: torch.Tensor, parent_ids: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Creates an attention mask and position IDs for hierarchical attention based on node IDs and their parent IDs.\n",
    "\n",
    "    Args:\n",
    "        ids: A tensor of shape (batch_size, sequence_length) containing node IDs\n",
    "        parent_ids: A tensor of shape (batch_size, sequence_length) containing parent IDs for each node\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - mask: A boolean tensor of shape (batch_size, sequence_length, sequence_length) where True indicates\n",
    "          allowed attention connections. Each position can attend to itself and any of its ancestors\n",
    "          in the hierarchy, but only for previous positions (due to causal masking).\n",
    "        - pos_ids: A tensor of shape (batch_size, sequence_length, sequence_length) containing relative\n",
    "          position IDs for each allowed attention connection, with -1 for masked positions.\n",
    "    \"\"\"\n",
    "    mask = ids.unsqueeze(1) == ids.unsqueeze(2)\n",
    "    _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    while torch.any(mask != _mask):\n",
    "        parent_ids = parent_ids.gather(\n",
    "            1, torch.argmax((parent_ids.unsqueeze(2) == ids.unsqueeze(1)).int(), dim=2)\n",
    "        )\n",
    "        mask = _mask\n",
    "        _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    mask &= torch.tril(torch.ones_like(mask, dtype=torch.bool, device=ids.device))\n",
    "    # mask = torch.linalg.matrix_power(mask.float(), mask.size(1) - 1) > 0\n",
    "    pos_ids = (torch.where(mask, mask.cumsum(2), 0) - 1).max(1).values\n",
    "    return mask, pos_ids\n",
    "\n",
    "\n",
    "def test_mask_and_pos_ids(\n",
    "    ids: list[int],\n",
    "    parent_ids: list[int],\n",
    "    expected_mask: list[list[int]],\n",
    "    expected_pos_ids: list[int],\n",
    "):\n",
    "    mask, pos_ids = mask_and_pos_ids(\n",
    "        ids=torch.tensor([ids]), parent_ids=torch.tensor([parent_ids])\n",
    "    )\n",
    "    assert torch.all(mask.int() == torch.tensor([expected_mask])), f\"\\n{mask.int()[0]}\"\n",
    "    assert torch.all(\n",
    "        pos_ids == torch.tensor([expected_pos_ids])\n",
    "    ), f\"{pos_ids[0].tolist()}\"\n",
    "\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1],\n",
    "    parent_ids=[0, 1],\n",
    "    expected_mask=[[1, 0], [0, 1]],\n",
    "    expected_pos_ids=[0, 0],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 1],\n",
    "    parent_ids=[0, 0, 0],\n",
    "    expected_mask=[[1, 0, 0], [1, 1, 0], [1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 0, 1, 2],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 0, 1, 1],\n",
    "    parent_ids=[0, 0, 1, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 0, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 1, 0, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [0, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 1]],\n",
    "    expected_pos_ids=[0, 0, 1, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 2, 3, 3],\n",
    "    parent_ids=[0, 1, 0, 0, 1, 1],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 0, 1, 2, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3, 4, 4, 5, 5],\n",
    "    parent_ids=[0, 0, 1, 1, 2, 2, 3, 3],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 2, 2, 3, 4, 3, 4],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[2, 1, 0],\n",
    "    parent_ids=[2, 2, 0],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 0, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_reachability_matrix(adj_matrix):\n",
    "    # Number of nodes\n",
    "    num_nodes = adj_matrix.size(0)\n",
    "\n",
    "    # Start with the adjacency matrix as reachability\n",
    "    reachability = adj_matrix.clone()\n",
    "\n",
    "    # Add paths of length 2 to N-1\n",
    "    for _ in range(num_nodes - 1):\n",
    "        # Update reachability with additional paths\n",
    "        reachability = reachability + torch.mm(reachability, adj_matrix)\n",
    "\n",
    "    # Convert to binary (reachable or not)\n",
    "    reachability = (reachability > 0).float()\n",
    "\n",
    "    return reachability\n",
    "\n",
    "# Example adjacency matrix\n",
    "adj_matrix = torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 1, 0, 0],\n",
    "    [0, 1, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "reachability_matrix = compute_reachability_matrix(adj_matrix)\n",
    "print(reachability_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def compute_reachability_matrix_one_swoop(adj_matrix):\n",
    "    num_nodes = adj_matrix.size(0)\n",
    "\n",
    "    # Compute the series sum: (I + A + A^2 + ... + A^(N-1))\n",
    "    reachability = torch.matrix_power(adj_matrix, num_nodes - 1)\n",
    "\n",
    "    # Binarize the result\n",
    "    reachability = (reachability > 0).float()\n",
    "\n",
    "    return reachability\n",
    "\n",
    "reachability_matrix = compute_reachability_matrix_one_swoop(adj_matrix)\n",
    "print(reachability_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   1,   2,  ..., 127, 127, 127],\n",
       "        [  0,   1,   1,  ...,  43,  43,  43]])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id: int, parent: \"Node | None\" = None, size: int = 1):\n",
    "        self.id = id\n",
    "        self.parent = parent\n",
    "        self.size = size\n",
    "\n",
    "\n",
    "nodes = [Node(id=0, size=random.randint(1, 1)), Node(id=1, size=random.randint(1, 1))]\n",
    "for i in range(2, 128):\n",
    "    parent = random.choice(nodes)\n",
    "    size = random.randint(1, 128)\n",
    "    node = Node(id=i, parent=parent, size=size)\n",
    "    nodes.append(node)\n",
    "ids = torch.tensor([node.id for node in nodes for _ in range(node.size)])\n",
    "parent_ids = torch.tensor(\n",
    "    [\n",
    "        node.parent.id if node.parent else node.id\n",
    "        for node in nodes\n",
    "        for _ in range(node.size)\n",
    "    ]\n",
    ")\n",
    "# mask = get_mask(ids, parent_ids)\n",
    "# faster_mask = get_faster_mask(ids, parent_ids)\n",
    "torch.stack([ids, parent_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8292"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.9 s ± 85.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mask_and_pos_ids(torch.stack([ids, ids]), torch.stack([parent_ids, parent_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(mask):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"binary\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Attention Mask\")\n",
    "    plt.xlabel(\"Source Position\")\n",
    "    plt.ylabel(\"Target Position\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-18 18:14:23 api_server.py:528] vLLM API server version dev\n",
      "\n",
      "INFO 11-18 18:14:23 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x781c10210cc0>)\n",
      "\n",
      "INFO 11-18 18:14:23 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/cb2e4a1c-be7a-4775-832d-34b9eaf7e1f3 for IPC Path.\n",
      "\n",
      "INFO 11-18 18:14:23 api_server.py:179] Started engine process with PID 8558\n",
      "\n",
      "INFO 11-18 18:14:33 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "\n",
      "INFO 11-18 18:14:34 model_runner.py:1060] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "\n",
      "INFO 11-18 18:14:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\n",
      "INFO 11-18 18:14:38 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "\n",
      "INFO 11-18 18:14:39 gpu_executor.py:122] # GPU blocks: 9604, # CPU blocks: 2048\n",
      "\n",
      "INFO 11-18 18:14:39 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 18.76x\n",
      "\n",
      "INFO 11-18 18:14:42 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\n",
      "INFO 11-18 18:14:42 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\n",
      "INFO 11-18 18:14:55 model_runner.py:1530] Graph capturing finished in 12 secs.\n",
      "\n",
      "INFO 11-18 18:14:55 api_server.py:232] vLLM to use /tmp/tmpop4p9wim as PROMETHEUS_MULTIPROC_DIR\n",
      "\n",
      "WARNING 11-18 18:14:55 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:19] Available routes are:\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /health, Methods: GET\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /version, Methods: GET\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "\n",
      "INFO 11-18 18:15:05 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "INFO 11-18 18:15:15 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "INFO 11-18 18:15:25 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "INFO 11-18 18:15:35 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "process = await asyncio.create_subprocess_exec(\n",
    "    \"vllm\",\n",
    "    \"serve\",\n",
    "    \"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE,\n",
    ")\n",
    "while True:\n",
    "    print((await process.stdout.readline()).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.vllm import start_vllm_server, vllm_server_metrics\n",
    "import os\n",
    "\n",
    "model = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "os.environ[\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\"] = \"1\"\n",
    "shutdown_server, client = await start_vllm_server(\n",
    "    disable_log_requests=True,\n",
    "    max_model_len=16384,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from lib.rl.episode import Episode\n",
    "from typing import AsyncIterable, Literal\n",
    "\n",
    "Split = Literal[\"train\", \"val\", \"test\"]\n",
    "\n",
    "\n",
    "async def episodes(split: Split) -> AsyncIterable[Episode]:\n",
    "    for _ in range(10):\n",
    "        await asyncio.sleep(1)\n",
    "        yield Episode()  # type: ignore\n",
    "\n",
    "\n",
    "async for episode in episodes(split=\"val\"):\n",
    "    print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.trainer import Trainer\n",
    "\n",
    "episode = Episode()\n",
    "\n",
    "Trainer(\n",
    "    base_model=model,\n",
    "    episodes={\n",
    "        \"train\": [episode],\n",
    "        \"val\": [episode],\n",
    "        \"test\": [episode],\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
