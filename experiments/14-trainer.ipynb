{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from /home/ubuntu/atreides/experiments/models/rl2/0003\n",
      "INFO 11-27 22:44:41 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import Trainer\n",
    "from lib.utils import return_exception\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "@return_exception\n",
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": random.sample(Clue.suspects, k=3),\n",
    "            \"weapon\": random.sample(Clue.weapons, k=3),\n",
    "            \"room\": random.sample(Clue.rooms, k=3),\n",
    "            # \"motive\": random.sample(Clue.motives, k=3),\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt, follow_up, solution = game.get_prompt_and_follow_up_and_solution()\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{key}: {value}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for key, value in solution.items()\n",
    "            ]\n",
    "        ) / len(solution)\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_episodes():\n",
    "    while True:\n",
    "        yield sample_random_episode()\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=\"./models/rl2\",\n",
    "    samples_per_episode=27,\n",
    "    branch_factor=3,\n",
    "    sample_probability_power=1 / 3,\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=1024,\n",
    "    patience_per_sample=1 / 27,\n",
    "    sampling_kwargs={\n",
    "        \"max_tokens\": 4096,\n",
    "    },\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=(sample_random_episode() for _ in range(256)),\n",
    "    val_samples_per_episode=3,\n",
    "    torchrun_kwargs=dict(nnodes=1, nproc_per_node=torch.cuda.device_count()),\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=2,\n",
    "        epochs=2,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=5e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            clip_epsilon=0.1,\n",
    "            entropy_coef=0.5,\n",
    "            kl_coef=0.3,\n",
    "            weighted_ce_coef=0.2,\n",
    "            weighted_entropy=True,\n",
    "            weighted_kl=True,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=2,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "    vllm_kwargs=dict(\n",
    "        block_size=32,\n",
    "        disable_log_requests=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enforce_eager=True,\n",
    "        gpu_memory_utilization=0.999,\n",
    "        max_model_len=16384,\n",
    "        max_num_seqs=4096,\n",
    "        max_num_batched_tokens=16384 * 8,\n",
    "        swap_space=8,\n",
    "        # scheduling_policy=\"priority\",\n",
    "        # tensor_parallel_size=torch.cuda.device_count() // 8,\n",
    "    ),\n",
    "    vllm_max_concurrent_samples=4096,\n",
    "    vllm_min_time_between_requests=0.0,\n",
    "    vllm_num=8,\n",
    "    vllm_timeout=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.patience_per_val_sample = 1.0\n",
    "trainer.patience_per_test_sample = 1.0\n",
    "trainer.tune_recipe_config.optimizer.lr = 8e-6\n",
    "trainer.tune_recipe_config.loss.clip_epsilon = 0.1\n",
    "trainer.tune_recipe_config.loss.weighted_ce_coef = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0003 --port=8086 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0003 --port=8088 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0003 --port=8090 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0003 --port=8092 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0003 --port=8093 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0003 --port=8094 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0003 --port=8095 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0003 --port=8096 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "INFO 11-27 22:44:56 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 22:44:56 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0003', config='', host=None, port=8086, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7b8c1fb8aac0>)\n",
      "INFO 11-27 22:44:56 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/52a7c06e-bdd8-469c-995e-807d7c43b726 for IPC Path.\n",
      "INFO 11-27 22:44:56 api_server.py:179] Started engine process with PID 355913\n",
      "WARNING 11-27 22:44:56 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 22:44:56 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 22:44:56 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0003', config='', host=None, port=8096, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7ba85f79aac0>)\n",
      "INFO 11-27 22:44:56 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/99543284-d592-4fab-8bb3-bd09b8aa4f97 for IPC Path.\n",
      "INFO 11-27 22:44:56 api_server.py:179] Started engine process with PID 355917\n",
      "WARNING 11-27 22:44:56 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 22:44:56 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 22:44:56 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0003', config='', host=None, port=8088, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x76cf4cf8aac0>)\n",
      "INFO 11-27 22:44:56 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/f19750bd-c781-47ca-8294-e946d8c1b2cb for IPC Path.\n",
      "INFO 11-27 22:44:56 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 22:44:56 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 22:44:56 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0003', config='', host=None, port=8093, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x77c896192ac0>)\n",
      "INFO 11-27 22:44:56 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0003', config='', host=None, port=8094, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7b63e4b96ac0>)\n",
      "INFO 11-27 22:44:56 api_server.py:179] Started engine process with PID 355933\n",
      "INFO 11-27 22:44:56 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 22:44:56 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0003', config='', host=None, port=8092, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x79b18938aac0>)\n",
      "WARNING 11-27 22:44:56 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 22:44:56 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 22:44:56 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0003', config='', host=None, port=8090, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7e51f218aac0>)\n",
      "INFO 11-27 22:44:56 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/82d50502-de5f-4d9b-a905-e130cc3c8627 for IPC Path.\n",
      "INFO 11-27 22:44:56 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/3cc5473f-a3bd-4878-a45e-2a6b88d8140e for IPC Path.\n",
      "INFO 11-27 22:44:56 api_server.py:179] Started engine process with PID 355940\n",
      "INFO 11-27 22:44:56 api_server.py:179] Started engine process with PID 355941\n",
      "INFO 11-27 22:44:56 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/39a8c8fc-66dd-41df-b8d1-0e4a86b88df2 for IPC Path.\n",
      "INFO 11-27 22:44:56 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/365a0551-d973-487d-af61-64ae06d255e8 for IPC Path.\n",
      "INFO 11-27 22:44:56 api_server.py:179] Started engine process with PID 355944\n",
      "WARNING 11-27 22:44:56 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:44:56 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 22:44:56 api_server.py:179] Started engine process with PID 355945\n",
      "WARNING 11-27 22:44:56 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:44:56 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 22:44:56 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 22:44:56 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0003', config='', host=None, port=8095, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7b9c75f7eac0>)\n",
      "INFO 11-27 22:44:56 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/dca799ca-17c0-435b-abed-86d55ef97f98 for IPC Path.\n",
      "INFO 11-27 22:44:56 api_server.py:179] Started engine process with PID 355952\n",
      "WARNING 11-27 22:44:56 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 22:45:04 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:04 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 22:45:04 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:04 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 22:45:04 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:04 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 22:45:04 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:04 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 22:45:05 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:05 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 22:45:05 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:05 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 22:45:05 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:05 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 22:45:05 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:05 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 22:45:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:11 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 22:45:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 22:45:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:11 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 22:45:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 22:45:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:11 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 22:45:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 22:45:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:11 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 22:45:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 22:45:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:11 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 22:45:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 22:45:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:11 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 22:45:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 22:45:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:11 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 22:45:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 22:45:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 22:45:11 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 22:45:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 11-27 22:45:19 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0003...\n",
      "INFO 11-27 22:45:19 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0003...\n",
      "INFO 11-27 22:45:19 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0003...\n",
      "INFO 11-27 22:45:19 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0003...\n",
      "INFO 11-27 22:45:19 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:19 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0003...\n",
      "INFO 11-27 22:45:19 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:19 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.43s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.46s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.43s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.60s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.43s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.49s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.53s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:05<00:15,  5.02s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.42s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.44s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.43s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.58s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.37s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.39s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.39s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.89s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.40s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.40s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.40s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.55s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.32s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.32s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.34s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.06s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.56s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.05s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.55s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.05s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.55s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:34 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 22:45:34 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 22:45:34 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.16s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.68s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.00s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.50s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.00s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.50s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  2.99s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.50s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:34 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 22:45:34 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 22:45:34 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 22:45:34 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.70s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.24s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.82s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:36 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 22:45:37 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 22:45:37 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 22:45:38 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 22:45:38 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 22:45:38 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 22:45:38 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 22:45:39 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 22:45:39 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 22:45:39 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 22:45:39 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 22:45:39 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 22:45:39 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 22:45:39 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 22:45:39 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 22:45:40 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 22:45:40 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 22:45:41 api_server.py:232] vLLM to use /tmp/tmph_xhaojs as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 22:45:41 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 22:45:41 launcher.py:19] Available routes are:\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [355304]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8096) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:41 api_server.py:232] vLLM to use /tmp/tmp7gl4n3kt as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 22:45:41 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 22:45:41 launcher.py:19] Available routes are:\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [355298]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8093) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:41 api_server.py:232] vLLM to use /tmp/tmpvedt2keb as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 22:45:41 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 22:45:41 launcher.py:19] Available routes are:\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 22:45:41 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [355290]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8086) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:39380 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:60668 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:47234 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO 11-27 22:45:43 api_server.py:232] vLLM to use /tmp/tmpdn6xmko_ as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 22:45:43 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 22:45:43 launcher.py:19] Available routes are:\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 11-27 22:45:43 api_server.py:232] vLLM to use /tmp/tmplwl732jv as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 22:45:43 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 22:45:43 launcher.py:19] Available routes are:\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [355294]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8090) (Press CTRL+C to quit)\n",
      "INFO:     Started server process [355302]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8095) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:43 api_server.py:232] vLLM to use /tmp/tmp7gjknnbz as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 22:45:43 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 22:45:43 launcher.py:19] Available routes are:\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [355296]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8092) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:43 api_server.py:232] vLLM to use /tmp/tmpccvut8t2 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 22:45:43 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 22:45:43 launcher.py:19] Available routes are:\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [355292]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8088) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 22:45:43 api_server.py:232] vLLM to use /tmp/tmp75nw7eo8 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 22:45:43 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 22:45:43 launcher.py:19] Available routes are:\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 22:45:43 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [355300]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8094) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:55416 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:42776 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:57720 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:53642 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:38280 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efec6046e6147289ed8183fd5c4b605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val: 0episode [00:00, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe1b05038e046408d5f29424dce8854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/1024 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (9 remaining samples x 5.0 patience per sample = 45.0 seconds)\n",
      "Early stopping exploration due to expired patience (546.91020000032 remaining samples x 0.037037037037037035 patience per sample = 20.255933333345183 seconds)\n",
      "$ tune run --nnodes=1 --nproc-per-node=8 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl2/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1127 22:55:08.717000 130632901413760 torch/distributed/run.py:779] \n",
      "W1127 22:55:08.717000 130632901413760 torch/distributed/run.py:779] *****************************************\n",
      "W1127 22:55:08.717000 130632901413760 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1127 22:55:08.717000 130632901413760 torch/distributed/run.py:779] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl2/0003\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0003/hf_model_0003_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0003/hf_model_0002_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0003/hf_model_0001_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0003/hf_model_0004_1.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl2\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl2/tensors\n",
      "  num_sequences: 85\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 2\n",
      "gradient_accumulation_steps: 2\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.1\n",
      "  entropy_coef: 0.5\n",
      "  kl_coef: 0.3\n",
      "  normalize_advantages: false\n",
      "  weighted_ce_coef: 0.2\n",
      "  weighted_entropy: true\n",
      "  weighted_kl: true\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/models/rl2/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/models/rl2/logs/log_1732748124.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 16.87 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 2.91 GiB\n",
      "\tGPU peak memory reserved: 3.03 GiB\n",
      "\tGPU peak memory active: 2.91 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|2|Loss: 0.0310: 100%|██████████| 2/2 [02:22<00:00, 69.06s/it, entropy=0.0002, kl_div=-0.0001, policy=0.0313, weighted_ce=-0.0009]\n",
      "1|2|Loss: 0.0310: 100%|██████████| 2/2 [02:36<00:00, 78.05s/it, entropy=0.0002, kl_div=-0.0001, policy=0.0313, weighted_ce=-0.0009]\n",
      "\n",
      " 50%|█████     | 1/2 [00:33<00:33, 33.87s/it]\u001b[A\n",
      "2|3|Loss: 0.0131:  50%|█████     | 1/2 [00:33<00:33, 33.87s/it]\u001b[A\n",
      "2|3|Loss: 0.0131:  50%|█████     | 1/2 [00:33<00:33, 33.87s/it, entropy=0.0004, kl_div=0.0031, policy=0.0139, weighted_ce=-0.0074]\u001b[A\n",
      "2|3|Loss: 0.0131: 100%|██████████| 2/2 [01:06<00:00, 32.95s/it, entropy=0.0004, kl_div=0.0031, policy=0.0139, weighted_ce=-0.0074]\u001b[A\n",
      "2|4|Loss: 0.0175: 100%|██████████| 2/2 [01:06<00:00, 32.95s/it, entropy=0.0004, kl_div=0.0031, policy=0.0139, weighted_ce=-0.0074]\u001b[A\n",
      "2|4|Loss: 0.0175: 100%|██████████| 2/2 [01:06<00:00, 32.95s/it, entropy=0.0003, kl_div=-0.0043, policy=0.0184, weighted_ce=0.0026]\u001b[AINFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 13.43 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0003_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0004_1.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 18.35 secs\n",
      "2|4|Loss: 0.0175: 100%|██████████| 2/2 [01:52<00:00, 56.40s/it, entropy=0.0003, kl_div=-0.0043, policy=0.0184, weighted_ce=0.0026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 4 model files to /home/ubuntu/atreides/experiments/models/rl2/0004\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0004 --port=8088 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0004 --port=8097 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0004 --port=8098 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0004 --port=8099 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0004 --port=8100 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0004 --port=8101 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0004 --port=8102 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0004 --port=8103 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "INFO 11-27 23:00:26 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:00:26 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0004', config='', host=None, port=8098, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x79653b39aac0>)\n",
      "INFO 11-27 23:00:26 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/53368289-ca41-4c98-b572-bfbbd234f441 for IPC Path.\n",
      "INFO 11-27 23:00:26 api_server.py:179] Started engine process with PID 370724\n",
      "WARNING 11-27 23:00:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:00:26 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:00:26 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0004', config='', host=None, port=8099, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7d106a192ac0>)\n",
      "INFO 11-27 23:00:26 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/e0187266-8d87-4794-8013-2f1025dd2b96 for IPC Path.\n",
      "INFO 11-27 23:00:26 api_server.py:179] Started engine process with PID 370728\n",
      "WARNING 11-27 23:00:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:00:26 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:00:26 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0004', config='', host=None, port=8101, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x743136192ac0>)\n",
      "INFO 11-27 23:00:26 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:00:26 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0004', config='', host=None, port=8100, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7a837338eac0>)\n",
      "INFO 11-27 23:00:26 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/4fbd892a-9c73-42ea-85e8-0687304246e4 for IPC Path.\n",
      "INFO 11-27 23:00:26 api_server.py:179] Started engine process with PID 370733\n",
      "WARNING 11-27 23:00:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:00:26 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/95cc3a30-9d75-4ecd-af95-c3f33e3be669 for IPC Path.\n",
      "INFO 11-27 23:00:26 api_server.py:179] Started engine process with PID 370736\n",
      "WARNING 11-27 23:00:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:00:26 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:00:26 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0004', config='', host=None, port=8097, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x761276b8aac0>)\n",
      "INFO 11-27 23:00:26 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/0b000e9f-fc5a-4abb-a21f-82bbd4889365 for IPC Path.\n",
      "INFO 11-27 23:00:26 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:00:26 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0004', config='', host=None, port=8088, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x74dd43d7eac0>)\n",
      "INFO 11-27 23:00:26 api_server.py:179] Started engine process with PID 370742\n",
      "WARNING 11-27 23:00:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:00:26 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:00:26 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0004', config='', host=None, port=8102, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7e72eb38eac0>)\n",
      "INFO 11-27 23:00:26 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:00:26 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0004', config='', host=None, port=8103, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7dc2e138eac0>)\n",
      "INFO 11-27 23:00:26 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/c5957301-4900-4408-b98f-7a8ff9e2d2a3 for IPC Path.\n",
      "INFO 11-27 23:00:26 api_server.py:179] Started engine process with PID 370747\n",
      "WARNING 11-27 23:00:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:00:26 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/e563da9d-c558-4a5c-b59a-24559963440d for IPC Path.\n",
      "INFO 11-27 23:00:26 api_server.py:179] Started engine process with PID 370750\n",
      "INFO 11-27 23:00:26 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/351ae77a-db65-4bb5-af82-b2cd922deb1c for IPC Path.\n",
      "WARNING 11-27 23:00:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:00:26 api_server.py:179] Started engine process with PID 370752\n",
      "WARNING 11-27 23:00:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:35 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:36 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:36 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:36 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:36 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:36 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:36 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:00:36 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:36 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:00:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:36 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:00:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:36 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:00:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:36 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:00:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:36 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:00:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:36 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:00:36 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:36 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:00:37 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:37 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:00:42 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:42 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:00:42 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:00:43 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:43 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:00:43 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:00:43 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:43 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:00:43 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:00:43 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:43 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:00:43 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:00:43 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:43 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:00:43 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:00:43 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:43 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:00:43 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:00:43 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:43 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:00:43 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:00:43 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:00:43 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:00:43 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 11-27 23:00:48 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0004...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:00:50 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0004...\n",
      "INFO 11-27 23:00:50 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0004...\n",
      "INFO 11-27 23:00:50 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0004...\n",
      "INFO 11-27 23:00:50 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0004...\n",
      "INFO 11-27 23:00:50 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0004...\n",
      "INFO 11-27 23:00:50 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0004...\n",
      "INFO 11-27 23:00:50 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0004...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.58s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.55s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.61s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.80s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.80s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.79s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.87s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.84s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.55s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.56s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.59s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.67s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.70s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.69s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.67s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:11<00:12,  6.16s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.47s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.53s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.54s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.58s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.57s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.59s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.59s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.12s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.63s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:04 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.14s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.66s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.14s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.66s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.15s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.71s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.15s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.70s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.15s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.70s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.16s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.72s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:05 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:01:05 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:01:05 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:01:05 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:01:05 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:01:05 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:16<00:05,  5.47s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  3.71s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:17<00:00,  4.39s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:08 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:01:09 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:01:09 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:01:09 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:01:09 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:01:09 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:01:09 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:01:09 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:01:09 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:01:09 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:01:09 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:01:09 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:01:09 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:01:09 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:01:09 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:01:12 api_server.py:232] vLLM to use /tmp/tmpqcbfdou1 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:01:12 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:01:12 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [370094]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8099) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:12 api_server.py:232] vLLM to use /tmp/tmpfoyzrvwl as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:01:12 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:01:12 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:01:12 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [370102]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8103) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:13 api_server.py:232] vLLM to use /tmp/tmpfgi3qol0 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:01:13 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:01:13 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [370096]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8100) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:13 api_server.py:232] vLLM to use /tmp/tmpc_2n7gtt as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:01:13 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:01:13 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 11-27 23:01:13 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:01:13 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [370098]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8101) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:13 api_server.py:232] vLLM to use /tmp/tmpfbciz0pi as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:01:13 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:01:13 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [370088]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8088) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:13 api_server.py:232] vLLM to use /tmp/tmpzgjvicn4 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:01:13 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:01:13 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [370090]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8097) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:01:13 api_server.py:232] vLLM to use /tmp/tmpv7z1i8en as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:01:13 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:01:13 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:01:13 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [370100]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8102) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:46996 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:35106 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:48058 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:45172 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:37874 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:53438 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:35850 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO 11-27 23:01:16 api_server.py:232] vLLM to use /tmp/tmp556xknwx as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:01:16 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:01:16 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:01:16 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [370092]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8098) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:39534 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f802447e363487fbef70d87392edb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869d1b36a01e451cb672643f23b60295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/1024 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ProcessLookupError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=8 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl2/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1127 23:07:26.141000 137382885362560 torch/distributed/run.py:779] \n",
      "W1127 23:07:26.141000 137382885362560 torch/distributed/run.py:779] *****************************************\n",
      "W1127 23:07:26.141000 137382885362560 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1127 23:07:26.141000 137382885362560 torch/distributed/run.py:779] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl2/0004\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0004/hf_model_0003_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0004/hf_model_0002_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0004/hf_model_0001_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0004/hf_model_0004_1.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl2\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl2/tensors\n",
      "  num_sequences: 11\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 2\n",
      "gradient_accumulation_steps: 2\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.1\n",
      "  entropy_coef: 0.5\n",
      "  kl_coef: 0.3\n",
      "  normalize_advantages: false\n",
      "  weighted_ce_coef: 0.2\n",
      "  weighted_entropy: true\n",
      "  weighted_kl: true\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/models/rl2/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/models/rl2/logs/log_1732748860.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 17.70 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 2.91 GiB\n",
      "\tGPU peak memory reserved: 3.03 GiB\n",
      "\tGPU peak memory active: 2.91 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "0it [00:00, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "\n",
      "0it [00:17, ?it/s]\u001b[A\n",
      "INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 14.53 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0003_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0004_1.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 18.21 secs\n",
      "0it [00:48, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 5 model files to /home/ubuntu/atreides/experiments/models/rl2/0005\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0005 --port=8088 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0005 --port=8097 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0005 --port=8098 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0005 --port=8099 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0005 --port=8100 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0005 --port=8101 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0005 --port=8102 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0005 --port=8103 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "INFO 11-27 23:09:18 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:09:18 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0005', config='', host=None, port=8101, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0005', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x77128c48aac0>)\n",
      "INFO 11-27 23:09:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/dd809ed1-9985-4d34-8646-5c50f792f980 for IPC Path.\n",
      "INFO 11-27 23:09:18 api_server.py:179] Started engine process with PID 381271\n",
      "WARNING 11-27 23:09:18 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:09:18 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:09:18 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0005', config='', host=None, port=8100, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0005', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7e031db92ac0>)\n",
      "INFO 11-27 23:09:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/833b9d9b-ffee-4693-8fd0-e3ae4d70c7db for IPC Path.\n",
      "INFO 11-27 23:09:18 api_server.py:179] Started engine process with PID 381275\n",
      "WARNING 11-27 23:09:18 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:09:18 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:09:18 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0005', config='', host=None, port=8097, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0005', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x77c29a38eac0>)\n",
      "INFO 11-27 23:09:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/48912b32-7c90-4938-bc7a-2bb53aa34b9c for IPC Path.\n",
      "INFO 11-27 23:09:18 api_server.py:179] Started engine process with PID 381279\n",
      "WARNING 11-27 23:09:18 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:09:18 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:09:18 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0005', config='', host=None, port=8103, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0005', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x72802bb8eac0>)\n",
      "INFO 11-27 23:09:18 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:09:18 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0005', config='', host=None, port=8088, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0005', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7094fcf92ac0>)\n",
      "INFO 11-27 23:09:18 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:09:18 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0005', config='', host=None, port=8098, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0005', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7ad09938aac0>)\n",
      "INFO 11-27 23:09:18 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:09:18 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0005', config='', host=None, port=8099, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0005', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x778408796ac0>)\n",
      "INFO 11-27 23:09:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/54e9e1db-3c92-4aaa-a23e-1fbdcf3a881a for IPC Path.\n",
      "INFO 11-27 23:09:18 api_server.py:179] Started engine process with PID 381286\n",
      "WARNING 11-27 23:09:18 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:09:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/b69e20e5-e550-4466-ab5d-5db35fb4b2a1 for IPC Path.\n",
      "INFO 11-27 23:09:18 api_server.py:179] Started engine process with PID 381289\n",
      "WARNING 11-27 23:09:18 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:09:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/307171bd-f539-4e6b-9b81-9ae8d27588af for IPC Path.\n",
      "INFO 11-27 23:09:18 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/965543c3-ac3a-46d4-af89-468d4f4e7d5b for IPC Path.\n",
      "INFO 11-27 23:09:18 api_server.py:179] Started engine process with PID 381292\n",
      "INFO 11-27 23:09:18 api_server.py:179] Started engine process with PID 381294\n",
      "WARNING 11-27 23:09:18 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:18 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:09:19 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:09:19 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0005', config='', host=None, port=8102, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0005', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x79f6bf57eac0>)\n",
      "INFO 11-27 23:09:19 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/08e01e25-b74b-4556-ab27-ea4f14f301a5 for IPC Path.\n",
      "INFO 11-27 23:09:19 api_server.py:179] Started engine process with PID 381299\n",
      "WARNING 11-27 23:09:19 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:26 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:27 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:27 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:27 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:09:27 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:27 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:09:27 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:27 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:09:27 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:09:27 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:09:27 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:09:27 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:09:27 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:09:27 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:27 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:09:33 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:33 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:09:33 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0005', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0005', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0005, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:09:34 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:09:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0005', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0005', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0005, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:09:34 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:09:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0005', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0005', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0005, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:09:34 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:09:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0005', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0005', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0005, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:09:34 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:09:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0005', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0005', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0005, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:09:34 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:09:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0005', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0005', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0005, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:09:34 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:09:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0005', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0005', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0005, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:09:34 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:09:34 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:09:34 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0005', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0005', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0005, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 11-27 23:09:38 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0005...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:09:41 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0005...\n",
      "INFO 11-27 23:09:41 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0005...\n",
      "INFO 11-27 23:09:41 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0005...\n",
      "INFO 11-27 23:09:41 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0005...\n",
      "INFO 11-27 23:09:41 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0005...\n",
      "INFO 11-27 23:09:41 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0005...\n",
      "INFO 11-27 23:09:41 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0005...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.49s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.46s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.71s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.74s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.79s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.81s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.86s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:05<00:15,  5.01s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.53s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.39s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.70s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.66s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.65s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.70s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.70s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.86s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.51s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.38s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.57s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.55s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.55s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.57s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.60s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.15s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.65s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.06s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.55s/it]\n",
      "\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:09:56 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:09:56 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.17s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.71s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.16s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.70s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.16s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.71s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.15s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.71s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.17s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.71s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:09:56 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:09:56 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:09:56 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:09:56 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:09:56 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.26s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.84s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:09:57 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:10:00 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:10:00 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:10:00 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:10:00 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:10:00 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:10:00 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:10:00 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:10:00 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:10:00 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:10:00 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:10:00 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:10:00 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:10:02 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:10:02 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:10:02 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:10:02 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:10:03 api_server.py:232] vLLM to use /tmp/tmpoea1uevd as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:10:03 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:10:03 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [380631]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8088) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:10:03 api_server.py:232] vLLM to use /tmp/tmpjucwenki as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:10:03 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:10:03 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:10:03 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [380637]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8099) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:10:04 api_server.py:232] vLLM to use /tmp/tmp6xcn4nxw as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:10:04 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:10:04 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [380643]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8102) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:10:04 api_server.py:232] vLLM to use /tmp/tmpolk5suih as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:10:04 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:10:04 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:10:04 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [380633]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8097) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:10:05 api_server.py:232] vLLM to use /tmp/tmptz_7js3r as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:10:05 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:10:05 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [380639]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8100) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:10:05 api_server.py:232] vLLM to use /tmp/tmpzirdnpgb as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:10:05 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:10:05 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [380645]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8103) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42410 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:50492 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO 11-27 23:10:05 api_server.py:232] vLLM to use /tmp/tmp9ry_1usy as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:10:05 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:10:05 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 11-27 23:10:05 api_server.py:232] vLLM to use /tmp/tmpwjwhgtza as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:10:05 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:10:05 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:10:05 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [380635]\n",
      "INFO:     Started server process [380641]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8101) (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8098) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58262 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:45262 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:43716 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:52330 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:36082 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:44340 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fd82d4dd8a427ea7569c4015cc7fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b4b2e14b5a49489426eb2557e908f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/1024 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n",
      "Exception ignored in: <function vLLM.__del__ at 0x7eaf43b451c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ProcessLookupError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=8 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl2/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1127 23:14:49.861000 140174152137600 torch/distributed/run.py:779] \n",
      "W1127 23:14:49.861000 140174152137600 torch/distributed/run.py:779] *****************************************\n",
      "W1127 23:14:49.861000 140174152137600 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1127 23:14:49.861000 140174152137600 torch/distributed/run.py:779] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl2/0005\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0005/hf_model_0003_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0005/hf_model_0002_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0005/hf_model_0001_1.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0005/hf_model_0004_1.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl2\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl2/tensors\n",
      "  num_sequences: 11\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 2\n",
      "gradient_accumulation_steps: 2\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.1\n",
      "  entropy_coef: 0.5\n",
      "  kl_coef: 0.3\n",
      "  normalize_advantages: false\n",
      "  weighted_ce_coef: 0.2\n",
      "  weighted_entropy: true\n",
      "  weighted_kl: true\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/models/rl2/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/models/rl2/logs/log_1732749304.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 17.35 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 2.91 GiB\n",
      "\tGPU peak memory reserved: 3.03 GiB\n",
      "\tGPU peak memory active: 2.91 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "0it [00:00, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "\n",
      "0it [00:17, ?it/s]\u001b[A\n",
      "INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 14.38 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0003_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl2/hf_model_0004_1.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 18.02 secs\n",
      "0it [00:47, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 6 model files to /home/ubuntu/atreides/experiments/models/rl2/0006\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0006 --port=8088 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0006 --port=8097 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0006 --port=8098 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0006 --port=8099 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0006 --port=8100 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0006 --port=8101 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0006 --port=8102 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl2/0006 --port=8103 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --api-key=default\n",
      "INFO 11-27 23:16:41 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:16:41 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0006', config='', host=None, port=8088, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0006', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x74c8ed992ac0>)\n",
      "INFO 11-27 23:16:41 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/88bba507-3e99-4a64-a929-8b13d6091ba1 for IPC Path.\n",
      "INFO 11-27 23:16:41 api_server.py:179] Started engine process with PID 391056\n",
      "WARNING 11-27 23:16:41 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:16:41 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:16:41 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0006', config='', host=None, port=8098, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0006', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x756e3588aac0>)\n",
      "INFO 11-27 23:16:41 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/892146a9-e1a8-47bc-abe4-82e6ba63c670 for IPC Path.\n",
      "INFO 11-27 23:16:41 api_server.py:179] Started engine process with PID 391061\n",
      "WARNING 11-27 23:16:41 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:16:41 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:16:41 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0006', config='', host=None, port=8097, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0006', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x721e5748aac0>)\n",
      "INFO 11-27 23:16:41 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/d2ccfca1-c730-4748-813b-9567d03ac071 for IPC Path.\n",
      "INFO 11-27 23:16:41 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:16:41 api_server.py:179] Started engine process with PID 391066\n",
      "INFO 11-27 23:16:41 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0006', config='', host=None, port=8099, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0006', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x799a4858eac0>)\n",
      "WARNING 11-27 23:16:41 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:16:41 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/6f50a3d6-3c7d-4a12-a70a-80880d23125c for IPC Path.\n",
      "INFO 11-27 23:16:41 api_server.py:179] Started engine process with PID 391069\n",
      "WARNING 11-27 23:16:41 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:16:41 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:16:41 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0006', config='', host=None, port=8100, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0006', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7241c2f8eac0>)\n",
      "INFO 11-27 23:16:42 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:16:42 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0006', config='', host=None, port=8102, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0006', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x78d4d958aac0>)\n",
      "INFO 11-27 23:16:42 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/2c034562-ec98-4e45-9e6f-2069718dbd57 for IPC Path.\n",
      "INFO 11-27 23:16:42 api_server.py:179] Started engine process with PID 391074\n",
      "WARNING 11-27 23:16:42 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:16:42 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/cf104a66-c5f2-4083-b59b-828011a8a9f5 for IPC Path.\n",
      "INFO 11-27 23:16:42 api_server.py:179] Started engine process with PID 391077\n",
      "WARNING 11-27 23:16:42 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:16:42 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:16:42 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0006', config='', host=None, port=8103, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0006', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7fdf3ed8aac0>)\n",
      "INFO 11-27 23:16:42 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/1b5aa89a-afbb-4561-a1a0-c78c0f2edeb3 for IPC Path.\n",
      "INFO 11-27 23:16:42 api_server.py:179] Started engine process with PID 391081\n",
      "INFO 11-27 23:16:42 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 23:16:42 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl2/0006', config='', host=None, port=8101, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl2/0006', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x70172118aac0>)\n",
      "WARNING 11-27 23:16:42 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 11-27 23:16:42 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/8984c590-3eb2-4a0a-aa56-912c141fe18c for IPC Path.\n",
      "INFO 11-27 23:16:42 api_server.py:179] Started engine process with PID 391085\n",
      "WARNING 11-27 23:16:42 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:49 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:49 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:49 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:49 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:49 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:49 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:50 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:50 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:16:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 23:16:50 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:50 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:16:50 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:50 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:16:50 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:50 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:16:50 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:50 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:16:50 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:50 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:16:50 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:50 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:16:50 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:50 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 23:16:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:16:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0006', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0006', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0006, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:16:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:16:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0006', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0006', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0006, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:16:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:16:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0006', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0006', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0006, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:16:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:16:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0006', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0006', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0006, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:16:57 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:57 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:16:57 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0006', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0006', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0006, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:16:57 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:57 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:16:57 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0006', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0006', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0006, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:16:57 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:57 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:16:57 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0006', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0006', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0006, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 23:16:57 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 23:16:57 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 23:16:57 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl2/0006', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl2/0006', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl2/0006, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 11-27 23:17:04 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0006...\n",
      "INFO 11-27 23:17:04 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0006...\n",
      "INFO 11-27 23:17:04 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0006...\n",
      "INFO 11-27 23:17:04 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0006...\n",
      "INFO 11-27 23:17:04 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0006...\n",
      "INFO 11-27 23:17:04 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0006...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:17:04 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0006...\n",
      "INFO 11-27 23:17:04 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl2/0006...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.65s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.52s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.98s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.89s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.97s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.94s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.81s/it]\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:14,  4.79s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.56s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.49s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.88s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.93s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.81s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.89s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.84s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.94s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.48s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:13<00:04,  4.50s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.74s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.81s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.13s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.65s/it]\n",
      "\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.81s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.85s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.84s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.13s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.63s/it]\n",
      "\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:14<00:04,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:17:19 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:17:20 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.32s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.89s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.30s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.89s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.28s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.83s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.30s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.87s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.31s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.88s/it]\n",
      "\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.36s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:15<00:00,  3.91s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:17:20 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:17:20 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:17:20 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:17:20 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:17:20 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:17:21 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 23:17:23 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:17:23 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:17:24 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:17:24 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:17:25 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:17:25 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:17:25 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:17:25 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:17:25 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:17:25 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:17:25 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:17:25 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:17:25 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:17:25 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:17:25 gpu_executor.py:122] # GPU blocks: 9744, # CPU blocks: 1024\n",
      "INFO 11-27 23:17:25 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.03x\n",
      "INFO 11-27 23:17:27 api_server.py:232] vLLM to use /tmp/tmpanem3y_e as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:17:27 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:17:27 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:17:27 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [390436]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8101) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:60832 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO 11-27 23:17:28 api_server.py:232] vLLM to use /tmp/tmpl8jjzgng as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:17:28 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:17:28 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:17:28 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [390434]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8100) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:17:29 api_server.py:232] vLLM to use /tmp/tmpd1yayqck as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:17:29 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:17:29 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [390426]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8088) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:17:29 api_server.py:232] vLLM to use /tmp/tmpdfpxdfzt as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:17:29 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:17:29 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [390430]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8098) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:17:29 api_server.py:232] vLLM to use /tmp/tmpmycc0mf2 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:17:29 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:17:29 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [390432]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8099) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:17:29 api_server.py:232] vLLM to use /tmp/tmps970igjp as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:17:29 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:17:29 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [390438]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8102) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 23:17:29 api_server.py:232] vLLM to use /tmp/tmpm9t_e7y3 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:17:29 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:17:29 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 11-27 23:17:29 api_server.py:232] vLLM to use /tmp/tmp3m8g2i0_ as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 23:17:29 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 23:17:29 launcher.py:19] Available routes are:\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 23:17:29 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [390428]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8097) (Press CTRL+C to quit)\n",
      "INFO:     Started server process [390440]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8103) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:47914 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:53318 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:38874 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:54354 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:46298 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:57028 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:47678 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d1311a48af49a1a9950f3622e3b405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': {'NousResearch/Hermes-2-Theta-Llama-3-8B': 0.4756944444444444,\n",
       "  '/home/ubuntu/atreides/experiments/models/rl2/0001': 0.7967303240740741,\n",
       "  '/home/ubuntu/atreides/experiments/models/rl2/0002': 0.7921006944444444,\n",
       "  '/home/ubuntu/atreides/experiments/models/rl2/0003': 0.8974247685185185,\n",
       "  '/home/ubuntu/atreides/experiments/models/rl2/0004': 0.8171757414785303,\n",
       "  '/home/ubuntu/atreides/experiments/models/rl2/0005': 0.7960731816153503},\n",
       " 'test': {}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.pack import packed_tensors_from_dir\n",
    "\n",
    "tensors = packed_tensors_from_dir(dir=\"./models/rl/tensors\", num_sequences=50, sequence_length=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[\"mask\"][0][2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --max-model-len=16384 --max-num-seqs=2048 --max-num-batched-tokens=65536 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8001 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --max-model-len=16384 --max-num-seqs=2048 --max-num-batched-tokens=65536 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8002 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --max-model-len=16384 --max-num-seqs=2048 --max-num-batched-tokens=65536 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8003 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --max-model-len=16384 --max-num-seqs=2048 --max-num-batched-tokens=65536 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8004 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --max-model-len=16384 --max-num-seqs=2048 --max-num-batched-tokens=65536 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8005 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --max-model-len=16384 --max-num-seqs=2048 --max-num-batched-tokens=65536 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8006 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --max-model-len=16384 --max-num-seqs=2048 --max-num-batched-tokens=65536 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8007 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --max-model-len=16384 --max-num-seqs=2048 --max-num-batched-tokens=65536 --api-key=default\n",
      "INFO 11-27 13:10:44 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 13:10:44 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=2048, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7e138158ab60>)\n",
      "INFO 11-27 13:10:44 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/a888dd0a-6428-4a75-afcb-cf4e495bf6cb for IPC Path.\n",
      "INFO 11-27 13:10:44 api_server.py:179] Started engine process with PID 13408\n",
      "INFO 11-27 13:10:44 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 13:10:44 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8006, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=2048, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7be4ecb96ac0>)\n",
      "INFO 11-27 13:10:44 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 13:10:44 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8002, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=2048, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x77e81d98eac0>)\n",
      "INFO 11-27 13:10:44 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 13:10:44 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8005, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=2048, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x75aa1338aac0>)\n",
      "INFO 11-27 13:10:44 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/881ca67c-25a1-4bb9-86e8-173327d8b593 for IPC Path.\n",
      "INFO 11-27 13:10:44 api_server.py:179] Started engine process with PID 13413\n",
      "INFO 11-27 13:10:44 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/410481f6-4053-49ed-8e30-423d52f6dcd2 for IPC Path.\n",
      "INFO 11-27 13:10:44 api_server.py:179] Started engine process with PID 13415\n",
      "INFO 11-27 13:10:44 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/c393d683-43d6-4a1e-9022-8fbc2708d20c for IPC Path.\n",
      "INFO 11-27 13:10:44 api_server.py:179] Started engine process with PID 13417\n",
      "INFO 11-27 13:10:44 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 13:10:44 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8007, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=2048, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7e1071392ac0>)\n",
      "INFO 11-27 13:10:44 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 13:10:44 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=2048, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7ee3ef17eac0>)\n",
      "INFO 11-27 13:10:44 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 13:10:44 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8004, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=2048, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x71784c686ac0>)\n",
      "INFO 11-27 13:10:44 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/9b521d28-936b-497e-a057-4aa58ef825a1 for IPC Path.\n",
      "INFO 11-27 13:10:44 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/89f3228b-826c-4956-9d5b-c6bba1343161 for IPC Path.\n",
      "INFO 11-27 13:10:44 api_server.py:179] Started engine process with PID 13422\n",
      "INFO 11-27 13:10:44 api_server.py:179] Started engine process with PID 13424\n",
      "INFO 11-27 13:10:44 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/78f72304-656f-481d-9bdb-b623644c050c for IPC Path.\n",
      "INFO 11-27 13:10:44 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-27 13:10:44 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=2048, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x75d1f338eac0>)\n",
      "INFO 11-27 13:10:44 api_server.py:179] Started engine process with PID 13427\n",
      "INFO 11-27 13:10:44 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/877f582b-00d9-4fc9-8677-ee14ac93b4be for IPC Path.\n",
      "INFO 11-27 13:10:44 api_server.py:179] Started engine process with PID 13429\n",
      "WARNING 11-27 13:10:44 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:44 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:44 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:44 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:44 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:44 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:44 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:44 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:50 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 11-27 13:10:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 13:10:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 13:10:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 13:10:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 13:10:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 13:10:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 13:10:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 13:10:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 11-27 13:10:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 13:10:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 13:10:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 13:10:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 13:10:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 13:10:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 13:10:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 13:10:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 13:10:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 13:10:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 13:10:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 13:10:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 13:10:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 13:10:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-27 13:10:56 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 11-27 13:10:56 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-27 13:10:56 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 11-27 13:11:01 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-27 13:11:01 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-27 13:11:01 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-27 13:11:01 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-27 13:11:01 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-27 13:11:01 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-27 13:11:01 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-27 13:11:01 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-27 13:11:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-27 13:11:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-27 13:11:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-27 13:11:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-27 13:11:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-27 13:11:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-27 13:11:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-27 13:11:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.92it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.13it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.13it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.79it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.72it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.53it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.21it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.83it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.91it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.15it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.88it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.79it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.77it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.73it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.69it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.67it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.33it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.23it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.37it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.39it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:26 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 13:11:26 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.21it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.37it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.31it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:26 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:27 model_runner.py:1067] Loading model weights took 14.9595 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.41it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:27 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 13:11:27 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 13:11:27 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 13:11:27 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 11-27 13:11:28 gpu_executor.py:122] # GPU blocks: 11040, # CPU blocks: 1024\n",
      "INFO 11-27 13:11:28 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 21.56x\n",
      "INFO 11-27 13:11:28 gpu_executor.py:122] # GPU blocks: 11040, # CPU blocks: 1024\n",
      "INFO 11-27 13:11:28 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 21.56x\n",
      "INFO 11-27 13:11:29 gpu_executor.py:122] # GPU blocks: 11040, # CPU blocks: 1024\n",
      "INFO 11-27 13:11:29 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 21.56x\n",
      "INFO 11-27 13:11:29 gpu_executor.py:122] # GPU blocks: 11040, # CPU blocks: 1024\n",
      "INFO 11-27 13:11:29 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 21.56x\n",
      "INFO 11-27 13:11:29 gpu_executor.py:122] # GPU blocks: 11040, # CPU blocks: 1024\n",
      "INFO 11-27 13:11:29 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 21.56x\n",
      "INFO 11-27 13:11:29 gpu_executor.py:122] # GPU blocks: 11040, # CPU blocks: 1024\n",
      "INFO 11-27 13:11:29 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 21.56x\n",
      "INFO 11-27 13:11:29 gpu_executor.py:122] # GPU blocks: 11040, # CPU blocks: 1024\n",
      "INFO 11-27 13:11:29 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 21.56x\n",
      "INFO 11-27 13:11:29 gpu_executor.py:122] # GPU blocks: 11040, # CPU blocks: 1024\n",
      "INFO 11-27 13:11:29 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 21.56x\n",
      "INFO 11-27 13:11:30 api_server.py:232] vLLM to use /tmp/tmpt58w3oo4 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 13:11:30 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 13:11:30 launcher.py:19] Available routes are:\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 13:11:30 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12850]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8006) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:31 api_server.py:232] vLLM to use /tmp/tmpn906spvt as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 13:11:31 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 13:11:31 launcher.py:19] Available routes are:\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12844]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8003) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:31 api_server.py:232] vLLM to use /tmp/tmp0kv57xgl as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 13:11:31 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 13:11:31 launcher.py:19] Available routes are:\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12846]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8004) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:31 api_server.py:232] vLLM to use /tmp/tmp_6wz6baf as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 13:11:31 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 13:11:31 launcher.py:19] Available routes are:\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12842]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8002) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:31 api_server.py:232] vLLM to use /tmp/tmp19chf6hb as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 13:11:31 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 13:11:31 launcher.py:19] Available routes are:\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12840]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8001) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:31 api_server.py:232] vLLM to use /tmp/tmpx1pkhmif as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 13:11:31 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 13:11:31 launcher.py:19] Available routes are:\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12838]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:31 api_server.py:232] vLLM to use /tmp/tmpu2zytmih as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 13:11:31 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 13:11:31 launcher.py:19] Available routes are:\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12848]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8005) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 13:11:31 api_server.py:232] vLLM to use /tmp/tmp76hm73wl as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-27 13:11:31 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-27 13:11:31 launcher.py:19] Available routes are:\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-27 13:11:31 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12852]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8007) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:60442 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:48150 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:32806 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:59086 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:48914 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:48038 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:59052 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO:     127.0.0.1:37890 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edad3d8b088b417095e010c89d049b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (14.0 remaining samples x 0.25 patience per sample = 3.5 seconds)\n",
      "Generated 1,032,959 tokens\n",
      "Packed sequences in 0.45s ✓\n",
      "Prepared tensors in 6.26s ✓\n",
      "Created mask in 17.41s ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.max_mask_sequence_batch_size = 16\n",
    "# (eval_score, eval_exceptions), \n",
    "result, = await asyncio.gather(\n",
    "    # trainer.eval(\"val\", 0, return_exceptions=True),\n",
    "    trainer.explore(1, return_exceptions=True),\n",
    ")\n",
    "# print(f\"Eval score: {eval_score:.2%}\")\n",
    "print(\n",
    "    f\"Generated {sum(completion.num_token_logprobs() for episode in result.episodes for completion in episode.completion.descendants()):,} tokens\"\n",
    ")\n",
    "tensors = trainer.tensors(result.episodes)\n",
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed sequences in 0.23s ✓\n",
      "Prepared tensors in 4.05s ✓\n",
      "Created mask in 11.62s ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors = trainer.tensors(result.episodes)\n",
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2113536)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(tensors[\"advantages\"].shape).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([127, 16384, 16384])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "i = 1\n",
    "_tensors = tensors\n",
    "key = \"input_pos\"\n",
    "\n",
    "show(\n",
    "    _tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        _tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(_tensors[key][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(_tensors[key][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16384, 16384])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tensors()[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m127\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m127\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;28mprint\u001b[39m(i, j)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(127):\n",
    "    for j in range(127):\n",
    "        if (tensors[\"mask\"][i] == result.tensors()[\"mask\"][j]).all():\n",
    "            print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = \"advantages\"\n",
    "torch.isclose(tensors[key], result.tensors()[key], rtol=1e-5, atol=1e-8, equal_nan=True).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all((tensors[\"weights\"] == result.tensors()[\"weights\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mexceptions[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:43\u001b[0m, in \u001b[0;36mExploreResult.done_callback\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone_callback\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: asyncio\u001b[38;5;241m.\u001b[39mTask[Episode]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pack_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_exception(exception)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:112\u001b[0m, in \u001b[0;36mExploreResult._pack_episode\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mancestors(including_self\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence[c] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:157\u001b[0m, in \u001b[0;36mExploreResult._write_sequence\u001b[0;34m(self, force_write_mask)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    154\u001b[0m     force_write_mask\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_mask_sequence_batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    156\u001b[0m ):\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m packed_tensors\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:187\u001b[0m, in \u001b[0;36mExploreResult._write_mask\u001b[0;34m(self, packed_tensors)\u001b[0m\n\u001b[1;32m    179\u001b[0m     ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m*\u001b[39m (max_ancestors \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids))\n\u001b[1;32m    180\u001b[0m     ancestor_ids[completion] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ids)\n\u001b[1;32m    181\u001b[0m packed_tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][start:stop] \u001b[38;5;241m=\u001b[39m get_mask(\n\u001b[1;32m    182\u001b[0m     ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequences_to_tensor(\n\u001b[1;32m    183\u001b[0m         sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences[start:stop],\n\u001b[1;32m    184\u001b[0m         pad_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m completion: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_tensors[completion][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    186\u001b[0m     ),\n\u001b[0;32m--> 187\u001b[0m     ancestor_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sequences_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                \u001b[49m\u001b[43mancestor_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    199\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:305\u001b[0m, in \u001b[0;36mExploreResult._sequences_to_tensor\u001b[0;34m(self, sequences, pad_value, map)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sequences_to_tensor\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    299\u001b[0m     sequences: \u001b[38;5;28mlist\u001b[39m[Counter[Completion]],\n\u001b[1;32m    300\u001b[0m     pad_value: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mmap\u001b[39m: Callable[[Completion], torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    302\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    304\u001b[0m         [\n\u001b[0;32m--> 305\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sequence_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpad_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m    311\u001b[0m         ]\n\u001b[1;32m    312\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:321\u001b[0m, in \u001b[0;36mExploreResult._sequence_to_tensor\u001b[0;34m(self, sequence, pad_value, map)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sequence_to_tensor\u001b[39m(\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    316\u001b[0m     sequence: Counter[Completion],\n\u001b[1;32m    317\u001b[0m     pad_value: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mmap\u001b[39m: Callable[[Completion], torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    319\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m truncate_pad(\n\u001b[0;32m--> 321\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m completion \u001b[38;5;129;01min\u001b[39;00m sequence]),\n\u001b[1;32m    322\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length],\n\u001b[1;32m    323\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    324\u001b[0m         value\u001b[38;5;241m=\u001b[39mpad_value,\n\u001b[1;32m    325\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/explore_result.py:190\u001b[0m, in \u001b[0;36mExploreResult._write_mask.<locals>.<lambda>\u001b[0;34m(completion)\u001b[0m\n\u001b[1;32m    179\u001b[0m     ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [ids[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m*\u001b[39m (max_ancestors \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(ids))\n\u001b[1;32m    180\u001b[0m     ancestor_ids[completion] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ids)\n\u001b[1;32m    181\u001b[0m packed_tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][start:stop] \u001b[38;5;241m=\u001b[39m get_mask(\n\u001b[1;32m    182\u001b[0m     ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequences_to_tensor(\n\u001b[1;32m    183\u001b[0m         sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences[start:stop],\n\u001b[1;32m    184\u001b[0m         pad_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m completion: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_tensors[completion][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    186\u001b[0m     ),\n\u001b[1;32m    187\u001b[0m     ancestor_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequences_to_tensor(\n\u001b[1;32m    188\u001b[0m         sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequences[start:stop],\n\u001b[1;32m    189\u001b[0m         pad_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m completion: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                \u001b[49m\u001b[43mancestor_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_tensors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    198\u001b[0m     ),\n\u001b[1;32m    199\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "raise result.exceptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mexceptions\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result.exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478521.64705882355"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_033_717 / 4.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309918.3183183183"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_064_056 / 6.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192005.2093023256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_064_056 / 10.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258950.125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_071_601 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176306.46808510637"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2_071_601 / 11.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249638.84848484848"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4_119_041 / 16.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl/0003 --disable-log-requests --max-num-seqs=512 --scheduling-policy=priority --tensor-parallel-size=8 --port=8000 --api-key=default\n",
      "INFO 11-24 02:30:57 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-24 02:30:57 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl/0003', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl/0003', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='priority', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7443e69927a0>)\n",
      "INFO 11-24 02:30:57 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/30ed2f7f-cd62-4b95-9c23-f484c435b110 for IPC Path.\n",
      "INFO 11-24 02:30:57 api_server.py:179] Started engine process with PID 54698\n",
      "INFO 11-24 02:31:02 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-24 02:31:02 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-24 02:31:11 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-24 02:31:11 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-24 02:31:11 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl/0003', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl/0003', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl/0003, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-24 02:31:12 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 240 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-24 02:31:12 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:25 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:25 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:31:33 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x78ac2a596390>, local_subscribe_port=59765, remote_subscribe_port=None)\n",
      "INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:33 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.20s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:08<00:08,  4.08s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:31:45 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:31:45 model_runner.py:1067] Loading model weights took 1.8735 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.20s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.29s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:31:46 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "INFO 11-24 02:31:53 distributed_gpu_executor.py:57] # GPU blocks: 260871, # CPU blocks: 16384\n",
      "INFO 11-24 02:31:53 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 509.51x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:32:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:32:57 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55064)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55067)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55061)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55063)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55065)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55066)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=55062)\u001b[0;0m INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "INFO 11-24 02:32:57 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "INFO 11-24 02:33:00 api_server.py:232] vLLM to use /tmp/tmpg5rmsg7p as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-24 02:33:00 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-24 02:33:00 launcher.py:19] Available routes are:\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-24 02:33:00 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [54578]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37536 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e516f643d0354e5fa0302c300abd6ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405bcd9d59fe43b79f2fb94869d969ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed sequences in 7.92s ✓\n",
      "Prepared tensors in 18.11s ✓\n",
      "Created mask in 29.57s ✓\n",
      "$ tune run --nnodes=1 --nproc-per-node=8 lib.recipes.rl.RLRecipe --config /home/ubuntu/atreides/experiments/models/rl/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1124 02:46:47.268000 125095847155520 torch/distributed/run.py:779] \n",
      "W1124 02:46:47.268000 125095847155520 torch/distributed/run.py:779] *****************************************\n",
      "W1124 02:46:47.268000 125095847155520 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1124 02:46:47.268000 125095847155520 torch/distributed/run.py:779] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl/0003\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl/0003/hf_model_0002_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl/0003/hf_model_0003_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl/0003/hf_model_0004_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl/0003/hf_model_0001_0.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl/tensors\n",
      "  num_sequences: 202\n",
      "  sequence_length: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 2\n",
      "gradient_accumulation_steps: 2\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.3\n",
      "  entropy_coef: 0.04\n",
      "  kl_coef: 0.02\n",
      "  normalize_advantages: false\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/models/rl/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/models/rl/logs/log_1732416425.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 20.46 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 2.91 GiB\n",
      "\tGPU peak memory reserved: 3.03 GiB\n",
      "\tGPU peak memory active: 2.91 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|3|Loss: 0.0023: 100%|██████████| 3/3 [02:56<00:00, 51.93s/it, entropy=0.1170, kl_div=0.0372, policy=0.0062]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 9.97 secs\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict...\n",
      "INFO:torchtune.utils._logging:Getting optimizer state dict took 21.00 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Recipe checkpoint of size 32.12 GB saved to /home/ubuntu/atreides/experiments/models/rl/recipe_state.pt\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 84.50 secs\n",
      "\n",
      "1|3|Loss: 0.0023: 100%|██████████| 3/3 [04:56<00:00, 98.68s/it, entropy=0.1170, kl_div=0.0372, policy=0.0062]\n",
      "\n",
      " 33%|███▎      | 1/3 [00:37<01:15, 37.84s/it]\u001b[A\n",
      "2|4|Loss: -0.0010:  33%|███▎      | 1/3 [00:37<01:15, 37.84s/it]\u001b[A\n",
      "2|4|Loss: -0.0010:  33%|███▎      | 1/3 [00:37<01:15, 37.84s/it, entropy=0.1279, kl_div=0.0382, policy=0.0034]\u001b[A\n",
      "2|4|Loss: -0.0010:  67%|██████▋   | 2/3 [01:15<00:37, 37.65s/it, entropy=0.1279, kl_div=0.0382, policy=0.0034]\u001b[A\n",
      "2|5|Loss: -0.0018:  67%|██████▋   | 2/3 [01:15<00:37, 37.65s/it, entropy=0.1279, kl_div=0.0382, policy=0.0034]\u001b[A\n",
      "2|5|Loss: -0.0018:  67%|██████▋   | 2/3 [01:15<00:37, 37.65s/it, entropy=0.1210, kl_div=0.0351, policy=0.0024]\u001b[A\n",
      "2|5|Loss: -0.0018: 100%|██████████| 3/3 [01:53<00:00, 37.75s/it, entropy=0.1210, kl_div=0.0351, policy=0.0024]\u001b[A\n",
      "2|6|Loss: -0.0011: 100%|██████████| 3/3 [01:53<00:00, 37.75s/it, entropy=0.1210, kl_div=0.0351, policy=0.0024]\u001b[A\n",
      "2|6|Loss: -0.0011: 100%|██████████| 3/3 [01:53<00:00, 37.75s/it, entropy=0.1255, kl_div=0.0317, policy=0.0033]\u001b[AINFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 12.02 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0003_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0004_1.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 19.18 secs\n",
      "2|6|Loss: -0.0011: 100%|██████████| 3/3 [02:25<00:00, 48.62s/it, entropy=0.1255, kl_div=0.0317, policy=0.0033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 4 model files to /home/ubuntu/atreides/experiments/models/rl/0004\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl/0004 --disable-log-requests --max-num-seqs=512 --scheduling-policy=priority --tensor-parallel-size=8 --port=8000 --api-key=default\n",
      "INFO 11-24 02:55:10 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-24 02:55:10 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl/0004', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl/0004', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=8, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='priority', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7c3bcdd927a0>)\n",
      "INFO 11-24 02:55:10 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/52b9a20c-529a-4186-b7a7-cc696dd5ae19 for IPC Path.\n",
      "INFO 11-24 02:55:10 api_server.py:179] Started engine process with PID 64280\n",
      "INFO 11-24 02:55:15 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-24 02:55:15 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-24 02:55:24 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-24 02:55:24 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-24 02:55:24 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl/0004', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl/0004', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl/0004, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-24 02:55:24 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 240 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-24 02:55:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:55:38 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:55:38 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:55:46 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 11-24 02:55:46 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x798a7dc0ee40>, local_subscribe_port=44071, remote_subscribe_port=None)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:55:46 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl/0004...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m   state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:03<00:11,  3.93s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:07<00:07,  3.77s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.40s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  2.93s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:12<00:00,  3.03s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:55:58 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "INFO 11-24 02:55:58 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:56:00 model_runner.py:1067] Loading model weights took 1.8735 GB\n",
      "INFO 11-24 02:56:07 distributed_gpu_executor.py:57] # GPU blocks: 260871, # CPU blocks: 16384\n",
      "INFO 11-24 02:56:07 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 509.51x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:56:17 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:57:12 custom_all_reduce.py:233] Registering 4355 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64636)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64640)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64642)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64638)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64639)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64641)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=64637)\u001b[0;0m INFO 11-24 02:57:12 model_runner.py:1523] Graph capturing finished in 55 secs.\n",
      "INFO 11-24 02:57:15 api_server.py:232] vLLM to use /tmp/tmpc4rmlj4w as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-24 02:57:15 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-24 02:57:15 launcher.py:19] Available routes are:\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-24 02:57:15 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [64179]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:58728 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb47eeea50ec42cba9675af9ce87e2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/256 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --disable-log-requests --scheduling-policy=priority --tensor-parallel-size=2 --api-key=default\n",
      "INFO 11-23 18:31:25 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 11-23 18:31:25 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='priority', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7107af372840>)\n",
      "INFO 11-23 18:31:25 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/8ea271a9-8ace-46ef-ad84-78ace07a0439 for IPC Path.\n",
      "INFO 11-23 18:31:25 api_server.py:179] Started engine process with PID 24512\n",
      "INFO 11-23 18:31:29 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-23 18:31:29 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-23 18:31:32 config.py:905] Defaulting to use mp for distributed inference\n",
      "WARNING 11-23 18:31:32 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "INFO 11-23 18:31:32 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "WARNING 11-23 18:31:32 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 26 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-23 18:31:32 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-23 18:31:36 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-23 18:31:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 11-23 18:31:36 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 11-23 18:31:36 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7288f6d732f0>, local_subscribe_port=53275, remote_subscribe_port=None)\n",
      "INFO 11-23 18:31:36 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-23 18:31:36 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:36 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.52it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.58it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.99it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.94it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-23 18:31:39 model_runner.py:1067] Loading model weights took 7.4829 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:39 model_runner.py:1067] Loading model weights took 7.4829 GB\n",
      "INFO 11-23 18:31:40 distributed_gpu_executor.py:57] # GPU blocks: 61653, # CPU blocks: 4096\n",
      "INFO 11-23 18:31:40 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 120.42x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:42 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:42 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-23 18:31:42 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-23 18:31:42 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:51 custom_all_reduce.py:233] Registering 2275 cuda graph addresses\n",
      "INFO 11-23 18:31:51 custom_all_reduce.py:233] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=24769)\u001b[0;0m INFO 11-23 18:31:51 model_runner.py:1523] Graph capturing finished in 10 secs.\n",
      "INFO 11-23 18:31:51 model_runner.py:1523] Graph capturing finished in 10 secs.\n",
      "INFO 11-23 18:31:52 api_server.py:232] vLLM to use /tmp/tmpjv4iurcw as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-23 18:31:52 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-23 18:31:52 launcher.py:19] Available routes are:\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-23 18:31:52 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [24430]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:38090 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181dad2fd5ad46578ee0150fde392c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357a2e98b4364cc483bfe12e719c8ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_score, episodes = await asyncio.gather(trainer.eval(\"val\", 0), trainer.explore(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed sequences in 0.01s ✓\n",
      "Prepared tensors in 0.35s ✓\n",
      "Created mask in 0.65s ✓\n"
     ]
    }
   ],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from torchtune.training import cleanup_before_training\n",
    "from torchtune.training.metric_logging import DiskLogger\n",
    "from typing import Any\n",
    "\n",
    "from lib.recipes.rl import ComponentConfig, RLConfig, RLRecipe\n",
    "from lib.rl.pack import PackedDataset, packed_tensors_to_dir\n",
    "from lib.rl.ppo import PPOLoss\n",
    "\n",
    "\n",
    "tensors, checkpoint_dir, checkpoint_files = await trainer.tune_resources(episodes)\n",
    "\n",
    "PLACEHOLDER: Any = None\n",
    "\n",
    "config = RLConfig(\n",
    "    # Dataset\n",
    "    dataset=ComponentConfig(\n",
    "        PackedDataset, **packed_tensors_to_dir(tensors, trainer.output_dir + \"/tensors\")\n",
    "    ),\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    # Model\n",
    "    model=ComponentConfig(llama3_1_8b),\n",
    "    num_output_chunks=4,\n",
    "    # Checkpointer\n",
    "    checkpointer=ComponentConfig(\n",
    "        \"torchtune.training.FullModelHFCheckpointer\",\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_files=checkpoint_files,\n",
    "        recipe_checkpoint=None,\n",
    "        output_dir=trainer.output_dir,\n",
    "        model_type=\"LLAMA3\",\n",
    "    ),\n",
    "    resume_from_checkpoint=False,\n",
    "    # Fine-tuning arguments\n",
    "    batch_size=4,\n",
    "    epochs=1,\n",
    "    optimizer=ComponentConfig(\n",
    "        \"torch.optim.AdamW\",\n",
    "        # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "        # \"bitsandbytes.optim.AdamW\",\n",
    "        # params=PLACEHOLDER,\n",
    "        lr=5e-6,\n",
    "        fused=True,\n",
    "    ),\n",
    "    loss=ComponentConfig(\n",
    "        PPOLoss,\n",
    "        # clip_epsilon=0.3,\n",
    "        # entropy_coef=0.0,\n",
    "        # kl_coef=0.0,\n",
    "        clip_epsilon=0.3,\n",
    "        entropy_coef=0.025,\n",
    "        kl_coef=0.025,\n",
    "        normalize_advantages=False,\n",
    "    ),\n",
    "    max_steps_per_epoch=None,\n",
    "    compile=False,\n",
    "    optimizer_in_bwd=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Training env\n",
    "    device=\"cuda\",\n",
    "    # Memory management\n",
    "    enable_activation_checkpointing=True,\n",
    "    enable_activation_offloading=False,\n",
    "    custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "    # Reduced precision\n",
    "    dtype=\"bf16\",\n",
    "    # Logging\n",
    "    metric_logger=ComponentConfig(\n",
    "        DiskLogger, log_dir=\"/home/ubuntu/atreides/experiments/logs\"\n",
    "    ),\n",
    "    log_every_n_steps=1,\n",
    "    log_peak_memory_stats=True,\n",
    ")\n",
    "\n",
    "# recipe = RLRecipe(config)\n",
    "# recipe.setup(config)\n",
    "# recipe.train()\n",
    "# recipe.cleanup()\n",
    "# del tensors, recipe\n",
    "# cleanup_before_training()\n",
    "# trainer.save(base_checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "dict_config = config.dict_config()\n",
    "OmegaConf.save(dict_config, trainer.output_dir + \"/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.recipes.rl.RLRecipe --config /home/ubuntu/atreides/experiments/models/rl/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import IO\n",
    "\n",
    "torchrun_kwargs = {\"nnodes\": 1, \"nproc_per_node\": 2}\n",
    "kwargs = {}\n",
    "env = {\"CUDA_LAUNCH_BLOCKING\": \"1\"}\n",
    "\n",
    "args = [\n",
    "    \"tune\",\n",
    "    \"run\",\n",
    "    *[\n",
    "        f\"--{key.replace('_', '-')}{f'={value}' if value is not True else ''}\"\n",
    "        for key, value in torchrun_kwargs.items()\n",
    "    ],\n",
    "    \"lib.recipes.rl.RLRecipe\",\n",
    "    \"--config\",\n",
    "    trainer.output_dir + \"/config.yaml\",\n",
    "    *[\n",
    "        f\"--{key.replace('_', '-')}{f'={value}' if value != True else ''}\"\n",
    "        for key, value in kwargs.items()\n",
    "    ],\n",
    "]\n",
    "print(f\"$ {' '.join(args)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1123 18:47:25.110000 137426734001024 torch/distributed/run.py:779] \n",
      "W1123 18:47:25.110000 137426734001024 torch/distributed/run.py:779] *****************************************\n",
      "W1123 18:47:25.110000 137426734001024 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1123 18:47:25.110000 137426734001024 torch/distributed/run.py:779] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl/tensors\n",
      "  num_sequences: 11\n",
      "  sequence_length: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.3\n",
      "  entropy_coef: 0.025\n",
      "  kl_coef: 0.025\n",
      "  normalize_advantages: false\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/logs/log_1732387649.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 4.42 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 8.47 GiB\n",
      "\tGPU peak memory reserved: 8.62 GiB\n",
      "\tGPU peak memory active: 8.47 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|1|Loss: 0.0149: 100%|██████████| 1/1 [00:41<00:00, 41.20s/it, entropy=0.4231, kl_div=0.0643, policy=0.0239]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 7.35 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 19.64 secs\n",
      "1|1|Loss: 0.0149: 100%|██████████| 1/1 [01:08<00:00, 68.56s/it, entropy=0.4231, kl_div=0.0643, policy=0.0239]\n"
     ]
    }
   ],
   "source": [
    "process = await asyncio.create_subprocess_exec(\n",
    "    *args,\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE,\n",
    "    env={\n",
    "        **os.environ,\n",
    "        **(env or {}),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "async def log_output(stream: asyncio.StreamReader, io: IO[str]) -> None:\n",
    "    while True:\n",
    "        line = await stream.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        decoded_line = line.decode()\n",
    "        io.write(decoded_line)\n",
    "        io.flush()\n",
    "\n",
    "\n",
    "tasks = []\n",
    "if process.stdout:\n",
    "    tasks.append(asyncio.create_task(log_output(process.stdout, sys.stdout)))\n",
    "if process.stderr:\n",
    "    tasks.append(asyncio.create_task(log_output(process.stderr, sys.stderr)))\n",
    "_ = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl/tensors\n",
      "  num_sequences: 10\n",
      "  sequence_length: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.3\n",
      "  entropy_coef: 0.025\n",
      "  kl_coef: 0.025\n",
      "  normalize_advantages: false\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.DiskLogger\n",
      "  log_dir: /home/ubuntu/atreides/experiments/logs\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/logs/log_1732386575.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 3.00 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.89 GiB\n",
      "\tGPU peak memory reserved: 15.99 GiB\n",
      "\tGPU peak memory active: 15.89 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|1|Loss: 0.0031:  20%|██        | 1/5 [00:29<01:58, 29.72s/it, entropy=0.5079, kl_div=0.0659, policy=0.0141]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No backend type associated with device type cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWORLD_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRANK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mrecipe_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/experiments/lib/recipes/rl.py:1147\u001b[0m, in \u001b[0;36mrecipe_main\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m   1145\u001b[0m recipe \u001b[38;5;241m=\u001b[39m RLRecipe(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[1;32m   1146\u001b[0m recipe\u001b[38;5;241m.\u001b[39msetup(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m-> 1147\u001b[0m \u001b[43mrecipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m recipe\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m~/atreides/experiments/lib/recipes/rl.py:1019\u001b[0m, in \u001b[0;36mRLRecipe.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training\u001b[38;5;241m.\u001b[39mis_distributed():\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m running_result\u001b[38;5;241m.\u001b[39mnamed_tensors():\n\u001b[0;32m-> 1019\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# Manually scale the gradients from unnormalized loss by total # of tokens\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m training\u001b[38;5;241m.\u001b[39mscale_grads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m running_result\u001b[38;5;241m.\u001b[39mnum_tokens)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/distributed/c10d_logger.py:79\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     81\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:2288\u001b[0m, in \u001b[0;36mall_reduce\u001b[0;34m(tensor, op, group, async_op)\u001b[0m\n\u001b[1;32m   2285\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2286\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2288\u001b[0m work \u001b[38;5;241m=\u001b[39m \u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_op:\n\u001b[1;32m   2291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m work\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No backend type associated with device type cpu"
     ]
    }
   ],
   "source": [
    "from lib.recipes.rl import recipe_main\n",
    "import os\n",
    "from torch import distributed as dist\n",
    "from torchtune.training import is_distributed\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "\n",
    "\n",
    "recipe_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "dict_config = config.dict_config()\n",
    "OmegaConf.save(dict_config, trainer.output_dir + \"/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'lib.rl.completion.Completion'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.rl.completion import Completion\n",
    "\n",
    "\n",
    "OmegaConf.create(OmegaConf.to_yaml(DictConfig(dict(name=f\"{Completion.__module__}.{Completion.__name__}\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import sys\n",
    "\n",
    "traceback.clear_frames(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model checkpoint files found to save in output directory /home/ubuntu/atreides/experiments/models/rl\n"
     ]
    }
   ],
   "source": [
    "trainer.save(base_checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m show(\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mtensors\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m     19\u001b[0m         tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m&\u001b[39m (\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;241m~\u001b[39mtorch\u001b[38;5;241m.\u001b[39misnan(tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvantages\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m~\u001b[39mtorch\u001b[38;5;241m.\u001b[39misnan(tensors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvantages\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m         )\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensors' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "i = 1\n",
    "\n",
    "show(\n",
    "    tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    f'<div style=\"white-space: pre-wrap\">{list(episodes[2].completion.leaves())[0].html(30.0)}</div>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_pos_ids(\n",
    "    ids: torch.Tensor, parent_ids: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Creates an attention mask and position IDs for hierarchical attention based on node IDs and their parent IDs.\n",
    "\n",
    "    Args:\n",
    "        ids: A tensor of shape (batch_size, sequence_length) containing node IDs\n",
    "        parent_ids: A tensor of shape (batch_size, sequence_length) containing parent IDs for each node\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - mask: A boolean tensor of shape (batch_size, sequence_length, sequence_length) where True indicates\n",
    "          allowed attention connections. Each position can attend to itself and any of its ancestors\n",
    "          in the hierarchy, but only for previous positions (due to causal masking).\n",
    "        - pos_ids: A tensor of shape (batch_size, sequence_length, sequence_length) containing relative\n",
    "          position IDs for each allowed attention connection, with -1 for masked positions.\n",
    "    \"\"\"\n",
    "    mask = ids.unsqueeze(1) == ids.unsqueeze(2)\n",
    "    _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    while torch.any(mask != _mask):\n",
    "        parent_ids = parent_ids.gather(\n",
    "            1, torch.argmax((parent_ids.unsqueeze(2) == ids.unsqueeze(1)).int(), dim=2)\n",
    "        )\n",
    "        mask = _mask\n",
    "        _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    mask &= torch.tril(torch.ones_like(mask, dtype=torch.bool, device=ids.device))\n",
    "    # mask = torch.linalg.matrix_power(mask.float(), mask.size(1) - 1) > 0\n",
    "    pos_ids = (torch.where(mask, mask.cumsum(2), 0) - 1).max(1).values\n",
    "    return mask, pos_ids\n",
    "\n",
    "\n",
    "def test_mask_and_pos_ids(\n",
    "    ids: list[int],\n",
    "    parent_ids: list[int],\n",
    "    expected_mask: list[list[int]],\n",
    "    expected_pos_ids: list[int],\n",
    "):\n",
    "    mask, pos_ids = mask_and_pos_ids(\n",
    "        ids=torch.tensor([ids]), parent_ids=torch.tensor([parent_ids])\n",
    "    )\n",
    "    assert torch.all(mask.int() == torch.tensor([expected_mask])), f\"\\n{mask.int()[0]}\"\n",
    "    assert torch.all(\n",
    "        pos_ids == torch.tensor([expected_pos_ids])\n",
    "    ), f\"{pos_ids[0].tolist()}\"\n",
    "\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1],\n",
    "    parent_ids=[0, 1],\n",
    "    expected_mask=[[1, 0], [0, 1]],\n",
    "    expected_pos_ids=[0, 0],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 1],\n",
    "    parent_ids=[0, 0, 0],\n",
    "    expected_mask=[[1, 0, 0], [1, 1, 0], [1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 0, 1, 2],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 0, 1, 1],\n",
    "    parent_ids=[0, 0, 1, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 0, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 1, 0, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [0, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 1]],\n",
    "    expected_pos_ids=[0, 0, 1, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 2, 3, 3],\n",
    "    parent_ids=[0, 1, 0, 0, 1, 1],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 0, 1, 2, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3, 4, 4, 5, 5],\n",
    "    parent_ids=[0, 0, 1, 1, 2, 2, 3, 3],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 2, 2, 3, 4, 3, 4],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[2, 1, 0],\n",
    "    parent_ids=[2, 2, 0],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 0, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
