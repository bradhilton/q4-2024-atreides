{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20241213_024445-rl24</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl24' target=\"_blank\">rl24</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl24' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import ExploreOptions, Trainer, vLLMConfig\n",
    "from lib.utils import return_exception\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import Literal, Optional\n",
    "\n",
    "with open(\"./data/chain-of-thought-examples.json\") as f:\n",
    "    chain_of_thought_examples: list[dict[str, str]] = json.load(f)\n",
    "\n",
    "\n",
    "def get_variable_difficulty_game(\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    ") -> Clue:\n",
    "    num_players = random.randint(3, 6)\n",
    "    num_weapons = max(\n",
    "        3,\n",
    "        min(\n",
    "            num_players + random.randint(-1, 5),\n",
    "            len(Clue.weapons),\n",
    "        ),\n",
    "    )\n",
    "    num_suspects = min(\n",
    "        num_weapons + random.randint(0, num_weapons - 1), len(Clue.suspects)\n",
    "    )\n",
    "    num_rooms = min(num_suspects + random.randint(0, num_suspects - 2), len(Clue.rooms))\n",
    "    elements = {\n",
    "        \"suspect\": random.sample(Clue.suspects, k=num_suspects),\n",
    "        \"weapon\": random.sample(Clue.weapons, k=num_weapons),\n",
    "        \"room\": random.sample(Clue.rooms, k=num_rooms),\n",
    "    }\n",
    "    if random.random() < 0.1:\n",
    "        elements[\"motive\"] = random.sample(\n",
    "            Clue.motives,\n",
    "            k=max(3, min(num_weapons + random.randint(-1, 3), len(Clue.motives))),\n",
    "        )\n",
    "    if random.random() < 0.1:\n",
    "        frequency = random.choice([0.25, 0.5, 1.0])\n",
    "        start = 24.0 - frequency\n",
    "        end = 0.0\n",
    "        for _ in range(random.randint(1, num_weapons + 1)):\n",
    "            if random.randint(0, 1):\n",
    "                end += frequency\n",
    "            else:\n",
    "                start -= frequency\n",
    "\n",
    "        def format_time(time: float) -> str:\n",
    "            return f\"{int(time):02d}:{int(60 * (time - int(time))):02d}\"\n",
    "\n",
    "        elements[\"time\"] = Clue.get_times(\n",
    "            format_time(start), format_time(end), f\"{int(frequency * 60)}min\"\n",
    "        )\n",
    "    game = Clue(\n",
    "        num_players=num_players,\n",
    "        elements=elements,\n",
    "    )\n",
    "    difficulty_level = num_players + random.randint(-2, 3)\n",
    "    # print(f\"Players: {num_players}\")\n",
    "    # for element in elements:\n",
    "    #     print(f\"{element.capitalize()}: {len(elements[element])}\")\n",
    "    # print(f\"Difficulty level: {difficulty_level}\")\n",
    "    return game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=difficulty_level > 1,\n",
    "            check_one_of_constraints=difficulty_level > 2,\n",
    "            check_inverse_one_of_constraints=difficulty_level > 3,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=difficulty_level > 4,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        return_first_solver_as_winner=(\n",
    "            bool(random.randint(0, 1))\n",
    "            if return_first_solver_as_winner is None\n",
    "            else return_first_solver_as_winner\n",
    "        ),\n",
    "        print_playthrough=False,\n",
    "        max_turns=100,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_easy_game(return_first_solver_as_winner: Optional[bool] = None) -> Clue:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": random.sample(Clue.suspects, k=3),\n",
    "            \"weapon\": random.sample(Clue.weapons, k=3),\n",
    "            \"room\": random.sample(Clue.rooms, k=3),\n",
    "            # \"motive\": random.sample(Clue.motives, k=3),\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        return_first_solver_as_winner=return_first_solver_as_winner or False,\n",
    "        print_playthrough=False,\n",
    "        max_turns=100,\n",
    "    )\n",
    "    return game\n",
    "\n",
    "\n",
    "@return_exception\n",
    "def sample_random_episode(\n",
    "    difficulty: Literal[\"easy\", \"variable\"] = \"variable\",\n",
    "    example_probability: float = 0.0,\n",
    "    max_prompt_characters: int = 8192,\n",
    "    reward_follow_up_completion: bool = True,\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    ") -> Episode:\n",
    "    while True:\n",
    "        try:\n",
    "            game = (\n",
    "                get_easy_game if difficulty == \"easy\" else get_variable_difficulty_game\n",
    "            )(return_first_solver_as_winner=return_first_solver_as_winner)\n",
    "            prompt, follow_up, solution = game.get_prompt_and_follow_up_and_solution()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if len(prompt) <= max_prompt_characters:\n",
    "            break\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ],\n",
    "                max_tokens=16 + len(solution) * 16,\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        if reward_follow_up_completion:\n",
    "            completion = follow_up_completion\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    # Find first match of key followed by colon and capture following text\n",
    "                    (\n",
    "                        match := re.search(\n",
    "                            rf\"{key}: ([A-Za-z \\.:-]+)\",\n",
    "                            answer,\n",
    "                            re.IGNORECASE,\n",
    "                        )\n",
    "                    )\n",
    "                    # Check if captured group matches expected value\n",
    "                    and match.group(1).strip().lower() == value.strip().lower()\n",
    "                )\n",
    "                for key, value in solution.items()\n",
    "            ]\n",
    "        ) / len(solution)\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    example = (\n",
    "        random.choice(chain_of_thought_examples)\n",
    "        if random.random() < example_probability\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        examples=(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": example[\"chain_of_thought\"]\n",
    "                    + (example[\"answer\"] and f\"\\n\\n---\\n\\n{example['answer']}\"),\n",
    "                },\n",
    "            ]\n",
    "            if example\n",
    "            else []\n",
    "        ),\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_episodes():\n",
    "    while True:\n",
    "        yield sample_random_episode()\n",
    "\n",
    "\n",
    "model_name = \"rl24\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=ExploreOptions(\n",
    "        iterations=7,\n",
    "        num_parents=6,\n",
    "        branch_factor=3,\n",
    "        patience=5,\n",
    "        sample_probability_power=None,\n",
    "        sampling_kwargs={\n",
    "            \"max_tokens\": 1024,\n",
    "            \"logit_bias\": {\n",
    "\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=64 * torch.cuda.device_count(),\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=(\n",
    "        sample_random_episode() for _ in range(64 * torch.cuda.device_count())\n",
    "    ),\n",
    "    val_patience=15,\n",
    "    val_samples_per_episode=3,\n",
    "    val_sampling_kwargs={\"max_tokens\": 1024},\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=4e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=0.0,\n",
    "            clip_epsilon=0.2,\n",
    "            unclipped_policy_coef=0.0,\n",
    "            tanh_log_policy_coef=0.8,\n",
    "            value_coef=0.0,\n",
    "            entropy_coef=0.0,\n",
    "            entropy_target=0.6,\n",
    "            entropy_target_coef=0.15,\n",
    "            kl_coef=0.15,\n",
    "            weighted_entropy_coef=0.1,\n",
    "            weighted_kl_coef=0.2,\n",
    "            weighted_ce_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.85,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384 * 4,\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=32,\n",
    "            preemption_mode=\"swap\",\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(string: str) -> list[int]:\n",
    "    tokens = trainer.tokenizer.llm.get_tokenizer().encode(\n",
    "        string, add_special_tokens=False\n",
    "    )\n",
    "    if len(tokens) > 1:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "word_biases = [\n",
    "    # Critical thinking markers (highest importance)\n",
    "    (\"wait\", 0.9),    # Frequent correction initiator\n",
    "    (\"hmm\", 0.8),     # Common uncertainty marker\n",
    "    (\"alternatively\", 0.8),  # Key for exploring options\n",
    "    \n",
    "    # Important correction/uncertainty markers\n",
    "    (\"perhaps\", 0.7),  # Common for tentative suggestions\n",
    "    (\"actually\", 0.7), # Important for corrections\n",
    "    (\"rather\", 0.7),   # Refinement marker\n",
    "    \n",
    "    # Analysis progression markers\n",
    "    (\"first\", 0.6),\n",
    "    (\"similarly\", 0.6),\n",
    "    (\"therefore\", 0.6),\n",
    "    (\"instead\", 0.6),\n",
    "    \n",
    "    # Mistake acknowledgment\n",
    "    (\"wrong\", 0.5),\n",
    "    (\"incorrect\", 0.5),\n",
    "    (\"no\", 0.5),\n",
    "    \n",
    "    # Supporting markers (lower importance)\n",
    "    (\"but\", 0.4),\n",
    "    (\"so\", 0.4),\n",
    "    (\"thus\", 0.4),\n",
    "    (\"now\", 0.4),\n",
    "    (\"given\", 0.4),\n",
    "    (\"suppose\", 0.4),\n",
    "    \n",
    "    # Light correction markers\n",
    "    (\"oh\", 0.3),\n",
    "    (\"oops\", 0.3),\n",
    "    (\"right\", 0.3)\n",
    "]\n",
    "\n",
    "logit_bias = {\n",
    "    token: bias\n",
    "    for word, bias in word_biases\n",
    "    for transform in [\n",
    "        lambda x: x,\n",
    "        str.capitalize,\n",
    "        lambda x: \" \" + x\n",
    "    ]\n",
    "    for token in tokens(transform(word))\n",
    "}\n",
    "logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 81122]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.tokenizer.llm.get_tokenizer().encode(\"...Hmm\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128002,    882,    198,   4071,   3868])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.tokenizer.encode([{\"role\": \"user\", \"content\": \"But wait\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl24/0005 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.85 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --preemption-mode=swap --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86df5526ae940d8a4156fb538e991c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val: 0episode [00:00, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.20867374870072775, [])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await trainer.eval(\"val\", verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl24/0001 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.85 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6bb4563f6344edb7becc540e280e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val: 0episode [00:00, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642d6497b2d94844bd27e41a0d483dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl24/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|53|Loss: 0.0343: 100%|██████████| 53/53 [15:55<00:00, 17.74s/it, entropy=0.6107, entropy_target=0.0107, kl_div=0.2170, policy=-0.0028, tanh_log_policy=0.0002, unclipped_policy=-0.0045, value=1.6921, weighted_ce=0.0023, weighted_entropy=-0.0026, weighted_kl_div=-0.0014] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 2 model files to /home/ubuntu/atreides/experiments/models/rl24/0002\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl24/0002 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.85 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fff7391eee410e9f9338055ce7e4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa41c8c1144498490f080bcadd3bdd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl24/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|68|Loss: 0.0379: 100%|██████████| 68/68 [20:17<00:00, 17.77s/it, entropy=0.4352, entropy_target=0.1648, kl_div=0.0902, policy=0.0004, tanh_log_policy=-0.0003, unclipped_policy=-0.0006, value=2.6518, weighted_ce=-0.0016, weighted_entropy=0.0020, weighted_kl_div=0.0005]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 3 model files to /home/ubuntu/atreides/experiments/models/rl24/0003\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl24/0003 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.85 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ae28f8992d429da905abd015bcd101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389362ef298146adaca801847c6a5cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ProcessLookupError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl24/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|6|Loss: 0.0219: 100%|██████████| 6/6 [01:58<00:00, 18.32s/it, entropy=0.6080, entropy_target=0.0080, kl_div=0.1713, policy=0.0608, tanh_log_policy=-0.0041, unclipped_policy=0.0551, value=1.4917, weighted_ce=-0.0248, weighted_entropy=0.0363, weighted_kl_div=0.0096]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 4 model files to /home/ubuntu/atreides/experiments/models/rl24/0004\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl24/0004 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.85 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c73f1c1f0e94411ade82a3110f9bf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=3, verbosity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
