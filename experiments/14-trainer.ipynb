{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 / 2 /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4381744384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "1 in torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'b': 6, 'a': 3})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counters = [\n",
    "    Counter({'a': 1, 'b': 2}),\n",
    "    Counter({'a': 1, 'b': 2}),\n",
    "    Counter({'a': 1, 'b': 2}),\n",
    "]\n",
    "sum(counters, Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bddd3946e834fc3acb118f3ebab3046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a770c3633c2440abec5c86fb2846503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pbar0 = tqdm(total=100, position=0)\n",
    "pbar1 = tqdm(total=100, position=1)\n",
    "\n",
    "for i in range(100):\n",
    "    pbar0.update(1)\n",
    "    pbar1.update(1)\n",
    "    pbar0.set_description(f'val-eval')\n",
    "    pbar1.set_description(f'explore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vllm_program(model: str) -> str:\n",
    "    return f\"vllm serve {model}\"\n",
    "\n",
    "vllm_program(\"NousResearch/Hermes-2-Theta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<async_generator object episodes at 0x75ae3906d380>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, AsyncIterable, Iterable, Optional, Union\n",
    "\n",
    "from lib.rl.episode import Episode\n",
    "\n",
    "\n",
    "Episodes = Union[Iterable[Episode], AsyncIterable[Episode]]\n",
    "\n",
    "\n",
    "def asyncify_episodes(episodes: Episodes) -> AsyncIterable[Episode]:\n",
    "    if isinstance(episodes, AsyncIterable):\n",
    "        return episodes\n",
    "    else:\n",
    "\n",
    "        async def async_episodes() -> AsyncIterable[Episode]:\n",
    "            for episode in episodes:\n",
    "                yield episode\n",
    "\n",
    "        return async_episodes()\n",
    "\n",
    "async def episodes() -> AsyncIterable[Episode]:\n",
    "    yield Episode() # type: ignore\n",
    "\n",
    "asyncify_episodes(episodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<async_generator object islice at 0x75ae39367440>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aioitertools.itertools import islice\n",
    "\n",
    "islice(episodes(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-18 18:14:23 api_server.py:528] vLLM API server version dev\n",
      "\n",
      "INFO 11-18 18:14:23 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x781c10210cc0>)\n",
      "\n",
      "INFO 11-18 18:14:23 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/cb2e4a1c-be7a-4775-832d-34b9eaf7e1f3 for IPC Path.\n",
      "\n",
      "INFO 11-18 18:14:23 api_server.py:179] Started engine process with PID 8558\n",
      "\n",
      "INFO 11-18 18:14:33 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "\n",
      "INFO 11-18 18:14:34 model_runner.py:1060] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "\n",
      "INFO 11-18 18:14:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\n",
      "INFO 11-18 18:14:38 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "\n",
      "INFO 11-18 18:14:39 gpu_executor.py:122] # GPU blocks: 9604, # CPU blocks: 2048\n",
      "\n",
      "INFO 11-18 18:14:39 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 18.76x\n",
      "\n",
      "INFO 11-18 18:14:42 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\n",
      "INFO 11-18 18:14:42 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\n",
      "INFO 11-18 18:14:55 model_runner.py:1530] Graph capturing finished in 12 secs.\n",
      "\n",
      "INFO 11-18 18:14:55 api_server.py:232] vLLM to use /tmp/tmpop4p9wim as PROMETHEUS_MULTIPROC_DIR\n",
      "\n",
      "WARNING 11-18 18:14:55 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:19] Available routes are:\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /health, Methods: GET\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /version, Methods: GET\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "\n",
      "INFO 11-18 18:14:55 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "\n",
      "INFO 11-18 18:15:05 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "INFO 11-18 18:15:15 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "INFO 11-18 18:15:25 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\n",
      "INFO 11-18 18:15:35 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "process = await asyncio.create_subprocess_exec(\n",
    "    \"vllm\",\n",
    "    \"serve\",\n",
    "    \"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE,\n",
    ")\n",
    "while True:\n",
    "    print((await process.stdout.readline()).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.vllm import start_vllm_server, vllm_server_metrics\n",
    "import os\n",
    "\n",
    "model = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "os.environ[\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\"] = \"1\"\n",
    "shutdown_server, client = await start_vllm_server(\n",
    "    disable_log_requests=True,\n",
    "    max_model_len=16384,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from lib.rl.episode import Episode\n",
    "from typing import AsyncIterable, Literal\n",
    "\n",
    "Split = Literal[\"train\", \"val\", \"test\"]\n",
    "\n",
    "\n",
    "async def episodes(split: Split) -> AsyncIterable[Episode]:\n",
    "    for _ in range(10):\n",
    "        await asyncio.sleep(1)\n",
    "        yield Episode()  # type: ignore\n",
    "\n",
    "\n",
    "async for episode in episodes(split=\"val\"):\n",
    "    print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.trainer import Trainer\n",
    "\n",
    "episode = Episode()\n",
    "\n",
    "Trainer(\n",
    "    base_model=model,\n",
    "    episodes={\n",
    "        \"train\": [episode],\n",
    "        \"val\": [episode],\n",
    "        \"test\": [episode],\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
