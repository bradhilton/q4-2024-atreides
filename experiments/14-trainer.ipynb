{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1969bb9e2968401492b962b0c685423a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/716 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 03:40:01 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbb623c7a3e4e87a3046113bd21381d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368421e56a734f7cb59c6b9257e1e045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14b4e0794534477bc9d1e6694eccb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ea50fe41f64aa18c40d1a2b51522f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/169 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.trainer import Trainer\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": random.sample(Clue.suspects, k=3),\n",
    "            \"weapon\": random.sample(Clue.weapons, k=3),\n",
    "            \"room\": random.sample(Clue.rooms, k=3),\n",
    "            # \"motive\": random.choices(Clue.motives, k=3),\n",
    "            # \"time\": random.choices(Clue.get_times(\"21:00\", \"03:00\", \"1h\"), k=3),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt = game.get_prompt()\n",
    "    follow_up = \"Fill out your answer like this:\\n\" + \"\\n\".join(\n",
    "        f\"{element.capitalize()}: <#{element.upper()}#>\" for element in game.elements\n",
    "    )\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{element}: {solution}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for element, solution in game.solution.items()\n",
    "            ]\n",
    "        ) / len(game.solution)\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_episodes():\n",
    "    while True:\n",
    "        yield sample_random_episode()\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    samples_per_episode=8,\n",
    "    branch_factor=2,\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=32,\n",
    "    val_episodes=[sample_random_episode() for _ in range(32)],\n",
    "    tune_sequence_length=8192,\n",
    "    vllm_kwargs=dict(disable_log_requests=True, scheduling_policy=\"priority\"),\n",
    "    vllm_max_concurrent_requests=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --disable-log-requests --scheduling-policy=priority --api-key=default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 03:40:06 api_server.py:528] vLLM API server version dev\n",
      "INFO 11-22 03:40:06 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='priority', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x718ffe7fcd60>)\n",
      "INFO 11-22 03:40:06 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/9e205a7d-0f7c-45c1-b955-f2562869370c for IPC Path.\n",
      "INFO 11-22 03:40:06 api_server.py:179] Started engine process with PID 4370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 03:40:17 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 11-22 03:40:18 model_runner.py:1060] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 11-22 03:40:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.32it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.21it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.33it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.36it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-22 03:41:04 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "INFO 11-22 03:41:05 gpu_executor.py:122] # GPU blocks: 27864, # CPU blocks: 2048\n",
      "INFO 11-22 03:41:05 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 54.42x\n",
      "INFO 11-22 03:41:07 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-22 03:41:07 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-22 03:41:16 model_runner.py:1530] Graph capturing finished in 9 secs.\n",
      "INFO 11-22 03:41:17 api_server.py:232] vLLM to use /tmp/tmpvc_tgll2 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-22 03:41:17 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-22 03:41:17 launcher.py:19] Available routes are:\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-22 03:41:17 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [4333]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:44182 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8f7edcf9f742a9ba2f3cb1166ef854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fba6e3b7f0d4fdca733c889cd4af775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_score, episodes = await asyncio.gather(trainer.eval(\"val\", 0), trainer.explore(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm = await trainer.vllm()\n",
    "vllm.process.terminate()\n",
    "trainer._vllm_task, trainer._completion_sampler = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed sequences in 0.02s ✓\n",
      "Prepared tensors in 0.61s ✓\n",
      "Created mask in 1.14s ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "Writing logs to /home/ubuntu/atreides/experiments/logs/log_1732249559.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 2.22 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.08 GiB\n",
      "\tGPU peak memory reserved: 15.20 GiB\n",
      "\tGPU peak memory active: 15.08 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|2|Loss: 0.0204: 100%|██████████| 2/2 [00:34<00:00, 17.32s/it, entropy=0.2895, kl_div=0.0556, policy=0.0263]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 0.00 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 18.91 secs\n",
      "1|2|Loss: 0.0204: 100%|██████████| 2/2 [00:53<00:00, 26.84s/it, entropy=0.2895, kl_div=0.0556, policy=0.0263]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from torchtune.training import cleanup_before_training\n",
    "from torchtune.training.metric_logging import DiskLogger\n",
    "from typing import Any\n",
    "\n",
    "from lib.recipes.rl import ComponentConfig, RLConfig, RLRecipe\n",
    "from lib.rl.pack import PackedDataset, packed_tensors\n",
    "from lib.rl.ppo import PPOLoss\n",
    "\n",
    "tensors = packed_tensors(\n",
    "    episodes,\n",
    "    model=trainer.model,\n",
    "    sequence_length=trainer.tune_sequence_length,\n",
    "    trajectories_per_episode=(\n",
    "        int(trainer.samples_per_episode * trainer.tune_episode_sample_fraction)\n",
    "        if trainer.tune_episode_sample_fraction < 1.0\n",
    "        else None\n",
    "    ),\n",
    "    tokenizer=trainer.tokenizer,\n",
    ")\n",
    "\n",
    "checkpoint_dir = subprocess.run(\n",
    "    f\"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download {trainer.model}\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "print(\"Checkpoint directory:\", checkpoint_dir)\n",
    "\n",
    "checkpoint_output_dir = \"/home/ubuntu/atreides/experiments/models/rl\"\n",
    "os.makedirs(checkpoint_output_dir, exist_ok=True)\n",
    "\n",
    "PLACEHOLDER: Any = None\n",
    "\n",
    "config = RLConfig(\n",
    "    # Dataset\n",
    "    dataset=ComponentConfig(PackedDataset, tensors=tensors),\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    # Model\n",
    "    model=ComponentConfig(llama3_1_8b),\n",
    "    num_output_chunks=4,\n",
    "    # Checkpointer\n",
    "    checkpointer=ComponentConfig(\n",
    "        \"torchtune.training.FullModelHFCheckpointer\",\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_files=[\n",
    "            \"model-00001-of-00004.safetensors\",\n",
    "            \"model-00002-of-00004.safetensors\",\n",
    "            \"model-00003-of-00004.safetensors\",\n",
    "            \"model-00004-of-00004.safetensors\",\n",
    "        ],\n",
    "        recipe_checkpoint=None,\n",
    "        output_dir=checkpoint_output_dir,\n",
    "        model_type=\"LLAMA3\",\n",
    "    ),\n",
    "    resume_from_checkpoint=False,\n",
    "    # Fine-tuning arguments\n",
    "    batch_size=4,\n",
    "    epochs=1,\n",
    "    optimizer=ComponentConfig(\n",
    "        # AdamW,\n",
    "        \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "        params=PLACEHOLDER,\n",
    "        lr=5e-6,\n",
    "        # fused=True,\n",
    "    ),\n",
    "    loss=ComponentConfig(\n",
    "        PPOLoss,\n",
    "        # clip_epsilon=0.3,\n",
    "        # entropy_coef=0.0,\n",
    "        # kl_coef=0.0,\n",
    "        clip_epsilon=0.3,\n",
    "        entropy_coef=0.025,\n",
    "        kl_coef=0.025,\n",
    "    ),\n",
    "    max_steps_per_epoch=None,\n",
    "    compile=False,\n",
    "    optimizer_in_bwd=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Training env\n",
    "    device=\"cuda\",\n",
    "    # Memory management\n",
    "    enable_activation_checkpointing=True,\n",
    "    enable_activation_offloading=False,\n",
    "    custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "    # Reduced precision\n",
    "    dtype=\"bf16\",\n",
    "    # Logging\n",
    "    metric_logger=ComponentConfig(\n",
    "        DiskLogger, log_dir=\"/home/ubuntu/atreides/experiments/logs\"\n",
    "    ),\n",
    "    output_dir=\"/home/ubuntu/atreides/experiments/logs\",\n",
    "    log_every_n_steps=1,\n",
    "    log_peak_memory_stats=True,\n",
    ")\n",
    "\n",
    "recipe = RLRecipe(config)\n",
    "recipe.setup(config)\n",
    "recipe.train()\n",
    "recipe.cleanup()\n",
    "del recipe\n",
    "cleanup_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = subprocess.run(\n",
    "    f\"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download {trainer.model}\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "print(\"Checkpoint directory:\", checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Find the latest epoch number from model checkpoint files\n",
    "epoch = max(\n",
    "    (\n",
    "        int(result.group(1))\n",
    "        for result in (\n",
    "            re.search(r\"hf_model_\\d+_(\\d+)\\.pt\", file)\n",
    "            for file in glob.glob(f\"{checkpoint_output_dir}/hf_model_*_*.pt\")\n",
    "        )\n",
    "        if result\n",
    "    ),\n",
    "    default=-1,\n",
    ")\n",
    "\n",
    "# Find the next iteration number by looking at existing subdirectories\n",
    "iteration = (\n",
    "    max(\n",
    "        (\n",
    "            int(subdir)\n",
    "            for subdir in os.listdir(checkpoint_output_dir)\n",
    "            if os.path.isdir(os.path.join(checkpoint_output_dir, subdir))\n",
    "            and subdir.isdigit()\n",
    "        ),\n",
    "        default=-1,\n",
    "    )\n",
    "    + 1\n",
    ")\n",
    "\n",
    "# Create a new directory for this iteration\n",
    "os.makedirs(f\"{checkpoint_output_dir}/{iteration:04d}\", exist_ok=True)\n",
    "\n",
    "# Copy configuration files (non-model files) to the iteration directory\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    if not any(\n",
    "        file.endswith(suffix)\n",
    "        for suffix in (\".safetensors\", \".pt\", \".ckpt\", \".bin\", \".pth\", \".h5\")\n",
    "    ):\n",
    "        src = os.path.join(checkpoint_dir, file)\n",
    "        dst = os.path.join(f\"{checkpoint_output_dir}/{iteration:04d}\", file)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "# Move model checkpoint files to the iteration directory\n",
    "for src in glob.glob(f\"{checkpoint_output_dir}/hf_model_*_{epoch}.pt\"):\n",
    "    dst = f\"{checkpoint_output_dir}/{iteration:04d}/{os.path.basename(src)}\"\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "# Delete remaining model checkpoint files in checkpoint_output_dir root\n",
    "for file in glob.glob(f\"{checkpoint_output_dir}/hf_model_*_*.pt\"):\n",
    "    if os.path.isfile(file):\n",
    "        os.remove(file)\n",
    "\n",
    "print(f\"Saved model checkpoint to {checkpoint_output_dir}/{iteration:04d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 1\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(f\"{checkpoint_output_dir}/{iter:04d}\", exist_ok=True)\n",
    "\n",
    "# Copy all non-safetensors files from model_dir to target\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    if not file.endswith(\".safetensors\") and not file.endswith(\".pt\"):\n",
    "        src = os.path.join(checkpoint_dir, file)\n",
    "        dst = os.path.join(f\"{checkpoint_dir}/{iter:04d}\", file)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "# Move all .pt files from ./models/test to ./models/test/0000\n",
    "for file in os.listdir(checkpoint_output_dir):\n",
    "    if file.endswith(\".pt\"):\n",
    "        src = os.path.join(checkpoint_output_dir, file)\n",
    "        dst = os.path.join(f\"{checkpoint_output_dir}/{iter:04d}\", file)\n",
    "        shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del recipe\n",
    "cleanup_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "i = 1\n",
    "\n",
    "show(\n",
    "    tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    f'<div style=\"white-space: pre-wrap\">{list(episodes[2].completion.leaves())[0].html(30.0)}</div>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_pos_ids(\n",
    "    ids: torch.Tensor, parent_ids: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Creates an attention mask and position IDs for hierarchical attention based on node IDs and their parent IDs.\n",
    "\n",
    "    Args:\n",
    "        ids: A tensor of shape (batch_size, sequence_length) containing node IDs\n",
    "        parent_ids: A tensor of shape (batch_size, sequence_length) containing parent IDs for each node\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - mask: A boolean tensor of shape (batch_size, sequence_length, sequence_length) where True indicates\n",
    "          allowed attention connections. Each position can attend to itself and any of its ancestors\n",
    "          in the hierarchy, but only for previous positions (due to causal masking).\n",
    "        - pos_ids: A tensor of shape (batch_size, sequence_length, sequence_length) containing relative\n",
    "          position IDs for each allowed attention connection, with -1 for masked positions.\n",
    "    \"\"\"\n",
    "    mask = ids.unsqueeze(1) == ids.unsqueeze(2)\n",
    "    _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    while torch.any(mask != _mask):\n",
    "        parent_ids = parent_ids.gather(\n",
    "            1, torch.argmax((parent_ids.unsqueeze(2) == ids.unsqueeze(1)).int(), dim=2)\n",
    "        )\n",
    "        mask = _mask\n",
    "        _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    mask &= torch.tril(torch.ones_like(mask, dtype=torch.bool, device=ids.device))\n",
    "    # mask = torch.linalg.matrix_power(mask.float(), mask.size(1) - 1) > 0\n",
    "    pos_ids = (torch.where(mask, mask.cumsum(2), 0) - 1).max(1).values\n",
    "    return mask, pos_ids\n",
    "\n",
    "\n",
    "def test_mask_and_pos_ids(\n",
    "    ids: list[int],\n",
    "    parent_ids: list[int],\n",
    "    expected_mask: list[list[int]],\n",
    "    expected_pos_ids: list[int],\n",
    "):\n",
    "    mask, pos_ids = mask_and_pos_ids(\n",
    "        ids=torch.tensor([ids]), parent_ids=torch.tensor([parent_ids])\n",
    "    )\n",
    "    assert torch.all(mask.int() == torch.tensor([expected_mask])), f\"\\n{mask.int()[0]}\"\n",
    "    assert torch.all(\n",
    "        pos_ids == torch.tensor([expected_pos_ids])\n",
    "    ), f\"{pos_ids[0].tolist()}\"\n",
    "\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1],\n",
    "    parent_ids=[0, 1],\n",
    "    expected_mask=[[1, 0], [0, 1]],\n",
    "    expected_pos_ids=[0, 0],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 1],\n",
    "    parent_ids=[0, 0, 0],\n",
    "    expected_mask=[[1, 0, 0], [1, 1, 0], [1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 0, 1, 2],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 0, 1, 1],\n",
    "    parent_ids=[0, 0, 1, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 0, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 1, 0, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [0, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 1]],\n",
    "    expected_pos_ids=[0, 0, 1, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 2, 3, 3],\n",
    "    parent_ids=[0, 1, 0, 0, 1, 1],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 0, 1, 2, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3, 4, 4, 5, 5],\n",
    "    parent_ids=[0, 0, 1, 1, 2, 2, 3, 3],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 2, 2, 3, 4, 3, 4],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[2, 1, 0],\n",
    "    parent_ids=[2, 2, 0],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 0, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 0],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
