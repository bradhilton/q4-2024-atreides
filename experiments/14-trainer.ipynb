{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --block-size=32 --disable-log-requests --enforce-eager --gpu-memory-utilization=0.999 --max-model-len=16384 --max-num-seqs=4096 --max-num-batched-tokens=131072 --return-tokens-as-token-ids --swap-space=8 --port=8003 --api-key=default\n",
      "INFO 12-03 21:11:36 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 12-03 21:11:36 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8003, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=8.0, cpu_offload_gb=0, gpu_memory_utilization=0.999, num_gpu_blocks_override=None, max_num_batched_tokens=131072, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7cf5d9aa0cc0>)\n",
      "INFO 12-03 21:11:36 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/e1c0025c-dc79-4170-b3ee-402d1a0ea4c1 for IPC Path.\n",
      "INFO 12-03 21:11:36 api_server.py:179] Started engine process with PID 21385\n",
      "WARNING 12-03 21:11:36 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-03 21:11:40 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-03 21:11:40 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-03 21:11:41 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-03 21:11:47 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-03 21:11:47 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-03 21:11:47 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 12-03 21:11:48 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 12-03 21:11:48 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  4.56it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.93it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.58it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.48it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.63it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-03 21:11:51 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 12-03 21:11:57 gpu_executor.py:122] # GPU blocks: 9747, # CPU blocks: 2048\n",
      "INFO 12-03 21:11:57 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 19.04x\n",
      "INFO 12-03 21:12:01 api_server.py:232] vLLM to use /tmp/tmp4yua16u4 as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 12-03 21:12:01 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 12-03 21:12:01 launcher.py:19] Available routes are:\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 12-03 21:12:01 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [21344]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8003) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:36894 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    }
   ],
   "source": [
    "from lib.vllm import start_vllm\n",
    "\n",
    "vllm = await start_vllm(\n",
    "    \"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "    block_size=32,\n",
    "    disable_log_requests=True,\n",
    "    # enable_prefix_caching=True,\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=0.999,\n",
    "    max_model_len=16384,\n",
    "    max_num_seqs=4096,\n",
    "    max_num_batched_tokens=16384 * 8,\n",
    "    return_tokens_as_token_ids=True,\n",
    "    swap_space=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = await vllm.client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    logprobs=True,\n",
    "    stop=[\".\"],\n",
    "    extra_body={\n",
    "        \"guided_regex\": r\"The capital of France is \\w+\\.\",\n",
    "        \"skip_special_tokens\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionTokenLogprob(token='token_id:791', bytes=[84, 104, 101], logprob=0.0, top_logprobs=[]),\n",
       " ChatCompletionTokenLogprob(token='token_id:6864', bytes=[32, 99, 97, 112, 105, 116, 97, 108], logprob=0.0, top_logprobs=[]),\n",
       " ChatCompletionTokenLogprob(token='token_id:315', bytes=[32, 111, 102], logprob=0.0, top_logprobs=[]),\n",
       " ChatCompletionTokenLogprob(token='token_id:9822', bytes=[32, 70, 114, 97, 110, 99, 101], logprob=0.0, top_logprobs=[]),\n",
       " ChatCompletionTokenLogprob(token='token_id:374', bytes=[32, 105, 115], logprob=0.0, top_logprobs=[]),\n",
       " ChatCompletionTokenLogprob(token='token_id:12366', bytes=[32, 80, 97, 114, 105, 115], logprob=0.0, top_logprobs=[]),\n",
       " ChatCompletionTokenLogprob(token='token_id:13', bytes=[46], logprob=0.0, top_logprobs=[])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].logprobs.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.utils import get_token\n",
    "\n",
    "get_token(chat_completion.choices[0].logprobs.content[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm.process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await vllm.client.completions.create(\n",
    "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    # prompt=\"<|begin_of_text|><|im_start|>user\\n\",\n",
    "    prompt=[128000, 128002, 882],\n",
    "    max_tokens=1,\n",
    "    extra_body={\n",
    "        \"prompt_logprobs\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from /home/ubuntu/atreides/experiments/models/rl2/0001\n",
      "INFO 12-03 21:39:06 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import Trainer\n",
    "from lib.utils import return_exception\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from torchtune.training.metric_logging import WandBLogger\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "@return_exception\n",
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": random.sample(Clue.suspects, k=3),\n",
    "            \"weapon\": random.sample(Clue.weapons, k=3),\n",
    "            \"room\": random.sample(Clue.rooms, k=3),\n",
    "            # \"motive\": random.sample(Clue.motives, k=3),\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt, follow_up, solution = game.get_prompt_and_follow_up_and_solution()\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{key}: {value}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for key, value in solution.items()\n",
    "            ]\n",
    "        ) / len(solution)\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_episodes():\n",
    "    while True:\n",
    "        yield sample_random_episode()\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=\"./models/rl2\",\n",
    "    samples_per_episode=27,\n",
    "    branch_factor=3,\n",
    "    sample_probability_power=1 / 3,\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=128 * torch.cuda.device_count(),\n",
    "    patience_per_sample=1 / 27,\n",
    "    sampling_kwargs={\n",
    "        \"max_tokens\": 2048,\n",
    "    },\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=(\n",
    "        sample_random_episode() for _ in range(32 * torch.cuda.device_count())\n",
    "    ),\n",
    "    val_samples_per_episode=3,\n",
    "    torchrun_kwargs=dict(nnodes=1, nproc_per_node=torch.cuda.device_count()),\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=2,\n",
    "        epochs=1,\n",
    "        metric_logger=ComponentConfig(\n",
    "            WandBLogger,\n",
    "        ),\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=5e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=1.0,\n",
    "            clip_epsilon=0.2,\n",
    "            entropy_coef=0.02,\n",
    "            kl_coef=0.0,\n",
    "            weighted_ce_coef=0.0,\n",
    "            weighted_entropy_coef=0.0,\n",
    "            weighted_kl_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=8192,\n",
    "    # vllm_env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "    vllm_kwargs=dict(\n",
    "        block_size=32,\n",
    "        disable_log_requests=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enforce_eager=True,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        # max_model_len=16384,\n",
    "        max_num_seqs=512 * torch.cuda.device_count(),\n",
    "        max_num_batched_tokens=8192 * 4,\n",
    "        swap_space=8,\n",
    "        # scheduling_policy=\"priority\",\n",
    "        # tensor_parallel_size=torch.cuda.device_count() // 8,\n",
    "    ),\n",
    "    vllm_max_concurrent_samples=512 * torch.cuda.device_count(),\n",
    "    vllm_min_time_between_requests=0.0,\n",
    "    vllm_num=torch.cuda.device_count(),\n",
    "    vllm_timeout=120 + 15 * torch.cuda.device_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128002,  78191,    198,      1,      1, 128003,    198])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = trainer.tokenizer.encode(\n",
    "    [{\"role\": \"assistant\", \"content\": \"!\"}],\n",
    "    # remove_bos=True,\n",
    "    # first_message_is_continuation=True,\n",
    "    continue_final_message=False,\n",
    ")\n",
    "\n",
    "zero_idx = (tokens == 0).nonzero()[0]\n",
    "new_tokens = torch.cat([tokens[:zero_idx], torch.tensor([1, 1]), tokens[zero_idx+1:]])\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizers.Tokenizer"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainer.tokenizer.llm.get_tokenizer()._tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Ä capital']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.tokenizer.llm.get_tokenizer().convert_ids_to_tokens([791, 6864])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerBase.decode of CachedPreTrainedTokenizerFast(name_or_path='NousResearch/Hermes-2-Theta-Llama-3-8B', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|im_end|>', 'pad_token': '<|end_of_text|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128002: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128003: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128004: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128005: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128008: AddedToken(\"<tools>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128010: AddedToken(\"</tools>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128011: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128012: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128013: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128014: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128015: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128016: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128017: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128018: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128019: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128020: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128021: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128022: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128023: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128024: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128025: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128026: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128027: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128028: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128029: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128030: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128031: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128032: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128033: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128034: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128035: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128036: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128037: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128038: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128039: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128040: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128041: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128042: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128043: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128044: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128045: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128046: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128047: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128048: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128049: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128050: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128051: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128052: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128053: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128054: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128055: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128056: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128057: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128058: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128059: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128060: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128061: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128062: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128063: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128064: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128065: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128066: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128067: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128068: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128069: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128070: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128071: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128072: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128073: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128074: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128075: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128076: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128077: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128078: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128079: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128080: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128081: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128082: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128083: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128084: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128085: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128086: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128087: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128088: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128089: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128090: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128091: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128092: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128093: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128094: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128095: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128096: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128097: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128098: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128099: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128100: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128101: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128102: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128103: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128104: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128105: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128106: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128107: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128108: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128109: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128110: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128111: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128112: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128113: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128114: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128115: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128116: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128117: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128118: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128119: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128120: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128121: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128122: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128123: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128124: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128125: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128126: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128127: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128128: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128129: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128130: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128131: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128132: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128133: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128134: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128135: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128136: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128137: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128138: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128139: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128140: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128141: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128142: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128143: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128144: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128145: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128146: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128147: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128148: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128149: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128150: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128151: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128152: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128153: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128154: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128155: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128156: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128157: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128158: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128159: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128160: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128161: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128162: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128163: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128164: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128165: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128166: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128167: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128168: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128169: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128170: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128171: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128172: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128173: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128174: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128175: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128176: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128177: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128178: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128179: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128180: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128181: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128182: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128183: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128184: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128185: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128186: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128187: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128188: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128189: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128190: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128191: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128192: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128193: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128194: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128195: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128196: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128197: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128198: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128199: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128200: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128201: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128202: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128203: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128204: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128205: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128206: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128207: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128208: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128209: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128210: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128211: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128212: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128213: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128214: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128215: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128216: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128217: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128218: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128219: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128220: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128221: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128222: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128223: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128224: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128225: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128226: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128227: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128228: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128229: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128230: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128231: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128232: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128233: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128234: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128235: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128236: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128237: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128238: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128239: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128240: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128241: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128242: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128243: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128244: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128245: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128246: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128247: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128248: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128249: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128250: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128251: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128252: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128253: AddedToken(\"<|reserved_special_token_248|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128254: AddedToken(\"<|reserved_special_token_249|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128255: AddedToken(\"<|reserved_special_token_250|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t128256: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.tokenizer.llm.get_tokenizer().decode  # ([791, 6864])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16384 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8002 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-num-seqs=512 --max-num-batched-tokens=32768 --swap-space=8 --api-key=default\n",
      "INFO 12-03 18:49:14 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 12-03 18:49:14 api_server.py:529] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8002, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=8.0, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=32768, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x70a4d68a0cc0>)\n",
      "INFO 12-03 18:49:14 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/c4dda09b-ab27-458e-aab6-ce6f13f498f0 for IPC Path.\n",
      "INFO 12-03 18:49:14 api_server.py:179] Started engine process with PID 12328\n",
      "WARNING 12-03 18:49:20 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-03 18:49:20 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-03 18:49:25 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-03 18:49:25 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-03 18:49:25 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 12-03 18:49:27 model_runner.py:1056] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 12-03 18:49:27 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  4.20it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.59it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.27it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-03 18:49:31 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 12-03 18:49:32 gpu_executor.py:122] # GPU blocks: 14333, # CPU blocks: 2048\n",
      "INFO 12-03 18:49:32 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 55.99x\n",
      "INFO 12-03 18:49:37 api_server.py:232] vLLM to use /tmp/tmpxmysilan as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 12-03 18:49:37 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 12-03 18:49:37 launcher.py:19] Available routes are:\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 12-03 18:49:37 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12293]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8002) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:49442 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cec87a9d4984ed29a38a5b8b860bd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (18.115199999989372 remaining samples x 0.037037037037037035 patience per sample = 0.6709333333329397 seconds)\n"
     ]
    }
   ],
   "source": [
    "explore_result = await trainer.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.dtype = \"bf16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.2450e-09)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_result.tensors()[\"values\"][\n",
    "    ~torch.isnan(explore_result.tensors()[\"values\"])\n",
    "].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.optimizer.lr = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.num_output_chunks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33100d000e34b04aff219c8a83975a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-eon-24</strong> at: <a href='https://wandb.ai/bradhilton/torchtune/runs/rqbpj3ze' target=\"_blank\">https://wandb.ai/bradhilton/torchtune/runs/rqbpj3ze</a><br/> View project at: <a href='https://wandb.ai/bradhilton/torchtune' target=\"_blank\">https://wandb.ai/bradhilton/torchtune</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241203_201017-rqbpj3ze/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2388: UserWarning: Run (rqbpj3ze) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "  0%|          | 0/93 [04:03<?, ?it/s]\n",
      "DEBUG:torchtune.utils._logging:Training is not distributed. If you want to train on multiple GPUs and are using the tune CLI, specify --nnodes 1 and --nproc_per_node [num_gpus]\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl2/0001\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0001/hf_model_0003_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0001/hf_model_0004_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0001/hf_model_0001_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl2/0001/hf_model_0002_0.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl2\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl2/tensors\n",
      "  num_sequences: 93\n",
      "  sequence_length: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.02\n",
      "  kl_coef: 0.0\n",
      "  normalize_advantages: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 1.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.0\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 1\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 1.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20241203_201433-movqaf05</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bradhilton/torchtune/runs/movqaf05' target=\"_blank\">dry-snowball-25</a></strong> to <a href='https://wandb.ai/bradhilton/torchtune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/torchtune' target=\"_blank\">https://wandb.ai/bradhilton/torchtune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/torchtune/runs/movqaf05' target=\"_blank\">https://wandb.ai/bradhilton/torchtune/runs/movqaf05</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/atreides/experiments/models/rl2/0001/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 2.68 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.05 GiB\n",
      "\tGPU peak memory reserved: 15.17 GiB\n",
      "\tGPU peak memory active: 15.05 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/93 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First token with KL divergence > 10: 1231\n",
      "Token: 98996\n",
      "Logprob: -0.0048474413342773914\n",
      "New Logprob: -10.3125\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "KL Divergence Debugging",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtune(explore_result)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:601\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, result, verbosity)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     cleanup_before_training()\n\u001b[0;32m--> 601\u001b[0m     \u001b[43mrecipe_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_recipe_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(checkpoint_dir)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/recipe.py:1191\u001b[0m, in \u001b[0;36mrecipe_main\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m   1189\u001b[0m recipe \u001b[38;5;241m=\u001b[39m TuneRecipe(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[1;32m   1190\u001b[0m recipe\u001b[38;5;241m.\u001b[39msetup(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[43mrecipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m recipe\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/recipe.py:1007\u001b[0m, in \u001b[0;36mTuneRecipe.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m mask, input_pos\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m current_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43madvantages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madvantages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreference_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreference_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m logits, batch\n\u001b[1;32m   1019\u001b[0m running_result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_result\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/ppo.py:217\u001b[0m, in \u001b[0;36mPPOLoss.forward\u001b[0;34m(self, logits, tokens, values, advantages, logprobs, reference_logprobs, weights, bos_id)\u001b[0m\n\u001b[1;32m    195\u001b[0m     num_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(logits)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunked_args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    197\u001b[0m         logits,\n\u001b[1;32m    198\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mchunk(num_chunks, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         ),\n\u001b[1;32m    216\u001b[0m     ):\n\u001b[0;32m--> 217\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchunked_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_chunk(\n\u001b[1;32m    221\u001b[0m     logits,\n\u001b[1;32m    222\u001b[0m     tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     bos_id\u001b[38;5;241m=\u001b[39mbos_id,\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/ppo.py:300\u001b[0m, in \u001b[0;36mPPOLoss._forward_chunk\u001b[0;34m(self, logits, tokens, values, advantages, logprobs, reference_logprobs, weights, bos_id)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo tokens found with KL divergence > 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL Divergence Debugging\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Calculate entropy for each token\u001b[39;00m\n\u001b[1;32m    303\u001b[0m entropy \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mentropy()  \u001b[38;5;66;03m# Shape: (batch_size * sequence_length,)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: KL Divergence Debugging"
     ]
    }
   ],
   "source": [
    "await trainer.tune(explore_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128002,    882,  ...,  28877,     25,  98996])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([        nan,         nan,         nan,  ..., -2.1458e-06,\n",
       "         0.0000e+00, -4.8474e-03])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors = explore_result.tensors()\n",
    "display(tensors[\"tokens\"][0][:1233])\n",
    "tensors[\"logprobs\"][0][:1233]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' when asked by Alexandria.\\n\\nPutting it all together, the facedown cards in the center of the table are:\\n\\n* Suspect: Madame Rose\\n* Weapon: Rope'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.tokenizer.decode(tensors[\"tokens\"][0][1200:1233])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "completion_sampler = await trainer.get_completion_sampler()\n",
    "client: AsyncOpenAI = completion_sampler.samplers[0].client  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm.client.chat.completions.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "episode = explore_result.episodes[0]\n",
    "completion = next(iter(episode.completion.leaves()))\n",
    "tokens = completion.all_tokens(trainer.tokenizer, cache=True).tolist()\n",
    "plain_completion = await vllm.client.completions.create(\n",
    "    model=trainer.model,\n",
    "    prompt=tokens,\n",
    "    max_tokens=1,\n",
    "    extra_body={\n",
    "        \"prompt_logprobs\": True,\n",
    "    },\n",
    ")\n",
    "prompt_logprobs: list[dict[str, dict[str, Any]]] = plain_completion.choices[0].prompt_logprobs  # type: ignore\n",
    "prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(prompt_logprobs) == len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_logprobs = [\n",
    "    prompt_logprob[str(token)][\"logprob\"] if prompt_logprob else torch.nan\n",
    "    for token, prompt_logprob in zip(tokens, prompt_logprobs)\n",
    "]\n",
    "for c in completion.ancestors(including_self=True, reverse=True):\n",
    "    count = c.token_count(trainer.tokenizer, cache=True)\n",
    "    c.reference_logprobs, reference_logprobs = (\n",
    "        torch.tensor(reference_logprobs[:count]),\n",
    "        reference_logprobs[count:],\n",
    "    )\n",
    "\n",
    "completion.reference_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(c._reference_logprobs.shape[0] for c in completion.ancestors(including_self=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(\n",
    "    [\n",
    "        prompt_logprob[str(token)][\"logprob\"] if prompt_logprob else torch.nan\n",
    "        for token, prompt_logprob in zip(tokens, prompt_logprobs)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt_logprobs), len(completion.all_tokens(trainer.tokenizer, cache=True).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/tmp/err_execute_model_input_20241201-005319.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(plain_completion, \"prompt_logprobs\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_completion.choices[0].prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.all_tokens(trainer.tokenizer, cache=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.patience_per_val_sample = 1.0\n",
    "trainer.patience_per_test_sample = 1.0\n",
    "trainer.tune_recipe_config.optimizer.lr = 8e-6\n",
    "trainer.tune_recipe_config.loss.clip_epsilon = 0.1\n",
    "trainer.tune_recipe_config.loss.weighted_ce_coef = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await trainer.train(iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.pack import packed_tensors_from_dir\n",
    "\n",
    "tensors = packed_tensors_from_dir(\n",
    "    dir=\"./models/rl/tensors\", num_sequences=50, sequence_length=16384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors[\"mask\"][0][2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.max_mask_sequence_batch_size = 16\n",
    "# (eval_score, eval_exceptions),\n",
    "(result,) = await asyncio.gather(\n",
    "    # trainer.eval(\"val\", 0, return_exceptions=True),\n",
    "    trainer.explore(1, return_exceptions=True),\n",
    ")\n",
    "# print(f\"Eval score: {eval_score:.2%}\")\n",
    "print(\n",
    "    f\"Generated {sum(completion.num_token_logprobs() for episode in result.episodes for completion in episode.completion.descendants()):,} tokens\"\n",
    ")\n",
    "tensors = trainer.tensors(result.episodes)\n",
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = trainer.tensors(result.episodes)\n",
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tensors[\"advantages\"].shape).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "i = 1\n",
    "_tensors = tensors\n",
    "key = \"input_pos\"\n",
    "\n",
    "show(\n",
    "    _tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        _tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(_tensors[key][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(_tensors[key][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tensors()[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(127):\n",
    "    for j in range(127):\n",
    "        if (tensors[\"mask\"][i] == result.tensors()[\"mask\"][j]).all():\n",
    "            print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"advantages\"\n",
    "torch.isclose(\n",
    "    tensors[key], result.tensors()[key], rtol=1e-5, atol=1e-8, equal_nan=True\n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all((tensors[\"weights\"] == result.tensors()[\"weights\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise result.exceptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_033_717 / 4.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_064_056 / 6.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_064_056 / 10.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_071_601 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_071_601 / 11.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4_119_041 / 16.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await trainer.train(iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score, episodes = await asyncio.gather(trainer.eval(\"val\", 0), trainer.explore(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from torchtune.training import cleanup_before_training\n",
    "from torchtune.training.metric_logging import DiskLogger\n",
    "from typing import Any\n",
    "\n",
    "from lib.recipes.rl import ComponentConfig, RLConfig, RLRecipe\n",
    "from lib.rl.pack import PackedDataset, packed_tensors_to_dir\n",
    "from lib.rl.ppo import PPOLoss\n",
    "\n",
    "\n",
    "tensors, checkpoint_dir, checkpoint_files = await trainer.tune_resources(episodes)\n",
    "\n",
    "PLACEHOLDER: Any = None\n",
    "\n",
    "config = RLConfig(\n",
    "    # Dataset\n",
    "    dataset=ComponentConfig(\n",
    "        PackedDataset, **packed_tensors_to_dir(tensors, trainer.output_dir + \"/tensors\")\n",
    "    ),\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    # Model\n",
    "    model=ComponentConfig(llama3_1_8b),\n",
    "    num_output_chunks=4,\n",
    "    # Checkpointer\n",
    "    checkpointer=ComponentConfig(\n",
    "        \"torchtune.training.FullModelHFCheckpointer\",\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_files=checkpoint_files,\n",
    "        recipe_checkpoint=None,\n",
    "        output_dir=trainer.output_dir,\n",
    "        model_type=\"LLAMA3\",\n",
    "    ),\n",
    "    resume_from_checkpoint=False,\n",
    "    # Fine-tuning arguments\n",
    "    batch_size=4,\n",
    "    epochs=1,\n",
    "    optimizer=ComponentConfig(\n",
    "        \"torch.optim.AdamW\",\n",
    "        # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "        # \"bitsandbytes.optim.AdamW\",\n",
    "        # params=PLACEHOLDER,\n",
    "        lr=5e-6,\n",
    "        fused=True,\n",
    "    ),\n",
    "    loss=ComponentConfig(\n",
    "        PPOLoss,\n",
    "        # clip_epsilon=0.3,\n",
    "        # entropy_coef=0.0,\n",
    "        # kl_coef=0.0,\n",
    "        clip_epsilon=0.3,\n",
    "        entropy_coef=0.025,\n",
    "        kl_coef=0.025,\n",
    "        normalize_advantages=False,\n",
    "    ),\n",
    "    max_steps_per_epoch=None,\n",
    "    compile=False,\n",
    "    optimizer_in_bwd=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Training env\n",
    "    device=\"cuda\",\n",
    "    # Memory management\n",
    "    enable_activation_checkpointing=True,\n",
    "    enable_activation_offloading=False,\n",
    "    custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "    # Reduced precision\n",
    "    dtype=\"bf16\",\n",
    "    # Logging\n",
    "    metric_logger=ComponentConfig(\n",
    "        DiskLogger, log_dir=\"/home/ubuntu/atreides/experiments/logs\"\n",
    "    ),\n",
    "    log_every_n_steps=1,\n",
    "    log_peak_memory_stats=True,\n",
    ")\n",
    "\n",
    "# recipe = RLRecipe(config)\n",
    "# recipe.setup(config)\n",
    "# recipe.train()\n",
    "# recipe.cleanup()\n",
    "# del tensors, recipe\n",
    "# cleanup_before_training()\n",
    "# trainer.save(base_checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "dict_config = config.dict_config()\n",
    "OmegaConf.save(dict_config, trainer.output_dir + \"/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import IO\n",
    "\n",
    "torchrun_kwargs = {\"nnodes\": 1, \"nproc_per_node\": 2}\n",
    "kwargs = {}\n",
    "env = {\"CUDA_LAUNCH_BLOCKING\": \"1\"}\n",
    "\n",
    "args = [\n",
    "    \"tune\",\n",
    "    \"run\",\n",
    "    *[\n",
    "        f\"--{key.replace('_', '-')}{f'={value}' if value is not True else ''}\"\n",
    "        for key, value in torchrun_kwargs.items()\n",
    "    ],\n",
    "    \"lib.recipes.rl.RLRecipe\",\n",
    "    \"--config\",\n",
    "    trainer.output_dir + \"/config.yaml\",\n",
    "    *[\n",
    "        f\"--{key.replace('_', '-')}{f'={value}' if value != True else ''}\"\n",
    "        for key, value in kwargs.items()\n",
    "    ],\n",
    "]\n",
    "print(f\"$ {' '.join(args)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = await asyncio.create_subprocess_exec(\n",
    "    *args,\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE,\n",
    "    env={\n",
    "        **os.environ,\n",
    "        **(env or {}),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "async def log_output(stream: asyncio.StreamReader, io: IO[str]) -> None:\n",
    "    while True:\n",
    "        line = await stream.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        decoded_line = line.decode()\n",
    "        io.write(decoded_line)\n",
    "        io.flush()\n",
    "\n",
    "\n",
    "tasks = []\n",
    "if process.stdout:\n",
    "    tasks.append(asyncio.create_task(log_output(process.stdout, sys.stdout)))\n",
    "if process.stderr:\n",
    "    tasks.append(asyncio.create_task(log_output(process.stderr, sys.stderr)))\n",
    "_ = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.recipes.rl import recipe_main\n",
    "import os\n",
    "from torch import distributed as dist\n",
    "from torchtune.training import is_distributed\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "\n",
    "\n",
    "recipe_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "dict_config = config.dict_config()\n",
    "OmegaConf.save(dict_config, trainer.output_dir + \"/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.completion import Completion\n",
    "\n",
    "\n",
    "OmegaConf.create(\n",
    "    OmegaConf.to_yaml(\n",
    "        DictConfig(dict(name=f\"{Completion.__module__}.{Completion.__name__}\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import sys\n",
    "\n",
    "traceback.clear_frames(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(base_checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "i = 1\n",
    "\n",
    "show(\n",
    "    tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    f'<div style=\"white-space: pre-wrap\">{list(episodes[2].completion.leaves())[0].html(30.0)}</div>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_pos_ids(\n",
    "    ids: torch.Tensor, parent_ids: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Creates an attention mask and position IDs for hierarchical attention based on node IDs and their parent IDs.\n",
    "\n",
    "    Args:\n",
    "        ids: A tensor of shape (batch_size, sequence_length) containing node IDs\n",
    "        parent_ids: A tensor of shape (batch_size, sequence_length) containing parent IDs for each node\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - mask: A boolean tensor of shape (batch_size, sequence_length, sequence_length) where True indicates\n",
    "          allowed attention connections. Each position can attend to itself and any of its ancestors\n",
    "          in the hierarchy, but only for previous positions (due to causal masking).\n",
    "        - pos_ids: A tensor of shape (batch_size, sequence_length, sequence_length) containing relative\n",
    "          position IDs for each allowed attention connection, with -1 for masked positions.\n",
    "    \"\"\"\n",
    "    mask = ids.unsqueeze(1) == ids.unsqueeze(2)\n",
    "    _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    while torch.any(mask != _mask):\n",
    "        parent_ids = parent_ids.gather(\n",
    "            1, torch.argmax((parent_ids.unsqueeze(2) == ids.unsqueeze(1)).int(), dim=2)\n",
    "        )\n",
    "        mask = _mask\n",
    "        _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    mask &= torch.tril(torch.ones_like(mask, dtype=torch.bool, device=ids.device))\n",
    "    # mask = torch.linalg.matrix_power(mask.float(), mask.size(1) - 1) > 0\n",
    "    pos_ids = (torch.where(mask, mask.cumsum(2), 0) - 1).max(1).values\n",
    "    return mask, pos_ids\n",
    "\n",
    "\n",
    "def test_mask_and_pos_ids(\n",
    "    ids: list[int],\n",
    "    parent_ids: list[int],\n",
    "    expected_mask: list[list[int]],\n",
    "    expected_pos_ids: list[int],\n",
    "):\n",
    "    mask, pos_ids = mask_and_pos_ids(\n",
    "        ids=torch.tensor([ids]), parent_ids=torch.tensor([parent_ids])\n",
    "    )\n",
    "    assert torch.all(mask.int() == torch.tensor([expected_mask])), f\"\\n{mask.int()[0]}\"\n",
    "    assert torch.all(\n",
    "        pos_ids == torch.tensor([expected_pos_ids])\n",
    "    ), f\"{pos_ids[0].tolist()}\"\n",
    "\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1],\n",
    "    parent_ids=[0, 1],\n",
    "    expected_mask=[[1, 0], [0, 1]],\n",
    "    expected_pos_ids=[0, 0],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 1],\n",
    "    parent_ids=[0, 0, 0],\n",
    "    expected_mask=[[1, 0, 0], [1, 1, 0], [1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 0, 1, 2],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 0, 1, 1],\n",
    "    parent_ids=[0, 0, 1, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 0, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 1, 0, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [0, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 1]],\n",
    "    expected_pos_ids=[0, 0, 1, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 2, 3, 3],\n",
    "    parent_ids=[0, 1, 0, 0, 1, 1],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 0, 1, 2, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3, 4, 4, 5, 5],\n",
    "    parent_ids=[0, 0, 1, 1, 2, 2, 3, 3],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 2, 2, 3, 4, 3, 4],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[2, 1, 0],\n",
    "    parent_ids=[2, 2, 0],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 0, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
