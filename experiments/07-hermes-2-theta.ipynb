{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-24 16:11:53 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-24 16:11:55 model_runner.py:1060] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 10-24 16:11:55 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9ea3d583b241eb9595e04bd894d98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-24 16:11:58 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "INFO 10-24 16:11:59 gpu_executor.py:122] # GPU blocks: 27864, # CPU blocks: 2048\n",
      "INFO 10-24 16:11:59 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 54.42x\n",
      "INFO 10-24 16:12:01 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-24 16:12:01 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-24 16:12:09 model_runner.py:1530] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1024/1024 [02:04<00:00,  8.21it/s, est. speed input: 4800.26 toks/s, output: 3163.30 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "prompt = \"\"\"\n",
    "On a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\n",
    "\n",
    "They assembled 3 decks of cards, each for a separate type of information composed of the following:\n",
    "\n",
    "Suspect:\n",
    "- Miss Scarlet\n",
    "- Mr. Green\n",
    "- Mrs. White\n",
    "\n",
    "Weapon:\n",
    "- Candlestick\n",
    "- Knife\n",
    "- Lead Pipe\n",
    "\n",
    "Room:\n",
    "- Hall\n",
    "- Lounge\n",
    "- Dining Room\n",
    "\n",
    "After randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\n",
    "\n",
    "- Summer: 2 cards\n",
    "- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\n",
    "- Connor: 2 cards\n",
    "\n",
    "The game proceeded as follows:\n",
    "\n",
    "1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\n",
    "2. The player directed this question to the other players in clockwise order, starting with the player to their left.\n",
    "3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\n",
    "4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\n",
    "5. This continued until either:\n",
    "    a) A player showed a card to the asking player, or\n",
    "    b) All the queried players had stated they didn't have any of the asked-about cards.\n",
    "6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\n",
    "\n",
    "Here is how the game played out:\n",
    "\n",
    "Summer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\n",
    "- Giselle did not have any of the cards\n",
    "- Connor showed Summer a card\n",
    "\n",
    "Giselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\n",
    "- Connor did not have any of the cards\n",
    "- Summer did not have any of the cards\n",
    "\n",
    "Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\n",
    "- Summer did not have any of the cards\n",
    "- Giselle showed Connor 'Miss Scarlet'\n",
    "\n",
    "Summer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\n",
    "- Giselle did not have any of the cards\n",
    "- Connor did not have any of the cards\n",
    "\n",
    "At this point, Giselle was able to correctly infer the solution and win the game.\n",
    "\n",
    "What were the facedown cards in the middle of the table?\n",
    "\"\"\".strip()\n",
    "\n",
    "output = llm.chat([[dict(role=\"user\", content=prompt)]] * 1024, sampling_params=SamplingParams(max_tokens=10_000, logprobs=20))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm.model_executor.models.llama import LlamaForCausalLM\n",
    "\n",
    "# Get the model\n",
    "vllm_model: LlamaForCausalLM = llm.llm_engine.model_executor.driver_worker.model_runner.model  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in vllm_model.parameters():\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 10.35it/s, est. speed input: 6066.81 toks/s, output: 10.37 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states: tensor([[ 3.9844, -0.4453, -1.9688,  ..., -3.7969,  1.1797,  2.4844],\n",
      "        [ 3.9844, -0.4453, -1.9688,  ..., -3.7969,  1.1797,  2.4844],\n",
      "        [ 0.1885,  1.5547, -1.3047,  ..., -1.7422,  0.4277, -2.5781],\n",
      "        ...,\n",
      "        [-2.5156, -0.7031,  5.4375,  ...,  1.2031,  1.0156,  1.4531],\n",
      "        [-2.7812,  1.2344,  0.0713,  ...,  0.1885, -0.7422,  3.2969],\n",
      "        [ 0.3613,  1.2578,  2.9531,  ..., -0.2891,  0.9922,  0.7617]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MmBackward0>)\n",
      "logits: tensor([[ 6.7812,  9.0000, 13.4375,  ..., -2.9688, -2.9688, -2.9688],\n",
      "        [ 6.7812,  9.0000, 13.4375,  ..., -2.9688, -2.9688, -2.9688],\n",
      "        [ 6.2188,  4.0312,  4.1875,  ..., -3.0938, -3.0938, -3.0938],\n",
      "        ...,\n",
      "        [ 2.5625,  5.2500,  5.5312,  ...,  1.7812,  1.7812,  1.7812],\n",
      "        [ 3.1406,  7.3438,  5.6562,  ...,  3.1250,  3.1250,  3.1094],\n",
      "        [ 6.2500,  9.6875,  9.0000,  ..., -1.7891, -1.7891, -1.7969]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AliasBackward0>)\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "lm_head.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "forward = vllm_model.forward\n",
    "hidden_states = None\n",
    "\n",
    "\n",
    "@torch.inference_mode(False)\n",
    "def hook1(*args, **kwargs):\n",
    "    global hidden_states\n",
    "    hidden_states = forward(*args, **kwargs)\n",
    "    print(\"hidden_states:\", hidden_states)\n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "vllm_model.forward = hook1\n",
    "compute_logits = vllm_model.compute_logits\n",
    "logits = None\n",
    "\n",
    "\n",
    "@torch.inference_mode(False)\n",
    "def hook2(*args, **kwargs):\n",
    "    global logits\n",
    "    output = compute_logits(*args, **kwargs)\n",
    "    if output.shape[0] > 1:\n",
    "        logits = output\n",
    "        print(\"logits:\", logits)\n",
    "    return output\n",
    "\n",
    "\n",
    "vllm_model.compute_logits = hook2\n",
    "vllm_model.train()\n",
    "output = llm.chat([dict(role=\"user\", content=prompt)], sampling_params=SamplingParams(max_tokens=1, prompt_logprobs=True))  # type: ignore\n",
    "vllm_model.forward = forward\n",
    "vllm_model.compute_logits = compute_logits\n",
    "loss: torch.Tensor = logits.mean()\n",
    "loss.backward()\n",
    "\n",
    "for name, parameter in vllm_model.named_parameters():\n",
    "    if parameter.grad is not None:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.31.mlp.down_proj.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([585, 128256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " {128000: Logprob(logprob=-18.54166603088379, rank=101723, decoded_token=''),\n",
       "  14924: Logprob(logprob=-1.3854166269302368, rank=1, decoded_token='Question')},\n",
       " {128002: Logprob(logprob=-18.44791603088379, rank=99124, decoded_token='<|im_start|>'),\n",
       "  14924: Logprob(logprob=-1.3854166269302368, rank=1, decoded_token='Question')},\n",
       " {882: Logprob(logprob=-10.944562911987305, rank=4594, decoded_token='user'),\n",
       "  2176: Logprob(logprob=-3.2414374351501465, rank=1, decoded_token='ee')},\n",
       " {198: Logprob(logprob=-0.553187906742096, rank=1, decoded_token='\\n')},\n",
       " {1966: Logprob(logprob=-7.796374320983887, rank=245, decoded_token='On'),\n",
       "  3923: Logprob(logprob=-2.733874559402466, rank=1, decoded_token='What')},\n",
       " {264: Logprob(logprob=-1.5976474285125732, rank=1, decoded_token=' a')},\n",
       " {8369: Logprob(logprob=-5.455681800842285, rank=32, decoded_token=' warm'),\n",
       "  3738: Logprob(logprob=-1.8306820392608643, rank=1, decoded_token=' certain')},\n",
       " {10683: Logprob(logprob=-3.4472498893737793, rank=6, decoded_token=' spring'),\n",
       "  7474: Logprob(logprob=-0.3222498595714569, rank=1, decoded_token=' summer')},\n",
       " {1938: Logprob(logprob=-0.4509103298187256, rank=1, decoded_token=' day')},\n",
       " {19367: Logprob(logprob=-15.36823844909668, rank=2146, decoded_token=' Summer'),\n",
       "  11: Logprob(logprob=-0.5244888067245483, rank=1, decoded_token=',')},\n",
       " {11: Logprob(logprob=-2.2873878479003906, rank=1, decoded_token=',')},\n",
       " {480: Logprob(logprob=-9.22768497467041, rank=302, decoded_token=' G'),\n",
       "  264: Logprob(logprob=-0.9151847958564758, rank=1, decoded_token=' a')},\n",
       " {285: Logprob(logprob=-4.2275166511535645, rank=16, decoded_token='is'),\n",
       "  8393: Logprob(logprob=-2.3525166511535645, rank=1, decoded_token='abe')},\n",
       " {6853: Logprob(logprob=-0.08439937978982925, rank=1, decoded_token='elle')},\n",
       " {323: Logprob(logprob=-1.4573984146118164, rank=2, decoded_token=' and'),\n",
       "  11: Logprob(logprob=-0.332398384809494, rank=1, decoded_token=',')},\n",
       " {58280: Logprob(logprob=-8.015483856201172, rank=462, decoded_token=' Connor'),\n",
       "  358: Logprob(logprob=-2.3279833793640137, rank=1, decoded_token=' I')},\n",
       " {7731: Logprob(logprob=-3.4316506385803223, rank=6, decoded_token=' sat'),\n",
       "  1051: Logprob(logprob=-1.6191505193710327, rank=1, decoded_token=' were')},\n",
       " {1523: Logprob(logprob=-2.1066811084747314, rank=3, decoded_token=' down'),\n",
       "  389: Logprob(logprob=-1.2316811084747314, rank=1, decoded_token=' on')},\n",
       " {311: Logprob(logprob=-0.5838345289230347, rank=1, decoded_token=' to')},\n",
       " {1514: Logprob(logprob=-3.156933069229126, rank=5, decoded_token=' play'),\n",
       "  617: Logprob(logprob=-1.281933069229126, rank=1, decoded_token=' have')},\n",
       " {264: Logprob(logprob=-0.5257243514060974, rank=1, decoded_token=' a')},\n",
       " {16736: Logprob(logprob=-9.128568649291992, rank=62, decoded_token=' casual'),\n",
       "  1847: Logprob(logprob=-0.1285683661699295, rank=1, decoded_token=' game')},\n",
       " {23347: Logprob(logprob=-15.159106254577637, rank=1085, decoded_token=' mystery'),\n",
       "  1847: Logprob(logprob=-0.034105803817510605, rank=1, decoded_token=' game')},\n",
       " {1847: Logprob(logprob=-0.11057211458683014, rank=1, decoded_token=' game')},\n",
       " {382: Logprob(logprob=-5.2546796798706055, rank=14, decoded_token='.\\n\\n'),\n",
       "  13: Logprob(logprob=-0.3796795606613159, rank=1, decoded_token='.')},\n",
       " {7009: Logprob(logprob=-3.601668357849121, rank=5, decoded_token='They'),\n",
       "  51787: Logprob(logprob=-1.2891682386398315, rank=1, decoded_token='Summer')},\n",
       " {35105: Logprob(logprob=-10.885279655456543, rank=416, decoded_token=' assembled'),\n",
       "  6773: Logprob(logprob=-1.885279893875122, rank=1, decoded_token=' decided')},\n",
       " {220: Logprob(logprob=-3.5382347106933594, rank=6, decoded_token=' '),\n",
       "  264: Logprob(logprob=-0.7882347702980042, rank=1, decoded_token=' a')},\n",
       " {18: Logprob(logprob=-2.148672103881836, rank=2, decoded_token='3'),\n",
       "  20: Logprob(logprob=-1.8986719846725464, rank=1, decoded_token='5')},\n",
       " {30881: Logprob(logprob=-2.5165648460388184, rank=2, decoded_token=' decks'),\n",
       "  15039: Logprob(logprob=-1.8915647268295288, rank=1, decoded_token=' boxes')},\n",
       " {315: Logprob(logprob=-0.003379469271749258, rank=1, decoded_token=' of')},\n",
       " {7563: Logprob(logprob=-0.05385422706604004, rank=1, decoded_token=' cards')},\n",
       " {11: Logprob(logprob=-0.8350951075553894, rank=1, decoded_token=',')},\n",
       " {1855: Logprob(logprob=-1.4302443265914917, rank=2, decoded_token=' each'),\n",
       "  75371: Logprob(logprob=-1.1802443265914917, rank=1, decoded_token=' shuffled')},\n",
       " {369: Logprob(logprob=-10.880779266357422, rank=71, decoded_token=' for'),\n",
       "  8649: Logprob(logprob=-0.7557791471481323, rank=1, decoded_token=' containing')},\n",
       " {264: Logprob(logprob=-0.891236424446106, rank=1, decoded_token=' a')},\n",
       " {8821: Logprob(logprob=-6.522312164306641, rank=13, decoded_token=' separate'),\n",
       "  2204: Logprob(logprob=-0.5223122835159302, rank=1, decoded_token=' different')},\n",
       " {955: Logprob(logprob=-6.162716388702393, rank=23, decoded_token=' type'),\n",
       "  1847: Logprob(logprob=-0.4752163290977478, rank=1, decoded_token=' game')},\n",
       " {315: Logprob(logprob=-0.02479039505124092, rank=1, decoded_token=' of')},\n",
       " {2038: Logprob(logprob=-4.917187213897705, rank=15, decoded_token=' information'),\n",
       "  31089: Logprob(logprob=-0.8546873331069946, rank=1, decoded_token=' clue')},\n",
       " {24306: Logprob(logprob=-15.040443420410156, rank=2733, decoded_token=' composed'),\n",
       "  25: Logprob(logprob=-1.0091934204101562, rank=1, decoded_token=':')},\n",
       " {315: Logprob(logprob=-0.08899322152137756, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-3.841841697692871, rank=4, decoded_token=' the'),\n",
       "  220: Logprob(logprob=-0.4043416976928711, rank=1, decoded_token=' ')},\n",
       " {2768: Logprob(logprob=-0.6187441945075989, rank=1, decoded_token=' following')},\n",
       " {1473: Logprob(logprob=-1.2924702167510986, rank=1, decoded_token=':\\n\\n')},\n",
       " {78524: Logprob(logprob=-10.53060245513916, rank=239, decoded_token='Sus'),\n",
       "  16: Logprob(logprob=-1.3118520975112915, rank=1, decoded_token='1')},\n",
       " {1002: Logprob(logprob=-2.5835845470428467, rank=2, decoded_token='pect'),\n",
       "  8132: Logprob(logprob=-0.08358456939458847, rank=1, decoded_token='pects')},\n",
       " {512: Logprob(logprob=-6.102409362792969, rank=11, decoded_token=':\\n'),\n",
       "  7563: Logprob(logprob=-0.5399091243743896, rank=1, decoded_token=' cards')},\n",
       " {12: Logprob(logprob=-1.310943841934204, rank=1, decoded_token='-')},\n",
       " {9083: Logprob(logprob=-7.089393615722656, rank=113, decoded_token=' Miss'),\n",
       "  220: Logprob(logprob=-1.9018934965133667, rank=1, decoded_token=' ')},\n",
       " {81818: Logprob(logprob=-0.379814088344574, rank=1, decoded_token=' Scarlet')},\n",
       " {198: Logprob(logprob=-0.36853140592575073, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.0003987947420682758, rank=1, decoded_token='-')},\n",
       " {4491: Logprob(logprob=-2.887190103530884, rank=3, decoded_token=' Mr'),\n",
       "  52798: Logprob(logprob=-0.637190043926239, rank=1, decoded_token=' Colonel')},\n",
       " {13: Logprob(logprob=-0.00421231659129262, rank=1, decoded_token='.')},\n",
       " {7997: Logprob(logprob=-0.26900556683540344, rank=1, decoded_token=' Green')},\n",
       " {198: Logprob(logprob=-0.0033079448621720076, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.00011062010162277147, rank=1, decoded_token='-')},\n",
       " {18083: Logprob(logprob=-1.1306151151657104, rank=2, decoded_token=' Mrs'),\n",
       "  52798: Logprob(logprob=-1.0056151151657104, rank=1, decoded_token=' Colonel')},\n",
       " {13: Logprob(logprob=-0.001383420079946518, rank=1, decoded_token='.')},\n",
       " {5929: Logprob(logprob=-0.35018905997276306, rank=1, decoded_token=' White')},\n",
       " {271: Logprob(logprob=-3.776289701461792, rank=2, decoded_token='\\n\\n'),\n",
       "  198: Logprob(logprob=-0.026289647445082664, rank=1, decoded_token='\\n')},\n",
       " {29314: Logprob(logprob=-0.345918744802475, rank=1, decoded_token='Weapon')},\n",
       " {512: Logprob(logprob=-0.02073702961206436, rank=1, decoded_token=':\\n')},\n",
       " {12: Logprob(logprob=-0.0008211340173147619, rank=1, decoded_token='-')},\n",
       " {73997: Logprob(logprob=-1.2189891338348389, rank=1, decoded_token=' Candle')},\n",
       " {30133: Logprob(logprob=-0.008107253350317478, rank=1, decoded_token='stick')},\n",
       " {198: Logprob(logprob=-0.0018499656580388546, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.00011383838864276186, rank=1, decoded_token='-')},\n",
       " {62302: Logprob(logprob=-4.279404163360596, rank=4, decoded_token=' Knife'),\n",
       "  10315: Logprob(logprob=-0.27940434217453003, rank=1, decoded_token=' Rev')},\n",
       " {198: Logprob(logprob=-0.0025397446006536484, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.0001113352773245424, rank=1, decoded_token='-')},\n",
       " {30982: Logprob(logprob=-7.485208034515381, rank=9, decoded_token=' Lead'),\n",
       "  10315: Logprob(logprob=-0.29770782589912415, rank=1, decoded_token=' Rev')},\n",
       " {28905: Logprob(logprob=-0.14719322323799133, rank=1, decoded_token=' Pipe')},\n",
       " {271: Logprob(logprob=-0.1331680715084076, rank=1, decoded_token='\\n\\n')},\n",
       " {14330: Logprob(logprob=-0.36083629727363586, rank=1, decoded_token='Room')},\n",
       " {512: Logprob(logprob=-0.002720823511481285, rank=1, decoded_token=':\\n')},\n",
       " {12: Logprob(logprob=-0.0008968859910964966, rank=1, decoded_token='-')},\n",
       " {11166: Logprob(logprob=-5.332056999206543, rank=8, decoded_token=' Hall'),\n",
       "  19915: Logprob(logprob=-0.33205676078796387, rank=1, decoded_token=' Kitchen')},\n",
       " {198: Logprob(logprob=-0.1145668476819992, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.00012635385792236775, rank=1, decoded_token='-')},\n",
       " {50767: Logprob(logprob=-3.709294557571411, rank=2, decoded_token=' Lounge'),\n",
       "  19915: Logprob(logprob=-0.08429452776908875, rank=1, decoded_token=' Kitchen')},\n",
       " {198: Logprob(logprob=-0.0012859179405495524, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.00032550760079175234, rank=1, decoded_token='-')},\n",
       " {39190: Logprob(logprob=-3.3751633167266846, rank=4, decoded_token=' Dining'),\n",
       "  19915: Logprob(logprob=-0.25016340613365173, rank=1, decoded_token=' Kitchen')},\n",
       " {10637: Logprob(logprob=-0.08335362374782562, rank=1, decoded_token=' Room')},\n",
       " {271: Logprob(logprob=-0.08694136142730713, rank=1, decoded_token='\\n\\n')},\n",
       " {6153: Logprob(logprob=-4.208308219909668, rank=10, decoded_token='After'),\n",
       "  791: Logprob(logprob=-1.2083081007003784, rank=1, decoded_token='The')},\n",
       " {27716: Logprob(logprob=-4.392561912536621, rank=9, decoded_token=' randomly'),\n",
       "  559: Logprob(logprob=-1.017561912536621, rank=1, decoded_token=' sh')},\n",
       " {320: Logprob(logprob=-10.907690048217773, rank=126, decoded_token=' ('),\n",
       "  559: Logprob(logprob=-0.9701902270317078, rank=1, decoded_token=' sh')},\n",
       " {438: Logprob(logprob=-0.8110874891281128, rank=1, decoded_token='and')},\n",
       " {89447: Logprob(logprob=-3.9081239700317383, rank=6, decoded_token=' blindly'),\n",
       "  42839: Logprob(logprob=-0.7206239700317383, rank=1, decoded_token=' secretly')},\n",
       " {8: Logprob(logprob=-0.014193536713719368, rank=1, decoded_token=')')},\n",
       " {19301: Logprob(logprob=-2.7360739707946777, rank=5, decoded_token=' choosing'),\n",
       "  27397: Logprob(logprob=-0.9860739707946777, rank=1, decoded_token=' selecting')},\n",
       " {832: Logprob(logprob=-1.638723611831665, rank=2, decoded_token=' one'),\n",
       "  264: Logprob(logprob=-0.5137235522270203, rank=1, decoded_token=' a')},\n",
       " {3786: Logprob(logprob=-0.3287445902824402, rank=1, decoded_token=' card')},\n",
       " {505: Logprob(logprob=-0.14020445942878723, rank=1, decoded_token=' from')},\n",
       " {1855: Logprob(logprob=-0.020560937002301216, rank=1, decoded_token=' each')},\n",
       " {1912: Logprob(logprob=-6.299744606018066, rank=10, decoded_token=' group'),\n",
       "  9722: Logprob(logprob=-0.29974478483200073, rank=1, decoded_token=' deck')},\n",
       " {323: Logprob(logprob=-4.6074137687683105, rank=4, decoded_token=' and'),\n",
       "  11: Logprob(logprob=-0.1074138805270195, rank=1, decoded_token=',')},\n",
       " {25012: Logprob(logprob=-1.3117724657058716, rank=1, decoded_token=' placing')},\n",
       " {1124: Logprob(logprob=-0.4064086377620697, rank=1, decoded_token=' them')},\n",
       " {304: Logprob(logprob=-2.75386118888855, rank=3, decoded_token=' in'),\n",
       "  3663: Logprob(logprob=-0.503861129283905, rank=1, decoded_token=' face')},\n",
       " {279: Logprob(logprob=-2.4370861053466797, rank=3, decoded_token=' the'),\n",
       "  264: Logprob(logprob=-0.9370862245559692, rank=1, decoded_token=' a')},\n",
       " {6278: Logprob(logprob=-2.145519495010376, rank=3, decoded_token=' middle'),\n",
       "  4219: Logprob(logprob=-1.770519495010376, rank=1, decoded_token=' center')},\n",
       " {315: Logprob(logprob=-0.15742239356040955, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-0.01429824996739626, rank=1, decoded_token=' the')},\n",
       " {2007: Logprob(logprob=-0.007613688241690397, rank=1, decoded_token=' table')},\n",
       " {17011: Logprob(logprob=-8.882372856140137, rank=27, decoded_token=' faced'),\n",
       "  11: Logprob(logprob=-0.06987253576517105, rank=1, decoded_token=',')},\n",
       " {785: Logprob(logprob=-0.3752337694168091, rank=1, decoded_token='own')},\n",
       " {11: Logprob(logprob=-0.036271173506975174, rank=1, decoded_token=',')},\n",
       " {814: Logprob(logprob=-0.35026875138282776, rank=1, decoded_token=' they')},\n",
       " {75371: Logprob(logprob=-5.248581409454346, rank=36, decoded_token=' shuffled'),\n",
       "  6773: Logprob(logprob=-1.6235812902450562, rank=1, decoded_token=' decided')},\n",
       " {279: Logprob(logprob=-0.9257537126541138, rank=2, decoded_token=' the'),\n",
       "  1124: Logprob(logprob=-0.6757537126541138, rank=1, decoded_token=' them')},\n",
       " {9861: Logprob(logprob=-0.4128606915473938, rank=1, decoded_token=' remaining')},\n",
       " {7563: Logprob(logprob=-0.022718630731105804, rank=1, decoded_token=' cards')},\n",
       " {323: Logprob(logprob=-0.6719539165496826, rank=1, decoded_token=' and')},\n",
       " {27023: Logprob(logprob=-2.079308271408081, rank=2, decoded_token=' dealt'),\n",
       "  24465: Logprob(logprob=-1.579308271408081, rank=1, decoded_token=' drew')},\n",
       " {704: Logprob(logprob=-2.222588539123535, rank=5, decoded_token=' out'),\n",
       "  220: Logprob(logprob=-1.5975885391235352, rank=1, decoded_token=' ')},\n",
       " {279: Logprob(logprob=-1.5019941329956055, rank=2, decoded_token=' the'),\n",
       "  220: Logprob(logprob=-1.3769941329956055, rank=1, decoded_token=' ')},\n",
       " {2768: Logprob(logprob=-1.068999171257019, rank=1, decoded_token=' following')},\n",
       " {311: Logprob(logprob=-3.4684767723083496, rank=8, decoded_token=' to'),\n",
       "  1473: Logprob(logprob=-1.71847665309906, rank=1, decoded_token=':\\n\\n')},\n",
       " {1855: Logprob(logprob=-0.44311749935150146, rank=1, decoded_token=' each')},\n",
       " {2851: Logprob(logprob=-0.12322913110256195, rank=1, decoded_token=' player')},\n",
       " {1473: Logprob(logprob=-0.15954656898975372, rank=1, decoded_token=':\\n\\n')},\n",
       " {12: Logprob(logprob=-1.2980456352233887, rank=2, decoded_token='-'),\n",
       "  78524: Logprob(logprob=-0.6730456948280334, rank=1, decoded_token='Sus')},\n",
       " {19367: Logprob(logprob=-4.695617198944092, rank=7, decoded_token=' Summer'),\n",
       "  220: Logprob(logprob=-0.5706170201301575, rank=1, decoded_token=' ')},\n",
       " {25: Logprob(logprob=-0.2314300239086151, rank=1, decoded_token=':')},\n",
       " {220: Logprob(logprob=-0.9650859236717224, rank=2, decoded_token=' '),\n",
       "  16687: Logprob(logprob=-0.7150859236717224, rank=1, decoded_token=' Sus')},\n",
       " {17: Logprob(logprob=-1.2660695314407349, rank=1, decoded_token='2')},\n",
       " {7563: Logprob(logprob=-1.2065863609313965, rank=2, decoded_token=' cards'),\n",
       "  16687: Logprob(logprob=-0.5815863609313965, rank=1, decoded_token=' Sus')},\n",
       " {198: Logprob(logprob=-1.996482253074646, rank=3, decoded_token='\\n'),\n",
       "  505: Logprob(logprob=-0.996482253074646, rank=1, decoded_token=' from')},\n",
       " {12: Logprob(logprob=-0.00032658010604791343, rank=1, decoded_token='-')},\n",
       " {480: Logprob(logprob=-0.00043096792069263756, rank=1, decoded_token=' G')},\n",
       " {285: Logprob(logprob=-6.115249561844394e-05, rank=1, decoded_token='is')},\n",
       " {6853: Logprob(logprob=-1.0967194612021558e-05, rank=1, decoded_token='elle')},\n",
       " {25: Logprob(logprob=-0.0015958918957039714, rank=1, decoded_token=':')},\n",
       " {220: Logprob(logprob=-0.0003967689990531653, rank=1, decoded_token=' ')},\n",
       " {17: Logprob(logprob=-0.21889910101890564, rank=1, decoded_token='2')},\n",
       " {7563: Logprob(logprob=-0.0002873722987715155, rank=1, decoded_token=' cards')},\n",
       " {4417: Logprob(logprob=-15.880931854248047, rank=124, decoded_token=\" ('\"),\n",
       "  198: Logprob(logprob=-0.005931750405579805, rank=1, decoded_token='\\n')},\n",
       " {43: Logprob(logprob=-6.117431640625, rank=64, decoded_token='L'),\n",
       "  1593: Logprob(logprob=-2.179931640625, rank=1, decoded_token='cause')},\n",
       " {26645: Logprob(logprob=-0.14678475260734558, rank=1, decoded_token='ounge')},\n",
       " {518: Logprob(logprob=-2.639481544494629, rank=2, decoded_token=\"',\"),\n",
       "  6: Logprob(logprob=-0.13948148488998413, rank=1, decoded_token=\"'\")},\n",
       " {364: Logprob(logprob=-0.025918936356902122, rank=1, decoded_token=\" '\")},\n",
       " {36412: Logprob(logprob=-1.20095956325531, rank=1, decoded_token='Miss')},\n",
       " {81818: Logprob(logprob=-0.0022515917662531137, rank=1, decoded_token=' Scarlet')},\n",
       " {1329: Logprob(logprob=-0.25790935754776, rank=1, decoded_token=\"')\\n\")},\n",
       " {12: Logprob(logprob=-0.0009301149984821677, rank=1, decoded_token='-')},\n",
       " {58280: Logprob(logprob=-0.001042656716890633, rank=1, decoded_token=' Connor')},\n",
       " {25: Logprob(logprob=-0.0005050813779234886, rank=1, decoded_token=':')},\n",
       " {220: Logprob(logprob=-0.004198665264993906, rank=1, decoded_token=' ')},\n",
       " {17: Logprob(logprob=-0.10786325484514236, rank=1, decoded_token='2')},\n",
       " {7563: Logprob(logprob=-0.000624104228336364, rank=1, decoded_token=' cards')},\n",
       " {271: Logprob(logprob=-1.8301175832748413, rank=2, decoded_token='\\n\\n'),\n",
       "  4417: Logprob(logprob=-0.20511753857135773, rank=1, decoded_token=\" ('\")},\n",
       " {791: Logprob(logprob=-1.5572378635406494, rank=1, decoded_token='The')},\n",
       " {1847: Logprob(logprob=-1.7868094444274902, rank=2, decoded_token=' game'),\n",
       "  9861: Logprob(logprob=-1.6618094444274902, rank=1, decoded_token=' remaining')},\n",
       " {45374: Logprob(logprob=-3.7718353271484375, rank=10, decoded_token=' proceeded'),\n",
       "  6137: Logprob(logprob=-1.6468353271484375, rank=1, decoded_token=' began')},\n",
       " {439: Logprob(logprob=-1.2140145301818848, rank=2, decoded_token=' as'),\n",
       "  449: Logprob(logprob=-1.0890145301818848, rank=1, decoded_token=' with')},\n",
       " {11263: Logprob(logprob=-0.2535204291343689, rank=1, decoded_token=' follows')},\n",
       " {1473: Logprob(logprob=-0.34585040807724, rank=1, decoded_token=':\\n\\n')},\n",
       " {16: Logprob(logprob=-0.7139461040496826, rank=1, decoded_token='1')},\n",
       " {13: Logprob(logprob=-0.016224611550569534, rank=1, decoded_token='.')},\n",
       " {1952: Logprob(logprob=-5.558688163757324, rank=8, decoded_token=' On'),\n",
       "  19367: Logprob(logprob=-0.1836879998445511, rank=1, decoded_token=' Summer')},\n",
       " {872: Logprob(logprob=-0.767135739326477, rank=1, decoded_token=' their')},\n",
       " {2543: Logprob(logprob=-0.3355826139450073, rank=1, decoded_token=' turn')},\n",
       " {11: Logprob(logprob=-0.00655713863670826, rank=1, decoded_token=',')},\n",
       " {264: Logprob(logprob=-0.1433786153793335, rank=1, decoded_token=' a')},\n",
       " {2851: Logprob(logprob=-0.000448841426987201, rank=1, decoded_token=' player')},\n",
       " {4691: Logprob(logprob=-5.61923360824585, rank=9, decoded_token=' asked'),\n",
       "  1436: Logprob(logprob=-0.5567334294319153, rank=1, decoded_token=' could')},\n",
       " {922: Logprob(logprob=-5.721358776092529, rank=10, decoded_token=' about'),\n",
       "  264: Logprob(logprob=-0.34635859727859497, rank=1, decoded_token=' a')},\n",
       " {264: Logprob(logprob=-1.130542516708374, rank=1, decoded_token=' a')},\n",
       " {743: Logprob(logprob=-7.704310417175293, rank=40, decoded_token=' set'),\n",
       "  3230: Logprob(logprob=-1.0168105363845825, rank=1, decoded_token=' specific')},\n",
       " {315: Logprob(logprob=-0.057823069393634796, rank=1, decoded_token=' of')},\n",
       " {7041: Logprob(logprob=-7.475235939025879, rank=49, decoded_token=' exactly'),\n",
       "  7563: Logprob(logprob=-1.350236177444458, rank=1, decoded_token=' cards')},\n",
       " {220: Logprob(logprob=-0.8643837571144104, rank=2, decoded_token=' '),\n",
       "  1403: Logprob(logprob=-0.7393837571144104, rank=1, decoded_token=' two')},\n",
       " {18: Logprob(logprob=-1.2791047096252441, rank=2, decoded_token='3'),\n",
       "  17: Logprob(logprob=-0.40410465002059937, rank=1, decoded_token='2')},\n",
       " {7563: Logprob(logprob=-0.10916943848133087, rank=1, decoded_token=' cards')},\n",
       " {11: Logprob(logprob=-1.9050877094268799, rank=2, decoded_token=','),\n",
       "  320: Logprob(logprob=-1.1550877094268799, rank=1, decoded_token=' (')},\n",
       " {832: Logprob(logprob=-1.640716791152954, rank=1, decoded_token=' one')},\n",
       " {505: Logprob(logprob=-0.15238957107067108, rank=1, decoded_token=' from')},\n",
       " {1855: Logprob(logprob=-0.03193250298500061, rank=1, decoded_token=' each')},\n",
       " {315: Logprob(logprob=-1.806157112121582, rank=3, decoded_token=' of'),\n",
       "  1912: Logprob(logprob=-0.931157112121582, rank=1, decoded_token=' group')},\n",
       " {279: Logprob(logprob=-0.011622808873653412, rank=1, decoded_token=' the')},\n",
       " {1847: Logprob(logprob=-8.931057929992676, rank=44, decoded_token=' game'),\n",
       "  220: Logprob(logprob=-0.8060575127601624, rank=1, decoded_token=' ')},\n",
       " {596: Logprob(logprob=-0.26362648606300354, rank=1, decoded_token=\"'s\")},\n",
       " {11306: Logprob(logprob=-1.446288824081421, rank=3, decoded_token=' categories'),\n",
       "  2380: Logprob(logprob=-0.9462888836860657, rank=1, decoded_token=' three')},\n",
       " {13: Logprob(logprob=-2.249990940093994, rank=2, decoded_token='.'),\n",
       "  320: Logprob(logprob=-0.37499094009399414, rank=1, decoded_token=' (')},\n",
       " {320: Logprob(logprob=-2.624049425125122, rank=4, decoded_token=' ('),\n",
       "  1789: Logprob(logprob=-0.8740493655204773, rank=1, decoded_token=' For')},\n",
       " {9290: Logprob(logprob=-5.408778190612793, rank=15, decoded_token='Note'),\n",
       "  78524: Logprob(logprob=-1.096278190612793, rank=1, decoded_token='Sus')},\n",
       " {25: Logprob(logprob=-1.337707757949829, rank=2, decoded_token=':'),\n",
       "  430: Logprob(logprob=-0.3377077281475067, rank=1, decoded_token=' that')},\n",
       " {25640: Logprob(logprob=-3.3828859329223633, rank=8, decoded_token=' Players'),\n",
       "  362: Logprob(logprob=-1.9453860521316528, rank=1, decoded_token=' A')},\n",
       " {1436: Logprob(logprob=-0.8328380584716797, rank=1, decoded_token=' could')},\n",
       " {2610: Logprob(logprob=-1.5822317600250244, rank=2, decoded_token=' ask'),\n",
       "  539: Logprob(logprob=-0.7072317600250244, rank=1, decoded_token=' not')},\n",
       " {922: Logprob(logprob=-0.44704532623291016, rank=1, decoded_token=' about')},\n",
       " {904: Logprob(logprob=-2.0507216453552246, rank=3, decoded_token=' any'),\n",
       "  279: Logprob(logprob=-0.9257216453552246, rank=1, decoded_token=' the')},\n",
       " {7563: Logprob(logprob=-2.5777547359466553, rank=4, decoded_token=' cards'),\n",
       "  220: Logprob(logprob=-0.8277547955513, rank=1, decoded_token=' ')},\n",
       " {11: Logprob(logprob=-1.3973209857940674, rank=2, decoded_token=','),\n",
       "  814: Logprob(logprob=-0.7723209857940674, rank=1, decoded_token=' they')},\n",
       " {2737: Logprob(logprob=-0.7038244009017944, rank=1, decoded_token=' including')},\n",
       " {1884: Logprob(logprob=-1.100905179977417, rank=2, decoded_token=' those'),\n",
       "  279: Logprob(logprob=-0.9759052395820618, rank=1, decoded_token=' the')},\n",
       " {304: Logprob(logprob=-2.305727481842041, rank=3, decoded_token=' in'),\n",
       "  814: Logprob(logprob=-0.5557273626327515, rank=1, decoded_token=' they')},\n",
       " {872: Logprob(logprob=-1.6468405723571777, rank=3, decoded_token=' their'),\n",
       "  279: Logprob(logprob=-0.646840512752533, rank=1, decoded_token=' the')},\n",
       " {1866: Logprob(logprob=-0.6307097673416138, rank=1, decoded_token=' own')},\n",
       " {1450: Logprob(logprob=-0.17157717049121857, rank=1, decoded_token=' hand')},\n",
       " {29275: Logprob(logprob=-0.8215624690055847, rank=1, decoded_token='.)\\n')},\n",
       " {17: Logprob(logprob=-0.012544183060526848, rank=1, decoded_token='2')},\n",
       " {13: Logprob(logprob=-0.00014757021563127637, rank=1, decoded_token='.')},\n",
       " {578: Logprob(logprob=-0.40760937333106995, rank=1, decoded_token=' The')},\n",
       " {2851: Logprob(logprob=-0.8912285566329956, rank=1, decoded_token=' player')},\n",
       " {15910: Logprob(logprob=-12.696619033813477, rank=729, decoded_token=' directed'),\n",
       "  889: Logprob(logprob=-0.9778693914413452, rank=1, decoded_token=' who')},\n",
       " {420: Logprob(logprob=-4.882519721984863, rank=7, decoded_token=' this'),\n",
       "  872: Logprob(logprob=-0.8825198411941528, rank=1, decoded_token=' their')},\n",
       " {3488: Logprob(logprob=-0.14748209714889526, rank=1, decoded_token=' question')},\n",
       " {311: Logprob(logprob=-0.1778113842010498, rank=1, decoded_token=' to')},\n",
       " {279: Logprob(logprob=-1.9358998537063599, rank=3, decoded_token=' the'),\n",
       "  2500: Logprob(logprob=-0.5608998537063599, rank=1, decoded_token=' another')},\n",
       " {1023: Logprob(logprob=-1.7499918937683105, rank=3, decoded_token=' other'),\n",
       "  1732: Logprob(logprob=-0.7499919533729553, rank=1, decoded_token=' person')},\n",
       " {4311: Logprob(logprob=-1.2636942863464355, rank=2, decoded_token=' players'),\n",
       "  1403: Logprob(logprob=-0.38869428634643555, rank=1, decoded_token=' two')},\n",
       " {304: Logprob(logprob=-4.384226322174072, rank=7, decoded_token=' in'),\n",
       "  11: Logprob(logprob=-0.3842262327671051, rank=1, decoded_token=',')},\n",
       " {66770: Logprob(logprob=-3.882011651992798, rank=4, decoded_token=' clockwise'),\n",
       "  279: Logprob(logprob=-0.382011741399765, rank=1, decoded_token=' the')},\n",
       " {2015: Logprob(logprob=-0.009073099121451378, rank=1, decoded_token=' order')},\n",
       " {11: Logprob(logprob=-1.0655381679534912, rank=1, decoded_token=',')},\n",
       " {6041: Logprob(logprob=-1.2308273315429688, rank=1, decoded_token=' starting')},\n",
       " {449: Logprob(logprob=-0.18090298771858215, rank=1, decoded_token=' with')},\n",
       " {279: Logprob(logprob=-0.24846547842025757, rank=1, decoded_token=' the')},\n",
       " {2851: Logprob(logprob=-0.2783622145652771, rank=1, decoded_token=' player')},\n",
       " {311: Logprob(logprob=-0.16238246858119965, rank=1, decoded_token=' to')},\n",
       " {872: Logprob(logprob=-0.09966182708740234, rank=1, decoded_token=' their')},\n",
       " {2163: Logprob(logprob=-0.6595715880393982, rank=1, decoded_token=' left')},\n",
       " {627: Logprob(logprob=-0.3883129358291626, rank=1, decoded_token='.\\n')},\n",
       " {18: Logprob(logprob=-0.000954768096562475, rank=1, decoded_token='3')},\n",
       " {13: Logprob(logprob=-0.0002233732520835474, rank=1, decoded_token='.')},\n",
       " {1442: Logprob(logprob=-3.3969879150390625, rank=3, decoded_token=' If'),\n",
       "  9062: Logprob(logprob=-0.2719878554344177, rank=1, decoded_token=' Each')},\n",
       " {264: Logprob(logprob=-0.9826712608337402, rank=1, decoded_token=' a')},\n",
       " {2851: Logprob(logprob=-0.011520172469317913, rank=1, decoded_token=' player')},\n",
       " {1047: Logprob(logprob=-1.5188438892364502, rank=1, decoded_token=' had')},\n",
       " {832: Logprob(logprob=-0.6579211950302124, rank=1, decoded_token=' one')},\n",
       " {477: Logprob(logprob=-1.604598879814148, rank=2, decoded_token=' or'),\n",
       "  315: Logprob(logprob=-0.22959887981414795, rank=1, decoded_token=' of')},\n",
       " {810: Logprob(logprob=-0.1841450333595276, rank=1, decoded_token=' more')},\n",
       " {315: Logprob(logprob=-0.1288255751132965, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-0.13974586129188538, rank=1, decoded_token=' the')},\n",
       " {4691: Logprob(logprob=-2.340703010559082, rank=3, decoded_token=' asked'),\n",
       "  7563: Logprob(logprob=-0.590703010559082, rank=1, decoded_token=' cards')},\n",
       " {69205: Logprob(logprob=-2.8580565452575684, rank=3, decoded_token='-about'),\n",
       "  7563: Logprob(logprob=-0.6080565452575684, rank=1, decoded_token=' cards')},\n",
       " {7563: Logprob(logprob=-0.0007331067463383079, rank=1, decoded_token=' cards')},\n",
       " {11: Logprob(logprob=-0.6402900815010071, rank=1, decoded_token=',')},\n",
       " {814: Logprob(logprob=-0.013682624325156212, rank=1, decoded_token=' they')},\n",
       " {1047: Logprob(logprob=-2.0093703269958496, rank=2, decoded_token=' had'),\n",
       "  10675: Logprob(logprob=-1.2593704462051392, rank=1, decoded_token=' revealed')},\n",
       " {311: Logprob(logprob=-0.017825989052653313, rank=1, decoded_token=' to')},\n",
       " {1501: Logprob(logprob=-2.750702381134033, rank=2, decoded_token=' show'),\n",
       "  16805: Logprob(logprob=-0.37570226192474365, rank=1, decoded_token=' reveal')},\n",
       " {832: Logprob(logprob=-5.152683258056641, rank=9, decoded_token=' one'),\n",
       "  1124: Logprob(logprob=-0.7776834964752197, rank=1, decoded_token=' them')},\n",
       " {315: Logprob(logprob=-0.48912346363067627, rank=1, decoded_token=' of')},\n",
       " {1884: Logprob(logprob=-1.7597016096115112, rank=2, decoded_token=' those'),\n",
       "  1124: Logprob(logprob=-0.5097016096115112, rank=1, decoded_token=' them')},\n",
       " {7563: Logprob(logprob=-0.0045918733812868595, rank=1, decoded_token=' cards')},\n",
       " {320: Logprob(logprob=-3.208832025527954, rank=7, decoded_token=' ('),\n",
       "  311: Logprob(logprob=-0.8338319659233093, rank=1, decoded_token=' to')},\n",
       " {1073: Logprob(logprob=-1.5862977504730225, rank=1, decoded_token='of')},\n",
       " {872: Logprob(logprob=-0.03060264140367508, rank=1, decoded_token=' their')},\n",
       " {5873: Logprob(logprob=-0.02583111636340618, rank=1, decoded_token=' choice')},\n",
       " {8: Logprob(logprob=-0.10824909806251526, rank=1, decoded_token=')')},\n",
       " {311: Logprob(logprob=-0.6118072271347046, rank=1, decoded_token=' to')},\n",
       " {279: Logprob(logprob=-0.10419268906116486, rank=1, decoded_token=' the')},\n",
       " {10371: Logprob(logprob=-2.1816844940185547, rank=2, decoded_token=' asking'),\n",
       "  113408: Logprob(logprob=-0.5566843748092651, rank=1, decoded_token=' asker')},\n",
       " {2851: Logprob(logprob=-0.00024768622824922204, rank=1, decoded_token=' player')},\n",
       " {38171: Logprob(logprob=-8.813794136047363, rank=26, decoded_token=' privately'),\n",
       "  627: Logprob(logprob=-0.563793957233429, rank=1, decoded_token='.\\n')},\n",
       " {13: Logprob(logprob=-1.2304961681365967, rank=2, decoded_token='.'),\n",
       "  627: Logprob(logprob=-0.7304962277412415, rank=1, decoded_token='.\\n')},\n",
       " {578: Logprob(logprob=-1.7265937328338623, rank=2, decoded_token=' The'),\n",
       "  1442: Logprob(logprob=-0.8515937328338623, rank=1, decoded_token=' If')},\n",
       " {2543: Logprob(logprob=-8.680020332336426, rank=61, decoded_token=' turn'),\n",
       "  3786: Logprob(logprob=-1.3675199747085571, rank=1, decoded_token=' card')},\n",
       " {1243: Logprob(logprob=-0.2683734595775604, rank=1, decoded_token=' then')},\n",
       " {9670: Logprob(logprob=-0.3394238352775574, rank=1, decoded_token=' ended')},\n",
       " {11: Logprob(logprob=-3.4420156478881836, rank=4, decoded_token=','),\n",
       "  627: Logprob(logprob=-0.19201554358005524, rank=1, decoded_token='.\\n')},\n",
       " {323: Logprob(logprob=-0.09365008026361465, rank=1, decoded_token=' and')},\n",
       " {1514: Logprob(logprob=-1.159101963043213, rank=2, decoded_token=' play'),\n",
       "  279: Logprob(logprob=-0.5341019034385681, rank=1, decoded_token=' the')},\n",
       " {5946: Logprob(logprob=-1.0178745985031128, rank=1, decoded_token=' passed')},\n",
       " {311: Logprob(logprob=-0.032954633235931396, rank=1, decoded_token=' to')},\n",
       " {279: Logprob(logprob=-0.0014642480527982116, rank=1, decoded_token=' the')},\n",
       " {1828: Logprob(logprob=-0.13132914900779724, rank=1, decoded_token=' next')},\n",
       " {2851: Logprob(logprob=-0.02849690057337284, rank=1, decoded_token=' player')},\n",
       " {627: Logprob(logprob=-0.030342470854520798, rank=1, decoded_token='.\\n')},\n",
       " {19: Logprob(logprob=-0.0011389919091016054, rank=1, decoded_token='4')},\n",
       " {13: Logprob(logprob=-0.00021586472576018423, rank=1, decoded_token='.')},\n",
       " {1442: Logprob(logprob=-0.014406594447791576, rank=1, decoded_token=' If')},\n",
       " {264: Logprob(logprob=-1.2511411905288696, rank=2, decoded_token=' a'),\n",
       "  912: Logprob(logprob=-1.0011411905288696, rank=1, decoded_token=' no')},\n",
       " {2851: Logprob(logprob=-0.006150722038000822, rank=1, decoded_token=' player')},\n",
       " {1550: Logprob(logprob=-0.6109917163848877, rank=1, decoded_token=' did')},\n",
       " {539: Logprob(logprob=-0.0017959432443603873, rank=1, decoded_token=' not')},\n",
       " {617: Logprob(logprob=-0.006423066835850477, rank=1, decoded_token=' have')},\n",
       " {904: Logprob(logprob=-0.02455662377178669, rank=1, decoded_token=' any')},\n",
       " {315: Logprob(logprob=-0.0026777861639857292, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-0.0006417360855266452, rank=1, decoded_token=' the')},\n",
       " {4691: Logprob(logprob=-0.01601100154221058, rank=1, decoded_token=' asked')},\n",
       " {69205: Logprob(logprob=-0.004960017278790474, rank=1, decoded_token='-about')},\n",
       " {7563: Logprob(logprob=-0.0001510267611593008, rank=1, decoded_token=' cards')},\n",
       " {11: Logprob(logprob=-0.0076561588793993, rank=1, decoded_token=',')},\n",
       " {814: Logprob(logprob=-0.040780529379844666, rank=1, decoded_token=' they')},\n",
       " {1071: Logprob(logprob=-0.8878980875015259, rank=1, decoded_token=' said')},\n",
       " {779: Logprob(logprob=-2.1425461769104004, rank=2, decoded_token=' so'),\n",
       "  330: Logprob(logprob=-0.3925461173057556, rank=1, decoded_token=' \"')},\n",
       " {11: Logprob(logprob=-0.721986711025238, rank=1, decoded_token=',')},\n",
       " {323: Logprob(logprob=-0.0271043349057436, rank=1, decoded_token=' and')},\n",
       " {279: Logprob(logprob=-0.3687107563018799, rank=1, decoded_token=' the')},\n",
       " {3488: Logprob(logprob=-5.667723655700684, rank=6, decoded_token=' question'),\n",
       "  2543: Logprob(logprob=-0.16772358119487762, rank=1, decoded_token=' turn')},\n",
       " {5946: Logprob(logprob=-5.725805759429932, rank=11, decoded_token=' passed'),\n",
       "  574: Logprob(logprob=-0.6008057594299316, rank=1, decoded_token=' was')},\n",
       " {311: Logprob(logprob=-0.09190858155488968, rank=1, decoded_token=' to')},\n",
       " {279: Logprob(logprob=-0.0003666205739136785, rank=1, decoded_token=' the')},\n",
       " {1828: Logprob(logprob=-0.009435700252652168, rank=1, decoded_token=' next')},\n",
       " {2851: Logprob(logprob=-0.004996551666408777, rank=1, decoded_token=' player')},\n",
       " {304: Logprob(logprob=-1.2136645317077637, rank=2, decoded_token=' in'),\n",
       "  627: Logprob(logprob=-0.5886645317077637, rank=1, decoded_token='.\\n')},\n",
       " {66770: Logprob(logprob=-0.27789562940597534, rank=1, decoded_token=' clockwise')},\n",
       " {2015: Logprob(logprob=-0.002996124094352126, rank=1, decoded_token=' order')},\n",
       " {627: Logprob(logprob=-0.3735848069190979, rank=1, decoded_token='.\\n')},\n",
       " {20: Logprob(logprob=-0.002673862734809518, rank=1, decoded_token='5')},\n",
       " {13: Logprob(logprob=-0.0003389737685211003, rank=1, decoded_token='.')},\n",
       " {1115: Logprob(logprob=-5.290360450744629, rank=11, decoded_token=' This'),\n",
       "  1442: Logprob(logprob=-1.0403605699539185, rank=1, decoded_token=' If')},\n",
       " {8738: Logprob(logprob=-0.9549012780189514, rank=2, decoded_token=' continued'),\n",
       "  1920: Logprob(logprob=-0.5799012780189514, rank=1, decoded_token=' process')},\n",
       " {3156: Logprob(logprob=-0.028179394081234932, rank=1, decoded_token=' until')},\n",
       " {3060: Logprob(logprob=-2.31520414352417, rank=5, decoded_token=' either'),\n",
       "  682: Logprob(logprob=-1.31520414352417, rank=1, decoded_token=' all')},\n",
       " {512: Logprob(logprob=-1.8746955394744873, rank=3, decoded_token=':\\n'),\n",
       "  682: Logprob(logprob=-0.9996955394744873, rank=1, decoded_token=' all')},\n",
       " {262: Logprob(logprob=-0.8488946557044983, rank=1, decoded_token='   ')},\n",
       " {264: Logprob(logprob=-1.127027988433838, rank=2, decoded_token=' a'),\n",
       "  482: Logprob(logprob=-0.8770279288291931, rank=1, decoded_token=' -')},\n",
       " {8: Logprob(logprob=-0.9761525988578796, rank=2, decoded_token=')'),\n",
       "  13: Logprob(logprob=-0.47615259885787964, rank=1, decoded_token='.')},\n",
       " {362: Logprob(logprob=-0.638878345489502, rank=1, decoded_token=' A')},\n",
       " {2851: Logprob(logprob=-0.01194809004664421, rank=1, decoded_token=' player')},\n",
       " {8710: Logprob(logprob=-5.151849746704102, rank=26, decoded_token=' showed'),\n",
       "  1047: Logprob(logprob=-1.5268495082855225, rank=1, decoded_token=' had')},\n",
       " {264: Logprob(logprob=-0.6279911398887634, rank=1, decoded_token=' a')},\n",
       " {3786: Logprob(logprob=-0.023507587611675262, rank=1, decoded_token=' card')},\n",
       " {311: Logprob(logprob=-1.8092539310455322, rank=2, decoded_token=' to'),\n",
       "  430: Logprob(logprob=-1.0592539310455322, rank=1, decoded_token=' that')},\n",
       " {279: Logprob(logprob=-0.3575044870376587, rank=1, decoded_token=' the')},\n",
       " {10371: Logprob(logprob=-0.33242061734199524, rank=1, decoded_token=' asking')},\n",
       " {2851: Logprob(logprob=-0.0002196785935666412, rank=1, decoded_token=' player')},\n",
       " {11: Logprob(logprob=-0.4773346781730652, rank=1, decoded_token=',')},\n",
       " {477: Logprob(logprob=-0.7422024607658386, rank=1, decoded_token=' or')},\n",
       " {198: Logprob(logprob=-0.03718574345111847, rank=1, decoded_token='\\n')},\n",
       " {262: Logprob(logprob=-0.00022766382608097047, rank=1, decoded_token='   ')},\n",
       " {293: Logprob(logprob=-0.0002960720448754728, rank=1, decoded_token=' b')},\n",
       " {8: Logprob(logprob=-3.397406908334233e-05, rank=1, decoded_token=')')},\n",
       " {2052: Logprob(logprob=-0.9510658383369446, rank=2, decoded_token=' All'),\n",
       "  578: Logprob(logprob=-0.8260658383369446, rank=1, decoded_token=' The')},\n",
       " {279: Logprob(logprob=-5.320565700531006, rank=7, decoded_token=' the'),\n",
       "  4311: Logprob(logprob=-0.44556570053100586, rank=1, decoded_token=' players')},\n",
       " {79002: Logprob(logprob=-6.989608287811279, rank=12, decoded_token=' queried'),\n",
       "  4691: Logprob(logprob=-0.7396082878112793, rank=1, decoded_token=' asked')},\n",
       " {4311: Logprob(logprob=-9.004351615905762, rank=8, decoded_token=' players'),\n",
       "  7563: Logprob(logprob=-0.004351670388132334, rank=1, decoded_token=' cards')},\n",
       " {1047: Logprob(logprob=-0.609744131565094, rank=1, decoded_token=' had')},\n",
       " {11224: Logprob(logprob=-4.267420768737793, rank=5, decoded_token=' stated'),\n",
       "  5946: Logprob(logprob=-0.642420768737793, rank=1, decoded_token=' passed')},\n",
       " {814: Logprob(logprob=-0.18161940574645996, rank=1, decoded_token=' they')},\n",
       " {3287: Logprob(logprob=-1.192499041557312, rank=2, decoded_token=' didn'),\n",
       "  1550: Logprob(logprob=-0.692499041557312, rank=1, decoded_token=' did')},\n",
       " {956: Logprob(logprob=-0.043501660227775574, rank=1, decoded_token=\"'t\")},\n",
       " {617: Logprob(logprob=-0.004967016167938709, rank=1, decoded_token=' have')},\n",
       " {904: Logprob(logprob=-0.11915019899606705, rank=1, decoded_token=' any')},\n",
       " {315: Logprob(logprob=-0.004625098779797554, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-0.007124850060790777, rank=1, decoded_token=' the')},\n",
       " {4691: Logprob(logprob=-0.28553465008735657, rank=1, decoded_token=' asked')},\n",
       " {69205: Logprob(logprob=-0.00945920031517744, rank=1, decoded_token='-about')},\n",
       " {7563: Logprob(logprob=-0.0002449450839776546, rank=1, decoded_token=' cards')},\n",
       " {627: Logprob(logprob=-2.1241111755371094, rank=3, decoded_token='.\\n'),\n",
       "  382: Logprob(logprob=-0.4991111159324646, rank=1, decoded_token='.\\n\\n')},\n",
       " {21: Logprob(logprob=-0.08519677817821503, rank=1, decoded_token='6')},\n",
       " {13: Logprob(logprob=-0.00019798702851403505, rank=1, decoded_token='.')},\n",
       " {4740: Logprob(logprob=-2.8411455154418945, rank=4, decoded_token=' After'),\n",
       "  1442: Logprob(logprob=-0.8411456346511841, rank=1, decoded_token=' If')},\n",
       " {264: Logprob(logprob=-1.1604206562042236, rank=1, decoded_token=' a')},\n",
       " {2851: Logprob(logprob=-0.8358668088912964, rank=1, decoded_token=' player')},\n",
       " {596: Logprob(logprob=-1.6672660112380981, rank=2, decoded_token=\"'s\"),\n",
       "  8710: Logprob(logprob=-1.5422660112380981, rank=1, decoded_token=' showed')},\n",
       " {2543: Logprob(logprob=-0.09275173395872116, rank=1, decoded_token=' turn')},\n",
       " {9670: Logprob(logprob=-2.5391745567321777, rank=2, decoded_token=' ended'),\n",
       "  11: Logprob(logprob=-0.1641746163368225, rank=1, decoded_token=',')},\n",
       " {320: Logprob(logprob=-4.76483154296875, rank=2, decoded_token=' ('),\n",
       "  11: Logprob(logprob=-0.014831752516329288, rank=1, decoded_token=',')},\n",
       " {50998: Logprob(logprob=-0.5966835618019104, rank=1, decoded_token='either')},\n",
       " {555: Logprob(logprob=-0.5995029807090759, rank=1, decoded_token=' by')},\n",
       " {1694: Logprob(logprob=-5.605626583099365, rank=12, decoded_token=' being'),\n",
       "  9204: Logprob(logprob=-0.35562649369239807, rank=1, decoded_token=' showing')},\n",
       " {6982: Logprob(logprob=-0.07738550752401352, rank=1, decoded_token=' shown')},\n",
       " {264: Logprob(logprob=-0.0032526941504329443, rank=1, decoded_token=' a')},\n",
       " {3786: Logprob(logprob=-0.001209957292303443, rank=1, decoded_token=' card')},\n",
       " {477: Logprob(logprob=-0.026974499225616455, rank=1, decoded_token=' or')},\n",
       " {3515: Logprob(logprob=-3.741024971008301, rank=6, decoded_token=' having'),\n",
       "  555: Logprob(logprob=-1.1160248517990112, rank=1, decoded_token=' by')},\n",
       " {682: Logprob(logprob=-1.7849493026733398, rank=3, decoded_token=' all'),\n",
       "  872: Logprob(logprob=-0.9099492430686951, rank=1, decoded_token=' their')},\n",
       " {79002: Logprob(logprob=-1.6027497053146362, rank=2, decoded_token=' queried'),\n",
       "  4311: Logprob(logprob=-0.6027497053146362, rank=1, decoded_token=' players')},\n",
       " {4311: Logprob(logprob=-0.005778512451797724, rank=1, decoded_token=' players')},\n",
       " {1522: Logprob(logprob=-0.6984614729881287, rank=1, decoded_token=' pass')},\n",
       " {705: Logprob(logprob=-0.030591079965233803, rank=1, decoded_token='),')},\n",
       " {1514: Logprob(logprob=-4.413486957550049, rank=5, decoded_token=' play'),\n",
       "  814: Logprob(logprob=-0.1634870022535324, rank=1, decoded_token=' they')},\n",
       " {7882: Logprob(logprob=-2.356072187423706, rank=4, decoded_token=' moved'),\n",
       "  5946: Logprob(logprob=-0.7310721278190613, rank=1, decoded_token=' passed')},\n",
       " {311: Logprob(logprob=-0.5329665541648865, rank=1, decoded_token=' to')},\n",
       " {279: Logprob(logprob=-0.00044050050200894475, rank=1, decoded_token=' the')},\n",
       " {1828: Logprob(logprob=-0.016445327550172806, rank=1, decoded_token=' next')},\n",
       " {2851: Logprob(logprob=-0.009896616451442242, rank=1, decoded_token=' player')},\n",
       " {304: Logprob(logprob=-0.22606392204761505, rank=1, decoded_token=' in')},\n",
       " {66770: Logprob(logprob=-0.06321380287408829, rank=1, decoded_token=' clockwise')},\n",
       " {2015: Logprob(logprob=-0.003957061562687159, rank=1, decoded_token=' order')},\n",
       " {382: Logprob(logprob=-0.11399008333683014, rank=1, decoded_token='.\\n\\n')},\n",
       " {8586: Logprob(logprob=-3.8537063598632812, rank=9, decoded_token='Here'),\n",
       "  791: Logprob(logprob=-1.8537063598632812, rank=1, decoded_token='The')},\n",
       " {374: Logprob(logprob=-1.2040846347808838, rank=2, decoded_token=' is'),\n",
       "  596: Logprob(logprob=-0.704084575176239, rank=1, decoded_token=\"'s\")},\n",
       " {1268: Logprob(logprob=-4.769900798797607, rank=5, decoded_token=' how'),\n",
       "  279: Logprob(logprob=-0.5199008584022522, rank=1, decoded_token=' the')},\n",
       " {279: Logprob(logprob=-0.08737587928771973, rank=1, decoded_token=' the')},\n",
       " {1847: Logprob(logprob=-0.23091453313827515, rank=1, decoded_token=' game')},\n",
       " {6476: Logprob(logprob=-1.8297064304351807, rank=2, decoded_token=' played'),\n",
       "  45374: Logprob(logprob=-0.7047063708305359, rank=1, decoded_token=' proceeded')},\n",
       " {704: Logprob(logprob=-0.0021205099765211344, rank=1, decoded_token=' out')},\n",
       " {1473: Logprob(logprob=-0.14081434905529022, rank=1, decoded_token=':\\n\\n')},\n",
       " {51787: Logprob(logprob=-0.6985453963279724, rank=1, decoded_token='Summer')},\n",
       " {4691: Logprob(logprob=-1.9339492321014404, rank=3, decoded_token=' asked'),\n",
       "  25: Logprob(logprob=-1.1839492321014404, rank=1, decoded_token=':')},\n",
       " {422: Logprob(logprob=-3.908238410949707, rank=10, decoded_token=' if'),\n",
       "  480: Logprob(logprob=-1.2832382917404175, rank=1, decoded_token=' G')},\n",
       " {5606: Logprob(logprob=-0.6028294563293457, rank=1, decoded_token=' anyone')},\n",
       " {1047: Logprob(logprob=-0.018247002735733986, rank=1, decoded_token=' had')},\n",
       " {364: Logprob(logprob=-1.9035842418670654, rank=2, decoded_token=\" '\"),\n",
       "  279: Logprob(logprob=-0.6535843014717102, rank=1, decoded_token=' the')},\n",
       " {50329: Logprob(logprob=-3.8713107109069824, rank=7, decoded_token='Mrs'),\n",
       "  36412: Logprob(logprob=-0.7463106513023376, rank=1, decoded_token='Miss')},\n",
       " {13: Logprob(logprob=-0.0004196478403173387, rank=1, decoded_token='.')},\n",
       " {5929: Logprob(logprob=-0.0008225633064284921, rank=1, decoded_token=' White')},\n",
       " {6: Logprob(logprob=-1.0889838933944702, rank=2, decoded_token=\"'\"),\n",
       "  518: Logprob(logprob=-0.5889838933944702, rank=1, decoded_token=\"',\")},\n",
       " {477: Logprob(logprob=-0.9096456170082092, rank=1, decoded_token=' or')},\n",
       " {364: Logprob(logprob=-0.30038943886756897, rank=1, decoded_token=\" '\")},\n",
       " {57505: Logprob(logprob=-1.6806507110595703, rank=2, decoded_token='Knife'),\n",
       "  34: Logprob(logprob=-0.9306507110595703, rank=1, decoded_token='C')},\n",
       " {6: Logprob(logprob=-0.19639302790164948, rank=1, decoded_token=\"'\")},\n",
       " {477: Logprob(logprob=-0.040818750858306885, rank=1, decoded_token=' or')},\n",
       " {364: Logprob(logprob=-0.0067458986304700375, rank=1, decoded_token=\" '\")},\n",
       " {35: Logprob(logprob=-0.35313358902931213, rank=1, decoded_token='D')},\n",
       " {5859: Logprob(logprob=-0.0002493547508493066, rank=1, decoded_token='ining')},\n",
       " {10637: Logprob(logprob=-0.000993116176687181, rank=1, decoded_token=' Room')},\n",
       " {3730: Logprob(logprob=-10.788990020751953, rank=26, decoded_token=\"':\\n\"),\n",
       "  4527: Logprob(logprob=-0.3514896035194397, rank=1, decoded_token=\"'.\")},\n",
       " {12: Logprob(logprob=-0.559437096118927, rank=1, decoded_token='-')},\n",
       " {480: Logprob(logprob=-0.020040063187479973, rank=1, decoded_token=' G')},\n",
       " {285: Logprob(logprob=-5.280832192511298e-05, rank=1, decoded_token='is')},\n",
       " {6853: Logprob(logprob=-8.583032467868179e-06, rank=1, decoded_token='elle')},\n",
       " {1550: Logprob(logprob=-3.2864840030670166, rank=8, decoded_token=' did'),\n",
       "  1071: Logprob(logprob=-1.7864840030670166, rank=1, decoded_token=' said')},\n",
       " {539: Logprob(logprob=-0.006038990803062916, rank=1, decoded_token=' not')},\n",
       " {617: Logprob(logprob=-0.027702363207936287, rank=1, decoded_token=' have')},\n",
       " {904: Logprob(logprob=-0.07207649201154709, rank=1, decoded_token=' any')},\n",
       " {315: Logprob(logprob=-0.04357378929853439, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-1.953387975692749, rank=3, decoded_token=' the'),\n",
       "  1884: Logprob(logprob=-0.8283879160881042, rank=1, decoded_token=' those')},\n",
       " {7563: Logprob(logprob=-0.8525530695915222, rank=1, decoded_token=' cards'),\n",
       "  4691: Logprob(logprob=-0.8525530695915222, rank=1, decoded_token=' asked')},\n",
       " {198: Logprob(logprob=-5.395191192626953, rank=13, decoded_token='\\n'),\n",
       "  11: Logprob(logprob=-0.7701911330223083, rank=1, decoded_token=',')},\n",
       " {12: Logprob(logprob=-0.0007489498239010572, rank=1, decoded_token='-')},\n",
       " {58280: Logprob(logprob=-0.008649740368127823, rank=1, decoded_token=' Connor')},\n",
       " {8710: Logprob(logprob=-1.4287762641906738, rank=1, decoded_token=' showed')},\n",
       " {19367: Logprob(logprob=-0.38277167081832886, rank=1, decoded_token=' Summer')},\n",
       " {264: Logprob(logprob=-3.568817377090454, rank=4, decoded_token=' a'),\n",
       "  279: Logprob(logprob=-0.6938173770904541, rank=1, decoded_token=' the')},\n",
       " {3786: Logprob(logprob=-2.247488260269165, rank=2, decoded_token=' card'),\n",
       "  364: Logprob(logprob=-0.12248818576335907, rank=1, decoded_token=\" '\")},\n",
       " {271: Logprob(logprob=-5.13369607925415, rank=15, decoded_token='\\n\\n'),\n",
       "  505: Logprob(logprob=-1.0711960792541504, rank=1, decoded_token=' from')},\n",
       " {38: Logprob(logprob=-0.8087801337242126, rank=1, decoded_token='G')},\n",
       " {285: Logprob(logprob=-0.0001436368766007945, rank=1, decoded_token='is')},\n",
       " {6853: Logprob(logprob=-4.291525328881107e-06, rank=1, decoded_token='elle')},\n",
       " {4691: Logprob(logprob=-0.12938851118087769, rank=1, decoded_token=' asked')},\n",
       " {422: Logprob(logprob=-0.01811612956225872, rank=1, decoded_token=' if')},\n",
       " {5606: Logprob(logprob=-0.002516319742426276, rank=1, decoded_token=' anyone')},\n",
       " {1047: Logprob(logprob=-0.0005371319712139666, rank=1, decoded_token=' had')},\n",
       " {364: Logprob(logprob=-0.011295536532998085, rank=1, decoded_token=\" '\")},\n",
       " {50329: Logprob(logprob=-6.242542743682861, rank=7, decoded_token='Mrs'),\n",
       "  36412: Logprob(logprob=-0.492542564868927, rank=1, decoded_token='Miss')},\n",
       " {13: Logprob(logprob=-0.0008419782971031964, rank=1, decoded_token='.')},\n",
       " {5929: Logprob(logprob=-0.012208379805088043, rank=1, decoded_token=' White')},\n",
       " {6: Logprob(logprob=-0.009371575899422169, rank=1, decoded_token=\"'\")},\n",
       " {477: Logprob(logprob=-0.000780754373408854, rank=1, decoded_token=' or')},\n",
       " {364: Logprob(logprob=-0.0032797851599752903, rank=1, decoded_token=\" '\")},\n",
       " {57505: Logprob(logprob=-3.9153759479522705, rank=6, decoded_token='Knife'),\n",
       "  34: Logprob(logprob=-0.41537585854530334, rank=1, decoded_token='C')},\n",
       " {6: Logprob(logprob=-0.0014518683310598135, rank=1, decoded_token=\"'\")},\n",
       " {477: Logprob(logprob=-0.00026222606538794935, rank=1, decoded_token=' or')},\n",
       " {364: Logprob(logprob=-0.004076267592608929, rank=1, decoded_token=\" '\")},\n",
       " {43: Logprob(logprob=-3.4285998344421387, rank=5, decoded_token='L'),\n",
       "  72945: Logprob(logprob=-0.8035999536514282, rank=1, decoded_token='Hall')},\n",
       " {26645: Logprob(logprob=-0.00024423000286333263, rank=1, decoded_token='ounge')},\n",
       " {3730: Logprob(logprob=-0.033849652856588364, rank=1, decoded_token=\"':\\n\")},\n",
       " {12: Logprob(logprob=-0.0005678709712810814, rank=1, decoded_token='-')},\n",
       " {58280: Logprob(logprob=-5.257193565368652, rank=2, decoded_token=' Connor'),\n",
       "  19367: Logprob(logprob=-0.0071934983134269714, rank=1, decoded_token=' Summer')},\n",
       " {1550: Logprob(logprob=-0.2519603669643402, rank=1, decoded_token=' did')},\n",
       " {539: Logprob(logprob=-0.00046492734691128135, rank=1, decoded_token=' not')},\n",
       " {617: Logprob(logprob=-0.0039267828688025475, rank=1, decoded_token=' have')},\n",
       " {904: Logprob(logprob=-0.14629867672920227, rank=1, decoded_token=' any')},\n",
       " {315: Logprob(logprob=-0.0018596036825329065, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-0.019741715863347054, rank=1, decoded_token=' the')},\n",
       " {7563: Logprob(logprob=-0.0008314966107718647, rank=1, decoded_token=' cards')},\n",
       " {198: Logprob(logprob=-0.039237409830093384, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.0006216024048626423, rank=1, decoded_token='-')},\n",
       " {19367: Logprob(logprob=-0.006452441215515137, rank=1, decoded_token=' Summer')},\n",
       " {1550: Logprob(logprob=-1.6443551778793335, rank=2, decoded_token=' did'),\n",
       "  8710: Logprob(logprob=-0.7693551778793335, rank=1, decoded_token=' showed')},\n",
       " {539: Logprob(logprob=-0.003816465148702264, rank=1, decoded_token=' not')},\n",
       " {617: Logprob(logprob=-0.05860157310962677, rank=1, decoded_token=' have')},\n",
       " {904: Logprob(logprob=-0.11158639937639236, rank=1, decoded_token=' any')},\n",
       " {315: Logprob(logprob=-0.018364284187555313, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-0.0009803733555600047, rank=1, decoded_token=' the')},\n",
       " {7563: Logprob(logprob=-0.0008554374799132347, rank=1, decoded_token=' cards')},\n",
       " {271: Logprob(logprob=-0.41248685121536255, rank=1, decoded_token='\\n\\n')},\n",
       " {57987: Logprob(logprob=-0.033962931483983994, rank=1, decoded_token='Connor')},\n",
       " {4691: Logprob(logprob=-0.022972753271460533, rank=1, decoded_token=' asked')},\n",
       " {422: Logprob(logprob=-0.001825810642912984, rank=1, decoded_token=' if')},\n",
       " {5606: Logprob(logprob=-0.0004757702990900725, rank=1, decoded_token=' anyone')},\n",
       " {1047: Logprob(logprob=-0.0001928620331455022, rank=1, decoded_token=' had')},\n",
       " {364: Logprob(logprob=-0.003631308674812317, rank=1, decoded_token=\" '\")},\n",
       " {36412: Logprob(logprob=-0.18036912381649017, rank=1, decoded_token='Miss')},\n",
       " {81818: Logprob(logprob=-0.00017355366435367614, rank=1, decoded_token=' Scarlet')},\n",
       " {6: Logprob(logprob=-0.0006140968762338161, rank=1, decoded_token=\"'\")},\n",
       " {477: Logprob(logprob=-0.00021073981770314276, rank=1, decoded_token=' or')},\n",
       " {364: Logprob(logprob=-0.0006756883230991662, rank=1, decoded_token=\" '\")},\n",
       " {34: Logprob(logprob=-0.46985000371932983, rank=1, decoded_token='C')},\n",
       " {3397: Logprob(logprob=-2.5987286790041253e-05, rank=1, decoded_token='andle')},\n",
       " {30133: Logprob(logprob=-6.460934673668817e-05, rank=1, decoded_token='stick')},\n",
       " {6: Logprob(logprob=-0.0004700509598478675, rank=1, decoded_token=\"'\")},\n",
       " {477: Logprob(logprob=-7.64102369430475e-05, rank=1, decoded_token=' or')},\n",
       " {364: Logprob(logprob=-0.00028689560713246465, rank=1, decoded_token=\" '\")},\n",
       " {72945: Logprob(logprob=-0.3266688287258148, rank=1, decoded_token='Hall')},\n",
       " {3730: Logprob(logprob=-0.0016452836571261287, rank=1, decoded_token=\"':\\n\")},\n",
       " {12: Logprob(logprob=-0.00025519452174194157, rank=1, decoded_token='-')},\n",
       " {19367: Logprob(logprob=-0.4915098547935486, rank=1, decoded_token=' Summer')},\n",
       " {1550: Logprob(logprob=-0.10784613341093063, rank=1, decoded_token=' did')},\n",
       " {539: Logprob(logprob=-0.00013410145766101778, rank=1, decoded_token=' not')},\n",
       " {617: Logprob(logprob=-0.00043418517452664673, rank=1, decoded_token=' have')},\n",
       " {904: Logprob(logprob=-0.10999443382024765, rank=1, decoded_token=' any')},\n",
       " {315: Logprob(logprob=-0.00017915551143232733, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-0.0005999195855110884, rank=1, decoded_token=' the')},\n",
       " {7563: Logprob(logprob=-0.0002208704245276749, rank=1, decoded_token=' cards')},\n",
       " {198: Logprob(logprob=-0.00441967835649848, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.0002687808300834149, rank=1, decoded_token='-')},\n",
       " {480: Logprob(logprob=-0.0008538890979252756, rank=1, decoded_token=' G')},\n",
       " {285: Logprob(logprob=-8.34461570775602e-06, rank=1, decoded_token='is')},\n",
       " {6853: Logprob(logprob=-1.3947389561508317e-05, rank=1, decoded_token='elle')},\n",
       " {8710: Logprob(logprob=-0.48412227630615234, rank=1, decoded_token=' showed')},\n",
       " {58280: Logprob(logprob=-0.6002931594848633, rank=1, decoded_token=' Connor')},\n",
       " {364: Logprob(logprob=-5.309264183044434, rank=5, decoded_token=\" '\"),\n",
       "  264: Logprob(logprob=-0.059264205396175385, rank=1, decoded_token=' a')},\n",
       " {36412: Logprob(logprob=-0.18344798684120178, rank=1, decoded_token='Miss')},\n",
       " {81818: Logprob(logprob=-7.199982064776123e-05, rank=1, decoded_token=' Scarlet')},\n",
       " {3961: Logprob(logprob=-0.22196096181869507, rank=1, decoded_token=\"'\\n\\n\")},\n",
       " {51787: Logprob(logprob=-2.22037410736084, rank=3, decoded_token='Summer'),\n",
       "  38: Logprob(logprob=-1.8453742265701294, rank=1, decoded_token='G')},\n",
       " {4691: Logprob(logprob=-0.34756773710250854, rank=1, decoded_token=' asked')},\n",
       " {422: Logprob(logprob=-0.007802603766322136, rank=1, decoded_token=' if')},\n",
       " {5606: Logprob(logprob=-0.00021252757869660854, rank=1, decoded_token=' anyone')},\n",
       " {1047: Logprob(logprob=-0.00012790338951162994, rank=1, decoded_token=' had')},\n",
       " {364: Logprob(logprob=-0.0027128581423312426, rank=1, decoded_token=\" '\")},\n",
       " {12555: Logprob(logprob=-0.8326566815376282, rank=1, decoded_token='Mr')},\n",
       " {13: Logprob(logprob=-0.0001656871900195256, rank=1, decoded_token='.')},\n",
       " {7997: Logprob(logprob=-0.0007832558476366103, rank=1, decoded_token=' Green')},\n",
       " {6: Logprob(logprob=-0.0003090619284193963, rank=1, decoded_token=\"'\")},\n",
       " {477: Logprob(logprob=-8.535020606359467e-05, rank=1, decoded_token=' or')},\n",
       " {364: Logprob(logprob=-0.0003457663697190583, rank=1, decoded_token=\" '\")},\n",
       " {57505: Logprob(logprob=-2.3337607383728027, rank=3, decoded_token='Knife'),\n",
       "  54963: Logprob(logprob=-0.33376070857048035, rank=1, decoded_token='Lead')},\n",
       " {6: Logprob(logprob=-6.151010165922344e-05, rank=1, decoded_token=\"'\")},\n",
       " {477: Logprob(logprob=-6.806619057897478e-05, rank=1, decoded_token=' or')},\n",
       " {364: Logprob(logprob=-0.00049650261644274, rank=1, decoded_token=\" '\")},\n",
       " {72945: Logprob(logprob=-1.9175450801849365, rank=3, decoded_token='Hall'),\n",
       "  35: Logprob(logprob=-0.4175450801849365, rank=1, decoded_token='D')},\n",
       " {3730: Logprob(logprob=-0.0012657972984015942, rank=1, decoded_token=\"':\\n\")},\n",
       " {12: Logprob(logprob=-0.0002953569928649813, rank=1, decoded_token='-')},\n",
       " {480: Logprob(logprob=-0.11567404121160507, rank=1, decoded_token=' G')},\n",
       " {285: Logprob(logprob=-1.1205610462639015e-05, rank=1, decoded_token='is')},\n",
       " {6853: Logprob(logprob=-3.3378546504536644e-06, rank=1, decoded_token='elle')},\n",
       " {1550: Logprob(logprob=-0.01811121217906475, rank=1, decoded_token=' did')},\n",
       " {539: Logprob(logprob=-1.490105023549404e-05, rank=1, decoded_token=' not')},\n",
       " {617: Logprob(logprob=-0.00025138078490272164, rank=1, decoded_token=' have')},\n",
       " {904: Logprob(logprob=-0.008110800758004189, rank=1, decoded_token=' any')},\n",
       " {315: Logprob(logprob=-3.4450891689630225e-05, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-0.00023541064001619816, rank=1, decoded_token=' the')},\n",
       " {7563: Logprob(logprob=-4.732496745418757e-05, rank=1, decoded_token=' cards')},\n",
       " {198: Logprob(logprob=-0.006270378362387419, rank=1, decoded_token='\\n')},\n",
       " {12: Logprob(logprob=-0.0001770101225702092, rank=1, decoded_token='-')},\n",
       " {58280: Logprob(logprob=-0.0008211340173147619, rank=1, decoded_token=' Connor')},\n",
       " {1550: Logprob(logprob=-0.2181394249200821, rank=1, decoded_token=' did')},\n",
       " {539: Logprob(logprob=-0.0005417786305770278, rank=1, decoded_token=' not')},\n",
       " {617: Logprob(logprob=-0.00042882305569946766, rank=1, decoded_token=' have')},\n",
       " {904: Logprob(logprob=-0.006002258043736219, rank=1, decoded_token=' any')},\n",
       " {315: Logprob(logprob=-0.00014137222024146467, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-4.672895011026412e-05, rank=1, decoded_token=' the')},\n",
       " {7563: Logprob(logprob=-8.523101132595912e-05, rank=1, decoded_token=' cards')},\n",
       " {271: Logprob(logprob=-0.03650440648198128, rank=1, decoded_token='\\n\\n')},\n",
       " {1688: Logprob(logprob=-2.100611448287964, rank=2, decoded_token='At'),\n",
       "  38: Logprob(logprob=-0.4756113886833191, rank=1, decoded_token='G')},\n",
       " {420: Logprob(logprob=-0.05243038013577461, rank=1, decoded_token=' this')},\n",
       " {1486: Logprob(logprob=-0.0020783983636647463, rank=1, decoded_token=' point')},\n",
       " {11: Logprob(logprob=-0.041237518191337585, rank=1, decoded_token=',')},\n",
       " {480: Logprob(logprob=-2.988436698913574, rank=5, decoded_token=' G'),\n",
       "  19367: Logprob(logprob=-1.1134365797042847, rank=1, decoded_token=' Summer')},\n",
       " {285: Logprob(logprob=-3.302042750874534e-05, rank=1, decoded_token='is')},\n",
       " {6853: Logprob(logprob=-9.417489309271332e-06, rank=1, decoded_token='elle')},\n",
       " {574: Logprob(logprob=-3.7594611644744873, rank=10, decoded_token=' was'),\n",
       "  1047: Logprob(logprob=-2.1969611644744873, rank=1, decoded_token=' had')},\n",
       " {3025: Logprob(logprob=-1.9437731504440308, rank=1, decoded_token=' able')},\n",
       " {311: Logprob(logprob=-0.0003510097449179739, rank=1, decoded_token=' to')},\n",
       " {12722: Logprob(logprob=-6.587961673736572, rank=17, decoded_token=' correctly'),\n",
       "  7836: Logprob(logprob=-0.33796173334121704, rank=1, decoded_token=' ded')},\n",
       " {24499: Logprob(logprob=-6.174746036529541, rank=11, decoded_token=' infer'),\n",
       "  7836: Logprob(logprob=-0.5497459173202515, rank=1, decoded_token=' ded')},\n",
       " {279: Logprob(logprob=-0.47611743211746216, rank=1, decoded_token=' the')},\n",
       " {6425: Logprob(logprob=-3.3960142135620117, rank=8, decoded_token=' solution'),\n",
       "  9861: Logprob(logprob=-1.2085140943527222, rank=1, decoded_token=' remaining')},\n",
       " {323: Logprob(logprob=-2.384042263031006, rank=3, decoded_token=' and'),\n",
       "  13: Logprob(logprob=-1.3840421438217163, rank=1, decoded_token='.')},\n",
       " {3243: Logprob(logprob=-5.313549041748047, rank=27, decoded_token=' win'),\n",
       "  1071: Logprob(logprob=-1.501049280166626, rank=1, decoded_token=' said')},\n",
       " {279: Logprob(logprob=-0.008135277777910233, rank=1, decoded_token=' the')},\n",
       " {1847: Logprob(logprob=-0.0021679725032299757, rank=1, decoded_token=' game')},\n",
       " {382: Logprob(logprob=-0.7247858047485352, rank=1, decoded_token='.\\n\\n')},\n",
       " {3923: Logprob(logprob=-0.5290403962135315, rank=1, decoded_token='What')},\n",
       " {1051: Logprob(logprob=-0.7675579190254211, rank=1, decoded_token=' were')},\n",
       " {279: Logprob(logprob=-0.07469486445188522, rank=1, decoded_token=' the')},\n",
       " {17011: Logprob(logprob=-6.62880802154541, rank=25, decoded_token=' faced'),\n",
       "  7563: Logprob(logprob=-0.5663081407546997, rank=1, decoded_token=' cards')},\n",
       " {785: Logprob(logprob=-0.00033206192892976105, rank=1, decoded_token='own')},\n",
       " {7563: Logprob(logprob=-0.0034057253506034613, rank=1, decoded_token=' cards')},\n",
       " {304: Logprob(logprob=-0.7778041958808899, rank=1, decoded_token=' in')},\n",
       " {279: Logprob(logprob=-0.0025022882036864758, rank=1, decoded_token=' the')},\n",
       " {6278: Logprob(logprob=-0.1236160397529602, rank=1, decoded_token=' middle')},\n",
       " {315: Logprob(logprob=-0.009488130919635296, rank=1, decoded_token=' of')},\n",
       " {279: Logprob(logprob=-5.1616290875244886e-05, rank=1, decoded_token=' the')},\n",
       " {2007: Logprob(logprob=-0.00013767725613433868, rank=1, decoded_token=' table')},\n",
       " {30: Logprob(logprob=-2.2721643447875977, rank=3, decoded_token='?'),\n",
       "  1980: Logprob(logprob=-0.7721644639968872, rank=1, decoded_token='?\\n\\n')},\n",
       " {128003: Logprob(logprob=-9.875120162963867, rank=226, decoded_token=''),\n",
       "  3639: Logprob(logprob=-1.3126202821731567, rank=1, decoded_token=' What')},\n",
       " {198: Logprob(logprob=-0.0008049347088672221, rank=1, decoded_token='\\n')},\n",
       " {128002: Logprob(logprob=-16.424718856811523, rank=66954, decoded_token='<|im_start|>'),\n",
       "  882: Logprob(logprob=-1.3465934991836548, rank=1, decoded_token='user')},\n",
       " {78191: Logprob(logprob=-19.19000816345215, rank=178, decoded_token='assistant'),\n",
       "  198: Logprob(logprob=-0.0025078770704567432, rank=1, decoded_token='\\n')},\n",
       " {198: Logprob(logprob=-0.00023195437097456306, rank=1, decoded_token='\\n')}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.executor.gpu_executor import GPUExecutor\n",
    "from vllm.worker.worker import Worker\n",
    "from vllm.worker.model_runner import ModelRunner\n",
    "\n",
    "gpu_executor: GPUExecutor = llm.llm_engine.model_executor  # type: ignore\n",
    "driver_worker: Worker = gpu_executor.driver_worker  # type: ignore\n",
    "model_runner: ModelRunner = driver_worker.model_runner  # type: ignore\n",
    "model: LlamaForCausalLM = model_runner.model  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"goddardexperiments/HermesProInstructV10\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128003,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model.safetensors.index.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128003\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274e2effbaf447c6b6c96d4ddca695de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at NousResearch/Hermes-2-Theta-Llama-3-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128003,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers_model = HFLlamaForCausalLM.from_pretrained(\"NousResearch/Hermes-2-Theta-Llama-3-8B\")\n",
    "transformers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 13.57it/s, est. speed input: 7958.83 toks/s, output: 13.60 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.7812,  9.0000, 13.4375,  ..., -2.9688, -2.9688, -2.9688],\n",
      "        [ 6.7812,  9.0000, 13.4375,  ..., -2.9688, -2.9688, -2.9688],\n",
      "        [ 6.2188,  4.0312,  4.1875,  ..., -3.0938, -3.0938, -3.0938],\n",
      "        ...,\n",
      "        [ 2.5625,  5.2500,  5.5312,  ...,  1.7812,  1.7812,  1.7812],\n",
      "        [ 3.1406,  7.3438,  5.6562,  ...,  3.1250,  3.1250,  3.1094],\n",
      "        [ 6.2500,  9.6875,  9.0000,  ..., -1.7891, -1.7891, -1.7969]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AliasBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5938,  7.2812,  4.3750,  ..., -3.0781, -3.0781, -3.0781]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "\n",
    "model = llama3_1_8b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight: (128256, 4096)\n",
      "layers.0.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.0.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.0.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.0.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.0.input_layernorm.weight: (4096,)\n",
      "layers.0.post_attention_layernorm.weight: (4096,)\n",
      "layers.1.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.1.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.1.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.1.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.1.input_layernorm.weight: (4096,)\n",
      "layers.1.post_attention_layernorm.weight: (4096,)\n",
      "layers.2.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.2.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.2.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.2.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.2.input_layernorm.weight: (4096,)\n",
      "layers.2.post_attention_layernorm.weight: (4096,)\n",
      "layers.3.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.3.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.3.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.3.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.3.input_layernorm.weight: (4096,)\n",
      "layers.3.post_attention_layernorm.weight: (4096,)\n",
      "layers.4.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.4.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.4.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.4.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.4.input_layernorm.weight: (4096,)\n",
      "layers.4.post_attention_layernorm.weight: (4096,)\n",
      "layers.5.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.5.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.5.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.5.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.5.input_layernorm.weight: (4096,)\n",
      "layers.5.post_attention_layernorm.weight: (4096,)\n",
      "layers.6.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.6.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.6.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.6.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.6.input_layernorm.weight: (4096,)\n",
      "layers.6.post_attention_layernorm.weight: (4096,)\n",
      "layers.7.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.7.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.7.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.7.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.7.input_layernorm.weight: (4096,)\n",
      "layers.7.post_attention_layernorm.weight: (4096,)\n",
      "layers.8.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.8.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.8.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.8.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.8.input_layernorm.weight: (4096,)\n",
      "layers.8.post_attention_layernorm.weight: (4096,)\n",
      "layers.9.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.9.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.9.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.9.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.9.input_layernorm.weight: (4096,)\n",
      "layers.9.post_attention_layernorm.weight: (4096,)\n",
      "layers.10.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.10.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.10.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.10.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.10.input_layernorm.weight: (4096,)\n",
      "layers.10.post_attention_layernorm.weight: (4096,)\n",
      "layers.11.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.11.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.11.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.11.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.11.input_layernorm.weight: (4096,)\n",
      "layers.11.post_attention_layernorm.weight: (4096,)\n",
      "layers.12.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.12.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.12.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.12.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.12.input_layernorm.weight: (4096,)\n",
      "layers.12.post_attention_layernorm.weight: (4096,)\n",
      "layers.13.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.13.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.13.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.13.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.13.input_layernorm.weight: (4096,)\n",
      "layers.13.post_attention_layernorm.weight: (4096,)\n",
      "layers.14.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.14.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.14.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.14.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.14.input_layernorm.weight: (4096,)\n",
      "layers.14.post_attention_layernorm.weight: (4096,)\n",
      "layers.15.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.15.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.15.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.15.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.15.input_layernorm.weight: (4096,)\n",
      "layers.15.post_attention_layernorm.weight: (4096,)\n",
      "layers.16.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.16.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.16.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.16.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.16.input_layernorm.weight: (4096,)\n",
      "layers.16.post_attention_layernorm.weight: (4096,)\n",
      "layers.17.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.17.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.17.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.17.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.17.input_layernorm.weight: (4096,)\n",
      "layers.17.post_attention_layernorm.weight: (4096,)\n",
      "layers.18.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.18.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.18.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.18.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.18.input_layernorm.weight: (4096,)\n",
      "layers.18.post_attention_layernorm.weight: (4096,)\n",
      "layers.19.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.19.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.19.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.19.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.19.input_layernorm.weight: (4096,)\n",
      "layers.19.post_attention_layernorm.weight: (4096,)\n",
      "layers.20.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.20.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.20.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.20.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.20.input_layernorm.weight: (4096,)\n",
      "layers.20.post_attention_layernorm.weight: (4096,)\n",
      "layers.21.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.21.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.21.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.21.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.21.input_layernorm.weight: (4096,)\n",
      "layers.21.post_attention_layernorm.weight: (4096,)\n",
      "layers.22.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.22.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.22.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.22.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.22.input_layernorm.weight: (4096,)\n",
      "layers.22.post_attention_layernorm.weight: (4096,)\n",
      "layers.23.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.23.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.23.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.23.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.23.input_layernorm.weight: (4096,)\n",
      "layers.23.post_attention_layernorm.weight: (4096,)\n",
      "layers.24.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.24.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.24.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.24.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.24.input_layernorm.weight: (4096,)\n",
      "layers.24.post_attention_layernorm.weight: (4096,)\n",
      "layers.25.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.25.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.25.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.25.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.25.input_layernorm.weight: (4096,)\n",
      "layers.25.post_attention_layernorm.weight: (4096,)\n",
      "layers.26.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.26.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.26.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.26.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.26.input_layernorm.weight: (4096,)\n",
      "layers.26.post_attention_layernorm.weight: (4096,)\n",
      "layers.27.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.27.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.27.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.27.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.27.input_layernorm.weight: (4096,)\n",
      "layers.27.post_attention_layernorm.weight: (4096,)\n",
      "layers.28.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.28.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.28.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.28.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.28.input_layernorm.weight: (4096,)\n",
      "layers.28.post_attention_layernorm.weight: (4096,)\n",
      "layers.29.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.29.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.29.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.29.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.29.input_layernorm.weight: (4096,)\n",
      "layers.29.post_attention_layernorm.weight: (4096,)\n",
      "layers.30.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.30.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.30.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.30.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.30.input_layernorm.weight: (4096,)\n",
      "layers.30.post_attention_layernorm.weight: (4096,)\n",
      "layers.31.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.31.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.31.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.31.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.31.input_layernorm.weight: (4096,)\n",
      "layers.31.post_attention_layernorm.weight: (4096,)\n",
      "norm.weight: (4096,)\n"
     ]
    }
   ],
   "source": [
    "vllm_state_dict = vllm_model.state_dict()\n",
    "\n",
    "for key in vllm_state_dict:\n",
    "    print(key + \":\", tuple(vllm_state_dict[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight: (128256, 4096)\n",
      "layers.0.attn.q_proj.weight: (4096, 4096)\n",
      "layers.0.attn.k_proj.weight: (1024, 4096)\n",
      "layers.0.attn.v_proj.weight: (1024, 4096)\n",
      "layers.0.attn.output_proj.weight: (4096, 4096)\n",
      "layers.0.mlp.w1.weight: (14336, 4096)\n",
      "layers.0.mlp.w2.weight: (4096, 14336)\n",
      "layers.0.mlp.w3.weight: (14336, 4096)\n",
      "layers.0.sa_norm.scale: (4096,)\n",
      "layers.0.mlp_norm.scale: (4096,)\n",
      "layers.1.attn.q_proj.weight: (4096, 4096)\n",
      "layers.1.attn.k_proj.weight: (1024, 4096)\n",
      "layers.1.attn.v_proj.weight: (1024, 4096)\n",
      "layers.1.attn.output_proj.weight: (4096, 4096)\n",
      "layers.1.mlp.w1.weight: (14336, 4096)\n",
      "layers.1.mlp.w2.weight: (4096, 14336)\n",
      "layers.1.mlp.w3.weight: (14336, 4096)\n",
      "layers.1.sa_norm.scale: (4096,)\n",
      "layers.1.mlp_norm.scale: (4096,)\n",
      "layers.2.attn.q_proj.weight: (4096, 4096)\n",
      "layers.2.attn.k_proj.weight: (1024, 4096)\n",
      "layers.2.attn.v_proj.weight: (1024, 4096)\n",
      "layers.2.attn.output_proj.weight: (4096, 4096)\n",
      "layers.2.mlp.w1.weight: (14336, 4096)\n",
      "layers.2.mlp.w2.weight: (4096, 14336)\n",
      "layers.2.mlp.w3.weight: (14336, 4096)\n",
      "layers.2.sa_norm.scale: (4096,)\n",
      "layers.2.mlp_norm.scale: (4096,)\n",
      "layers.3.attn.q_proj.weight: (4096, 4096)\n",
      "layers.3.attn.k_proj.weight: (1024, 4096)\n",
      "layers.3.attn.v_proj.weight: (1024, 4096)\n",
      "layers.3.attn.output_proj.weight: (4096, 4096)\n",
      "layers.3.mlp.w1.weight: (14336, 4096)\n",
      "layers.3.mlp.w2.weight: (4096, 14336)\n",
      "layers.3.mlp.w3.weight: (14336, 4096)\n",
      "layers.3.sa_norm.scale: (4096,)\n",
      "layers.3.mlp_norm.scale: (4096,)\n",
      "layers.4.attn.q_proj.weight: (4096, 4096)\n",
      "layers.4.attn.k_proj.weight: (1024, 4096)\n",
      "layers.4.attn.v_proj.weight: (1024, 4096)\n",
      "layers.4.attn.output_proj.weight: (4096, 4096)\n",
      "layers.4.mlp.w1.weight: (14336, 4096)\n",
      "layers.4.mlp.w2.weight: (4096, 14336)\n",
      "layers.4.mlp.w3.weight: (14336, 4096)\n",
      "layers.4.sa_norm.scale: (4096,)\n",
      "layers.4.mlp_norm.scale: (4096,)\n",
      "layers.5.attn.q_proj.weight: (4096, 4096)\n",
      "layers.5.attn.k_proj.weight: (1024, 4096)\n",
      "layers.5.attn.v_proj.weight: (1024, 4096)\n",
      "layers.5.attn.output_proj.weight: (4096, 4096)\n",
      "layers.5.mlp.w1.weight: (14336, 4096)\n",
      "layers.5.mlp.w2.weight: (4096, 14336)\n",
      "layers.5.mlp.w3.weight: (14336, 4096)\n",
      "layers.5.sa_norm.scale: (4096,)\n",
      "layers.5.mlp_norm.scale: (4096,)\n",
      "layers.6.attn.q_proj.weight: (4096, 4096)\n",
      "layers.6.attn.k_proj.weight: (1024, 4096)\n",
      "layers.6.attn.v_proj.weight: (1024, 4096)\n",
      "layers.6.attn.output_proj.weight: (4096, 4096)\n",
      "layers.6.mlp.w1.weight: (14336, 4096)\n",
      "layers.6.mlp.w2.weight: (4096, 14336)\n",
      "layers.6.mlp.w3.weight: (14336, 4096)\n",
      "layers.6.sa_norm.scale: (4096,)\n",
      "layers.6.mlp_norm.scale: (4096,)\n",
      "layers.7.attn.q_proj.weight: (4096, 4096)\n",
      "layers.7.attn.k_proj.weight: (1024, 4096)\n",
      "layers.7.attn.v_proj.weight: (1024, 4096)\n",
      "layers.7.attn.output_proj.weight: (4096, 4096)\n",
      "layers.7.mlp.w1.weight: (14336, 4096)\n",
      "layers.7.mlp.w2.weight: (4096, 14336)\n",
      "layers.7.mlp.w3.weight: (14336, 4096)\n",
      "layers.7.sa_norm.scale: (4096,)\n",
      "layers.7.mlp_norm.scale: (4096,)\n",
      "layers.8.attn.q_proj.weight: (4096, 4096)\n",
      "layers.8.attn.k_proj.weight: (1024, 4096)\n",
      "layers.8.attn.v_proj.weight: (1024, 4096)\n",
      "layers.8.attn.output_proj.weight: (4096, 4096)\n",
      "layers.8.mlp.w1.weight: (14336, 4096)\n",
      "layers.8.mlp.w2.weight: (4096, 14336)\n",
      "layers.8.mlp.w3.weight: (14336, 4096)\n",
      "layers.8.sa_norm.scale: (4096,)\n",
      "layers.8.mlp_norm.scale: (4096,)\n",
      "layers.9.attn.q_proj.weight: (4096, 4096)\n",
      "layers.9.attn.k_proj.weight: (1024, 4096)\n",
      "layers.9.attn.v_proj.weight: (1024, 4096)\n",
      "layers.9.attn.output_proj.weight: (4096, 4096)\n",
      "layers.9.mlp.w1.weight: (14336, 4096)\n",
      "layers.9.mlp.w2.weight: (4096, 14336)\n",
      "layers.9.mlp.w3.weight: (14336, 4096)\n",
      "layers.9.sa_norm.scale: (4096,)\n",
      "layers.9.mlp_norm.scale: (4096,)\n",
      "layers.10.attn.q_proj.weight: (4096, 4096)\n",
      "layers.10.attn.k_proj.weight: (1024, 4096)\n",
      "layers.10.attn.v_proj.weight: (1024, 4096)\n",
      "layers.10.attn.output_proj.weight: (4096, 4096)\n",
      "layers.10.mlp.w1.weight: (14336, 4096)\n",
      "layers.10.mlp.w2.weight: (4096, 14336)\n",
      "layers.10.mlp.w3.weight: (14336, 4096)\n",
      "layers.10.sa_norm.scale: (4096,)\n",
      "layers.10.mlp_norm.scale: (4096,)\n",
      "layers.11.attn.q_proj.weight: (4096, 4096)\n",
      "layers.11.attn.k_proj.weight: (1024, 4096)\n",
      "layers.11.attn.v_proj.weight: (1024, 4096)\n",
      "layers.11.attn.output_proj.weight: (4096, 4096)\n",
      "layers.11.mlp.w1.weight: (14336, 4096)\n",
      "layers.11.mlp.w2.weight: (4096, 14336)\n",
      "layers.11.mlp.w3.weight: (14336, 4096)\n",
      "layers.11.sa_norm.scale: (4096,)\n",
      "layers.11.mlp_norm.scale: (4096,)\n",
      "layers.12.attn.q_proj.weight: (4096, 4096)\n",
      "layers.12.attn.k_proj.weight: (1024, 4096)\n",
      "layers.12.attn.v_proj.weight: (1024, 4096)\n",
      "layers.12.attn.output_proj.weight: (4096, 4096)\n",
      "layers.12.mlp.w1.weight: (14336, 4096)\n",
      "layers.12.mlp.w2.weight: (4096, 14336)\n",
      "layers.12.mlp.w3.weight: (14336, 4096)\n",
      "layers.12.sa_norm.scale: (4096,)\n",
      "layers.12.mlp_norm.scale: (4096,)\n",
      "layers.13.attn.q_proj.weight: (4096, 4096)\n",
      "layers.13.attn.k_proj.weight: (1024, 4096)\n",
      "layers.13.attn.v_proj.weight: (1024, 4096)\n",
      "layers.13.attn.output_proj.weight: (4096, 4096)\n",
      "layers.13.mlp.w1.weight: (14336, 4096)\n",
      "layers.13.mlp.w2.weight: (4096, 14336)\n",
      "layers.13.mlp.w3.weight: (14336, 4096)\n",
      "layers.13.sa_norm.scale: (4096,)\n",
      "layers.13.mlp_norm.scale: (4096,)\n",
      "layers.14.attn.q_proj.weight: (4096, 4096)\n",
      "layers.14.attn.k_proj.weight: (1024, 4096)\n",
      "layers.14.attn.v_proj.weight: (1024, 4096)\n",
      "layers.14.attn.output_proj.weight: (4096, 4096)\n",
      "layers.14.mlp.w1.weight: (14336, 4096)\n",
      "layers.14.mlp.w2.weight: (4096, 14336)\n",
      "layers.14.mlp.w3.weight: (14336, 4096)\n",
      "layers.14.sa_norm.scale: (4096,)\n",
      "layers.14.mlp_norm.scale: (4096,)\n",
      "layers.15.attn.q_proj.weight: (4096, 4096)\n",
      "layers.15.attn.k_proj.weight: (1024, 4096)\n",
      "layers.15.attn.v_proj.weight: (1024, 4096)\n",
      "layers.15.attn.output_proj.weight: (4096, 4096)\n",
      "layers.15.mlp.w1.weight: (14336, 4096)\n",
      "layers.15.mlp.w2.weight: (4096, 14336)\n",
      "layers.15.mlp.w3.weight: (14336, 4096)\n",
      "layers.15.sa_norm.scale: (4096,)\n",
      "layers.15.mlp_norm.scale: (4096,)\n",
      "layers.16.attn.q_proj.weight: (4096, 4096)\n",
      "layers.16.attn.k_proj.weight: (1024, 4096)\n",
      "layers.16.attn.v_proj.weight: (1024, 4096)\n",
      "layers.16.attn.output_proj.weight: (4096, 4096)\n",
      "layers.16.mlp.w1.weight: (14336, 4096)\n",
      "layers.16.mlp.w2.weight: (4096, 14336)\n",
      "layers.16.mlp.w3.weight: (14336, 4096)\n",
      "layers.16.sa_norm.scale: (4096,)\n",
      "layers.16.mlp_norm.scale: (4096,)\n",
      "layers.17.attn.q_proj.weight: (4096, 4096)\n",
      "layers.17.attn.k_proj.weight: (1024, 4096)\n",
      "layers.17.attn.v_proj.weight: (1024, 4096)\n",
      "layers.17.attn.output_proj.weight: (4096, 4096)\n",
      "layers.17.mlp.w1.weight: (14336, 4096)\n",
      "layers.17.mlp.w2.weight: (4096, 14336)\n",
      "layers.17.mlp.w3.weight: (14336, 4096)\n",
      "layers.17.sa_norm.scale: (4096,)\n",
      "layers.17.mlp_norm.scale: (4096,)\n",
      "layers.18.attn.q_proj.weight: (4096, 4096)\n",
      "layers.18.attn.k_proj.weight: (1024, 4096)\n",
      "layers.18.attn.v_proj.weight: (1024, 4096)\n",
      "layers.18.attn.output_proj.weight: (4096, 4096)\n",
      "layers.18.mlp.w1.weight: (14336, 4096)\n",
      "layers.18.mlp.w2.weight: (4096, 14336)\n",
      "layers.18.mlp.w3.weight: (14336, 4096)\n",
      "layers.18.sa_norm.scale: (4096,)\n",
      "layers.18.mlp_norm.scale: (4096,)\n",
      "layers.19.attn.q_proj.weight: (4096, 4096)\n",
      "layers.19.attn.k_proj.weight: (1024, 4096)\n",
      "layers.19.attn.v_proj.weight: (1024, 4096)\n",
      "layers.19.attn.output_proj.weight: (4096, 4096)\n",
      "layers.19.mlp.w1.weight: (14336, 4096)\n",
      "layers.19.mlp.w2.weight: (4096, 14336)\n",
      "layers.19.mlp.w3.weight: (14336, 4096)\n",
      "layers.19.sa_norm.scale: (4096,)\n",
      "layers.19.mlp_norm.scale: (4096,)\n",
      "layers.20.attn.q_proj.weight: (4096, 4096)\n",
      "layers.20.attn.k_proj.weight: (1024, 4096)\n",
      "layers.20.attn.v_proj.weight: (1024, 4096)\n",
      "layers.20.attn.output_proj.weight: (4096, 4096)\n",
      "layers.20.mlp.w1.weight: (14336, 4096)\n",
      "layers.20.mlp.w2.weight: (4096, 14336)\n",
      "layers.20.mlp.w3.weight: (14336, 4096)\n",
      "layers.20.sa_norm.scale: (4096,)\n",
      "layers.20.mlp_norm.scale: (4096,)\n",
      "layers.21.attn.q_proj.weight: (4096, 4096)\n",
      "layers.21.attn.k_proj.weight: (1024, 4096)\n",
      "layers.21.attn.v_proj.weight: (1024, 4096)\n",
      "layers.21.attn.output_proj.weight: (4096, 4096)\n",
      "layers.21.mlp.w1.weight: (14336, 4096)\n",
      "layers.21.mlp.w2.weight: (4096, 14336)\n",
      "layers.21.mlp.w3.weight: (14336, 4096)\n",
      "layers.21.sa_norm.scale: (4096,)\n",
      "layers.21.mlp_norm.scale: (4096,)\n",
      "layers.22.attn.q_proj.weight: (4096, 4096)\n",
      "layers.22.attn.k_proj.weight: (1024, 4096)\n",
      "layers.22.attn.v_proj.weight: (1024, 4096)\n",
      "layers.22.attn.output_proj.weight: (4096, 4096)\n",
      "layers.22.mlp.w1.weight: (14336, 4096)\n",
      "layers.22.mlp.w2.weight: (4096, 14336)\n",
      "layers.22.mlp.w3.weight: (14336, 4096)\n",
      "layers.22.sa_norm.scale: (4096,)\n",
      "layers.22.mlp_norm.scale: (4096,)\n",
      "layers.23.attn.q_proj.weight: (4096, 4096)\n",
      "layers.23.attn.k_proj.weight: (1024, 4096)\n",
      "layers.23.attn.v_proj.weight: (1024, 4096)\n",
      "layers.23.attn.output_proj.weight: (4096, 4096)\n",
      "layers.23.mlp.w1.weight: (14336, 4096)\n",
      "layers.23.mlp.w2.weight: (4096, 14336)\n",
      "layers.23.mlp.w3.weight: (14336, 4096)\n",
      "layers.23.sa_norm.scale: (4096,)\n",
      "layers.23.mlp_norm.scale: (4096,)\n",
      "layers.24.attn.q_proj.weight: (4096, 4096)\n",
      "layers.24.attn.k_proj.weight: (1024, 4096)\n",
      "layers.24.attn.v_proj.weight: (1024, 4096)\n",
      "layers.24.attn.output_proj.weight: (4096, 4096)\n",
      "layers.24.mlp.w1.weight: (14336, 4096)\n",
      "layers.24.mlp.w2.weight: (4096, 14336)\n",
      "layers.24.mlp.w3.weight: (14336, 4096)\n",
      "layers.24.sa_norm.scale: (4096,)\n",
      "layers.24.mlp_norm.scale: (4096,)\n",
      "layers.25.attn.q_proj.weight: (4096, 4096)\n",
      "layers.25.attn.k_proj.weight: (1024, 4096)\n",
      "layers.25.attn.v_proj.weight: (1024, 4096)\n",
      "layers.25.attn.output_proj.weight: (4096, 4096)\n",
      "layers.25.mlp.w1.weight: (14336, 4096)\n",
      "layers.25.mlp.w2.weight: (4096, 14336)\n",
      "layers.25.mlp.w3.weight: (14336, 4096)\n",
      "layers.25.sa_norm.scale: (4096,)\n",
      "layers.25.mlp_norm.scale: (4096,)\n",
      "layers.26.attn.q_proj.weight: (4096, 4096)\n",
      "layers.26.attn.k_proj.weight: (1024, 4096)\n",
      "layers.26.attn.v_proj.weight: (1024, 4096)\n",
      "layers.26.attn.output_proj.weight: (4096, 4096)\n",
      "layers.26.mlp.w1.weight: (14336, 4096)\n",
      "layers.26.mlp.w2.weight: (4096, 14336)\n",
      "layers.26.mlp.w3.weight: (14336, 4096)\n",
      "layers.26.sa_norm.scale: (4096,)\n",
      "layers.26.mlp_norm.scale: (4096,)\n",
      "layers.27.attn.q_proj.weight: (4096, 4096)\n",
      "layers.27.attn.k_proj.weight: (1024, 4096)\n",
      "layers.27.attn.v_proj.weight: (1024, 4096)\n",
      "layers.27.attn.output_proj.weight: (4096, 4096)\n",
      "layers.27.mlp.w1.weight: (14336, 4096)\n",
      "layers.27.mlp.w2.weight: (4096, 14336)\n",
      "layers.27.mlp.w3.weight: (14336, 4096)\n",
      "layers.27.sa_norm.scale: (4096,)\n",
      "layers.27.mlp_norm.scale: (4096,)\n",
      "layers.28.attn.q_proj.weight: (4096, 4096)\n",
      "layers.28.attn.k_proj.weight: (1024, 4096)\n",
      "layers.28.attn.v_proj.weight: (1024, 4096)\n",
      "layers.28.attn.output_proj.weight: (4096, 4096)\n",
      "layers.28.mlp.w1.weight: (14336, 4096)\n",
      "layers.28.mlp.w2.weight: (4096, 14336)\n",
      "layers.28.mlp.w3.weight: (14336, 4096)\n",
      "layers.28.sa_norm.scale: (4096,)\n",
      "layers.28.mlp_norm.scale: (4096,)\n",
      "layers.29.attn.q_proj.weight: (4096, 4096)\n",
      "layers.29.attn.k_proj.weight: (1024, 4096)\n",
      "layers.29.attn.v_proj.weight: (1024, 4096)\n",
      "layers.29.attn.output_proj.weight: (4096, 4096)\n",
      "layers.29.mlp.w1.weight: (14336, 4096)\n",
      "layers.29.mlp.w2.weight: (4096, 14336)\n",
      "layers.29.mlp.w3.weight: (14336, 4096)\n",
      "layers.29.sa_norm.scale: (4096,)\n",
      "layers.29.mlp_norm.scale: (4096,)\n",
      "layers.30.attn.q_proj.weight: (4096, 4096)\n",
      "layers.30.attn.k_proj.weight: (1024, 4096)\n",
      "layers.30.attn.v_proj.weight: (1024, 4096)\n",
      "layers.30.attn.output_proj.weight: (4096, 4096)\n",
      "layers.30.mlp.w1.weight: (14336, 4096)\n",
      "layers.30.mlp.w2.weight: (4096, 14336)\n",
      "layers.30.mlp.w3.weight: (14336, 4096)\n",
      "layers.30.sa_norm.scale: (4096,)\n",
      "layers.30.mlp_norm.scale: (4096,)\n",
      "layers.31.attn.q_proj.weight: (4096, 4096)\n",
      "layers.31.attn.k_proj.weight: (1024, 4096)\n",
      "layers.31.attn.v_proj.weight: (1024, 4096)\n",
      "layers.31.attn.output_proj.weight: (4096, 4096)\n",
      "layers.31.mlp.w1.weight: (14336, 4096)\n",
      "layers.31.mlp.w2.weight: (4096, 14336)\n",
      "layers.31.mlp.w3.weight: (14336, 4096)\n",
      "layers.31.sa_norm.scale: (4096,)\n",
      "layers.31.mlp_norm.scale: (4096,)\n",
      "norm.scale: (4096,)\n",
      "output.weight: (128256, 4096)\n"
     ]
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "\n",
    "for key in state_dict:\n",
    "    print(key + \":\", tuple(state_dict[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight: (128256, 4096)\n",
      "layers.0.attn.q_proj.weight: (4096, 4096)\n",
      "layers.0.attn.k_proj.weight: (1024, 4096)\n",
      "layers.0.attn.v_proj.weight: (1024, 4096)\n",
      "layers.0.attn.output_proj.weight: (4096, 4096)\n",
      "layers.0.mlp.w1.weight: (14336, 4096)\n",
      "layers.0.mlp.w3.weight: (14336, 4096)\n",
      "layers.0.mlp.w2.weight: (4096, 14336)\n",
      "layers.0.sa_norm.scale: (4096,)\n",
      "layers.0.mlp_norm.scale: (4096,)\n",
      "layers.1.attn.q_proj.weight: (4096, 4096)\n",
      "layers.1.attn.k_proj.weight: (1024, 4096)\n",
      "layers.1.attn.v_proj.weight: (1024, 4096)\n",
      "layers.1.attn.output_proj.weight: (4096, 4096)\n",
      "layers.1.mlp.w1.weight: (14336, 4096)\n",
      "layers.1.mlp.w3.weight: (14336, 4096)\n",
      "layers.1.mlp.w2.weight: (4096, 14336)\n",
      "layers.1.sa_norm.scale: (4096,)\n",
      "layers.1.mlp_norm.scale: (4096,)\n",
      "layers.2.attn.q_proj.weight: (4096, 4096)\n",
      "layers.2.attn.k_proj.weight: (1024, 4096)\n",
      "layers.2.attn.v_proj.weight: (1024, 4096)\n",
      "layers.2.attn.output_proj.weight: (4096, 4096)\n",
      "layers.2.mlp.w1.weight: (14336, 4096)\n",
      "layers.2.mlp.w3.weight: (14336, 4096)\n",
      "layers.2.mlp.w2.weight: (4096, 14336)\n",
      "layers.2.sa_norm.scale: (4096,)\n",
      "layers.2.mlp_norm.scale: (4096,)\n",
      "layers.3.attn.q_proj.weight: (4096, 4096)\n",
      "layers.3.attn.k_proj.weight: (1024, 4096)\n",
      "layers.3.attn.v_proj.weight: (1024, 4096)\n",
      "layers.3.attn.output_proj.weight: (4096, 4096)\n",
      "layers.3.mlp.w1.weight: (14336, 4096)\n",
      "layers.3.mlp.w3.weight: (14336, 4096)\n",
      "layers.3.mlp.w2.weight: (4096, 14336)\n",
      "layers.3.sa_norm.scale: (4096,)\n",
      "layers.3.mlp_norm.scale: (4096,)\n",
      "layers.4.attn.q_proj.weight: (4096, 4096)\n",
      "layers.4.attn.k_proj.weight: (1024, 4096)\n",
      "layers.4.attn.v_proj.weight: (1024, 4096)\n",
      "layers.4.attn.output_proj.weight: (4096, 4096)\n",
      "layers.4.mlp.w1.weight: (14336, 4096)\n",
      "layers.4.mlp.w3.weight: (14336, 4096)\n",
      "layers.4.mlp.w2.weight: (4096, 14336)\n",
      "layers.4.sa_norm.scale: (4096,)\n",
      "layers.4.mlp_norm.scale: (4096,)\n",
      "layers.5.attn.q_proj.weight: (4096, 4096)\n",
      "layers.5.attn.k_proj.weight: (1024, 4096)\n",
      "layers.5.attn.v_proj.weight: (1024, 4096)\n",
      "layers.5.attn.output_proj.weight: (4096, 4096)\n",
      "layers.5.mlp.w1.weight: (14336, 4096)\n",
      "layers.5.mlp.w3.weight: (14336, 4096)\n",
      "layers.5.mlp.w2.weight: (4096, 14336)\n",
      "layers.5.sa_norm.scale: (4096,)\n",
      "layers.5.mlp_norm.scale: (4096,)\n",
      "layers.6.attn.q_proj.weight: (4096, 4096)\n",
      "layers.6.attn.k_proj.weight: (1024, 4096)\n",
      "layers.6.attn.v_proj.weight: (1024, 4096)\n",
      "layers.6.attn.output_proj.weight: (4096, 4096)\n",
      "layers.6.mlp.w1.weight: (14336, 4096)\n",
      "layers.6.mlp.w3.weight: (14336, 4096)\n",
      "layers.6.mlp.w2.weight: (4096, 14336)\n",
      "layers.6.sa_norm.scale: (4096,)\n",
      "layers.6.mlp_norm.scale: (4096,)\n",
      "layers.7.attn.q_proj.weight: (4096, 4096)\n",
      "layers.7.attn.k_proj.weight: (1024, 4096)\n",
      "layers.7.attn.v_proj.weight: (1024, 4096)\n",
      "layers.7.attn.output_proj.weight: (4096, 4096)\n",
      "layers.7.mlp.w1.weight: (14336, 4096)\n",
      "layers.7.mlp.w3.weight: (14336, 4096)\n",
      "layers.7.mlp.w2.weight: (4096, 14336)\n",
      "layers.7.sa_norm.scale: (4096,)\n",
      "layers.7.mlp_norm.scale: (4096,)\n",
      "layers.8.attn.q_proj.weight: (4096, 4096)\n",
      "layers.8.attn.k_proj.weight: (1024, 4096)\n",
      "layers.8.attn.v_proj.weight: (1024, 4096)\n",
      "layers.8.attn.output_proj.weight: (4096, 4096)\n",
      "layers.8.mlp.w1.weight: (14336, 4096)\n",
      "layers.8.mlp.w3.weight: (14336, 4096)\n",
      "layers.8.mlp.w2.weight: (4096, 14336)\n",
      "layers.8.sa_norm.scale: (4096,)\n",
      "layers.8.mlp_norm.scale: (4096,)\n",
      "layers.9.attn.q_proj.weight: (4096, 4096)\n",
      "layers.9.attn.k_proj.weight: (1024, 4096)\n",
      "layers.9.attn.v_proj.weight: (1024, 4096)\n",
      "layers.9.attn.output_proj.weight: (4096, 4096)\n",
      "layers.9.mlp.w1.weight: (14336, 4096)\n",
      "layers.9.mlp.w3.weight: (14336, 4096)\n",
      "layers.9.mlp.w2.weight: (4096, 14336)\n",
      "layers.9.sa_norm.scale: (4096,)\n",
      "layers.9.mlp_norm.scale: (4096,)\n",
      "layers.10.attn.q_proj.weight: (4096, 4096)\n",
      "layers.10.attn.k_proj.weight: (1024, 4096)\n",
      "layers.10.attn.v_proj.weight: (1024, 4096)\n",
      "layers.10.attn.output_proj.weight: (4096, 4096)\n",
      "layers.10.mlp.w1.weight: (14336, 4096)\n",
      "layers.10.mlp.w3.weight: (14336, 4096)\n",
      "layers.10.mlp.w2.weight: (4096, 14336)\n",
      "layers.10.sa_norm.scale: (4096,)\n",
      "layers.10.mlp_norm.scale: (4096,)\n",
      "layers.11.attn.q_proj.weight: (4096, 4096)\n",
      "layers.11.attn.k_proj.weight: (1024, 4096)\n",
      "layers.11.attn.v_proj.weight: (1024, 4096)\n",
      "layers.11.attn.output_proj.weight: (4096, 4096)\n",
      "layers.11.mlp.w1.weight: (14336, 4096)\n",
      "layers.11.mlp.w3.weight: (14336, 4096)\n",
      "layers.11.mlp.w2.weight: (4096, 14336)\n",
      "layers.11.sa_norm.scale: (4096,)\n",
      "layers.11.mlp_norm.scale: (4096,)\n",
      "layers.12.attn.q_proj.weight: (4096, 4096)\n",
      "layers.12.attn.k_proj.weight: (1024, 4096)\n",
      "layers.12.attn.v_proj.weight: (1024, 4096)\n",
      "layers.12.attn.output_proj.weight: (4096, 4096)\n",
      "layers.12.mlp.w1.weight: (14336, 4096)\n",
      "layers.12.mlp.w3.weight: (14336, 4096)\n",
      "layers.12.mlp.w2.weight: (4096, 14336)\n",
      "layers.12.sa_norm.scale: (4096,)\n",
      "layers.12.mlp_norm.scale: (4096,)\n",
      "layers.13.attn.q_proj.weight: (4096, 4096)\n",
      "layers.13.attn.k_proj.weight: (1024, 4096)\n",
      "layers.13.attn.v_proj.weight: (1024, 4096)\n",
      "layers.13.attn.output_proj.weight: (4096, 4096)\n",
      "layers.13.mlp.w1.weight: (14336, 4096)\n",
      "layers.13.mlp.w3.weight: (14336, 4096)\n",
      "layers.13.mlp.w2.weight: (4096, 14336)\n",
      "layers.13.sa_norm.scale: (4096,)\n",
      "layers.13.mlp_norm.scale: (4096,)\n",
      "layers.14.attn.q_proj.weight: (4096, 4096)\n",
      "layers.14.attn.k_proj.weight: (1024, 4096)\n",
      "layers.14.attn.v_proj.weight: (1024, 4096)\n",
      "layers.14.attn.output_proj.weight: (4096, 4096)\n",
      "layers.14.mlp.w1.weight: (14336, 4096)\n",
      "layers.14.mlp.w3.weight: (14336, 4096)\n",
      "layers.14.mlp.w2.weight: (4096, 14336)\n",
      "layers.14.sa_norm.scale: (4096,)\n",
      "layers.14.mlp_norm.scale: (4096,)\n",
      "layers.15.attn.q_proj.weight: (4096, 4096)\n",
      "layers.15.attn.k_proj.weight: (1024, 4096)\n",
      "layers.15.attn.v_proj.weight: (1024, 4096)\n",
      "layers.15.attn.output_proj.weight: (4096, 4096)\n",
      "layers.15.mlp.w1.weight: (14336, 4096)\n",
      "layers.15.mlp.w3.weight: (14336, 4096)\n",
      "layers.15.mlp.w2.weight: (4096, 14336)\n",
      "layers.15.sa_norm.scale: (4096,)\n",
      "layers.15.mlp_norm.scale: (4096,)\n",
      "layers.16.attn.q_proj.weight: (4096, 4096)\n",
      "layers.16.attn.k_proj.weight: (1024, 4096)\n",
      "layers.16.attn.v_proj.weight: (1024, 4096)\n",
      "layers.16.attn.output_proj.weight: (4096, 4096)\n",
      "layers.16.mlp.w1.weight: (14336, 4096)\n",
      "layers.16.mlp.w3.weight: (14336, 4096)\n",
      "layers.16.mlp.w2.weight: (4096, 14336)\n",
      "layers.16.sa_norm.scale: (4096,)\n",
      "layers.16.mlp_norm.scale: (4096,)\n",
      "layers.17.attn.q_proj.weight: (4096, 4096)\n",
      "layers.17.attn.k_proj.weight: (1024, 4096)\n",
      "layers.17.attn.v_proj.weight: (1024, 4096)\n",
      "layers.17.attn.output_proj.weight: (4096, 4096)\n",
      "layers.17.mlp.w1.weight: (14336, 4096)\n",
      "layers.17.mlp.w3.weight: (14336, 4096)\n",
      "layers.17.mlp.w2.weight: (4096, 14336)\n",
      "layers.17.sa_norm.scale: (4096,)\n",
      "layers.17.mlp_norm.scale: (4096,)\n",
      "layers.18.attn.q_proj.weight: (4096, 4096)\n",
      "layers.18.attn.k_proj.weight: (1024, 4096)\n",
      "layers.18.attn.v_proj.weight: (1024, 4096)\n",
      "layers.18.attn.output_proj.weight: (4096, 4096)\n",
      "layers.18.mlp.w1.weight: (14336, 4096)\n",
      "layers.18.mlp.w3.weight: (14336, 4096)\n",
      "layers.18.mlp.w2.weight: (4096, 14336)\n",
      "layers.18.sa_norm.scale: (4096,)\n",
      "layers.18.mlp_norm.scale: (4096,)\n",
      "layers.19.attn.q_proj.weight: (4096, 4096)\n",
      "layers.19.attn.k_proj.weight: (1024, 4096)\n",
      "layers.19.attn.v_proj.weight: (1024, 4096)\n",
      "layers.19.attn.output_proj.weight: (4096, 4096)\n",
      "layers.19.mlp.w1.weight: (14336, 4096)\n",
      "layers.19.mlp.w3.weight: (14336, 4096)\n",
      "layers.19.mlp.w2.weight: (4096, 14336)\n",
      "layers.19.sa_norm.scale: (4096,)\n",
      "layers.19.mlp_norm.scale: (4096,)\n",
      "layers.20.attn.q_proj.weight: (4096, 4096)\n",
      "layers.20.attn.k_proj.weight: (1024, 4096)\n",
      "layers.20.attn.v_proj.weight: (1024, 4096)\n",
      "layers.20.attn.output_proj.weight: (4096, 4096)\n",
      "layers.20.mlp.w1.weight: (14336, 4096)\n",
      "layers.20.mlp.w3.weight: (14336, 4096)\n",
      "layers.20.mlp.w2.weight: (4096, 14336)\n",
      "layers.20.sa_norm.scale: (4096,)\n",
      "layers.20.mlp_norm.scale: (4096,)\n",
      "layers.21.attn.q_proj.weight: (4096, 4096)\n",
      "layers.21.attn.k_proj.weight: (1024, 4096)\n",
      "layers.21.attn.v_proj.weight: (1024, 4096)\n",
      "layers.21.attn.output_proj.weight: (4096, 4096)\n",
      "layers.21.mlp.w1.weight: (14336, 4096)\n",
      "layers.21.mlp.w3.weight: (14336, 4096)\n",
      "layers.21.mlp.w2.weight: (4096, 14336)\n",
      "layers.21.sa_norm.scale: (4096,)\n",
      "layers.21.mlp_norm.scale: (4096,)\n",
      "layers.22.attn.q_proj.weight: (4096, 4096)\n",
      "layers.22.attn.k_proj.weight: (1024, 4096)\n",
      "layers.22.attn.v_proj.weight: (1024, 4096)\n",
      "layers.22.attn.output_proj.weight: (4096, 4096)\n",
      "layers.22.mlp.w1.weight: (14336, 4096)\n",
      "layers.22.mlp.w3.weight: (14336, 4096)\n",
      "layers.22.mlp.w2.weight: (4096, 14336)\n",
      "layers.22.sa_norm.scale: (4096,)\n",
      "layers.22.mlp_norm.scale: (4096,)\n",
      "layers.23.attn.q_proj.weight: (4096, 4096)\n",
      "layers.23.attn.k_proj.weight: (1024, 4096)\n",
      "layers.23.attn.v_proj.weight: (1024, 4096)\n",
      "layers.23.attn.output_proj.weight: (4096, 4096)\n",
      "layers.23.mlp.w1.weight: (14336, 4096)\n",
      "layers.23.mlp.w3.weight: (14336, 4096)\n",
      "layers.23.mlp.w2.weight: (4096, 14336)\n",
      "layers.23.sa_norm.scale: (4096,)\n",
      "layers.23.mlp_norm.scale: (4096,)\n",
      "layers.24.attn.q_proj.weight: (4096, 4096)\n",
      "layers.24.attn.k_proj.weight: (1024, 4096)\n",
      "layers.24.attn.v_proj.weight: (1024, 4096)\n",
      "layers.24.attn.output_proj.weight: (4096, 4096)\n",
      "layers.24.mlp.w1.weight: (14336, 4096)\n",
      "layers.24.mlp.w3.weight: (14336, 4096)\n",
      "layers.24.mlp.w2.weight: (4096, 14336)\n",
      "layers.24.sa_norm.scale: (4096,)\n",
      "layers.24.mlp_norm.scale: (4096,)\n",
      "layers.25.attn.q_proj.weight: (4096, 4096)\n",
      "layers.25.attn.k_proj.weight: (1024, 4096)\n",
      "layers.25.attn.v_proj.weight: (1024, 4096)\n",
      "layers.25.attn.output_proj.weight: (4096, 4096)\n",
      "layers.25.mlp.w1.weight: (14336, 4096)\n",
      "layers.25.mlp.w3.weight: (14336, 4096)\n",
      "layers.25.mlp.w2.weight: (4096, 14336)\n",
      "layers.25.sa_norm.scale: (4096,)\n",
      "layers.25.mlp_norm.scale: (4096,)\n",
      "layers.26.attn.q_proj.weight: (4096, 4096)\n",
      "layers.26.attn.k_proj.weight: (1024, 4096)\n",
      "layers.26.attn.v_proj.weight: (1024, 4096)\n",
      "layers.26.attn.output_proj.weight: (4096, 4096)\n",
      "layers.26.mlp.w1.weight: (14336, 4096)\n",
      "layers.26.mlp.w3.weight: (14336, 4096)\n",
      "layers.26.mlp.w2.weight: (4096, 14336)\n",
      "layers.26.sa_norm.scale: (4096,)\n",
      "layers.26.mlp_norm.scale: (4096,)\n",
      "layers.27.attn.q_proj.weight: (4096, 4096)\n",
      "layers.27.attn.k_proj.weight: (1024, 4096)\n",
      "layers.27.attn.v_proj.weight: (1024, 4096)\n",
      "layers.27.attn.output_proj.weight: (4096, 4096)\n",
      "layers.27.mlp.w1.weight: (14336, 4096)\n",
      "layers.27.mlp.w3.weight: (14336, 4096)\n",
      "layers.27.mlp.w2.weight: (4096, 14336)\n",
      "layers.27.sa_norm.scale: (4096,)\n",
      "layers.27.mlp_norm.scale: (4096,)\n",
      "layers.28.attn.q_proj.weight: (4096, 4096)\n",
      "layers.28.attn.k_proj.weight: (1024, 4096)\n",
      "layers.28.attn.v_proj.weight: (1024, 4096)\n",
      "layers.28.attn.output_proj.weight: (4096, 4096)\n",
      "layers.28.mlp.w1.weight: (14336, 4096)\n",
      "layers.28.mlp.w3.weight: (14336, 4096)\n",
      "layers.28.mlp.w2.weight: (4096, 14336)\n",
      "layers.28.sa_norm.scale: (4096,)\n",
      "layers.28.mlp_norm.scale: (4096,)\n",
      "layers.29.attn.q_proj.weight: (4096, 4096)\n",
      "layers.29.attn.k_proj.weight: (1024, 4096)\n",
      "layers.29.attn.v_proj.weight: (1024, 4096)\n",
      "layers.29.attn.output_proj.weight: (4096, 4096)\n",
      "layers.29.mlp.w1.weight: (14336, 4096)\n",
      "layers.29.mlp.w3.weight: (14336, 4096)\n",
      "layers.29.mlp.w2.weight: (4096, 14336)\n",
      "layers.29.sa_norm.scale: (4096,)\n",
      "layers.29.mlp_norm.scale: (4096,)\n",
      "layers.30.attn.q_proj.weight: (4096, 4096)\n",
      "layers.30.attn.k_proj.weight: (1024, 4096)\n",
      "layers.30.attn.v_proj.weight: (1024, 4096)\n",
      "layers.30.attn.output_proj.weight: (4096, 4096)\n",
      "layers.30.mlp.w1.weight: (14336, 4096)\n",
      "layers.30.mlp.w3.weight: (14336, 4096)\n",
      "layers.30.mlp.w2.weight: (4096, 14336)\n",
      "layers.30.sa_norm.scale: (4096,)\n",
      "layers.30.mlp_norm.scale: (4096,)\n",
      "layers.31.attn.q_proj.weight: (4096, 4096)\n",
      "layers.31.attn.k_proj.weight: (1024, 4096)\n",
      "layers.31.attn.v_proj.weight: (1024, 4096)\n",
      "layers.31.attn.output_proj.weight: (4096, 4096)\n",
      "layers.31.mlp.w1.weight: (14336, 4096)\n",
      "layers.31.mlp.w3.weight: (14336, 4096)\n",
      "layers.31.mlp.w2.weight: (4096, 14336)\n",
      "layers.31.sa_norm.scale: (4096,)\n",
      "layers.31.mlp_norm.scale: (4096,)\n",
      "norm.scale: (4096,)\n"
     ]
    }
   ],
   "source": [
    "mapped_vllm_state_dict = {}\n",
    "\n",
    "# Map embedding and norm layers directly\n",
    "mapped_vllm_state_dict['tok_embeddings.weight'] = vllm_state_dict['embed_tokens.weight']\n",
    "\n",
    "\n",
    "# Map each transformer layer\n",
    "for i in range(32):\n",
    "    prefix = f'layers.{i}.'\n",
    "    vllm_prefix = f'layers.{i}.'\n",
    "    \n",
    "    # Split QKV projection\n",
    "    qkv = vllm_state_dict[vllm_prefix + 'self_attn.qkv_proj.weight']\n",
    "    q, k, v = torch.split(qkv, [4096, 1024, 1024], dim=0)\n",
    "    mapped_vllm_state_dict[prefix + 'attn.q_proj.weight'] = q\n",
    "    mapped_vllm_state_dict[prefix + 'attn.k_proj.weight'] = k \n",
    "    mapped_vllm_state_dict[prefix + 'attn.v_proj.weight'] = v\n",
    "\n",
    "    # Map attention output projection\n",
    "    mapped_vllm_state_dict[prefix + 'attn.output_proj.weight'] = vllm_state_dict[vllm_prefix + 'self_attn.o_proj.weight']\n",
    "\n",
    "    # Split gate/up projection\n",
    "    gate_up = vllm_state_dict[vllm_prefix + 'mlp.gate_up_proj.weight']\n",
    "    gate, up = torch.split(gate_up, [14336, 14336], dim=0)\n",
    "    mapped_vllm_state_dict[prefix + 'mlp.w1.weight'] = gate\n",
    "    mapped_vllm_state_dict[prefix + 'mlp.w3.weight'] = up\n",
    "\n",
    "    # Map down projection\n",
    "    mapped_vllm_state_dict[prefix + 'mlp.w2.weight'] = vllm_state_dict[vllm_prefix + 'mlp.down_proj.weight']\n",
    "\n",
    "    # Map norms\n",
    "    mapped_vllm_state_dict[prefix + 'sa_norm.scale'] = vllm_state_dict[vllm_prefix + 'input_layernorm.weight']\n",
    "    mapped_vllm_state_dict[prefix + 'mlp_norm.scale'] = vllm_state_dict[vllm_prefix + 'post_attention_layernorm.weight']\n",
    "\n",
    "mapped_vllm_state_dict['norm.scale'] = vllm_state_dict['norm.weight']\n",
    "\n",
    "for key in mapped_vllm_state_dict:\n",
    "    print(key + \":\", tuple(mapped_vllm_state_dict[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_vllm_state_dict[\"output.weight\"] = llm.llm_engine.model_executor.driver_worker.model_runner.model.lm_head.state_dict()[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight: 0.0000\n",
      "layers.0.attn.q_proj.weight: 0.0000\n",
      "layers.0.attn.k_proj.weight: 0.0001\n",
      "layers.0.attn.v_proj.weight: -0.0000\n",
      "layers.0.attn.output_proj.weight: -0.0000\n",
      "layers.0.mlp.w1.weight: 0.0001\n",
      "layers.0.mlp.w2.weight: -0.0000\n",
      "layers.0.mlp.w3.weight: -0.0001\n",
      "layers.0.sa_norm.scale: nan\n",
      "layers.0.mlp_norm.scale: nan\n",
      "layers.1.attn.q_proj.weight: -0.0003\n",
      "layers.1.attn.k_proj.weight: -0.0004\n",
      "layers.1.attn.v_proj.weight: -0.0000\n",
      "layers.1.attn.output_proj.weight: -0.0000\n",
      "layers.1.mlp.w1.weight: -0.0000\n",
      "layers.1.mlp.w2.weight: -0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m mapped_vllm_state_dict:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Stack the tensors into a 2xN matrix before computing correlation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     stacked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m      6\u001b[0m         state_dict[key]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m      7\u001b[0m         mapped_vllm_state_dict[key]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     ])\n\u001b[0;32m----> 9\u001b[0m     correlation \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrcoef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrelation\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Print correlation between state_dict and mapped_vllm_state_dict tensors\n",
    "for key in state_dict:\n",
    "    if key in mapped_vllm_state_dict:\n",
    "        # Stack the tensors into a 2xN matrix before computing correlation\n",
    "        stacked = torch.stack([\n",
    "            state_dict[key].flatten().float(),\n",
    "            mapped_vllm_state_dict[key].flatten().float().to(\"cpu\")\n",
    "        ])\n",
    "        correlation = torch.corrcoef(stacked)[0,1]\n",
    "        print(f\"{key}: {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'vllm.attention.backends.flash_attn.FlashAttentionMetadata'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it, est. speed input: 113.81 toks/s, output: 70.43 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=6, prompt=\"<|begin_of_text|><|im_start|>user\\nOn a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\\n\\nThey assembled 3 decks of cards, each for a separate type of information composed of the following:\\n\\nSuspect:\\n- Miss Scarlet\\n- Mr. Green\\n- Mrs. White\\n\\nWeapon:\\n- Candlestick\\n- Knife\\n- Lead Pipe\\n\\nRoom:\\n- Hall\\n- Lounge\\n- Dining Room\\n\\nAfter randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\\n\\n- Summer: 2 cards\\n- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\\n- Connor: 2 cards\\n\\nThe game proceeded as follows:\\n\\n1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\\n2. The player directed this question to the other players in clockwise order, starting with the player to their left.\\n3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\\n4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\\n5. This continued until either:\\n    a) A player showed a card to the asking player, or\\n    b) All the queried players had stated they didn't have any of the asked-about cards.\\n6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\\n\\nHere is how the game played out:\\n\\nSummer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\\n- Giselle did not have any of the cards\\n- Connor showed Summer a card\\n\\nGiselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\\n- Connor did not have any of the cards\\n- Summer did not have any of the cards\\n\\nConnor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\\n- Summer did not have any of the cards\\n- Giselle showed Connor 'Miss Scarlet'\\n\\nSummer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\\n- Giselle did not have any of the cards\\n- Connor did not have any of the cards\\n\\nAt this point, Giselle was able to correctly infer the solution and win the game.\\n\\nWhat were the facedown cards in the middle of the table?<|im_end|>\\n<|im_start|>assistant\\n\", prompt_token_ids=[128000, 128000, 128002, 882, 198, 1966, 264, 8369, 10683, 1938, 19367, 11, 480, 285, 6853, 323, 58280, 7731, 1523, 311, 1514, 264, 16736, 23347, 1847, 382, 7009, 35105, 220, 18, 30881, 315, 7563, 11, 1855, 369, 264, 8821, 955, 315, 2038, 24306, 315, 279, 2768, 1473, 78524, 1002, 512, 12, 9083, 81818, 198, 12, 4491, 13, 7997, 198, 12, 18083, 13, 5929, 271, 29314, 512, 12, 73997, 30133, 198, 12, 62302, 198, 12, 30982, 28905, 271, 14330, 512, 12, 11166, 198, 12, 50767, 198, 12, 39190, 10637, 271, 6153, 27716, 320, 438, 89447, 8, 19301, 832, 3786, 505, 1855, 1912, 323, 25012, 1124, 304, 279, 6278, 315, 279, 2007, 17011, 785, 11, 814, 75371, 279, 9861, 7563, 323, 27023, 704, 279, 2768, 311, 1855, 2851, 1473, 12, 19367, 25, 220, 17, 7563, 198, 12, 480, 285, 6853, 25, 220, 17, 7563, 4417, 43, 26645, 518, 364, 36412, 81818, 1329, 12, 58280, 25, 220, 17, 7563, 271, 791, 1847, 45374, 439, 11263, 1473, 16, 13, 1952, 872, 2543, 11, 264, 2851, 4691, 922, 264, 743, 315, 7041, 220, 18, 7563, 11, 832, 505, 1855, 315, 279, 1847, 596, 11306, 13, 320, 9290, 25, 25640, 1436, 2610, 922, 904, 7563, 11, 2737, 1884, 304, 872, 1866, 1450, 29275, 17, 13, 578, 2851, 15910, 420, 3488, 311, 279, 1023, 4311, 304, 66770, 2015, 11, 6041, 449, 279, 2851, 311, 872, 2163, 627, 18, 13, 1442, 264, 2851, 1047, 832, 477, 810, 315, 279, 4691, 69205, 7563, 11, 814, 1047, 311, 1501, 832, 315, 1884, 7563, 320, 1073, 872, 5873, 8, 311, 279, 10371, 2851, 38171, 13, 578, 2543, 1243, 9670, 11, 323, 1514, 5946, 311, 279, 1828, 2851, 627, 19, 13, 1442, 264, 2851, 1550, 539, 617, 904, 315, 279, 4691, 69205, 7563, 11, 814, 1071, 779, 11, 323, 279, 3488, 5946, 311, 279, 1828, 2851, 304, 66770, 2015, 627, 20, 13, 1115, 8738, 3156, 3060, 512, 262, 264, 8, 362, 2851, 8710, 264, 3786, 311, 279, 10371, 2851, 11, 477, 198, 262, 293, 8, 2052, 279, 79002, 4311, 1047, 11224, 814, 3287, 956, 617, 904, 315, 279, 4691, 69205, 7563, 627, 21, 13, 4740, 264, 2851, 596, 2543, 9670, 320, 50998, 555, 1694, 6982, 264, 3786, 477, 3515, 682, 79002, 4311, 1522, 705, 1514, 7882, 311, 279, 1828, 2851, 304, 66770, 2015, 382, 8586, 374, 1268, 279, 1847, 6476, 704, 1473, 51787, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 35, 5859, 10637, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 8710, 19367, 264, 3786, 271, 38, 285, 6853, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 43, 26645, 3730, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 271, 57987, 4691, 422, 5606, 1047, 364, 36412, 81818, 6, 477, 364, 34, 3397, 30133, 6, 477, 364, 72945, 3730, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 480, 285, 6853, 8710, 58280, 364, 36412, 81818, 3961, 51787, 4691, 422, 5606, 1047, 364, 12555, 13, 7997, 6, 477, 364, 57505, 6, 477, 364, 72945, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 271, 1688, 420, 1486, 11, 480, 285, 6853, 574, 3025, 311, 12722, 24499, 279, 6425, 323, 3243, 279, 1847, 382, 3923, 1051, 279, 17011, 785, 7563, 304, 279, 6278, 315, 279, 2007, 30, 128003, 198, 128002, 78191, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='We know that Giselle held \\'Lounge\\' and \\'Miss Scarlet\\' cards and that Connor showed Summer a card at some point. Also, Giselle was able to infer the solution at the end.\\n\\nLet\\'s analyze the possible scenarios:\\n\\n1. If \"Knife\" is in the middle: Summer would have shown \"Knife\" during her second turn to Connor since it\\'s the only card Summer knew was in the middle. However, this contradicts the fact that Summer did not show any cards.\\n\\n2. If \"Dining Room\" is in the middle: Summer would have shown \"Dining Room\" during her first turn to Giselle since she knew Giselle held \"Lounge\". However, this also contradicts the fact that Summer did not show any cards.\\n\\n3. If \"Hall\" is in the middle: Summer could not have asked about it during her first turn since she didn\\'t know if Connor held \"Hall\" or not. However, she could have asked about it during her second turn, but Connor did not have the card.\\n\\nSo, we are left with the remaining possibilities. Since Summer did not show any cards, it must be that none of the facedown cards matched with any of the cards Giselle or Connor already had. Hence, it is impossible for Giselle to deduce the correct solution based on the given information.\\n\\nThis conundrum arises because the game\\'s rules allow players to ask about any cards, not limited to those they have in their hand or which are facedown. This discrepancy in the rules creates an anomaly where the game\\'s logical foundation is breached, rendering the game no longer playable according to its intended design. It is therefore impossible to determine the facedown cards with the given information within this flawed game setting.', token_ids=(1687, 1440, 430, 480, 285, 6853, 5762, 364, 43, 26645, 6, 323, 364, 36412, 81818, 6, 7563, 323, 430, 58280, 8710, 19367, 264, 3786, 520, 1063, 1486, 13, 7429, 11, 480, 285, 6853, 574, 3025, 311, 24499, 279, 6425, 520, 279, 842, 382, 10267, 596, 24564, 279, 3284, 26350, 1473, 16, 13, 1442, 330, 57505, 1, 374, 304, 279, 6278, 25, 19367, 1053, 617, 6982, 330, 57505, 1, 2391, 1077, 2132, 2543, 311, 58280, 2533, 433, 596, 279, 1193, 3786, 19367, 7020, 574, 304, 279, 6278, 13, 4452, 11, 420, 23093, 31095, 279, 2144, 430, 19367, 1550, 539, 1501, 904, 7563, 382, 17, 13, 1442, 330, 35, 5859, 10637, 1, 374, 304, 279, 6278, 25, 19367, 1053, 617, 6982, 330, 35, 5859, 10637, 1, 2391, 1077, 1176, 2543, 311, 480, 285, 6853, 2533, 1364, 7020, 480, 285, 6853, 5762, 330, 43, 26645, 3343, 4452, 11, 420, 1101, 23093, 31095, 279, 2144, 430, 19367, 1550, 539, 1501, 904, 7563, 382, 18, 13, 1442, 330, 72945, 1, 374, 304, 279, 6278, 25, 19367, 1436, 539, 617, 4691, 922, 433, 2391, 1077, 1176, 2543, 2533, 1364, 3287, 956, 1440, 422, 58280, 5762, 330, 72945, 1, 477, 539, 13, 4452, 11, 1364, 1436, 617, 4691, 922, 433, 2391, 1077, 2132, 2543, 11, 719, 58280, 1550, 539, 617, 279, 3786, 382, 4516, 11, 584, 527, 2163, 449, 279, 9861, 24525, 13, 8876, 19367, 1550, 539, 1501, 904, 7563, 11, 433, 2011, 387, 430, 7000, 315, 279, 17011, 785, 7563, 18545, 449, 904, 315, 279, 7563, 480, 285, 6853, 477, 58280, 2736, 1047, 13, 32140, 11, 433, 374, 12266, 369, 480, 285, 6853, 311, 7836, 10743, 279, 4495, 6425, 3196, 389, 279, 2728, 2038, 382, 2028, 390, 1263, 10952, 48282, 1606, 279, 1847, 596, 5718, 2187, 4311, 311, 2610, 922, 904, 7563, 11, 539, 7347, 311, 1884, 814, 617, 304, 872, 1450, 477, 902, 527, 17011, 785, 13, 1115, 79105, 304, 279, 5718, 11705, 459, 64048, 1405, 279, 1847, 596, 20406, 16665, 374, 82166, 11, 21568, 279, 1847, 912, 5129, 52135, 4184, 311, 1202, 10825, 2955, 13, 1102, 374, 9093, 12266, 311, 8417, 279, 17011, 785, 7563, 449, 279, 2728, 2038, 2949, 420, 48008, 1847, 6376, 13, 128003), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729721783.637649, last_token_time=1729721783.637649, first_scheduled_time=1729721783.6405513, first_token_time=1729721783.700682, time_in_queue=0.0029022693634033203, finished_time=1729721788.7675114, scheduler_time=0.025968797000587074, model_forward_time=None, model_execute_time=None), lora_request=None)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm.forward_context import get_forward_context\n",
    "\n",
    "compute_logits = vllm_model.forward\n",
    "\n",
    "def hook(*args, **kwargs):\n",
    "    print(type(get_forward_context()))\n",
    "    return compute_logits(*args, **kwargs)\n",
    "\n",
    "vllm_model.forward = hook\n",
    "\n",
    "output = llm.chat([[dict(role=\"user\", content=prompt)]] * 1, sampling_params=SamplingParams(max_tokens=10_000))  # type: ignore\n",
    "\n",
    "vllm_model.forward = compute_logits\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.49s/it, est. speed input: 106.64 toks/s, output: 70.55 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=8, prompt=\"<|begin_of_text|><|im_start|>user\\nOn a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\\n\\nThey assembled 3 decks of cards, each for a separate type of information composed of the following:\\n\\nSuspect:\\n- Miss Scarlet\\n- Mr. Green\\n- Mrs. White\\n\\nWeapon:\\n- Candlestick\\n- Knife\\n- Lead Pipe\\n\\nRoom:\\n- Hall\\n- Lounge\\n- Dining Room\\n\\nAfter randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\\n\\n- Summer: 2 cards\\n- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\\n- Connor: 2 cards\\n\\nThe game proceeded as follows:\\n\\n1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\\n2. The player directed this question to the other players in clockwise order, starting with the player to their left.\\n3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\\n4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\\n5. This continued until either:\\n    a) A player showed a card to the asking player, or\\n    b) All the queried players had stated they didn't have any of the asked-about cards.\\n6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\\n\\nHere is how the game played out:\\n\\nSummer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\\n- Giselle did not have any of the cards\\n- Connor showed Summer a card\\n\\nGiselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\\n- Connor did not have any of the cards\\n- Summer did not have any of the cards\\n\\nConnor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\\n- Summer did not have any of the cards\\n- Giselle showed Connor 'Miss Scarlet'\\n\\nSummer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\\n- Giselle did not have any of the cards\\n- Connor did not have any of the cards\\n\\nAt this point, Giselle was able to correctly infer the solution and win the game.\\n\\nWhat were the facedown cards in the middle of the table?<|im_end|>\\n<|im_start|>assistant\\n\", prompt_token_ids=[128000, 128000, 128002, 882, 198, 1966, 264, 8369, 10683, 1938, 19367, 11, 480, 285, 6853, 323, 58280, 7731, 1523, 311, 1514, 264, 16736, 23347, 1847, 382, 7009, 35105, 220, 18, 30881, 315, 7563, 11, 1855, 369, 264, 8821, 955, 315, 2038, 24306, 315, 279, 2768, 1473, 78524, 1002, 512, 12, 9083, 81818, 198, 12, 4491, 13, 7997, 198, 12, 18083, 13, 5929, 271, 29314, 512, 12, 73997, 30133, 198, 12, 62302, 198, 12, 30982, 28905, 271, 14330, 512, 12, 11166, 198, 12, 50767, 198, 12, 39190, 10637, 271, 6153, 27716, 320, 438, 89447, 8, 19301, 832, 3786, 505, 1855, 1912, 323, 25012, 1124, 304, 279, 6278, 315, 279, 2007, 17011, 785, 11, 814, 75371, 279, 9861, 7563, 323, 27023, 704, 279, 2768, 311, 1855, 2851, 1473, 12, 19367, 25, 220, 17, 7563, 198, 12, 480, 285, 6853, 25, 220, 17, 7563, 4417, 43, 26645, 518, 364, 36412, 81818, 1329, 12, 58280, 25, 220, 17, 7563, 271, 791, 1847, 45374, 439, 11263, 1473, 16, 13, 1952, 872, 2543, 11, 264, 2851, 4691, 922, 264, 743, 315, 7041, 220, 18, 7563, 11, 832, 505, 1855, 315, 279, 1847, 596, 11306, 13, 320, 9290, 25, 25640, 1436, 2610, 922, 904, 7563, 11, 2737, 1884, 304, 872, 1866, 1450, 29275, 17, 13, 578, 2851, 15910, 420, 3488, 311, 279, 1023, 4311, 304, 66770, 2015, 11, 6041, 449, 279, 2851, 311, 872, 2163, 627, 18, 13, 1442, 264, 2851, 1047, 832, 477, 810, 315, 279, 4691, 69205, 7563, 11, 814, 1047, 311, 1501, 832, 315, 1884, 7563, 320, 1073, 872, 5873, 8, 311, 279, 10371, 2851, 38171, 13, 578, 2543, 1243, 9670, 11, 323, 1514, 5946, 311, 279, 1828, 2851, 627, 19, 13, 1442, 264, 2851, 1550, 539, 617, 904, 315, 279, 4691, 69205, 7563, 11, 814, 1071, 779, 11, 323, 279, 3488, 5946, 311, 279, 1828, 2851, 304, 66770, 2015, 627, 20, 13, 1115, 8738, 3156, 3060, 512, 262, 264, 8, 362, 2851, 8710, 264, 3786, 311, 279, 10371, 2851, 11, 477, 198, 262, 293, 8, 2052, 279, 79002, 4311, 1047, 11224, 814, 3287, 956, 617, 904, 315, 279, 4691, 69205, 7563, 627, 21, 13, 4740, 264, 2851, 596, 2543, 9670, 320, 50998, 555, 1694, 6982, 264, 3786, 477, 3515, 682, 79002, 4311, 1522, 705, 1514, 7882, 311, 279, 1828, 2851, 304, 66770, 2015, 382, 8586, 374, 1268, 279, 1847, 6476, 704, 1473, 51787, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 35, 5859, 10637, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 8710, 19367, 264, 3786, 271, 38, 285, 6853, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 43, 26645, 3730, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 271, 57987, 4691, 422, 5606, 1047, 364, 36412, 81818, 6, 477, 364, 34, 3397, 30133, 6, 477, 364, 72945, 3730, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 480, 285, 6853, 8710, 58280, 364, 36412, 81818, 3961, 51787, 4691, 422, 5606, 1047, 364, 12555, 13, 7997, 6, 477, 364, 57505, 6, 477, 364, 72945, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 271, 1688, 420, 1486, 11, 480, 285, 6853, 574, 3025, 311, 12722, 24499, 279, 6425, 323, 3243, 279, 1847, 382, 3923, 1051, 279, 17011, 785, 7563, 304, 279, 6278, 315, 279, 2007, 30, 128003, 198, 128002, 78191, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"Let's analyze the game:\\n\\nAfter the first turn, Summer sees that Connor has a 'Miss Scarlet', which means the cards in the middle of the table cannot be ('Miss Scarlet', 'Candlestick', and 'Hall'), or else Connor would have shown one of those cards to Summer.\\n\\nAfter Giselle's turn, we know that she had a 'Lounge' and had not seen a 'Mrs. White' or a 'Knife'. This means that Summer's cards cannot be ('Mrs. White', 'Knife', and 'Lounge'), otherwise, Giselle would have seen them in Summer's hand. \\n\\nAfter Connor's turn, we know that he had not seen a 'Miss Scarlet', a 'Candlestick', or a 'Hall', which means he must have two other cards. He showed a card to Summer, which means it was not one of the cards he needed, so it must be ('Mrs. White', 'Knife'), or ('Mr. Green', 'Lead Pipe').\\n\\nNow, let's consider the possibilities:\\n\\n- If Connor showed ('Mrs. White', 'Knife') to Summer, then Summer must have ('Mrs. White', 'Knife') or ('Mr. Green', 'Lead Pipe'). In this case, Giselle would have seen ('Mrs. White', 'Mrs. White') or ('Mrs. White', 'Mr. Green'), which is impossible since she only saw 'Miss Scarlet'.\\n- If Connor showed ('Mr. Green', 'Lead Pipe') to Summer, then Summer must have ('Mrs. White', 'Knife') and ('Miss Scarlet', 'Candlestick'). In this case, Giselle would have seen ('Miss Scarlet', 'Mrs. White'), which is possible, so this seems to be the solution.\\n\\nThe facedown cards in the middle of the table are ('Mrs. White', 'Knife', 'Dining Room').\", token_ids=(10267, 596, 24564, 279, 1847, 1473, 6153, 279, 1176, 2543, 11, 19367, 16008, 430, 58280, 706, 264, 364, 36412, 81818, 518, 902, 3445, 279, 7563, 304, 279, 6278, 315, 279, 2007, 4250, 387, 4417, 36412, 81818, 518, 364, 34, 3397, 30133, 518, 323, 364, 72945, 4670, 477, 775, 58280, 1053, 617, 6982, 832, 315, 1884, 7563, 311, 19367, 382, 6153, 480, 285, 6853, 596, 2543, 11, 584, 1440, 430, 1364, 1047, 264, 364, 43, 26645, 6, 323, 1047, 539, 3970, 264, 364, 50329, 13, 5929, 6, 477, 264, 364, 57505, 4527, 1115, 3445, 430, 19367, 596, 7563, 4250, 387, 4417, 50329, 13, 5929, 518, 364, 57505, 518, 323, 364, 43, 26645, 4670, 6062, 11, 480, 285, 6853, 1053, 617, 3970, 1124, 304, 19367, 596, 1450, 13, 4815, 6153, 58280, 596, 2543, 11, 584, 1440, 430, 568, 1047, 539, 3970, 264, 364, 36412, 81818, 518, 264, 364, 34, 3397, 30133, 518, 477, 264, 364, 72945, 518, 902, 3445, 568, 2011, 617, 1403, 1023, 7563, 13, 1283, 8710, 264, 3786, 311, 19367, 11, 902, 3445, 433, 574, 539, 832, 315, 279, 7563, 568, 4460, 11, 779, 433, 2011, 387, 4417, 50329, 13, 5929, 518, 364, 57505, 4670, 477, 4417, 12555, 13, 7997, 518, 364, 54963, 28905, 873, 382, 7184, 11, 1095, 596, 2980, 279, 24525, 1473, 12, 1442, 58280, 8710, 4417, 50329, 13, 5929, 518, 364, 57505, 873, 311, 19367, 11, 1243, 19367, 2011, 617, 4417, 50329, 13, 5929, 518, 364, 57505, 873, 477, 4417, 12555, 13, 7997, 518, 364, 54963, 28905, 1861, 763, 420, 1162, 11, 480, 285, 6853, 1053, 617, 3970, 4417, 50329, 13, 5929, 518, 364, 50329, 13, 5929, 873, 477, 4417, 50329, 13, 5929, 518, 364, 12555, 13, 7997, 4670, 902, 374, 12266, 2533, 1364, 1193, 5602, 364, 36412, 81818, 24482, 12, 1442, 58280, 8710, 4417, 12555, 13, 7997, 518, 364, 54963, 28905, 873, 311, 19367, 11, 1243, 19367, 2011, 617, 4417, 50329, 13, 5929, 518, 364, 57505, 873, 323, 4417, 36412, 81818, 518, 364, 34, 3397, 30133, 1861, 763, 420, 1162, 11, 480, 285, 6853, 1053, 617, 3970, 4417, 36412, 81818, 518, 364, 50329, 13, 5929, 4670, 902, 374, 3284, 11, 779, 420, 5084, 311, 387, 279, 6425, 382, 791, 17011, 785, 7563, 304, 279, 6278, 315, 279, 2007, 527, 4417, 50329, 13, 5929, 518, 364, 57505, 518, 364, 35, 5859, 10637, 1861, 128003), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729722069.1363103, last_token_time=1729722069.1363103, first_scheduled_time=1729722069.1391888, first_token_time=1729722069.199031, time_in_queue=0.002878427505493164, finished_time=1729722074.6116498, scheduler_time=0.02783600099337491, model_forward_time=None, model_execute_time=None), lora_request=None)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_model.train()\n",
    "llm.chat([[dict(role=\"user\", content=prompt)]] * 1, sampling_params=SamplingParams(max_tokens=10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mvllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Dummy loss - just use mean of output\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:345\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer):\n\u001b[1;32m    344\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 345\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: hidden_states,\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m: residual\n\u001b[1;32m    353\u001b[0m     })\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:257\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(\n\u001b[1;32m    256\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    263\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    264\u001b[0m     hidden_states, residual)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:187\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    185\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    186\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(positions, q, k)\n\u001b[0;32m--> 187\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/layer.py:100\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, attn_type)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     92\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     attn_type: AttentionType \u001b[38;5;241m=\u001b[39m AttentionType\u001b[38;5;241m.\u001b[39mDECODER,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_v_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py:586\u001b[0m, in \u001b[0;36mFlashAttentionImpl.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, k_scale, v_scale, attn_type)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# NOTE(woosuk): FlashAttention does not support FP8 KV cache.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m, (\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey/v_scale is not supported in FlashAttention.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 586\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munified_flash_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_soft_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/autograd.py:98\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.autograd_impl\u001b[0;34m(keyset, *args, **keyword_only_args)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautograd_impl\u001b[39m(keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeyword_only_args):\n\u001b[0;32m---> 98\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mGenerated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_only_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/autograd.py:40\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.forward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m     38\u001b[0m keyset \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyset\n\u001b[1;32m     39\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyword_only_args\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_autograd_keyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39m_setup_context_fn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# The Dispatcher will remove args that are equal to their default\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# values from (args, kwargs). We're going to add it back so that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# their setup_context (along with the rest of their operator\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# registrations)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfill_defaults(op\u001b[38;5;241m.\u001b[39m_schema, args, kwargs)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:672\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self_, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredispatch\u001b[39m(self_, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch_boxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:494\u001b[0m, in \u001b[0;36mCustomOpDef._register_to_dispatcher.<locals>.adinplaceorview_impl\u001b[0;34m(keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m                 autograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mincrement_version(v)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39m_AutoDispatchBelowADInplaceOrView():\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opoverload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_ADInplaceOrView_keyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:672\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self_, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredispatch\u001b[39m(self_, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch_boxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:236\u001b[0m, in \u001b[0;36mCustomOpDef.register_kernel.<locals>.inner.<locals>.backend_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackend_impl\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Checks the assumption that outputs cannot alias\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# inputs or other outputs.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     storages \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28mid\u001b[39m(tensor\u001b[38;5;241m.\u001b[39muntyped_storage())\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m iter_tensors(args, kwargs)\n\u001b[1;32m    234\u001b[0m     }\n\u001b[0;32m--> 236\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     tuple_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py:626\u001b[0m, in \u001b[0;36munified_flash_attention\u001b[0;34m(query, key, value, num_heads, head_size, num_kv_heads, kv_cache, kv_cache_dtype, k_scale, v_scale, softmax_scale, window_size, alibi_slopes, logits_soft_cap)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mcustom_op(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm::unified_flash_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    607\u001b[0m                          mutates_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkv_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munified_flash_attention\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m     logits_soft_cap: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    625\u001b[0m     current_metadata \u001b[38;5;241m=\u001b[39m get_forward_context()\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m current_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_metadata, FlashAttentionMetadata)\n\u001b[1;32m    628\u001b[0m     attn_metadata: FlashAttentionMetadata \u001b[38;5;241m=\u001b[39m current_metadata\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# I have this from earlier\n",
    "prompt_token_ids: list[int] = output[0].prompt_token_ids  # type: ignore\n",
    "\n",
    "# Create inputs based on your existing prompt_token_ids\n",
    "input_ids = torch.tensor([prompt_token_ids], dtype=torch.long, device=\"cuda:0\")\n",
    "positions = torch.arange(len(prompt_token_ids), device=\"cuda:0\").unsqueeze(0)\n",
    "kv_caches = [\n",
    "    torch.zeros(\n",
    "        1,\n",
    "        vllm_model.layers[0].self_attn.num_heads,\n",
    "        len(prompt_token_ids),\n",
    "        vllm_model.layers[0].self_attn.head_dim,\n",
    "        device=\"cuda:0\",\n",
    "    )\n",
    "    for _ in range(len(vllm_model.layers))\n",
    "]\n",
    "attn_metadata = {\n",
    "    \"attention_mask\": torch.ones(1, len(prompt_token_ids), device=\"cuda:0\"),\n",
    "    \"sequence_lengths\": torch.tensor([len(prompt_token_ids)], device=\"cuda:0\"),\n",
    "    \"max_sequence_length\": len(prompt_token_ids),\n",
    "}\n",
    "\n",
    "# Try training step\n",
    "optimizer = torch.optim.Adam(vllm_model.parameters())\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "# try:\n",
    "output = vllm_model(input_ids, positions, kv_caches, attn_metadata, None)\n",
    "\n",
    "# Dummy loss - just use mean of output\n",
    "loss = output.mean()\n",
    "\n",
    "# Check if backprop works\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Model is trainable! Gradients computed and parameters updated.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Model is not trainable. Error: {type(e)}\")\n",
    "\n",
    "#     # Let's get more info about what might be wrong\n",
    "#     print(\"\\nChecking parameter properties:\")\n",
    "#     for name, param in model.named_parameters():\n",
    "#         print(f\"{name}:\")\n",
    "#         print(f\"  requires_grad: {param.requires_grad}\")\n",
    "#         print(f\"  grad_fn: {param.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "model_dir = subprocess.run(\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.training.checkpointing import FullModelHFCheckpointer\n",
    "\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "    checkpoint_dir=model_dir,\n",
    "    checkpoint_files=glob.glob(f\"{model_dir}/*.safetensors\"),\n",
    "    output_dir=model_dir,\n",
    "    model_type='LLAMA3' # type: ignore\n",
    ")\n",
    "state_dict = checkpointer.load_checkpoint()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(mapped_vllm_state_dict, assign=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0100,  0.0172,  0.0035,  ...,  0.0012, -0.0167, -0.0111],\n",
       "        [-0.0067,  0.0117,  0.0114,  ..., -0.0090,  0.0092, -0.0007],\n",
       "        [ 0.0142,  0.0093,  0.0083,  ..., -0.0025, -0.0064, -0.0140],\n",
       "        ...,\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077],\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077],\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0100,  0.0172,  0.0035,  ...,  0.0012, -0.0167, -0.0111],\n",
       "        [-0.0067,  0.0117,  0.0114,  ..., -0.0090,  0.0092, -0.0007],\n",
       "        [ 0.0142,  0.0093,  0.0083,  ..., -0.0025, -0.0064, -0.0140],\n",
       "        ...,\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077],\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077],\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_vllm_state_dict[\"output.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "\n",
    "model = llama3_1_8b()\n",
    "model.load_state_dict(state_dict[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128000,\n",
       " 128002,\n",
       " 882,\n",
       " 198,\n",
       " 1966,\n",
       " 264,\n",
       " 8369,\n",
       " 10683,\n",
       " 1938,\n",
       " 19367,\n",
       " 11,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 323,\n",
       " 58280,\n",
       " 7731,\n",
       " 1523,\n",
       " 311,\n",
       " 1514,\n",
       " 264,\n",
       " 16736,\n",
       " 23347,\n",
       " 1847,\n",
       " 382,\n",
       " 7009,\n",
       " 35105,\n",
       " 220,\n",
       " 18,\n",
       " 30881,\n",
       " 315,\n",
       " 7563,\n",
       " 11,\n",
       " 1855,\n",
       " 369,\n",
       " 264,\n",
       " 8821,\n",
       " 955,\n",
       " 315,\n",
       " 2038,\n",
       " 24306,\n",
       " 315,\n",
       " 279,\n",
       " 2768,\n",
       " 1473,\n",
       " 78524,\n",
       " 1002,\n",
       " 512,\n",
       " 12,\n",
       " 9083,\n",
       " 81818,\n",
       " 198,\n",
       " 12,\n",
       " 4491,\n",
       " 13,\n",
       " 7997,\n",
       " 198,\n",
       " 12,\n",
       " 18083,\n",
       " 13,\n",
       " 5929,\n",
       " 271,\n",
       " 29314,\n",
       " 512,\n",
       " 12,\n",
       " 73997,\n",
       " 30133,\n",
       " 198,\n",
       " 12,\n",
       " 62302,\n",
       " 198,\n",
       " 12,\n",
       " 30982,\n",
       " 28905,\n",
       " 271,\n",
       " 14330,\n",
       " 512,\n",
       " 12,\n",
       " 11166,\n",
       " 198,\n",
       " 12,\n",
       " 50767,\n",
       " 198,\n",
       " 12,\n",
       " 39190,\n",
       " 10637,\n",
       " 271,\n",
       " 6153,\n",
       " 27716,\n",
       " 320,\n",
       " 438,\n",
       " 89447,\n",
       " 8,\n",
       " 19301,\n",
       " 832,\n",
       " 3786,\n",
       " 505,\n",
       " 1855,\n",
       " 1912,\n",
       " 323,\n",
       " 25012,\n",
       " 1124,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 17011,\n",
       " 785,\n",
       " 11,\n",
       " 814,\n",
       " 75371,\n",
       " 279,\n",
       " 9861,\n",
       " 7563,\n",
       " 323,\n",
       " 27023,\n",
       " 704,\n",
       " 279,\n",
       " 2768,\n",
       " 311,\n",
       " 1855,\n",
       " 2851,\n",
       " 1473,\n",
       " 12,\n",
       " 19367,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 4417,\n",
       " 43,\n",
       " 26645,\n",
       " 518,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 1329,\n",
       " 12,\n",
       " 58280,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 271,\n",
       " 791,\n",
       " 1847,\n",
       " 45374,\n",
       " 439,\n",
       " 11263,\n",
       " 1473,\n",
       " 16,\n",
       " 13,\n",
       " 1952,\n",
       " 872,\n",
       " 2543,\n",
       " 11,\n",
       " 264,\n",
       " 2851,\n",
       " 4691,\n",
       " 922,\n",
       " 264,\n",
       " 743,\n",
       " 315,\n",
       " 7041,\n",
       " 220,\n",
       " 18,\n",
       " 7563,\n",
       " 11,\n",
       " 832,\n",
       " 505,\n",
       " 1855,\n",
       " 315,\n",
       " 279,\n",
       " 1847,\n",
       " 596,\n",
       " 11306,\n",
       " 13,\n",
       " 320,\n",
       " 9290,\n",
       " 25,\n",
       " 25640,\n",
       " 1436,\n",
       " 2610,\n",
       " 922,\n",
       " 904,\n",
       " 7563,\n",
       " 11,\n",
       " 2737,\n",
       " 1884,\n",
       " 304,\n",
       " 872,\n",
       " 1866,\n",
       " 1450,\n",
       " 29275,\n",
       " 17,\n",
       " 13,\n",
       " 578,\n",
       " 2851,\n",
       " 15910,\n",
       " 420,\n",
       " 3488,\n",
       " 311,\n",
       " 279,\n",
       " 1023,\n",
       " 4311,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 11,\n",
       " 6041,\n",
       " 449,\n",
       " 279,\n",
       " 2851,\n",
       " 311,\n",
       " 872,\n",
       " 2163,\n",
       " 627,\n",
       " 18,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 1047,\n",
       " 832,\n",
       " 477,\n",
       " 810,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1047,\n",
       " 311,\n",
       " 1501,\n",
       " 832,\n",
       " 315,\n",
       " 1884,\n",
       " 7563,\n",
       " 320,\n",
       " 1073,\n",
       " 872,\n",
       " 5873,\n",
       " 8,\n",
       " 311,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 38171,\n",
       " 13,\n",
       " 578,\n",
       " 2543,\n",
       " 1243,\n",
       " 9670,\n",
       " 11,\n",
       " 323,\n",
       " 1514,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 19,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1071,\n",
       " 779,\n",
       " 11,\n",
       " 323,\n",
       " 279,\n",
       " 3488,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 627,\n",
       " 20,\n",
       " 13,\n",
       " 1115,\n",
       " 8738,\n",
       " 3156,\n",
       " 3060,\n",
       " 512,\n",
       " 262,\n",
       " 264,\n",
       " 8,\n",
       " 362,\n",
       " 2851,\n",
       " 8710,\n",
       " 264,\n",
       " 3786,\n",
       " 311,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 11,\n",
       " 477,\n",
       " 198,\n",
       " 262,\n",
       " 293,\n",
       " 8,\n",
       " 2052,\n",
       " 279,\n",
       " 79002,\n",
       " 4311,\n",
       " 1047,\n",
       " 11224,\n",
       " 814,\n",
       " 3287,\n",
       " 956,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 627,\n",
       " 21,\n",
       " 13,\n",
       " 4740,\n",
       " 264,\n",
       " 2851,\n",
       " 596,\n",
       " 2543,\n",
       " 9670,\n",
       " 320,\n",
       " 50998,\n",
       " 555,\n",
       " 1694,\n",
       " 6982,\n",
       " 264,\n",
       " 3786,\n",
       " 477,\n",
       " 3515,\n",
       " 682,\n",
       " 79002,\n",
       " 4311,\n",
       " 1522,\n",
       " 705,\n",
       " 1514,\n",
       " 7882,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 382,\n",
       " 8586,\n",
       " 374,\n",
       " 1268,\n",
       " 279,\n",
       " 1847,\n",
       " 6476,\n",
       " 704,\n",
       " 1473,\n",
       " 51787,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 50329,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 5859,\n",
       " 10637,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 8710,\n",
       " 19367,\n",
       " 264,\n",
       " 3786,\n",
       " 271,\n",
       " 38,\n",
       " 285,\n",
       " 6853,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 50329,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 43,\n",
       " 26645,\n",
       " 3730,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 57987,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 3397,\n",
       " 30133,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 8710,\n",
       " 58280,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 3961,\n",
       " 51787,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 12555,\n",
       " 13,\n",
       " 7997,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 1688,\n",
       " 420,\n",
       " 1486,\n",
       " 11,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 574,\n",
       " 3025,\n",
       " 311,\n",
       " 12722,\n",
       " 24499,\n",
       " 279,\n",
       " 6425,\n",
       " 323,\n",
       " 3243,\n",
       " 279,\n",
       " 1847,\n",
       " 382,\n",
       " 3923,\n",
       " 1051,\n",
       " 279,\n",
       " 17011,\n",
       " 785,\n",
       " 7563,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 30,\n",
       " 128003,\n",
       " 198,\n",
       " 128002,\n",
       " 78191,\n",
       " 198]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens(messages: list[dict]) -> list[int]:\n",
    "    generate = llm.generate\n",
    "\n",
    "    def get_tokens(prompts: list[dict], *args: object, **kwargs: object) -> list[int]:\n",
    "        return llm.get_tokenizer().encode(prompts[0][\"prompt\"])\n",
    "\n",
    "    llm.generate = get_tokens  # type: ignore\n",
    "    tokens = llm.chat(messages)  # type: ignore\n",
    "    llm.generate = generate  # type: ignore\n",
    "    return tokens  # type: ignore\n",
    "\n",
    "\n",
    "get_tokens([dict(role=\"user\", content=prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "\n",
    "destroy_model_parallel()\n",
    "del llm.llm_engine.model_executor.driver_worker  # type: ignore\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move all model parameters and buffers to CUDA\n",
    "for param in model.parameters():\n",
    "    if param.device.type != 'cuda':\n",
    "        param.data = param.data.to('cuda')\n",
    "        \n",
    "for buffer in model.buffers():\n",
    "    if buffer.device.type != 'cuda':\n",
    "        buffer.data = buffer.data.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtune.generation import generate\n",
    "\n",
    "result = generate(\n",
    "    model,\n",
    "    torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])], device=\"cuda\"),\n",
    "    max_generated_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|im_start|>user\n",
      "On a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\n",
      "\n",
      "They assembled 3 decks of cards, each for a separate type of information composed of the following:\n",
      "\n",
      "Suspect:\n",
      "- Miss Scarlet\n",
      "- Mr. Green\n",
      "- Mrs. White\n",
      "\n",
      "Weapon:\n",
      "- Candlestick\n",
      "- Knife\n",
      "- Lead Pipe\n",
      "\n",
      "Room:\n",
      "- Hall\n",
      "- Lounge\n",
      "- Dining Room\n",
      "\n",
      "After randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\n",
      "\n",
      "- Summer: 2 cards\n",
      "- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\n",
      "- Connor: 2 cards\n",
      "\n",
      "The game proceeded as follows:\n",
      "\n",
      "1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\n",
      "2. The player directed this question to the other players in clockwise order, starting with the player to their left.\n",
      "3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\n",
      "4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\n",
      "5. This continued until either:\n",
      "    a) A player showed a card to the asking player, or\n",
      "    b) All the queried players had stated they didn't have any of the asked-about cards.\n",
      "6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\n",
      "\n",
      "Here is how the game played out:\n",
      "\n",
      "Summer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor showed Summer a card\n",
      "\n",
      "Giselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\n",
      "- Connor did not have any of the cards\n",
      "- Summer did not have any of the cards\n",
      "\n",
      "Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\n",
      "- Summer did not have any of the cards\n",
      "- Giselle showed Connor 'Miss Scarlet'\n",
      "\n",
      "Summer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor did not have any of the cards\n",
      "\n",
      "At this point, Giselle was able to correctly infer the solution and win the game.\n",
      "\n",
      "What were the facedown cards in the middle of the table?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " - hatch\n",
      " present allowed\n",
      "\n",
      " the - conc or\n",
      " weren-8\n",
      " and\n",
      "                          \n",
      " of today\n",
      " #- every world?\n",
      "\n",
      " As,\n",
      " Tomorrow, optimeter one were. teams: them\n",
      " a\n",
      " understanding-for\n",
      " the\n",
      " the the\n",
      " telephone\n",
      " \"\n",
      "\n",
      "- some \n",
      " had tricks\n",
      " me ( t, regulated[,\n",
      " completed that\n",
      " had left\n",
      " and\n",
      " Membership the:N Teller\n",
      " top( there Every\n",
      " following things\n",
      "\n",
      " 00- d,A intense entity\n",
      "\n",
      " ignored,the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_tokenizer().decode(result[0].squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questiondefee\n",
      "给 this certain sunny day in, aretaelle, Alex are on to have a game game game.\" had a36 different of cards, each containing themselves different game of clues: of  following:\n",
      "\n",
      "Deckpects cards- Mr Scarlet\n",
      "- Colonel. Green\n",
      "- Mrs. White\n",
      "Weapon:\n",
      "- Pistolstick\n",
      "- Rev\n",
      "- Rope Pipe\n",
      "\n",
      "Room:\n",
      "- Kitchen\n",
      "- Kitchen\n",
      "- Kitchen Room\n",
      "\n",
      "Each each shwithout secretly) selecting a card from each deck, putting them face a center of the table,own, G had them cards cards and drew  the following hands each player:Sus : Sus3 cards (- Giselle: 2 cards\n",
      "randomounge' 'Miss Scarlet')\n",
      "- Connor: 2 cards ('Summer rest was with such:\n",
      "\n",
      "1. Summer each turn, a player can a the specific of  two2 cards ( one from each group the three's three ( ForSus that A could only about the  they even those they front own hand.)\n",
      "2. The other then the question to one person two, a order.\n",
      " starting from the player to their right.3. Each a player had one of more of the asked cards cards in they revealed to reveal it of them cards fromof their choice) to the asker player.\n",
      ". \n",
      " other then ended.\n",
      " and playopl to the next player.\n",
      "4. If none player did青年 have any of the asked-about cards, they responded \", rebuilding the asking was to the next player.\n",
      " clockwise order.\n",
      "5. If process until one:\n",
      "    -. A player saw a card that the asking player ( ending\n",
      "    b) All players asked cards had passed they did't have any of the asked-about cards.\n",
      "\n",
      "6. The every card showed turn,,either by showing shown a card or not their players players pass), they passed to the next player in clockwise order.\n",
      "\n",
      "The is the the game proceeded out:\n",
      "\n",
      "Summer started G anyone had theMiss. White' in 'Knife' or 'Dining Room'.- Giselle, not have any of these cards,- Connor had G the ':Giselle asked if anyone had 'Miss. White startDate or 'C' or 'Hallounge':\n",
      "- Summer did not have any of the cards\n",
      "- Summer said not have any of the cards (Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'D':\n",
      "- Summer did not have any of the cards\n",
      "- Giselle said Summer aL Scarlet'\n",
      "\n",
      "G asked if anyone had 'Mr. Green' or 'Lead' or 'Hall':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor showed not have any of the cards\n",
      "\n",
      "G this point, Connoriselle decided sure to ded ded the identities. announced the game.\n",
      "\n",
      "What cards the threeown cards in the middle of the table?\n",
      "\n",
      " \n",
      "user\n",
      "\n",
      "Since\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tokens = torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])])\n",
    "seq_len = input_tokens.size(1)\n",
    "\n",
    "mask = torch.tril(torch.ones((1, seq_len, seq_len), dtype=torch.bool))\n",
    "input_pos = torch.arange(seq_len).unsqueeze(0).expand(1, seq_len)\n",
    "\n",
    "output = model.forward(input_tokens, mask=mask, input_pos=input_pos)\n",
    "\n",
    "# Handle output whether it's a tensor or list of tensors\n",
    "if isinstance(output, list):\n",
    "    output = output[-1]\n",
    "output = output.squeeze(0)\n",
    "\n",
    "# Apply temperature sampling\n",
    "temperature = 0.7\n",
    "logits = output / temperature\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Sample from the probability distribution\n",
    "predicted_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1).tolist()\n",
    "\n",
    "print(llm.get_tokenizer().decode(predicted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QuestionQuestionee\\nWhat a certain summer day, and aabeelle, I were on to have a game game game.Summer decided a5 boxes of cards, shuffled containing a different game of clue: of  following:\\n\\n1pects cards-  Scarlet\\n- Colonel. Green\\n- Colonel. White\\nWeapon:\\n- Candlestick\\n- Rev\\n- Rev Pipe\\n\\nRoom:\\n- Kitchen\\n- Kitchen\\n- Kitchen Room\\n\\nThe sh shand secretly) selecting a card from each deck, placing them face a center of the table,own, they decided them remaining cards and drew   following:\\n\\n each player:\\n\\nSus : Sus2 Sus from- Giselle: 2 cards\\ncauseounge\\' \\'C Scarlet\\')\\n- Connor: 2 cards (\\'The remaining began with follows:\\n\\n1. Summer their turn, a player could a a specific of cards two2 cards ( one from each group the\\'s three ( ForSus that A could not about the  they including the they the own hand.)\\n2. The player who their question to another person two, the order.\\n starting with the player to their left.\\n3. Each a player knew one of more of the cards cards cards, they revealed to reveal them of them cards toof their choice) to the asker player.\\n.\\n If card then ended.\\n and the passed to the next player.\\n4. If no player did not have any of the asked-about cards, they said \" and and the turn was to the next player.\\n clockwise order.\\n5. If process until all all    -. A player had a card that the asking player, or\\n    b) The players asked cards had passed they did\\'t have any of the asked-about cards.\\n\\n6. If a player showed turn,,either by showing shown a card or by their players players pass), they passed to the next player in clockwise order.\\n\\nSummer\\'s the the game proceeded out:\\n\\nSummer: G anyone had theMiss. White\\', or \\'C\\' or \\'Dining Room\\'.- Giselle said not have any of those asked,- Connor showed Summer the\\'fromGiselle asked if anyone had \\'Miss. White\\' or \\'C\\' or \\'Hallounge\\':\\n- Summer did not have any of the cards\\n- Summer showed not have any of the cards\\n\\nConnor asked if anyone had \\'Miss Scarlet\\' or \\'Candlestick\\' or \\'Hall\\':\\n- Summer did not have any of the cards\\n- Giselle showed Summer aMiss Scarlet\\'\\n\\nG asked if anyone had \\'Mr. Green\\' or \\'Lead\\' or \\'D\\':\\n- Giselle did not have any of the cards\\n- Connor did not have any of the cards\\n\\nG this point, Summeriselle had able to ded ded the remaining. said the game.\\n\\nWhat were the cardsown cards in the middle of the table?\\n\\n \\n\\n\\nuser\\n\\nLet'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14924,\n",
       " 14924,\n",
       " 2176,\n",
       " 198,\n",
       " 3923,\n",
       " 264,\n",
       " 3738,\n",
       " 7474,\n",
       " 1938,\n",
       " 11,\n",
       " 323,\n",
       " 264,\n",
       " 8393,\n",
       " 6853,\n",
       " 11,\n",
       " 358,\n",
       " 1051,\n",
       " 389,\n",
       " 311,\n",
       " 617,\n",
       " 264,\n",
       " 1847,\n",
       " 1847,\n",
       " 1847,\n",
       " 13,\n",
       " 51787,\n",
       " 6773,\n",
       " 264,\n",
       " 20,\n",
       " 15039,\n",
       " 315,\n",
       " 7563,\n",
       " 11,\n",
       " 75371,\n",
       " 8649,\n",
       " 264,\n",
       " 2204,\n",
       " 1847,\n",
       " 315,\n",
       " 31089,\n",
       " 25,\n",
       " 315,\n",
       " 220,\n",
       " 2768,\n",
       " 1473,\n",
       " 16,\n",
       " 8132,\n",
       " 7563,\n",
       " 12,\n",
       " 220,\n",
       " 81818,\n",
       " 198,\n",
       " 12,\n",
       " 52798,\n",
       " 13,\n",
       " 7997,\n",
       " 198,\n",
       " 12,\n",
       " 52798,\n",
       " 13,\n",
       " 5929,\n",
       " 198,\n",
       " 29314,\n",
       " 512,\n",
       " 12,\n",
       " 73997,\n",
       " 30133,\n",
       " 198,\n",
       " 12,\n",
       " 10315,\n",
       " 198,\n",
       " 12,\n",
       " 10315,\n",
       " 28905,\n",
       " 271,\n",
       " 14330,\n",
       " 512,\n",
       " 12,\n",
       " 19915,\n",
       " 198,\n",
       " 12,\n",
       " 19915,\n",
       " 198,\n",
       " 12,\n",
       " 19915,\n",
       " 10637,\n",
       " 271,\n",
       " 791,\n",
       " 559,\n",
       " 559,\n",
       " 438,\n",
       " 42839,\n",
       " 8,\n",
       " 27397,\n",
       " 264,\n",
       " 3786,\n",
       " 505,\n",
       " 1855,\n",
       " 9722,\n",
       " 11,\n",
       " 25012,\n",
       " 1124,\n",
       " 3663,\n",
       " 264,\n",
       " 4219,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 11,\n",
       " 785,\n",
       " 11,\n",
       " 814,\n",
       " 6773,\n",
       " 1124,\n",
       " 9861,\n",
       " 7563,\n",
       " 323,\n",
       " 24465,\n",
       " 220,\n",
       " 220,\n",
       " 2768,\n",
       " 1473,\n",
       " 1855,\n",
       " 2851,\n",
       " 1473,\n",
       " 78524,\n",
       " 220,\n",
       " 25,\n",
       " 16687,\n",
       " 17,\n",
       " 16687,\n",
       " 505,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 198,\n",
       " 1593,\n",
       " 26645,\n",
       " 6,\n",
       " 364,\n",
       " 34,\n",
       " 81818,\n",
       " 1329,\n",
       " 12,\n",
       " 58280,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 4417,\n",
       " 791,\n",
       " 9861,\n",
       " 6137,\n",
       " 449,\n",
       " 11263,\n",
       " 1473,\n",
       " 16,\n",
       " 13,\n",
       " 19367,\n",
       " 872,\n",
       " 2543,\n",
       " 11,\n",
       " 264,\n",
       " 2851,\n",
       " 1436,\n",
       " 264,\n",
       " 264,\n",
       " 3230,\n",
       " 315,\n",
       " 7563,\n",
       " 1403,\n",
       " 17,\n",
       " 7563,\n",
       " 320,\n",
       " 832,\n",
       " 505,\n",
       " 1855,\n",
       " 1912,\n",
       " 279,\n",
       " 220,\n",
       " 596,\n",
       " 2380,\n",
       " 320,\n",
       " 1789,\n",
       " 78524,\n",
       " 430,\n",
       " 362,\n",
       " 1436,\n",
       " 539,\n",
       " 922,\n",
       " 279,\n",
       " 220,\n",
       " 814,\n",
       " 2737,\n",
       " 279,\n",
       " 814,\n",
       " 279,\n",
       " 1866,\n",
       " 1450,\n",
       " 29275,\n",
       " 17,\n",
       " 13,\n",
       " 578,\n",
       " 2851,\n",
       " 889,\n",
       " 872,\n",
       " 3488,\n",
       " 311,\n",
       " 2500,\n",
       " 1732,\n",
       " 1403,\n",
       " 11,\n",
       " 279,\n",
       " 2015,\n",
       " 627,\n",
       " 6041,\n",
       " 449,\n",
       " 279,\n",
       " 2851,\n",
       " 311,\n",
       " 872,\n",
       " 2163,\n",
       " 627,\n",
       " 18,\n",
       " 13,\n",
       " 9062,\n",
       " 264,\n",
       " 2851,\n",
       " 7020,\n",
       " 832,\n",
       " 315,\n",
       " 810,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 7563,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 10675,\n",
       " 311,\n",
       " 16805,\n",
       " 1124,\n",
       " 315,\n",
       " 1124,\n",
       " 7563,\n",
       " 311,\n",
       " 1073,\n",
       " 872,\n",
       " 5873,\n",
       " 8,\n",
       " 311,\n",
       " 279,\n",
       " 113408,\n",
       " 2851,\n",
       " 627,\n",
       " 627,\n",
       " 1442,\n",
       " 3786,\n",
       " 1243,\n",
       " 9670,\n",
       " 627,\n",
       " 323,\n",
       " 279,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 19,\n",
       " 13,\n",
       " 1442,\n",
       " 912,\n",
       " 2851,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1071,\n",
       " 330,\n",
       " 323,\n",
       " 323,\n",
       " 279,\n",
       " 2543,\n",
       " 574,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 66770,\n",
       " 2015,\n",
       " 627,\n",
       " 20,\n",
       " 13,\n",
       " 1442,\n",
       " 1920,\n",
       " 3156,\n",
       " 682,\n",
       " 682,\n",
       " 262,\n",
       " 482,\n",
       " 13,\n",
       " 362,\n",
       " 2851,\n",
       " 1047,\n",
       " 264,\n",
       " 3786,\n",
       " 430,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 11,\n",
       " 477,\n",
       " 198,\n",
       " 262,\n",
       " 293,\n",
       " 8,\n",
       " 578,\n",
       " 4311,\n",
       " 4691,\n",
       " 7563,\n",
       " 1047,\n",
       " 5946,\n",
       " 814,\n",
       " 1550,\n",
       " 956,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 382,\n",
       " 21,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 8710,\n",
       " 2543,\n",
       " 11,\n",
       " 11,\n",
       " 50998,\n",
       " 555,\n",
       " 9204,\n",
       " 6982,\n",
       " 264,\n",
       " 3786,\n",
       " 477,\n",
       " 555,\n",
       " 872,\n",
       " 4311,\n",
       " 4311,\n",
       " 1522,\n",
       " 705,\n",
       " 814,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 382,\n",
       " 51787,\n",
       " 596,\n",
       " 279,\n",
       " 279,\n",
       " 1847,\n",
       " 45374,\n",
       " 704,\n",
       " 1473,\n",
       " 51787,\n",
       " 25,\n",
       " 480,\n",
       " 5606,\n",
       " 1047,\n",
       " 279,\n",
       " 36412,\n",
       " 13,\n",
       " 5929,\n",
       " 518,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 5859,\n",
       " 10637,\n",
       " 4527,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1071,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 1884,\n",
       " 4691,\n",
       " 11,\n",
       " 12,\n",
       " 58280,\n",
       " 8710,\n",
       " 19367,\n",
       " 279,\n",
       " 364,\n",
       " 505,\n",
       " 38,\n",
       " 285,\n",
       " 6853,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 26645,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 19367,\n",
       " 8710,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 57987,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 3397,\n",
       " 30133,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 8710,\n",
       " 19367,\n",
       " 264,\n",
       " 36412,\n",
       " 81818,\n",
       " 3961,\n",
       " 38,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 12555,\n",
       " 13,\n",
       " 7997,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 54963,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 38,\n",
       " 420,\n",
       " 1486,\n",
       " 11,\n",
       " 19367,\n",
       " 285,\n",
       " 6853,\n",
       " 1047,\n",
       " 3025,\n",
       " 311,\n",
       " 7836,\n",
       " 7836,\n",
       " 279,\n",
       " 9861,\n",
       " 13,\n",
       " 1071,\n",
       " 279,\n",
       " 1847,\n",
       " 382,\n",
       " 3923,\n",
       " 1051,\n",
       " 279,\n",
       " 7563,\n",
       " 785,\n",
       " 7563,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 1980,\n",
       " 4815,\n",
       " 198,\n",
       " 882,\n",
       " 198,\n",
       " 198,\n",
       " 10267]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.generation import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tiktoken.load import dump_tiktoken_bpe\n",
    "\n",
    "tokenizer_data = json.load(open(f\"{model_dir}/tokenizer.json\", \"r\"))\n",
    "dump_tiktoken_bpe(\n",
    "    {token.encode(): rank for token, rank in tokenizer_data[\"model\"][\"vocab\"].items()},\n",
    "    f\"{model_dir}/tokenizer.bpe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtune.models.llama3._tokenizer.Llama3Tokenizer at 0x736d0f378500>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.data import ChatMLTemplate\n",
    "from torchtune.models.llama3 import Llama3Tokenizer\n",
    "\n",
    "tokenizer = Llama3Tokenizer(\n",
    "    path=model_dir + \"/tokenizer.bpe\", prompt_template=ChatMLTemplate()  # type: ignore\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer.tt_model.tt_model = Tokenizer.get_instance().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128006, 882, 128007, 271, 9906, 11, 1917, 0, 128009]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.data import Message\n",
    "\n",
    "tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor([tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|begin_of_text|>': 128000,\n",
       " '<|end_of_text|>': 128001,\n",
       " '<|reserved_special_token_0|>': 128002,\n",
       " '<|reserved_special_token_1|>': 128003,\n",
       " '<|finetune_right_pad_id|>': 128004,\n",
       " '<|step_id|>': 128005,\n",
       " '<|start_header_id|>': 128006,\n",
       " '<|end_header_id|>': 128007,\n",
       " '<|eom_id|>': 128008,\n",
       " '<|eot_id|>': 128009,\n",
       " '<|python_tag|>': 128010,\n",
       " '<|image|>': 128256,\n",
       " '<|video|>': 128012,\n",
       " '<|reserved_special_token_2|>': 128013,\n",
       " '<|reserved_special_token_3|>': 128014,\n",
       " '<|reserved_special_token_4|>': 128015,\n",
       " '<|reserved_special_token_5|>': 128016,\n",
       " '<|reserved_special_token_6|>': 128017,\n",
       " '<|reserved_special_token_7|>': 128018,\n",
       " '<|reserved_special_token_8|>': 128019,\n",
       " '<|reserved_special_token_9|>': 128020,\n",
       " '<|reserved_special_token_10|>': 128021,\n",
       " '<|reserved_special_token_11|>': 128022,\n",
       " '<|reserved_special_token_12|>': 128023,\n",
       " '<|reserved_special_token_13|>': 128024,\n",
       " '<|reserved_special_token_14|>': 128025,\n",
       " '<|reserved_special_token_15|>': 128026,\n",
       " '<|reserved_special_token_16|>': 128027,\n",
       " '<|reserved_special_token_17|>': 128028,\n",
       " '<|reserved_special_token_18|>': 128029,\n",
       " '<|reserved_special_token_19|>': 128030,\n",
       " '<|reserved_special_token_20|>': 128031,\n",
       " '<|reserved_special_token_21|>': 128032,\n",
       " '<|reserved_special_token_22|>': 128033,\n",
       " '<|reserved_special_token_23|>': 128034,\n",
       " '<|reserved_special_token_24|>': 128035,\n",
       " '<|reserved_special_token_25|>': 128036,\n",
       " '<|reserved_special_token_26|>': 128037,\n",
       " '<|reserved_special_token_27|>': 128038,\n",
       " '<|reserved_special_token_28|>': 128039,\n",
       " '<|reserved_special_token_29|>': 128040,\n",
       " '<|reserved_special_token_30|>': 128041,\n",
       " '<|reserved_special_token_31|>': 128042,\n",
       " '<|reserved_special_token_32|>': 128043,\n",
       " '<|reserved_special_token_33|>': 128044,\n",
       " '<|reserved_special_token_34|>': 128045,\n",
       " '<|reserved_special_token_35|>': 128046,\n",
       " '<|reserved_special_token_36|>': 128047,\n",
       " '<|reserved_special_token_37|>': 128048,\n",
       " '<|reserved_special_token_38|>': 128049,\n",
       " '<|reserved_special_token_39|>': 128050,\n",
       " '<|reserved_special_token_40|>': 128051,\n",
       " '<|reserved_special_token_41|>': 128052,\n",
       " '<|reserved_special_token_42|>': 128053,\n",
       " '<|reserved_special_token_43|>': 128054,\n",
       " '<|reserved_special_token_44|>': 128055,\n",
       " '<|reserved_special_token_45|>': 128056,\n",
       " '<|reserved_special_token_46|>': 128057,\n",
       " '<|reserved_special_token_47|>': 128058,\n",
       " '<|reserved_special_token_48|>': 128059,\n",
       " '<|reserved_special_token_49|>': 128060,\n",
       " '<|reserved_special_token_50|>': 128061,\n",
       " '<|reserved_special_token_51|>': 128062,\n",
       " '<|reserved_special_token_52|>': 128063,\n",
       " '<|reserved_special_token_53|>': 128064,\n",
       " '<|reserved_special_token_54|>': 128065,\n",
       " '<|reserved_special_token_55|>': 128066,\n",
       " '<|reserved_special_token_56|>': 128067,\n",
       " '<|reserved_special_token_57|>': 128068,\n",
       " '<|reserved_special_token_58|>': 128069,\n",
       " '<|reserved_special_token_59|>': 128070,\n",
       " '<|reserved_special_token_60|>': 128071,\n",
       " '<|reserved_special_token_61|>': 128072,\n",
       " '<|reserved_special_token_62|>': 128073,\n",
       " '<|reserved_special_token_63|>': 128074,\n",
       " '<|reserved_special_token_64|>': 128075,\n",
       " '<|reserved_special_token_65|>': 128076,\n",
       " '<|reserved_special_token_66|>': 128077,\n",
       " '<|reserved_special_token_67|>': 128078,\n",
       " '<|reserved_special_token_68|>': 128079,\n",
       " '<|reserved_special_token_69|>': 128080,\n",
       " '<|reserved_special_token_70|>': 128081,\n",
       " '<|reserved_special_token_71|>': 128082,\n",
       " '<|reserved_special_token_72|>': 128083,\n",
       " '<|reserved_special_token_73|>': 128084,\n",
       " '<|reserved_special_token_74|>': 128085,\n",
       " '<|reserved_special_token_75|>': 128086,\n",
       " '<|reserved_special_token_76|>': 128087,\n",
       " '<|reserved_special_token_77|>': 128088,\n",
       " '<|reserved_special_token_78|>': 128089,\n",
       " '<|reserved_special_token_79|>': 128090,\n",
       " '<|reserved_special_token_80|>': 128091,\n",
       " '<|reserved_special_token_81|>': 128092,\n",
       " '<|reserved_special_token_82|>': 128093,\n",
       " '<|reserved_special_token_83|>': 128094,\n",
       " '<|reserved_special_token_84|>': 128095,\n",
       " '<|reserved_special_token_85|>': 128096,\n",
       " '<|reserved_special_token_86|>': 128097,\n",
       " '<|reserved_special_token_87|>': 128098,\n",
       " '<|reserved_special_token_88|>': 128099,\n",
       " '<|reserved_special_token_89|>': 128100,\n",
       " '<|reserved_special_token_90|>': 128101,\n",
       " '<|reserved_special_token_91|>': 128102,\n",
       " '<|reserved_special_token_92|>': 128103,\n",
       " '<|reserved_special_token_93|>': 128104,\n",
       " '<|reserved_special_token_94|>': 128105,\n",
       " '<|reserved_special_token_95|>': 128106,\n",
       " '<|reserved_special_token_96|>': 128107,\n",
       " '<|reserved_special_token_97|>': 128108,\n",
       " '<|reserved_special_token_98|>': 128109,\n",
       " '<|reserved_special_token_99|>': 128110,\n",
       " '<|reserved_special_token_100|>': 128111,\n",
       " '<|reserved_special_token_101|>': 128112,\n",
       " '<|reserved_special_token_102|>': 128113,\n",
       " '<|reserved_special_token_103|>': 128114,\n",
       " '<|reserved_special_token_104|>': 128115,\n",
       " '<|reserved_special_token_105|>': 128116,\n",
       " '<|reserved_special_token_106|>': 128117,\n",
       " '<|reserved_special_token_107|>': 128118,\n",
       " '<|reserved_special_token_108|>': 128119,\n",
       " '<|reserved_special_token_109|>': 128120,\n",
       " '<|reserved_special_token_110|>': 128121,\n",
       " '<|reserved_special_token_111|>': 128122,\n",
       " '<|reserved_special_token_112|>': 128123,\n",
       " '<|reserved_special_token_113|>': 128124,\n",
       " '<|reserved_special_token_114|>': 128125,\n",
       " '<|reserved_special_token_115|>': 128126,\n",
       " '<|reserved_special_token_116|>': 128127,\n",
       " '<|reserved_special_token_117|>': 128128,\n",
       " '<|reserved_special_token_118|>': 128129,\n",
       " '<|reserved_special_token_119|>': 128130,\n",
       " '<|reserved_special_token_120|>': 128131,\n",
       " '<|reserved_special_token_121|>': 128132,\n",
       " '<|reserved_special_token_122|>': 128133,\n",
       " '<|reserved_special_token_123|>': 128134,\n",
       " '<|reserved_special_token_124|>': 128135,\n",
       " '<|reserved_special_token_125|>': 128136,\n",
       " '<|reserved_special_token_126|>': 128137,\n",
       " '<|reserved_special_token_127|>': 128138,\n",
       " '<|reserved_special_token_128|>': 128139,\n",
       " '<|reserved_special_token_129|>': 128140,\n",
       " '<|reserved_special_token_130|>': 128141,\n",
       " '<|reserved_special_token_131|>': 128142,\n",
       " '<|reserved_special_token_132|>': 128143,\n",
       " '<|reserved_special_token_133|>': 128144,\n",
       " '<|reserved_special_token_134|>': 128145,\n",
       " '<|reserved_special_token_135|>': 128146,\n",
       " '<|reserved_special_token_136|>': 128147,\n",
       " '<|reserved_special_token_137|>': 128148,\n",
       " '<|reserved_special_token_138|>': 128149,\n",
       " '<|reserved_special_token_139|>': 128150,\n",
       " '<|reserved_special_token_140|>': 128151,\n",
       " '<|reserved_special_token_141|>': 128152,\n",
       " '<|reserved_special_token_142|>': 128153,\n",
       " '<|reserved_special_token_143|>': 128154,\n",
       " '<|reserved_special_token_144|>': 128155,\n",
       " '<|reserved_special_token_145|>': 128156,\n",
       " '<|reserved_special_token_146|>': 128157,\n",
       " '<|reserved_special_token_147|>': 128158,\n",
       " '<|reserved_special_token_148|>': 128159,\n",
       " '<|reserved_special_token_149|>': 128160,\n",
       " '<|reserved_special_token_150|>': 128161,\n",
       " '<|reserved_special_token_151|>': 128162,\n",
       " '<|reserved_special_token_152|>': 128163,\n",
       " '<|reserved_special_token_153|>': 128164,\n",
       " '<|reserved_special_token_154|>': 128165,\n",
       " '<|reserved_special_token_155|>': 128166,\n",
       " '<|reserved_special_token_156|>': 128167,\n",
       " '<|reserved_special_token_157|>': 128168,\n",
       " '<|reserved_special_token_158|>': 128169,\n",
       " '<|reserved_special_token_159|>': 128170,\n",
       " '<|reserved_special_token_160|>': 128171,\n",
       " '<|reserved_special_token_161|>': 128172,\n",
       " '<|reserved_special_token_162|>': 128173,\n",
       " '<|reserved_special_token_163|>': 128174,\n",
       " '<|reserved_special_token_164|>': 128175,\n",
       " '<|reserved_special_token_165|>': 128176,\n",
       " '<|reserved_special_token_166|>': 128177,\n",
       " '<|reserved_special_token_167|>': 128178,\n",
       " '<|reserved_special_token_168|>': 128179,\n",
       " '<|reserved_special_token_169|>': 128180,\n",
       " '<|reserved_special_token_170|>': 128181,\n",
       " '<|reserved_special_token_171|>': 128182,\n",
       " '<|reserved_special_token_172|>': 128183,\n",
       " '<|reserved_special_token_173|>': 128184,\n",
       " '<|reserved_special_token_174|>': 128185,\n",
       " '<|reserved_special_token_175|>': 128186,\n",
       " '<|reserved_special_token_176|>': 128187,\n",
       " '<|reserved_special_token_177|>': 128188,\n",
       " '<|reserved_special_token_178|>': 128189,\n",
       " '<|reserved_special_token_179|>': 128190,\n",
       " '<|reserved_special_token_180|>': 128191,\n",
       " '<|reserved_special_token_181|>': 128192,\n",
       " '<|reserved_special_token_182|>': 128193,\n",
       " '<|reserved_special_token_183|>': 128194,\n",
       " '<|reserved_special_token_184|>': 128195,\n",
       " '<|reserved_special_token_185|>': 128196,\n",
       " '<|reserved_special_token_186|>': 128197,\n",
       " '<|reserved_special_token_187|>': 128198,\n",
       " '<|reserved_special_token_188|>': 128199,\n",
       " '<|reserved_special_token_189|>': 128200,\n",
       " '<|reserved_special_token_190|>': 128201,\n",
       " '<|reserved_special_token_191|>': 128202,\n",
       " '<|reserved_special_token_192|>': 128203,\n",
       " '<|reserved_special_token_193|>': 128204,\n",
       " '<|reserved_special_token_194|>': 128205,\n",
       " '<|reserved_special_token_195|>': 128206,\n",
       " '<|reserved_special_token_196|>': 128207,\n",
       " '<|reserved_special_token_197|>': 128208,\n",
       " '<|reserved_special_token_198|>': 128209,\n",
       " '<|reserved_special_token_199|>': 128210,\n",
       " '<|reserved_special_token_200|>': 128211,\n",
       " '<|reserved_special_token_201|>': 128212,\n",
       " '<|reserved_special_token_202|>': 128213,\n",
       " '<|reserved_special_token_203|>': 128214,\n",
       " '<|reserved_special_token_204|>': 128215,\n",
       " '<|reserved_special_token_205|>': 128216,\n",
       " '<|reserved_special_token_206|>': 128217,\n",
       " '<|reserved_special_token_207|>': 128218,\n",
       " '<|reserved_special_token_208|>': 128219,\n",
       " '<|reserved_special_token_209|>': 128220,\n",
       " '<|reserved_special_token_210|>': 128221,\n",
       " '<|reserved_special_token_211|>': 128222,\n",
       " '<|reserved_special_token_212|>': 128223,\n",
       " '<|reserved_special_token_213|>': 128224,\n",
       " '<|reserved_special_token_214|>': 128225,\n",
       " '<|reserved_special_token_215|>': 128226,\n",
       " '<|reserved_special_token_216|>': 128227,\n",
       " '<|reserved_special_token_217|>': 128228,\n",
       " '<|reserved_special_token_218|>': 128229,\n",
       " '<|reserved_special_token_219|>': 128230,\n",
       " '<|reserved_special_token_220|>': 128231,\n",
       " '<|reserved_special_token_221|>': 128232,\n",
       " '<|reserved_special_token_222|>': 128233,\n",
       " '<|reserved_special_token_223|>': 128234,\n",
       " '<|reserved_special_token_224|>': 128235,\n",
       " '<|reserved_special_token_225|>': 128236,\n",
       " '<|reserved_special_token_226|>': 128237,\n",
       " '<|reserved_special_token_227|>': 128238,\n",
       " '<|reserved_special_token_228|>': 128239,\n",
       " '<|reserved_special_token_229|>': 128240,\n",
       " '<|reserved_special_token_230|>': 128241,\n",
       " '<|reserved_special_token_231|>': 128242,\n",
       " '<|reserved_special_token_232|>': 128243,\n",
       " '<|reserved_special_token_233|>': 128244,\n",
       " '<|reserved_special_token_234|>': 128245,\n",
       " '<|reserved_special_token_235|>': 128246,\n",
       " '<|reserved_special_token_236|>': 128247,\n",
       " '<|reserved_special_token_237|>': 128248,\n",
       " '<|reserved_special_token_238|>': 128249,\n",
       " '<|reserved_special_token_239|>': 128250,\n",
       " '<|reserved_special_token_240|>': 128251,\n",
       " '<|reserved_special_token_241|>': 128252,\n",
       " '<|reserved_special_token_242|>': 128253,\n",
       " '<|reserved_special_token_243|>': 128254,\n",
       " '<|reserved_special_token_244|>': 128255}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128001, 128009, 128008]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.stop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize_messages(\n",
    "                [\n",
    "                    Message(role=\"user\", content=\"Hello, world!\", masked=True),\n",
    "                    Message(role=\"assistant\", content=\"\", eot=False),\n",
    "                ],\n",
    "                add_end_tokens=False,\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|im_start|>userHello, world!<|im_end|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<|im_start|>assistantHello there, nice to meet you! How can I assist you today?<|im_end|><|reserved_special_token_1|>utterstock<|reserved_special_token_1|>\\n```\\n<|im_start|>userI am looking for a good way to learn Spanish. Can you recommend any apps or websites?<|im_end|>\\n```\\n\\n<|im_start|>assistantCertainly, I can help you with that!<|im_end|>Here are some popular language learning apps and websites that you might find helpful:\\n\\n1. Duolingo'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.generation import generate\n",
    "\n",
    "\n",
    "tokenizer.tokenize_messages\n",
    "\n",
    "tokens, logits = generate(\n",
    "    model=model,\n",
    "    prompt=torch.tensor(\n",
    "        [\n",
    "            tokenizer.tokenize_messages(\n",
    "                [\n",
    "                    Message(role=\"user\", content=\"Hello, world!\", masked=True),\n",
    "                ],\n",
    "                add_end_tokens=False,\n",
    "            )[0]\n",
    "        ]\n",
    "    ).to(\"cuda\"),\n",
    "    max_generated_tokens=100,\n",
    "    stop_tokens=tokenizer.stop_tokens,\n",
    ")\n",
    "tokenizer.decode(list(tokens[0]), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#\\n\\nI! I! Iitle'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.forward(torch.tensor([tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))]))\n",
    "\n",
    "tokenizer.decode(token_ids=response.argmax(dim=-1).squeeze().tolist(), skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
