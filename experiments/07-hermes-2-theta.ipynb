{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-23 20:51:14 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-23 20:51:15 model_runner.py:1060] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 10-23 20:51:15 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c6fa3e2a20417d912d6ab39733698b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-23 20:51:19 model_runner.py:1071] Loading model weights took 14.9595 GB\n",
      "INFO 10-23 20:51:20 gpu_executor.py:122] # GPU blocks: 13376, # CPU blocks: 2048\n",
      "INFO 10-23 20:51:20 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 26.12x\n",
      "INFO 10-23 20:51:25 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-23 20:51:25 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-23 20:51:39 model_runner.py:1530] Graph capturing finished in 15 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:19<00:00,  9.75s/it, est. speed input: 59.98 toks/s, output: 73.93 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "prompt = \"\"\"\n",
    "On a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\n",
    "\n",
    "They assembled 3 decks of cards, each for a separate type of information composed of the following:\n",
    "\n",
    "Suspect:\n",
    "- Miss Scarlet\n",
    "- Mr. Green\n",
    "- Mrs. White\n",
    "\n",
    "Weapon:\n",
    "- Candlestick\n",
    "- Knife\n",
    "- Lead Pipe\n",
    "\n",
    "Room:\n",
    "- Hall\n",
    "- Lounge\n",
    "- Dining Room\n",
    "\n",
    "After randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\n",
    "\n",
    "- Summer: 2 cards\n",
    "- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\n",
    "- Connor: 2 cards\n",
    "\n",
    "The game proceeded as follows:\n",
    "\n",
    "1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\n",
    "2. The player directed this question to the other players in clockwise order, starting with the player to their left.\n",
    "3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\n",
    "4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\n",
    "5. This continued until either:\n",
    "    a) A player showed a card to the asking player, or\n",
    "    b) All the queried players had stated they didn't have any of the asked-about cards.\n",
    "6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\n",
    "\n",
    "Here is how the game played out:\n",
    "\n",
    "Summer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\n",
    "- Giselle did not have any of the cards\n",
    "- Connor showed Summer a card\n",
    "\n",
    "Giselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\n",
    "- Connor did not have any of the cards\n",
    "- Summer did not have any of the cards\n",
    "\n",
    "Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\n",
    "- Summer did not have any of the cards\n",
    "- Giselle showed Connor 'Miss Scarlet'\n",
    "\n",
    "Summer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\n",
    "- Giselle did not have any of the cards\n",
    "- Connor did not have any of the cards\n",
    "\n",
    "At this point, Giselle was able to correctly infer the solution and win the game.\n",
    "\n",
    "What were the facedown cards in the middle of the table?\n",
    "\"\"\".strip()\n",
    "\n",
    "output = llm.chat([[dict(role=\"user\", content=prompt)]] * 1, sampling_params=SamplingParams(max_tokens=10_000))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm.model_executor.models.llama import LlamaModel\n",
    "\n",
    "# Get the model\n",
    "model: LlamaModel = llm.llm_engine.model_executor.driver_worker.model_runner.model.model  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttentionMetadata(num_prefills=1, num_prefill_tokens=585, num_decode_tokens=0, slot_mapping=tensor([1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083,\n",
      "        1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095,\n",
      "        1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107,\n",
      "        1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119,\n",
      "        1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131,\n",
      "        1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143,\n",
      "        1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155,\n",
      "        1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167,\n",
      "        1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179,\n",
      "        1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "        1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203,\n",
      "        1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215,\n",
      "        1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227,\n",
      "        1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239,\n",
      "        1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251,\n",
      "        1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263,\n",
      "        1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275,\n",
      "        1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287,\n",
      "        1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299,\n",
      "        1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311,\n",
      "        1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323,\n",
      "        1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335,\n",
      "        1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "        1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359,\n",
      "        1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371,\n",
      "        1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383,\n",
      "        1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395,\n",
      "        1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407,\n",
      "        1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419,\n",
      "        1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431,\n",
      "        1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443,\n",
      "        1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455,\n",
      "        1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467,\n",
      "        1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "        1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491,\n",
      "        1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503,\n",
      "        1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531,\n",
      "        1532, 1533, 1534, 1535, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559,\n",
      "        1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1584, 1585, 1586, 1587,\n",
      "        1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599,\n",
      "        1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627,\n",
      "        1628, 1629, 1630, 1631, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655,\n",
      "        1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1680, 1681, 1682, 1683,\n",
      "        1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695,\n",
      "        1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "        1724, 1725, 1726, 1727, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751,\n",
      "        1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1776, 1777, 1778, 1779,\n",
      "        1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791,\n",
      "        1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816], device='cuda:0'), seq_lens=[585], seq_lens_tensor=tensor([585], device='cuda:0', dtype=torch.int32), max_query_len=585, max_decode_query_len=1, max_prefill_seq_len=585, max_decode_seq_len=0, query_start_loc=tensor([  0, 585], device='cuda:0', dtype=torch.int32), seq_start_loc=tensor([  0, 585], device='cuda:0', dtype=torch.int32), context_lens_tensor=tensor([0], device='cuda:0', dtype=torch.int32), block_tables=tensor([], device='cuda:0', size=(1, 0), dtype=torch.int32), use_cuda_graph=False, _cached_prefill_metadata=FlashAttentionMetadata(num_prefills=1, num_prefill_tokens=585, num_decode_tokens=0, slot_mapping=tensor([1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083,\n",
      "        1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095,\n",
      "        1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107,\n",
      "        1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119,\n",
      "        1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131,\n",
      "        1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143,\n",
      "        1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155,\n",
      "        1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167,\n",
      "        1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179,\n",
      "        1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191,\n",
      "        1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203,\n",
      "        1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215,\n",
      "        1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227,\n",
      "        1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239,\n",
      "        1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251,\n",
      "        1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263,\n",
      "        1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275,\n",
      "        1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287,\n",
      "        1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299,\n",
      "        1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311,\n",
      "        1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323,\n",
      "        1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335,\n",
      "        1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
      "        1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359,\n",
      "        1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371,\n",
      "        1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383,\n",
      "        1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395,\n",
      "        1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407,\n",
      "        1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419,\n",
      "        1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431,\n",
      "        1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443,\n",
      "        1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455,\n",
      "        1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467,\n",
      "        1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479,\n",
      "        1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491,\n",
      "        1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503,\n",
      "        1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531,\n",
      "        1532, 1533, 1534, 1535, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559,\n",
      "        1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1584, 1585, 1586, 1587,\n",
      "        1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599,\n",
      "        1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627,\n",
      "        1628, 1629, 1630, 1631, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655,\n",
      "        1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1680, 1681, 1682, 1683,\n",
      "        1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695,\n",
      "        1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723,\n",
      "        1724, 1725, 1726, 1727, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751,\n",
      "        1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1776, 1777, 1778, 1779,\n",
      "        1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791,\n",
      "        1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816], device='cuda:0'), seq_lens=[585], seq_lens_tensor=tensor([585], device='cuda:0', dtype=torch.int32), max_query_len=585, max_decode_query_len=0, max_prefill_seq_len=585, max_decode_seq_len=0, query_start_loc=tensor([  0, 585], device='cuda:0', dtype=torch.int32), seq_start_loc=tensor([  0, 585], device='cuda:0', dtype=torch.int32), context_lens_tensor=tensor([0], device='cuda:0', dtype=torch.int32), block_tables=tensor([], device='cuda:0', size=(1, 0), dtype=torch.int32), use_cuda_graph=False, _cached_prefill_metadata=None, _cached_decode_metadata=None), _cached_decode_metadata=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.64s/it, est. speed input: 39.95 toks/s, output: 41.25 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=3, prompt=\"<|begin_of_text|><|im_start|>user\\nOn a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\\n\\nThey assembled 3 decks of cards, each for a separate type of information composed of the following:\\n\\nSuspect:\\n- Miss Scarlet\\n- Mr. Green\\n- Mrs. White\\n\\nWeapon:\\n- Candlestick\\n- Knife\\n- Lead Pipe\\n\\nRoom:\\n- Hall\\n- Lounge\\n- Dining Room\\n\\nAfter randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\\n\\n- Summer: 2 cards\\n- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\\n- Connor: 2 cards\\n\\nThe game proceeded as follows:\\n\\n1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\\n2. The player directed this question to the other players in clockwise order, starting with the player to their left.\\n3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\\n4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\\n5. This continued until either:\\n    a) A player showed a card to the asking player, or\\n    b) All the queried players had stated they didn't have any of the asked-about cards.\\n6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\\n\\nHere is how the game played out:\\n\\nSummer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\\n- Giselle did not have any of the cards\\n- Connor showed Summer a card\\n\\nGiselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\\n- Connor did not have any of the cards\\n- Summer did not have any of the cards\\n\\nConnor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\\n- Summer did not have any of the cards\\n- Giselle showed Connor 'Miss Scarlet'\\n\\nSummer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\\n- Giselle did not have any of the cards\\n- Connor did not have any of the cards\\n\\nAt this point, Giselle was able to correctly infer the solution and win the game.\\n\\nWhat were the facedown cards in the middle of the table?<|im_end|>\\n<|im_start|>assistant\\n\", prompt_token_ids=[128000, 128000, 128002, 882, 198, 1966, 264, 8369, 10683, 1938, 19367, 11, 480, 285, 6853, 323, 58280, 7731, 1523, 311, 1514, 264, 16736, 23347, 1847, 382, 7009, 35105, 220, 18, 30881, 315, 7563, 11, 1855, 369, 264, 8821, 955, 315, 2038, 24306, 315, 279, 2768, 1473, 78524, 1002, 512, 12, 9083, 81818, 198, 12, 4491, 13, 7997, 198, 12, 18083, 13, 5929, 271, 29314, 512, 12, 73997, 30133, 198, 12, 62302, 198, 12, 30982, 28905, 271, 14330, 512, 12, 11166, 198, 12, 50767, 198, 12, 39190, 10637, 271, 6153, 27716, 320, 438, 89447, 8, 19301, 832, 3786, 505, 1855, 1912, 323, 25012, 1124, 304, 279, 6278, 315, 279, 2007, 17011, 785, 11, 814, 75371, 279, 9861, 7563, 323, 27023, 704, 279, 2768, 311, 1855, 2851, 1473, 12, 19367, 25, 220, 17, 7563, 198, 12, 480, 285, 6853, 25, 220, 17, 7563, 4417, 43, 26645, 518, 364, 36412, 81818, 1329, 12, 58280, 25, 220, 17, 7563, 271, 791, 1847, 45374, 439, 11263, 1473, 16, 13, 1952, 872, 2543, 11, 264, 2851, 4691, 922, 264, 743, 315, 7041, 220, 18, 7563, 11, 832, 505, 1855, 315, 279, 1847, 596, 11306, 13, 320, 9290, 25, 25640, 1436, 2610, 922, 904, 7563, 11, 2737, 1884, 304, 872, 1866, 1450, 29275, 17, 13, 578, 2851, 15910, 420, 3488, 311, 279, 1023, 4311, 304, 66770, 2015, 11, 6041, 449, 279, 2851, 311, 872, 2163, 627, 18, 13, 1442, 264, 2851, 1047, 832, 477, 810, 315, 279, 4691, 69205, 7563, 11, 814, 1047, 311, 1501, 832, 315, 1884, 7563, 320, 1073, 872, 5873, 8, 311, 279, 10371, 2851, 38171, 13, 578, 2543, 1243, 9670, 11, 323, 1514, 5946, 311, 279, 1828, 2851, 627, 19, 13, 1442, 264, 2851, 1550, 539, 617, 904, 315, 279, 4691, 69205, 7563, 11, 814, 1071, 779, 11, 323, 279, 3488, 5946, 311, 279, 1828, 2851, 304, 66770, 2015, 627, 20, 13, 1115, 8738, 3156, 3060, 512, 262, 264, 8, 362, 2851, 8710, 264, 3786, 311, 279, 10371, 2851, 11, 477, 198, 262, 293, 8, 2052, 279, 79002, 4311, 1047, 11224, 814, 3287, 956, 617, 904, 315, 279, 4691, 69205, 7563, 627, 21, 13, 4740, 264, 2851, 596, 2543, 9670, 320, 50998, 555, 1694, 6982, 264, 3786, 477, 3515, 682, 79002, 4311, 1522, 705, 1514, 7882, 311, 279, 1828, 2851, 304, 66770, 2015, 382, 8586, 374, 1268, 279, 1847, 6476, 704, 1473, 51787, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 35, 5859, 10637, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 8710, 19367, 264, 3786, 271, 38, 285, 6853, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 43, 26645, 3730, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 271, 57987, 4691, 422, 5606, 1047, 364, 36412, 81818, 6, 477, 364, 34, 3397, 30133, 6, 477, 364, 72945, 3730, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 480, 285, 6853, 8710, 58280, 364, 36412, 81818, 3961, 51787, 4691, 422, 5606, 1047, 364, 12555, 13, 7997, 6, 477, 364, 57505, 6, 477, 364, 72945, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 271, 1688, 420, 1486, 11, 480, 285, 6853, 574, 3025, 311, 12722, 24499, 279, 6425, 323, 3243, 279, 1847, 382, 3923, 1051, 279, 17011, 785, 7563, 304, 279, 6278, 315, 279, 2007, 30, 128003, 198, 128002, 78191, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"Let's analyze the game step by step:\\n\\n1. Summer asked if anyone had 'Mrs. White', 'Knife', or 'Dining Room'. Giselle did not have any of those cards, and Connor showed Summer a card. This means Connor has either 'Mrs. White', 'Knife', or 'Dining Room'.\\n2. Giselle asked if anyone had 'Mrs. White', 'Knife', or 'Lounge'. Connor did not have any of those cards, and Summer did not have any of those cards. This means neither Connor nor Summer has 'Mrs. White', 'Knife', or 'Lounge'. Since Giselle already had 'Lounge', she knows she has 'Mrs. White' or 'Knife'.\\n3. Connor asked if anyone had 'Miss Scarlet', 'Candlestick', or 'Hall'. Summer did not have any of those cards, and Giselle showed Connor 'Miss Scarlet'. This means Giselle has either 'Candlestick' or 'Hall', but not 'Miss Scarlet'.\\n\\nAt this point, let's summarize what we know about each player's cards:\\n\\n- Connor has either 'Mrs. White', 'Knife', or 'Dining Room'.\\n- Giselle has 'Miss Scarlet' and either 'Candlestick' or 'Hall'.\\n- Summer must have the remaining card: if Connor has 'Mrs. White' or 'Knife', then Summer has 'Dining Room', and if Connor has 'Dining Room', then Summer has 'Mrs. White' or 'Knife'.\\n\\nNow, let's consider what would happen if Connor showed a card when asked about 'Miss Scarlet', 'Candlestick', or 'Hall':\\n\\n- If Connor showed a card from one of those, then Giselle would have the other two cards. Since Giselle doesn't have 'Miss Scarlet', that means she has both 'Candlestick' and 'Hall'. But this would mean Connor doesn't have 'Mrs. White' or 'Knife' (since Giselle doesn't have 'Lounge'), so he should have shown a card when asked about 'Mrs. White', 'Knife', or 'Dining Room', which contradicts the game's record. Therefore, Connor could not show a card when asked about 'Miss Scarlet', 'Candlestick', or 'Hall'.\\n\\nSince Connor did not show a card when asked about 'Miss Scarlet', 'Candlestick', or 'Hall', that means Giselle correctly deduced that Connor has 'Mrs. White' and Summer has 'Knife'. When Summer asked if anyone had 'Mr. Green', 'Knife', or 'Hall', both Giselle and Connor passed because they didn't have any of those cards, which confirms Giselle's deduction.\\n\\nSo, the facedown cards in the middle of the table are:\\n\\n- Suspect: Mrs. White (Connor)\\n- Weapon: Knife (Summer)\\n- Room: Dining Room (unknown)\", token_ids=(10267, 596, 24564, 279, 1847, 3094, 555, 3094, 1473, 16, 13, 19367, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 518, 364, 57505, 518, 477, 364, 35, 5859, 10637, 4527, 480, 285, 6853, 1550, 539, 617, 904, 315, 1884, 7563, 11, 323, 58280, 8710, 19367, 264, 3786, 13, 1115, 3445, 58280, 706, 3060, 364, 50329, 13, 5929, 518, 364, 57505, 518, 477, 364, 35, 5859, 10637, 24482, 17, 13, 480, 285, 6853, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 518, 364, 57505, 518, 477, 364, 43, 26645, 4527, 58280, 1550, 539, 617, 904, 315, 1884, 7563, 11, 323, 19367, 1550, 539, 617, 904, 315, 1884, 7563, 13, 1115, 3445, 14188, 58280, 6463, 19367, 706, 364, 50329, 13, 5929, 518, 364, 57505, 518, 477, 364, 43, 26645, 4527, 8876, 480, 285, 6853, 2736, 1047, 364, 43, 26645, 518, 1364, 8964, 1364, 706, 364, 50329, 13, 5929, 6, 477, 364, 57505, 24482, 18, 13, 58280, 4691, 422, 5606, 1047, 364, 36412, 81818, 518, 364, 34, 3397, 30133, 518, 477, 364, 72945, 4527, 19367, 1550, 539, 617, 904, 315, 1884, 7563, 11, 323, 480, 285, 6853, 8710, 58280, 364, 36412, 81818, 4527, 1115, 3445, 480, 285, 6853, 706, 3060, 364, 34, 3397, 30133, 6, 477, 364, 72945, 518, 719, 539, 364, 36412, 81818, 30736, 1688, 420, 1486, 11, 1095, 596, 63179, 1148, 584, 1440, 922, 1855, 2851, 596, 7563, 1473, 12, 58280, 706, 3060, 364, 50329, 13, 5929, 518, 364, 57505, 518, 477, 364, 35, 5859, 10637, 24482, 12, 480, 285, 6853, 706, 364, 36412, 81818, 6, 323, 3060, 364, 34, 3397, 30133, 6, 477, 364, 72945, 24482, 12, 19367, 2011, 617, 279, 9861, 3786, 25, 422, 58280, 706, 364, 50329, 13, 5929, 6, 477, 364, 57505, 518, 1243, 19367, 706, 364, 35, 5859, 10637, 518, 323, 422, 58280, 706, 364, 35, 5859, 10637, 518, 1243, 19367, 706, 364, 50329, 13, 5929, 6, 477, 364, 57505, 30736, 7184, 11, 1095, 596, 2980, 1148, 1053, 3621, 422, 58280, 8710, 264, 3786, 994, 4691, 922, 364, 36412, 81818, 518, 364, 34, 3397, 30133, 518, 477, 364, 72945, 56530, 12, 1442, 58280, 8710, 264, 3786, 505, 832, 315, 1884, 11, 1243, 480, 285, 6853, 1053, 617, 279, 1023, 1403, 7563, 13, 8876, 480, 285, 6853, 3250, 956, 617, 364, 36412, 81818, 518, 430, 3445, 1364, 706, 2225, 364, 34, 3397, 30133, 6, 323, 364, 72945, 4527, 2030, 420, 1053, 3152, 58280, 3250, 956, 617, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 320, 11536, 480, 285, 6853, 3250, 956, 617, 364, 43, 26645, 4670, 779, 568, 1288, 617, 6982, 264, 3786, 994, 4691, 922, 364, 50329, 13, 5929, 518, 364, 57505, 518, 477, 364, 35, 5859, 10637, 518, 902, 23093, 31095, 279, 1847, 596, 3335, 13, 15636, 11, 58280, 1436, 539, 1501, 264, 3786, 994, 4691, 922, 364, 36412, 81818, 518, 364, 34, 3397, 30133, 518, 477, 364, 72945, 30736, 12834, 58280, 1550, 539, 1501, 264, 3786, 994, 4691, 922, 364, 36412, 81818, 518, 364, 34, 3397, 30133, 518, 477, 364, 72945, 518, 430, 3445, 480, 285, 6853, 12722, 7836, 24921, 430, 58280, 706, 364, 50329, 13, 5929, 6, 323, 19367, 706, 364, 57505, 4527, 3277, 19367, 4691, 422, 5606, 1047, 364, 12555, 13, 7997, 518, 364, 57505, 518, 477, 364, 72945, 518, 2225, 480, 285, 6853, 323, 58280, 5946, 1606, 814, 3287, 956, 617, 904, 315, 1884, 7563, 11, 902, 43496, 480, 285, 6853, 596, 39943, 382, 4516, 11, 279, 17011, 785, 7563, 304, 279, 6278, 315, 279, 2007, 527, 1473, 12, 16687, 1002, 25, 18083, 13, 5929, 320, 57987, 340, 12, 28877, 25, 62302, 320, 51787, 340, 12, 10637, 25, 39190, 10637, 320, 16476, 8, 128003), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729717161.0136287, last_token_time=1729717161.0136287, first_scheduled_time=1729717161.018879, first_token_time=1729717161.136866, time_in_queue=0.005250215530395508, finished_time=1729717175.637966, scheduler_time=0.05214641500424477, model_forward_time=None, model_execute_time=None), lora_request=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm.forward_context import get_forward_context\n",
    "\n",
    "forward = model.forward\n",
    "\n",
    "def hook(*args, **kwargs):\n",
    "    print(get_forward_context())\n",
    "    return forward(*args, **kwargs)\n",
    "\n",
    "model.forward = hook\n",
    "\n",
    "output = llm.chat([[dict(role=\"user\", content=prompt)]] * 1, sampling_params=SamplingParams(max_tokens=10_000))  # type: ignore\n",
    "\n",
    "model.forward = forward\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Dummy loss - just use mean of output\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:345\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer):\n\u001b[1;32m    344\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 345\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: hidden_states,\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m: residual\n\u001b[1;32m    353\u001b[0m     })\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:257\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(\n\u001b[1;32m    256\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    263\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    264\u001b[0m     hidden_states, residual)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:187\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    185\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    186\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(positions, q, k)\n\u001b[0;32m--> 187\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/layer.py:100\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, attn_type)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     92\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     attn_type: AttentionType \u001b[38;5;241m=\u001b[39m AttentionType\u001b[38;5;241m.\u001b[39mDECODER,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_v_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py:586\u001b[0m, in \u001b[0;36mFlashAttentionImpl.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, k_scale, v_scale, attn_type)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# NOTE(woosuk): FlashAttention does not support FP8 KV cache.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m, (\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey/v_scale is not supported in FlashAttention.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 586\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munified_flash_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_soft_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/autograd.py:98\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.autograd_impl\u001b[0;34m(keyset, *args, **keyword_only_args)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautograd_impl\u001b[39m(keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeyword_only_args):\n\u001b[0;32m---> 98\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mGenerated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_only_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/autograd.py:40\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.forward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m     38\u001b[0m keyset \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyset\n\u001b[1;32m     39\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyword_only_args\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_autograd_keyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39m_setup_context_fn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# The Dispatcher will remove args that are equal to their default\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# values from (args, kwargs). We're going to add it back so that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# their setup_context (along with the rest of their operator\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# registrations)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfill_defaults(op\u001b[38;5;241m.\u001b[39m_schema, args, kwargs)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:672\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self_, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredispatch\u001b[39m(self_, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch_boxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:494\u001b[0m, in \u001b[0;36mCustomOpDef._register_to_dispatcher.<locals>.adinplaceorview_impl\u001b[0;34m(keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m                 autograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mincrement_version(v)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39m_AutoDispatchBelowADInplaceOrView():\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opoverload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_ADInplaceOrView_keyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:672\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self_, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredispatch\u001b[39m(self_, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch_boxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:236\u001b[0m, in \u001b[0;36mCustomOpDef.register_kernel.<locals>.inner.<locals>.backend_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackend_impl\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Checks the assumption that outputs cannot alias\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# inputs or other outputs.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     storages \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28mid\u001b[39m(tensor\u001b[38;5;241m.\u001b[39muntyped_storage())\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m iter_tensors(args, kwargs)\n\u001b[1;32m    234\u001b[0m     }\n\u001b[0;32m--> 236\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     tuple_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py:626\u001b[0m, in \u001b[0;36munified_flash_attention\u001b[0;34m(query, key, value, num_heads, head_size, num_kv_heads, kv_cache, kv_cache_dtype, k_scale, v_scale, softmax_scale, window_size, alibi_slopes, logits_soft_cap)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mcustom_op(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm::unified_flash_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    607\u001b[0m                          mutates_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkv_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munified_flash_attention\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m     logits_soft_cap: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    625\u001b[0m     current_metadata \u001b[38;5;241m=\u001b[39m get_forward_context()\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m current_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_metadata, FlashAttentionMetadata)\n\u001b[1;32m    628\u001b[0m     attn_metadata: FlashAttentionMetadata \u001b[38;5;241m=\u001b[39m current_metadata\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# I have this from earlier\n",
    "prompt_token_ids: list[int] = output[0].prompt_token_ids  # type: ignore\n",
    "\n",
    "# Create inputs based on your existing prompt_token_ids\n",
    "input_ids = torch.tensor([prompt_token_ids], dtype=torch.long, device=\"cuda:0\")\n",
    "positions = torch.arange(len(prompt_token_ids), device=\"cuda:0\").unsqueeze(0)\n",
    "kv_caches = [\n",
    "    torch.zeros(\n",
    "        1,\n",
    "        model.layers[0].self_attn.num_heads,\n",
    "        len(prompt_token_ids),\n",
    "        model.layers[0].self_attn.head_dim,\n",
    "        device=\"cuda:0\",\n",
    "    )\n",
    "    for _ in range(len(model.layers))\n",
    "]\n",
    "attn_metadata = {\n",
    "    \"attention_mask\": torch.ones(1, len(prompt_token_ids), device=\"cuda:0\"),\n",
    "    \"sequence_lengths\": torch.tensor([len(prompt_token_ids)], device=\"cuda:0\"),\n",
    "    \"max_sequence_length\": len(prompt_token_ids),\n",
    "}\n",
    "\n",
    "# Try training step\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "# try:\n",
    "output = model(input_ids, positions, kv_caches, attn_metadata, None)\n",
    "\n",
    "# Dummy loss - just use mean of output\n",
    "loss = output.mean()\n",
    "\n",
    "# Check if backprop works\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Model is trainable! Gradients computed and parameters updated.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Model is not trainable. Error: {type(e)}\")\n",
    "\n",
    "#     # Let's get more info about what might be wrong\n",
    "#     print(\"\\nChecking parameter properties:\")\n",
    "#     for name, param in model.named_parameters():\n",
    "#         print(f\"{name}:\")\n",
    "#         print(f\"  requires_grad: {param.requires_grad}\")\n",
    "#         print(f\"  grad_fn: {param.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "model_dir = subprocess.run(\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.training.checkpointing import FullModelHFCheckpointer\n",
    "\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "    checkpoint_dir=model_dir,\n",
    "    checkpoint_files=glob.glob(f\"{model_dir}/*.safetensors\"),\n",
    "    output_dir=model_dir,\n",
    "    model_type='LLAMA3' # type: ignore\n",
    ")\n",
    "state_dict = checkpointer.load_checkpoint()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "\n",
    "model = llama3_1_8b()\n",
    "model.load_state_dict(state_dict[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128000,\n",
       " 128002,\n",
       " 882,\n",
       " 198,\n",
       " 1966,\n",
       " 264,\n",
       " 8369,\n",
       " 10683,\n",
       " 1938,\n",
       " 19367,\n",
       " 11,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 323,\n",
       " 58280,\n",
       " 7731,\n",
       " 1523,\n",
       " 311,\n",
       " 1514,\n",
       " 264,\n",
       " 16736,\n",
       " 23347,\n",
       " 1847,\n",
       " 382,\n",
       " 7009,\n",
       " 35105,\n",
       " 220,\n",
       " 18,\n",
       " 30881,\n",
       " 315,\n",
       " 7563,\n",
       " 11,\n",
       " 1855,\n",
       " 369,\n",
       " 264,\n",
       " 8821,\n",
       " 955,\n",
       " 315,\n",
       " 2038,\n",
       " 24306,\n",
       " 315,\n",
       " 279,\n",
       " 2768,\n",
       " 1473,\n",
       " 78524,\n",
       " 1002,\n",
       " 512,\n",
       " 12,\n",
       " 9083,\n",
       " 81818,\n",
       " 198,\n",
       " 12,\n",
       " 4491,\n",
       " 13,\n",
       " 7997,\n",
       " 198,\n",
       " 12,\n",
       " 18083,\n",
       " 13,\n",
       " 5929,\n",
       " 271,\n",
       " 29314,\n",
       " 512,\n",
       " 12,\n",
       " 73997,\n",
       " 30133,\n",
       " 198,\n",
       " 12,\n",
       " 62302,\n",
       " 198,\n",
       " 12,\n",
       " 30982,\n",
       " 28905,\n",
       " 271,\n",
       " 14330,\n",
       " 512,\n",
       " 12,\n",
       " 11166,\n",
       " 198,\n",
       " 12,\n",
       " 50767,\n",
       " 198,\n",
       " 12,\n",
       " 39190,\n",
       " 10637,\n",
       " 271,\n",
       " 6153,\n",
       " 27716,\n",
       " 320,\n",
       " 438,\n",
       " 89447,\n",
       " 8,\n",
       " 19301,\n",
       " 832,\n",
       " 3786,\n",
       " 505,\n",
       " 1855,\n",
       " 1912,\n",
       " 323,\n",
       " 25012,\n",
       " 1124,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 17011,\n",
       " 785,\n",
       " 11,\n",
       " 814,\n",
       " 75371,\n",
       " 279,\n",
       " 9861,\n",
       " 7563,\n",
       " 323,\n",
       " 27023,\n",
       " 704,\n",
       " 279,\n",
       " 2768,\n",
       " 311,\n",
       " 1855,\n",
       " 2851,\n",
       " 1473,\n",
       " 12,\n",
       " 19367,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 4417,\n",
       " 43,\n",
       " 26645,\n",
       " 518,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 1329,\n",
       " 12,\n",
       " 58280,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 271,\n",
       " 791,\n",
       " 1847,\n",
       " 45374,\n",
       " 439,\n",
       " 11263,\n",
       " 1473,\n",
       " 16,\n",
       " 13,\n",
       " 1952,\n",
       " 872,\n",
       " 2543,\n",
       " 11,\n",
       " 264,\n",
       " 2851,\n",
       " 4691,\n",
       " 922,\n",
       " 264,\n",
       " 743,\n",
       " 315,\n",
       " 7041,\n",
       " 220,\n",
       " 18,\n",
       " 7563,\n",
       " 11,\n",
       " 832,\n",
       " 505,\n",
       " 1855,\n",
       " 315,\n",
       " 279,\n",
       " 1847,\n",
       " 596,\n",
       " 11306,\n",
       " 13,\n",
       " 320,\n",
       " 9290,\n",
       " 25,\n",
       " 25640,\n",
       " 1436,\n",
       " 2610,\n",
       " 922,\n",
       " 904,\n",
       " 7563,\n",
       " 11,\n",
       " 2737,\n",
       " 1884,\n",
       " 304,\n",
       " 872,\n",
       " 1866,\n",
       " 1450,\n",
       " 29275,\n",
       " 17,\n",
       " 13,\n",
       " 578,\n",
       " 2851,\n",
       " 15910,\n",
       " 420,\n",
       " 3488,\n",
       " 311,\n",
       " 279,\n",
       " 1023,\n",
       " 4311,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 11,\n",
       " 6041,\n",
       " 449,\n",
       " 279,\n",
       " 2851,\n",
       " 311,\n",
       " 872,\n",
       " 2163,\n",
       " 627,\n",
       " 18,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 1047,\n",
       " 832,\n",
       " 477,\n",
       " 810,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1047,\n",
       " 311,\n",
       " 1501,\n",
       " 832,\n",
       " 315,\n",
       " 1884,\n",
       " 7563,\n",
       " 320,\n",
       " 1073,\n",
       " 872,\n",
       " 5873,\n",
       " 8,\n",
       " 311,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 38171,\n",
       " 13,\n",
       " 578,\n",
       " 2543,\n",
       " 1243,\n",
       " 9670,\n",
       " 11,\n",
       " 323,\n",
       " 1514,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 19,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1071,\n",
       " 779,\n",
       " 11,\n",
       " 323,\n",
       " 279,\n",
       " 3488,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 627,\n",
       " 20,\n",
       " 13,\n",
       " 1115,\n",
       " 8738,\n",
       " 3156,\n",
       " 3060,\n",
       " 512,\n",
       " 262,\n",
       " 264,\n",
       " 8,\n",
       " 362,\n",
       " 2851,\n",
       " 8710,\n",
       " 264,\n",
       " 3786,\n",
       " 311,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 11,\n",
       " 477,\n",
       " 198,\n",
       " 262,\n",
       " 293,\n",
       " 8,\n",
       " 2052,\n",
       " 279,\n",
       " 79002,\n",
       " 4311,\n",
       " 1047,\n",
       " 11224,\n",
       " 814,\n",
       " 3287,\n",
       " 956,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 627,\n",
       " 21,\n",
       " 13,\n",
       " 4740,\n",
       " 264,\n",
       " 2851,\n",
       " 596,\n",
       " 2543,\n",
       " 9670,\n",
       " 320,\n",
       " 50998,\n",
       " 555,\n",
       " 1694,\n",
       " 6982,\n",
       " 264,\n",
       " 3786,\n",
       " 477,\n",
       " 3515,\n",
       " 682,\n",
       " 79002,\n",
       " 4311,\n",
       " 1522,\n",
       " 705,\n",
       " 1514,\n",
       " 7882,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 382,\n",
       " 8586,\n",
       " 374,\n",
       " 1268,\n",
       " 279,\n",
       " 1847,\n",
       " 6476,\n",
       " 704,\n",
       " 1473,\n",
       " 51787,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 50329,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 5859,\n",
       " 10637,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 8710,\n",
       " 19367,\n",
       " 264,\n",
       " 3786,\n",
       " 271,\n",
       " 38,\n",
       " 285,\n",
       " 6853,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 50329,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 43,\n",
       " 26645,\n",
       " 3730,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 57987,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 3397,\n",
       " 30133,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 8710,\n",
       " 58280,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 3961,\n",
       " 51787,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 12555,\n",
       " 13,\n",
       " 7997,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 1688,\n",
       " 420,\n",
       " 1486,\n",
       " 11,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 574,\n",
       " 3025,\n",
       " 311,\n",
       " 12722,\n",
       " 24499,\n",
       " 279,\n",
       " 6425,\n",
       " 323,\n",
       " 3243,\n",
       " 279,\n",
       " 1847,\n",
       " 382,\n",
       " 3923,\n",
       " 1051,\n",
       " 279,\n",
       " 17011,\n",
       " 785,\n",
       " 7563,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 30,\n",
       " 128003,\n",
       " 198,\n",
       " 128002,\n",
       " 78191,\n",
       " 198]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens(messages: list[dict]) -> list[int]:\n",
    "    generate = llm.generate\n",
    "\n",
    "    def get_tokens(prompts: list[dict], *args: object, **kwargs: object) -> list[int]:\n",
    "        return llm.get_tokenizer().encode(prompts[0][\"prompt\"])\n",
    "\n",
    "    llm.generate = get_tokens  # type: ignore\n",
    "    tokens = llm.chat(messages)  # type: ignore\n",
    "    llm.generate = generate  # type: ignore\n",
    "    return tokens  # type: ignore\n",
    "\n",
    "\n",
    "get_tokens([dict(role=\"user\", content=prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "\n",
    "destroy_model_parallel()\n",
    "del llm.llm_engine.model_executor.driver_worker  # type: ignore\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtune.generation import generate\n",
    "\n",
    "result = generate(\n",
    "    model,\n",
    "    torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])], device=\"cuda\"),\n",
    "    max_generated_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|im_start|>user\n",
      "On a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\n",
      "\n",
      "They assembled 3 decks of cards, each for a separate type of information composed of the following:\n",
      "\n",
      "Suspect:\n",
      "- Miss Scarlet\n",
      "- Mr. Green\n",
      "- Mrs. White\n",
      "\n",
      "Weapon:\n",
      "- Candlestick\n",
      "- Knife\n",
      "- Lead Pipe\n",
      "\n",
      "Room:\n",
      "- Hall\n",
      "- Lounge\n",
      "- Dining Room\n",
      "\n",
      "After randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\n",
      "\n",
      "- Summer: 2 cards\n",
      "- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\n",
      "- Connor: 2 cards\n",
      "\n",
      "The game proceeded as follows:\n",
      "\n",
      "1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\n",
      "2. The player directed this question to the other players in clockwise order, starting with the player to their left.\n",
      "3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\n",
      "4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\n",
      "5. This continued until either:\n",
      "    a) A player showed a card to the asking player, or\n",
      "    b) All the queried players had stated they didn't have any of the asked-about cards.\n",
      "6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\n",
      "\n",
      "Here is how the game played out:\n",
      "\n",
      "Summer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor showed Summer a card\n",
      "\n",
      "Giselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\n",
      "- Connor did not have any of the cards\n",
      "- Summer did not have any of the cards\n",
      "\n",
      "Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\n",
      "- Summer did not have any of the cards\n",
      "- Giselle showed Connor 'Miss Scarlet'\n",
      "\n",
      "Summer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor did not have any of the cards\n",
      "\n",
      "At this point, Giselle was able to correctly infer the solution and win the game.\n",
      "\n",
      "What were the facedown cards in the middle of the table?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's analyze the given information step by step:\n",
      "\n",
      "1. Summer asked for 'Mrs. White', 'Knife', and 'Dining Room'. Since Connor showed a card, he must have had at least one of those cards.\n",
      "2. Giselle asked for 'Mrs. White', 'Knife', and 'Lounge'. She knows that Summer doesn't have 'Mrs. White' or 'Knife' but she doesn't know about the 'Lounge'. Since she doesn't have any of\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_tokenizer().decode(result[0].squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questiondefee\n",
      "给 this certain sunny day in, aretaelle, Alex are on to have a game game game.\" had a36 different of cards, each containing themselves different game of clues: of  following:\n",
      "\n",
      "Deckpects cards- Mr Scarlet\n",
      "- Colonel. Green\n",
      "- Mrs. White\n",
      "Weapon:\n",
      "- Pistolstick\n",
      "- Rev\n",
      "- Rope Pipe\n",
      "\n",
      "Room:\n",
      "- Kitchen\n",
      "- Kitchen\n",
      "- Kitchen Room\n",
      "\n",
      "Each each shwithout secretly) selecting a card from each deck, putting them face a center of the table,own, G had them cards cards and drew  the following hands each player:Sus : Sus3 cards (- Giselle: 2 cards\n",
      "randomounge' 'Miss Scarlet')\n",
      "- Connor: 2 cards ('Summer rest was with such:\n",
      "\n",
      "1. Summer each turn, a player can a the specific of  two2 cards ( one from each group the three's three ( ForSus that A could only about the  they even those they front own hand.)\n",
      "2. The other then the question to one person two, a order.\n",
      " starting from the player to their right.3. Each a player had one of more of the asked cards cards in they revealed to reveal it of them cards fromof their choice) to the asker player.\n",
      ". \n",
      " other then ended.\n",
      " and playopl to the next player.\n",
      "4. If none player did青年 have any of the asked-about cards, they responded \", rebuilding the asking was to the next player.\n",
      " clockwise order.\n",
      "5. If process until one:\n",
      "    -. A player saw a card that the asking player ( ending\n",
      "    b) All players asked cards had passed they did't have any of the asked-about cards.\n",
      "\n",
      "6. The every card showed turn,,either by showing shown a card or not their players players pass), they passed to the next player in clockwise order.\n",
      "\n",
      "The is the the game proceeded out:\n",
      "\n",
      "Summer started G anyone had theMiss. White' in 'Knife' or 'Dining Room'.- Giselle, not have any of these cards,- Connor had G the ':Giselle asked if anyone had 'Miss. White startDate or 'C' or 'Hallounge':\n",
      "- Summer did not have any of the cards\n",
      "- Summer said not have any of the cards (Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'D':\n",
      "- Summer did not have any of the cards\n",
      "- Giselle said Summer aL Scarlet'\n",
      "\n",
      "G asked if anyone had 'Mr. Green' or 'Lead' or 'Hall':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor showed not have any of the cards\n",
      "\n",
      "G this point, Connoriselle decided sure to ded ded the identities. announced the game.\n",
      "\n",
      "What cards the threeown cards in the middle of the table?\n",
      "\n",
      " \n",
      "user\n",
      "\n",
      "Since\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tokens = torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])])\n",
    "seq_len = input_tokens.size(1)\n",
    "\n",
    "mask = torch.tril(torch.ones((1, seq_len, seq_len), dtype=torch.bool))\n",
    "input_pos = torch.arange(seq_len).unsqueeze(0).expand(1, seq_len)\n",
    "\n",
    "output = model.forward(input_tokens, mask=mask, input_pos=input_pos)\n",
    "\n",
    "# Handle output whether it's a tensor or list of tensors\n",
    "if isinstance(output, list):\n",
    "    output = output[-1]\n",
    "output = output.squeeze(0)\n",
    "\n",
    "# Apply temperature sampling\n",
    "temperature = 0.7\n",
    "logits = output / temperature\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Sample from the probability distribution\n",
    "predicted_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1).tolist()\n",
    "\n",
    "print(llm.get_tokenizer().decode(predicted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QuestionQuestionee\\nWhat a certain summer day, and aabeelle, I were on to have a game game game.Summer decided a5 boxes of cards, shuffled containing a different game of clue: of  following:\\n\\n1pects cards-  Scarlet\\n- Colonel. Green\\n- Colonel. White\\nWeapon:\\n- Candlestick\\n- Rev\\n- Rev Pipe\\n\\nRoom:\\n- Kitchen\\n- Kitchen\\n- Kitchen Room\\n\\nThe sh shand secretly) selecting a card from each deck, placing them face a center of the table,own, they decided them remaining cards and drew   following:\\n\\n each player:\\n\\nSus : Sus2 Sus from- Giselle: 2 cards\\ncauseounge\\' \\'C Scarlet\\')\\n- Connor: 2 cards (\\'The remaining began with follows:\\n\\n1. Summer their turn, a player could a a specific of cards two2 cards ( one from each group the\\'s three ( ForSus that A could not about the  they including the they the own hand.)\\n2. The player who their question to another person two, the order.\\n starting with the player to their left.\\n3. Each a player knew one of more of the cards cards cards, they revealed to reveal them of them cards toof their choice) to the asker player.\\n.\\n If card then ended.\\n and the passed to the next player.\\n4. If no player did not have any of the asked-about cards, they said \" and and the turn was to the next player.\\n clockwise order.\\n5. If process until all all    -. A player had a card that the asking player, or\\n    b) The players asked cards had passed they did\\'t have any of the asked-about cards.\\n\\n6. If a player showed turn,,either by showing shown a card or by their players players pass), they passed to the next player in clockwise order.\\n\\nSummer\\'s the the game proceeded out:\\n\\nSummer: G anyone had theMiss. White\\', or \\'C\\' or \\'Dining Room\\'.- Giselle said not have any of those asked,- Connor showed Summer the\\'fromGiselle asked if anyone had \\'Miss. White\\' or \\'C\\' or \\'Hallounge\\':\\n- Summer did not have any of the cards\\n- Summer showed not have any of the cards\\n\\nConnor asked if anyone had \\'Miss Scarlet\\' or \\'Candlestick\\' or \\'Hall\\':\\n- Summer did not have any of the cards\\n- Giselle showed Summer aMiss Scarlet\\'\\n\\nG asked if anyone had \\'Mr. Green\\' or \\'Lead\\' or \\'D\\':\\n- Giselle did not have any of the cards\\n- Connor did not have any of the cards\\n\\nG this point, Summeriselle had able to ded ded the remaining. said the game.\\n\\nWhat were the cardsown cards in the middle of the table?\\n\\n \\n\\n\\nuser\\n\\nLet'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14924,\n",
       " 14924,\n",
       " 2176,\n",
       " 198,\n",
       " 3923,\n",
       " 264,\n",
       " 3738,\n",
       " 7474,\n",
       " 1938,\n",
       " 11,\n",
       " 323,\n",
       " 264,\n",
       " 8393,\n",
       " 6853,\n",
       " 11,\n",
       " 358,\n",
       " 1051,\n",
       " 389,\n",
       " 311,\n",
       " 617,\n",
       " 264,\n",
       " 1847,\n",
       " 1847,\n",
       " 1847,\n",
       " 13,\n",
       " 51787,\n",
       " 6773,\n",
       " 264,\n",
       " 20,\n",
       " 15039,\n",
       " 315,\n",
       " 7563,\n",
       " 11,\n",
       " 75371,\n",
       " 8649,\n",
       " 264,\n",
       " 2204,\n",
       " 1847,\n",
       " 315,\n",
       " 31089,\n",
       " 25,\n",
       " 315,\n",
       " 220,\n",
       " 2768,\n",
       " 1473,\n",
       " 16,\n",
       " 8132,\n",
       " 7563,\n",
       " 12,\n",
       " 220,\n",
       " 81818,\n",
       " 198,\n",
       " 12,\n",
       " 52798,\n",
       " 13,\n",
       " 7997,\n",
       " 198,\n",
       " 12,\n",
       " 52798,\n",
       " 13,\n",
       " 5929,\n",
       " 198,\n",
       " 29314,\n",
       " 512,\n",
       " 12,\n",
       " 73997,\n",
       " 30133,\n",
       " 198,\n",
       " 12,\n",
       " 10315,\n",
       " 198,\n",
       " 12,\n",
       " 10315,\n",
       " 28905,\n",
       " 271,\n",
       " 14330,\n",
       " 512,\n",
       " 12,\n",
       " 19915,\n",
       " 198,\n",
       " 12,\n",
       " 19915,\n",
       " 198,\n",
       " 12,\n",
       " 19915,\n",
       " 10637,\n",
       " 271,\n",
       " 791,\n",
       " 559,\n",
       " 559,\n",
       " 438,\n",
       " 42839,\n",
       " 8,\n",
       " 27397,\n",
       " 264,\n",
       " 3786,\n",
       " 505,\n",
       " 1855,\n",
       " 9722,\n",
       " 11,\n",
       " 25012,\n",
       " 1124,\n",
       " 3663,\n",
       " 264,\n",
       " 4219,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 11,\n",
       " 785,\n",
       " 11,\n",
       " 814,\n",
       " 6773,\n",
       " 1124,\n",
       " 9861,\n",
       " 7563,\n",
       " 323,\n",
       " 24465,\n",
       " 220,\n",
       " 220,\n",
       " 2768,\n",
       " 1473,\n",
       " 1855,\n",
       " 2851,\n",
       " 1473,\n",
       " 78524,\n",
       " 220,\n",
       " 25,\n",
       " 16687,\n",
       " 17,\n",
       " 16687,\n",
       " 505,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 198,\n",
       " 1593,\n",
       " 26645,\n",
       " 6,\n",
       " 364,\n",
       " 34,\n",
       " 81818,\n",
       " 1329,\n",
       " 12,\n",
       " 58280,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 4417,\n",
       " 791,\n",
       " 9861,\n",
       " 6137,\n",
       " 449,\n",
       " 11263,\n",
       " 1473,\n",
       " 16,\n",
       " 13,\n",
       " 19367,\n",
       " 872,\n",
       " 2543,\n",
       " 11,\n",
       " 264,\n",
       " 2851,\n",
       " 1436,\n",
       " 264,\n",
       " 264,\n",
       " 3230,\n",
       " 315,\n",
       " 7563,\n",
       " 1403,\n",
       " 17,\n",
       " 7563,\n",
       " 320,\n",
       " 832,\n",
       " 505,\n",
       " 1855,\n",
       " 1912,\n",
       " 279,\n",
       " 220,\n",
       " 596,\n",
       " 2380,\n",
       " 320,\n",
       " 1789,\n",
       " 78524,\n",
       " 430,\n",
       " 362,\n",
       " 1436,\n",
       " 539,\n",
       " 922,\n",
       " 279,\n",
       " 220,\n",
       " 814,\n",
       " 2737,\n",
       " 279,\n",
       " 814,\n",
       " 279,\n",
       " 1866,\n",
       " 1450,\n",
       " 29275,\n",
       " 17,\n",
       " 13,\n",
       " 578,\n",
       " 2851,\n",
       " 889,\n",
       " 872,\n",
       " 3488,\n",
       " 311,\n",
       " 2500,\n",
       " 1732,\n",
       " 1403,\n",
       " 11,\n",
       " 279,\n",
       " 2015,\n",
       " 627,\n",
       " 6041,\n",
       " 449,\n",
       " 279,\n",
       " 2851,\n",
       " 311,\n",
       " 872,\n",
       " 2163,\n",
       " 627,\n",
       " 18,\n",
       " 13,\n",
       " 9062,\n",
       " 264,\n",
       " 2851,\n",
       " 7020,\n",
       " 832,\n",
       " 315,\n",
       " 810,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 7563,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 10675,\n",
       " 311,\n",
       " 16805,\n",
       " 1124,\n",
       " 315,\n",
       " 1124,\n",
       " 7563,\n",
       " 311,\n",
       " 1073,\n",
       " 872,\n",
       " 5873,\n",
       " 8,\n",
       " 311,\n",
       " 279,\n",
       " 113408,\n",
       " 2851,\n",
       " 627,\n",
       " 627,\n",
       " 1442,\n",
       " 3786,\n",
       " 1243,\n",
       " 9670,\n",
       " 627,\n",
       " 323,\n",
       " 279,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 19,\n",
       " 13,\n",
       " 1442,\n",
       " 912,\n",
       " 2851,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1071,\n",
       " 330,\n",
       " 323,\n",
       " 323,\n",
       " 279,\n",
       " 2543,\n",
       " 574,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 66770,\n",
       " 2015,\n",
       " 627,\n",
       " 20,\n",
       " 13,\n",
       " 1442,\n",
       " 1920,\n",
       " 3156,\n",
       " 682,\n",
       " 682,\n",
       " 262,\n",
       " 482,\n",
       " 13,\n",
       " 362,\n",
       " 2851,\n",
       " 1047,\n",
       " 264,\n",
       " 3786,\n",
       " 430,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 11,\n",
       " 477,\n",
       " 198,\n",
       " 262,\n",
       " 293,\n",
       " 8,\n",
       " 578,\n",
       " 4311,\n",
       " 4691,\n",
       " 7563,\n",
       " 1047,\n",
       " 5946,\n",
       " 814,\n",
       " 1550,\n",
       " 956,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 382,\n",
       " 21,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 8710,\n",
       " 2543,\n",
       " 11,\n",
       " 11,\n",
       " 50998,\n",
       " 555,\n",
       " 9204,\n",
       " 6982,\n",
       " 264,\n",
       " 3786,\n",
       " 477,\n",
       " 555,\n",
       " 872,\n",
       " 4311,\n",
       " 4311,\n",
       " 1522,\n",
       " 705,\n",
       " 814,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 382,\n",
       " 51787,\n",
       " 596,\n",
       " 279,\n",
       " 279,\n",
       " 1847,\n",
       " 45374,\n",
       " 704,\n",
       " 1473,\n",
       " 51787,\n",
       " 25,\n",
       " 480,\n",
       " 5606,\n",
       " 1047,\n",
       " 279,\n",
       " 36412,\n",
       " 13,\n",
       " 5929,\n",
       " 518,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 5859,\n",
       " 10637,\n",
       " 4527,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1071,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 1884,\n",
       " 4691,\n",
       " 11,\n",
       " 12,\n",
       " 58280,\n",
       " 8710,\n",
       " 19367,\n",
       " 279,\n",
       " 364,\n",
       " 505,\n",
       " 38,\n",
       " 285,\n",
       " 6853,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 26645,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 19367,\n",
       " 8710,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 57987,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 3397,\n",
       " 30133,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 8710,\n",
       " 19367,\n",
       " 264,\n",
       " 36412,\n",
       " 81818,\n",
       " 3961,\n",
       " 38,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 12555,\n",
       " 13,\n",
       " 7997,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 54963,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 38,\n",
       " 420,\n",
       " 1486,\n",
       " 11,\n",
       " 19367,\n",
       " 285,\n",
       " 6853,\n",
       " 1047,\n",
       " 3025,\n",
       " 311,\n",
       " 7836,\n",
       " 7836,\n",
       " 279,\n",
       " 9861,\n",
       " 13,\n",
       " 1071,\n",
       " 279,\n",
       " 1847,\n",
       " 382,\n",
       " 3923,\n",
       " 1051,\n",
       " 279,\n",
       " 7563,\n",
       " 785,\n",
       " 7563,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 1980,\n",
       " 4815,\n",
       " 198,\n",
       " 882,\n",
       " 198,\n",
       " 198,\n",
       " 10267]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.generation import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tiktoken.load import dump_tiktoken_bpe\n",
    "\n",
    "tokenizer_data = json.load(open(f\"{model_dir}/tokenizer.json\", \"r\"))\n",
    "dump_tiktoken_bpe(\n",
    "    {token.encode(): rank for token, rank in tokenizer_data[\"model\"][\"vocab\"].items()},\n",
    "    f\"{model_dir}/tokenizer.bpe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtune.models.llama3._tokenizer.Llama3Tokenizer at 0x736d0f378500>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.data import ChatMLTemplate\n",
    "from torchtune.models.llama3 import Llama3Tokenizer\n",
    "\n",
    "tokenizer = Llama3Tokenizer(\n",
    "    path=model_dir + \"/tokenizer.bpe\", prompt_template=ChatMLTemplate()  # type: ignore\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer.tt_model.tt_model = Tokenizer.get_instance().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128006, 882, 128007, 271, 9906, 11, 1917, 0, 128009]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.data import Message\n",
    "\n",
    "tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor([tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|begin_of_text|>': 128000,\n",
       " '<|end_of_text|>': 128001,\n",
       " '<|reserved_special_token_0|>': 128002,\n",
       " '<|reserved_special_token_1|>': 128003,\n",
       " '<|finetune_right_pad_id|>': 128004,\n",
       " '<|step_id|>': 128005,\n",
       " '<|start_header_id|>': 128006,\n",
       " '<|end_header_id|>': 128007,\n",
       " '<|eom_id|>': 128008,\n",
       " '<|eot_id|>': 128009,\n",
       " '<|python_tag|>': 128010,\n",
       " '<|image|>': 128256,\n",
       " '<|video|>': 128012,\n",
       " '<|reserved_special_token_2|>': 128013,\n",
       " '<|reserved_special_token_3|>': 128014,\n",
       " '<|reserved_special_token_4|>': 128015,\n",
       " '<|reserved_special_token_5|>': 128016,\n",
       " '<|reserved_special_token_6|>': 128017,\n",
       " '<|reserved_special_token_7|>': 128018,\n",
       " '<|reserved_special_token_8|>': 128019,\n",
       " '<|reserved_special_token_9|>': 128020,\n",
       " '<|reserved_special_token_10|>': 128021,\n",
       " '<|reserved_special_token_11|>': 128022,\n",
       " '<|reserved_special_token_12|>': 128023,\n",
       " '<|reserved_special_token_13|>': 128024,\n",
       " '<|reserved_special_token_14|>': 128025,\n",
       " '<|reserved_special_token_15|>': 128026,\n",
       " '<|reserved_special_token_16|>': 128027,\n",
       " '<|reserved_special_token_17|>': 128028,\n",
       " '<|reserved_special_token_18|>': 128029,\n",
       " '<|reserved_special_token_19|>': 128030,\n",
       " '<|reserved_special_token_20|>': 128031,\n",
       " '<|reserved_special_token_21|>': 128032,\n",
       " '<|reserved_special_token_22|>': 128033,\n",
       " '<|reserved_special_token_23|>': 128034,\n",
       " '<|reserved_special_token_24|>': 128035,\n",
       " '<|reserved_special_token_25|>': 128036,\n",
       " '<|reserved_special_token_26|>': 128037,\n",
       " '<|reserved_special_token_27|>': 128038,\n",
       " '<|reserved_special_token_28|>': 128039,\n",
       " '<|reserved_special_token_29|>': 128040,\n",
       " '<|reserved_special_token_30|>': 128041,\n",
       " '<|reserved_special_token_31|>': 128042,\n",
       " '<|reserved_special_token_32|>': 128043,\n",
       " '<|reserved_special_token_33|>': 128044,\n",
       " '<|reserved_special_token_34|>': 128045,\n",
       " '<|reserved_special_token_35|>': 128046,\n",
       " '<|reserved_special_token_36|>': 128047,\n",
       " '<|reserved_special_token_37|>': 128048,\n",
       " '<|reserved_special_token_38|>': 128049,\n",
       " '<|reserved_special_token_39|>': 128050,\n",
       " '<|reserved_special_token_40|>': 128051,\n",
       " '<|reserved_special_token_41|>': 128052,\n",
       " '<|reserved_special_token_42|>': 128053,\n",
       " '<|reserved_special_token_43|>': 128054,\n",
       " '<|reserved_special_token_44|>': 128055,\n",
       " '<|reserved_special_token_45|>': 128056,\n",
       " '<|reserved_special_token_46|>': 128057,\n",
       " '<|reserved_special_token_47|>': 128058,\n",
       " '<|reserved_special_token_48|>': 128059,\n",
       " '<|reserved_special_token_49|>': 128060,\n",
       " '<|reserved_special_token_50|>': 128061,\n",
       " '<|reserved_special_token_51|>': 128062,\n",
       " '<|reserved_special_token_52|>': 128063,\n",
       " '<|reserved_special_token_53|>': 128064,\n",
       " '<|reserved_special_token_54|>': 128065,\n",
       " '<|reserved_special_token_55|>': 128066,\n",
       " '<|reserved_special_token_56|>': 128067,\n",
       " '<|reserved_special_token_57|>': 128068,\n",
       " '<|reserved_special_token_58|>': 128069,\n",
       " '<|reserved_special_token_59|>': 128070,\n",
       " '<|reserved_special_token_60|>': 128071,\n",
       " '<|reserved_special_token_61|>': 128072,\n",
       " '<|reserved_special_token_62|>': 128073,\n",
       " '<|reserved_special_token_63|>': 128074,\n",
       " '<|reserved_special_token_64|>': 128075,\n",
       " '<|reserved_special_token_65|>': 128076,\n",
       " '<|reserved_special_token_66|>': 128077,\n",
       " '<|reserved_special_token_67|>': 128078,\n",
       " '<|reserved_special_token_68|>': 128079,\n",
       " '<|reserved_special_token_69|>': 128080,\n",
       " '<|reserved_special_token_70|>': 128081,\n",
       " '<|reserved_special_token_71|>': 128082,\n",
       " '<|reserved_special_token_72|>': 128083,\n",
       " '<|reserved_special_token_73|>': 128084,\n",
       " '<|reserved_special_token_74|>': 128085,\n",
       " '<|reserved_special_token_75|>': 128086,\n",
       " '<|reserved_special_token_76|>': 128087,\n",
       " '<|reserved_special_token_77|>': 128088,\n",
       " '<|reserved_special_token_78|>': 128089,\n",
       " '<|reserved_special_token_79|>': 128090,\n",
       " '<|reserved_special_token_80|>': 128091,\n",
       " '<|reserved_special_token_81|>': 128092,\n",
       " '<|reserved_special_token_82|>': 128093,\n",
       " '<|reserved_special_token_83|>': 128094,\n",
       " '<|reserved_special_token_84|>': 128095,\n",
       " '<|reserved_special_token_85|>': 128096,\n",
       " '<|reserved_special_token_86|>': 128097,\n",
       " '<|reserved_special_token_87|>': 128098,\n",
       " '<|reserved_special_token_88|>': 128099,\n",
       " '<|reserved_special_token_89|>': 128100,\n",
       " '<|reserved_special_token_90|>': 128101,\n",
       " '<|reserved_special_token_91|>': 128102,\n",
       " '<|reserved_special_token_92|>': 128103,\n",
       " '<|reserved_special_token_93|>': 128104,\n",
       " '<|reserved_special_token_94|>': 128105,\n",
       " '<|reserved_special_token_95|>': 128106,\n",
       " '<|reserved_special_token_96|>': 128107,\n",
       " '<|reserved_special_token_97|>': 128108,\n",
       " '<|reserved_special_token_98|>': 128109,\n",
       " '<|reserved_special_token_99|>': 128110,\n",
       " '<|reserved_special_token_100|>': 128111,\n",
       " '<|reserved_special_token_101|>': 128112,\n",
       " '<|reserved_special_token_102|>': 128113,\n",
       " '<|reserved_special_token_103|>': 128114,\n",
       " '<|reserved_special_token_104|>': 128115,\n",
       " '<|reserved_special_token_105|>': 128116,\n",
       " '<|reserved_special_token_106|>': 128117,\n",
       " '<|reserved_special_token_107|>': 128118,\n",
       " '<|reserved_special_token_108|>': 128119,\n",
       " '<|reserved_special_token_109|>': 128120,\n",
       " '<|reserved_special_token_110|>': 128121,\n",
       " '<|reserved_special_token_111|>': 128122,\n",
       " '<|reserved_special_token_112|>': 128123,\n",
       " '<|reserved_special_token_113|>': 128124,\n",
       " '<|reserved_special_token_114|>': 128125,\n",
       " '<|reserved_special_token_115|>': 128126,\n",
       " '<|reserved_special_token_116|>': 128127,\n",
       " '<|reserved_special_token_117|>': 128128,\n",
       " '<|reserved_special_token_118|>': 128129,\n",
       " '<|reserved_special_token_119|>': 128130,\n",
       " '<|reserved_special_token_120|>': 128131,\n",
       " '<|reserved_special_token_121|>': 128132,\n",
       " '<|reserved_special_token_122|>': 128133,\n",
       " '<|reserved_special_token_123|>': 128134,\n",
       " '<|reserved_special_token_124|>': 128135,\n",
       " '<|reserved_special_token_125|>': 128136,\n",
       " '<|reserved_special_token_126|>': 128137,\n",
       " '<|reserved_special_token_127|>': 128138,\n",
       " '<|reserved_special_token_128|>': 128139,\n",
       " '<|reserved_special_token_129|>': 128140,\n",
       " '<|reserved_special_token_130|>': 128141,\n",
       " '<|reserved_special_token_131|>': 128142,\n",
       " '<|reserved_special_token_132|>': 128143,\n",
       " '<|reserved_special_token_133|>': 128144,\n",
       " '<|reserved_special_token_134|>': 128145,\n",
       " '<|reserved_special_token_135|>': 128146,\n",
       " '<|reserved_special_token_136|>': 128147,\n",
       " '<|reserved_special_token_137|>': 128148,\n",
       " '<|reserved_special_token_138|>': 128149,\n",
       " '<|reserved_special_token_139|>': 128150,\n",
       " '<|reserved_special_token_140|>': 128151,\n",
       " '<|reserved_special_token_141|>': 128152,\n",
       " '<|reserved_special_token_142|>': 128153,\n",
       " '<|reserved_special_token_143|>': 128154,\n",
       " '<|reserved_special_token_144|>': 128155,\n",
       " '<|reserved_special_token_145|>': 128156,\n",
       " '<|reserved_special_token_146|>': 128157,\n",
       " '<|reserved_special_token_147|>': 128158,\n",
       " '<|reserved_special_token_148|>': 128159,\n",
       " '<|reserved_special_token_149|>': 128160,\n",
       " '<|reserved_special_token_150|>': 128161,\n",
       " '<|reserved_special_token_151|>': 128162,\n",
       " '<|reserved_special_token_152|>': 128163,\n",
       " '<|reserved_special_token_153|>': 128164,\n",
       " '<|reserved_special_token_154|>': 128165,\n",
       " '<|reserved_special_token_155|>': 128166,\n",
       " '<|reserved_special_token_156|>': 128167,\n",
       " '<|reserved_special_token_157|>': 128168,\n",
       " '<|reserved_special_token_158|>': 128169,\n",
       " '<|reserved_special_token_159|>': 128170,\n",
       " '<|reserved_special_token_160|>': 128171,\n",
       " '<|reserved_special_token_161|>': 128172,\n",
       " '<|reserved_special_token_162|>': 128173,\n",
       " '<|reserved_special_token_163|>': 128174,\n",
       " '<|reserved_special_token_164|>': 128175,\n",
       " '<|reserved_special_token_165|>': 128176,\n",
       " '<|reserved_special_token_166|>': 128177,\n",
       " '<|reserved_special_token_167|>': 128178,\n",
       " '<|reserved_special_token_168|>': 128179,\n",
       " '<|reserved_special_token_169|>': 128180,\n",
       " '<|reserved_special_token_170|>': 128181,\n",
       " '<|reserved_special_token_171|>': 128182,\n",
       " '<|reserved_special_token_172|>': 128183,\n",
       " '<|reserved_special_token_173|>': 128184,\n",
       " '<|reserved_special_token_174|>': 128185,\n",
       " '<|reserved_special_token_175|>': 128186,\n",
       " '<|reserved_special_token_176|>': 128187,\n",
       " '<|reserved_special_token_177|>': 128188,\n",
       " '<|reserved_special_token_178|>': 128189,\n",
       " '<|reserved_special_token_179|>': 128190,\n",
       " '<|reserved_special_token_180|>': 128191,\n",
       " '<|reserved_special_token_181|>': 128192,\n",
       " '<|reserved_special_token_182|>': 128193,\n",
       " '<|reserved_special_token_183|>': 128194,\n",
       " '<|reserved_special_token_184|>': 128195,\n",
       " '<|reserved_special_token_185|>': 128196,\n",
       " '<|reserved_special_token_186|>': 128197,\n",
       " '<|reserved_special_token_187|>': 128198,\n",
       " '<|reserved_special_token_188|>': 128199,\n",
       " '<|reserved_special_token_189|>': 128200,\n",
       " '<|reserved_special_token_190|>': 128201,\n",
       " '<|reserved_special_token_191|>': 128202,\n",
       " '<|reserved_special_token_192|>': 128203,\n",
       " '<|reserved_special_token_193|>': 128204,\n",
       " '<|reserved_special_token_194|>': 128205,\n",
       " '<|reserved_special_token_195|>': 128206,\n",
       " '<|reserved_special_token_196|>': 128207,\n",
       " '<|reserved_special_token_197|>': 128208,\n",
       " '<|reserved_special_token_198|>': 128209,\n",
       " '<|reserved_special_token_199|>': 128210,\n",
       " '<|reserved_special_token_200|>': 128211,\n",
       " '<|reserved_special_token_201|>': 128212,\n",
       " '<|reserved_special_token_202|>': 128213,\n",
       " '<|reserved_special_token_203|>': 128214,\n",
       " '<|reserved_special_token_204|>': 128215,\n",
       " '<|reserved_special_token_205|>': 128216,\n",
       " '<|reserved_special_token_206|>': 128217,\n",
       " '<|reserved_special_token_207|>': 128218,\n",
       " '<|reserved_special_token_208|>': 128219,\n",
       " '<|reserved_special_token_209|>': 128220,\n",
       " '<|reserved_special_token_210|>': 128221,\n",
       " '<|reserved_special_token_211|>': 128222,\n",
       " '<|reserved_special_token_212|>': 128223,\n",
       " '<|reserved_special_token_213|>': 128224,\n",
       " '<|reserved_special_token_214|>': 128225,\n",
       " '<|reserved_special_token_215|>': 128226,\n",
       " '<|reserved_special_token_216|>': 128227,\n",
       " '<|reserved_special_token_217|>': 128228,\n",
       " '<|reserved_special_token_218|>': 128229,\n",
       " '<|reserved_special_token_219|>': 128230,\n",
       " '<|reserved_special_token_220|>': 128231,\n",
       " '<|reserved_special_token_221|>': 128232,\n",
       " '<|reserved_special_token_222|>': 128233,\n",
       " '<|reserved_special_token_223|>': 128234,\n",
       " '<|reserved_special_token_224|>': 128235,\n",
       " '<|reserved_special_token_225|>': 128236,\n",
       " '<|reserved_special_token_226|>': 128237,\n",
       " '<|reserved_special_token_227|>': 128238,\n",
       " '<|reserved_special_token_228|>': 128239,\n",
       " '<|reserved_special_token_229|>': 128240,\n",
       " '<|reserved_special_token_230|>': 128241,\n",
       " '<|reserved_special_token_231|>': 128242,\n",
       " '<|reserved_special_token_232|>': 128243,\n",
       " '<|reserved_special_token_233|>': 128244,\n",
       " '<|reserved_special_token_234|>': 128245,\n",
       " '<|reserved_special_token_235|>': 128246,\n",
       " '<|reserved_special_token_236|>': 128247,\n",
       " '<|reserved_special_token_237|>': 128248,\n",
       " '<|reserved_special_token_238|>': 128249,\n",
       " '<|reserved_special_token_239|>': 128250,\n",
       " '<|reserved_special_token_240|>': 128251,\n",
       " '<|reserved_special_token_241|>': 128252,\n",
       " '<|reserved_special_token_242|>': 128253,\n",
       " '<|reserved_special_token_243|>': 128254,\n",
       " '<|reserved_special_token_244|>': 128255}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128001, 128009, 128008]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.stop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize_messages(\n",
    "                [\n",
    "                    Message(role=\"user\", content=\"Hello, world!\", masked=True),\n",
    "                    Message(role=\"assistant\", content=\"\", eot=False),\n",
    "                ],\n",
    "                add_end_tokens=False,\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|im_start|>userHello, world!<|im_end|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<|im_start|>assistantHello there, nice to meet you! How can I assist you today?<|im_end|><|reserved_special_token_1|>utterstock<|reserved_special_token_1|>\\n```\\n<|im_start|>userI am looking for a good way to learn Spanish. Can you recommend any apps or websites?<|im_end|>\\n```\\n\\n<|im_start|>assistantCertainly, I can help you with that!<|im_end|>Here are some popular language learning apps and websites that you might find helpful:\\n\\n1. Duolingo'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.generation import generate\n",
    "\n",
    "\n",
    "tokenizer.tokenize_messages\n",
    "\n",
    "tokens, logits = generate(\n",
    "    model=model,\n",
    "    prompt=torch.tensor(\n",
    "        [\n",
    "            tokenizer.tokenize_messages(\n",
    "                [\n",
    "                    Message(role=\"user\", content=\"Hello, world!\", masked=True),\n",
    "                ],\n",
    "                add_end_tokens=False,\n",
    "            )[0]\n",
    "        ]\n",
    "    ).to(\"cuda\"),\n",
    "    max_generated_tokens=100,\n",
    "    stop_tokens=tokenizer.stop_tokens,\n",
    ")\n",
    "tokenizer.decode(list(tokens[0]), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#\\n\\nI! I! Iitle'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.forward(torch.tensor([tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))]))\n",
    "\n",
    "tokenizer.decode(token_ids=response.argmax(dim=-1).squeeze().tolist(), skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
