{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b462f0ae7ea54064818af43fa684cbc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/716 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-23 18:26:13 llm_engine.py:60] Initializing an LLM engine with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer_mode=auto, dtype=torch.bfloat16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 10-23 18:26:13 tokenizer.py:28] For some LLaMA-based models, initializing the fast tokenizer may take a long time. To eliminate the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80163d5ae0534d75a338e5f6fd0eedcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f561889cd147419d29e0553ec5cbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc394569c53a4f58b7c196dc86dab912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/worker/worker.py:222: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  tokens_tensor = torch.cuda.LongTensor(input_tokens)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-23 18:26:16 llm_engine.py:131] # GPU blocks: 3090, # CPU blocks: 512\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'accelerate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conversation\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_conversation_template\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/fastchat/model/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_adapter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     load_model,\n\u001b[1;32m      3\u001b[0m     get_conversation_template,\n\u001b[1;32m      4\u001b[0m     add_model_args,\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/fastchat/model/model_adapter.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CPU_ISA\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conversation, get_conv_template\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_compress_model\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllama_condense_monkey_patch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m replace_llama_with_condense\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_chatglm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_stream_chatglm\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/fastchat/model/compression.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_empty_weights\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_module_tensor_to_device\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'accelerate'"
     ]
    }
   ],
   "source": [
    "from fastchat.conversation import Conversation\n",
    "from fastchat.model.model_adapter import get_conversation_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_conversation_template' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_conversation_template\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNousResearch/Hermes-2-Theta-Llama-3-8B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_conversation_template' is not defined"
     ]
    }
   ],
   "source": [
    "get_conversation_template(\"NousResearch/Hermes-2-Theta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='Hello', prompt_token_ids=[128000, 9906], outputs=[CompletionOutput(index=0, text=' AUDIO bathroomimento downloaded(rows quote � Leonard_short pred-------------QAbatis která_ENCOD Garc', token_ids=[53224, 15197, 15377, 24174, 33011, 12929, 107820, 41954, 17087, 4255, 20098, 48622, 37688, 102313, 54369, 30414], cumulative_logprob=-188.18853759765625, logprobs={}, finish_reason=length)], finished=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(Conversation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_dir)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "model_dir = subprocess.run(\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.training.checkpointing import FullModelHFCheckpointer\n",
    "\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "    checkpoint_dir=model_dir,\n",
    "    checkpoint_files=glob.glob(f\"{model_dir}/*.safetensors\"),\n",
    "    output_dir=model_dir,\n",
    "    model_type='LLAMA3' # type: ignore\n",
    ")\n",
    "state_dict = checkpointer.load_checkpoint()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "\n",
    "model = llama3_1_8b()\n",
    "model.load_state_dict(state_dict[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tiktoken.load import dump_tiktoken_bpe\n",
    "\n",
    "tokenizer_data = json.load(open(f\"{model_dir}/tokenizer.json\", \"r\"))\n",
    "dump_tiktoken_bpe(\n",
    "    {token.encode(): rank for token, rank in tokenizer_data[\"model\"][\"vocab\"].items()},\n",
    "    f\"{model_dir}/tokenizer.bpe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtune.models.llama3._tokenizer.Llama3Tokenizer at 0x736d0f378500>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.data import ChatMLTemplate\n",
    "from torchtune.models.llama3 import Llama3Tokenizer\n",
    "\n",
    "tokenizer = Llama3Tokenizer(\n",
    "    path=model_dir + \"/tokenizer.bpe\", prompt_template=ChatMLTemplate()  # type: ignore\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer.tt_model.tt_model = Tokenizer.get_instance().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128006, 882, 128007, 271, 9906, 11, 1917, 0, 128009]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.data import Message\n",
    "\n",
    "tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor([tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|begin_of_text|>': 128000,\n",
       " '<|end_of_text|>': 128001,\n",
       " '<|reserved_special_token_0|>': 128002,\n",
       " '<|reserved_special_token_1|>': 128003,\n",
       " '<|finetune_right_pad_id|>': 128004,\n",
       " '<|step_id|>': 128005,\n",
       " '<|start_header_id|>': 128006,\n",
       " '<|end_header_id|>': 128007,\n",
       " '<|eom_id|>': 128008,\n",
       " '<|eot_id|>': 128009,\n",
       " '<|python_tag|>': 128010,\n",
       " '<|image|>': 128256,\n",
       " '<|video|>': 128012,\n",
       " '<|reserved_special_token_2|>': 128013,\n",
       " '<|reserved_special_token_3|>': 128014,\n",
       " '<|reserved_special_token_4|>': 128015,\n",
       " '<|reserved_special_token_5|>': 128016,\n",
       " '<|reserved_special_token_6|>': 128017,\n",
       " '<|reserved_special_token_7|>': 128018,\n",
       " '<|reserved_special_token_8|>': 128019,\n",
       " '<|reserved_special_token_9|>': 128020,\n",
       " '<|reserved_special_token_10|>': 128021,\n",
       " '<|reserved_special_token_11|>': 128022,\n",
       " '<|reserved_special_token_12|>': 128023,\n",
       " '<|reserved_special_token_13|>': 128024,\n",
       " '<|reserved_special_token_14|>': 128025,\n",
       " '<|reserved_special_token_15|>': 128026,\n",
       " '<|reserved_special_token_16|>': 128027,\n",
       " '<|reserved_special_token_17|>': 128028,\n",
       " '<|reserved_special_token_18|>': 128029,\n",
       " '<|reserved_special_token_19|>': 128030,\n",
       " '<|reserved_special_token_20|>': 128031,\n",
       " '<|reserved_special_token_21|>': 128032,\n",
       " '<|reserved_special_token_22|>': 128033,\n",
       " '<|reserved_special_token_23|>': 128034,\n",
       " '<|reserved_special_token_24|>': 128035,\n",
       " '<|reserved_special_token_25|>': 128036,\n",
       " '<|reserved_special_token_26|>': 128037,\n",
       " '<|reserved_special_token_27|>': 128038,\n",
       " '<|reserved_special_token_28|>': 128039,\n",
       " '<|reserved_special_token_29|>': 128040,\n",
       " '<|reserved_special_token_30|>': 128041,\n",
       " '<|reserved_special_token_31|>': 128042,\n",
       " '<|reserved_special_token_32|>': 128043,\n",
       " '<|reserved_special_token_33|>': 128044,\n",
       " '<|reserved_special_token_34|>': 128045,\n",
       " '<|reserved_special_token_35|>': 128046,\n",
       " '<|reserved_special_token_36|>': 128047,\n",
       " '<|reserved_special_token_37|>': 128048,\n",
       " '<|reserved_special_token_38|>': 128049,\n",
       " '<|reserved_special_token_39|>': 128050,\n",
       " '<|reserved_special_token_40|>': 128051,\n",
       " '<|reserved_special_token_41|>': 128052,\n",
       " '<|reserved_special_token_42|>': 128053,\n",
       " '<|reserved_special_token_43|>': 128054,\n",
       " '<|reserved_special_token_44|>': 128055,\n",
       " '<|reserved_special_token_45|>': 128056,\n",
       " '<|reserved_special_token_46|>': 128057,\n",
       " '<|reserved_special_token_47|>': 128058,\n",
       " '<|reserved_special_token_48|>': 128059,\n",
       " '<|reserved_special_token_49|>': 128060,\n",
       " '<|reserved_special_token_50|>': 128061,\n",
       " '<|reserved_special_token_51|>': 128062,\n",
       " '<|reserved_special_token_52|>': 128063,\n",
       " '<|reserved_special_token_53|>': 128064,\n",
       " '<|reserved_special_token_54|>': 128065,\n",
       " '<|reserved_special_token_55|>': 128066,\n",
       " '<|reserved_special_token_56|>': 128067,\n",
       " '<|reserved_special_token_57|>': 128068,\n",
       " '<|reserved_special_token_58|>': 128069,\n",
       " '<|reserved_special_token_59|>': 128070,\n",
       " '<|reserved_special_token_60|>': 128071,\n",
       " '<|reserved_special_token_61|>': 128072,\n",
       " '<|reserved_special_token_62|>': 128073,\n",
       " '<|reserved_special_token_63|>': 128074,\n",
       " '<|reserved_special_token_64|>': 128075,\n",
       " '<|reserved_special_token_65|>': 128076,\n",
       " '<|reserved_special_token_66|>': 128077,\n",
       " '<|reserved_special_token_67|>': 128078,\n",
       " '<|reserved_special_token_68|>': 128079,\n",
       " '<|reserved_special_token_69|>': 128080,\n",
       " '<|reserved_special_token_70|>': 128081,\n",
       " '<|reserved_special_token_71|>': 128082,\n",
       " '<|reserved_special_token_72|>': 128083,\n",
       " '<|reserved_special_token_73|>': 128084,\n",
       " '<|reserved_special_token_74|>': 128085,\n",
       " '<|reserved_special_token_75|>': 128086,\n",
       " '<|reserved_special_token_76|>': 128087,\n",
       " '<|reserved_special_token_77|>': 128088,\n",
       " '<|reserved_special_token_78|>': 128089,\n",
       " '<|reserved_special_token_79|>': 128090,\n",
       " '<|reserved_special_token_80|>': 128091,\n",
       " '<|reserved_special_token_81|>': 128092,\n",
       " '<|reserved_special_token_82|>': 128093,\n",
       " '<|reserved_special_token_83|>': 128094,\n",
       " '<|reserved_special_token_84|>': 128095,\n",
       " '<|reserved_special_token_85|>': 128096,\n",
       " '<|reserved_special_token_86|>': 128097,\n",
       " '<|reserved_special_token_87|>': 128098,\n",
       " '<|reserved_special_token_88|>': 128099,\n",
       " '<|reserved_special_token_89|>': 128100,\n",
       " '<|reserved_special_token_90|>': 128101,\n",
       " '<|reserved_special_token_91|>': 128102,\n",
       " '<|reserved_special_token_92|>': 128103,\n",
       " '<|reserved_special_token_93|>': 128104,\n",
       " '<|reserved_special_token_94|>': 128105,\n",
       " '<|reserved_special_token_95|>': 128106,\n",
       " '<|reserved_special_token_96|>': 128107,\n",
       " '<|reserved_special_token_97|>': 128108,\n",
       " '<|reserved_special_token_98|>': 128109,\n",
       " '<|reserved_special_token_99|>': 128110,\n",
       " '<|reserved_special_token_100|>': 128111,\n",
       " '<|reserved_special_token_101|>': 128112,\n",
       " '<|reserved_special_token_102|>': 128113,\n",
       " '<|reserved_special_token_103|>': 128114,\n",
       " '<|reserved_special_token_104|>': 128115,\n",
       " '<|reserved_special_token_105|>': 128116,\n",
       " '<|reserved_special_token_106|>': 128117,\n",
       " '<|reserved_special_token_107|>': 128118,\n",
       " '<|reserved_special_token_108|>': 128119,\n",
       " '<|reserved_special_token_109|>': 128120,\n",
       " '<|reserved_special_token_110|>': 128121,\n",
       " '<|reserved_special_token_111|>': 128122,\n",
       " '<|reserved_special_token_112|>': 128123,\n",
       " '<|reserved_special_token_113|>': 128124,\n",
       " '<|reserved_special_token_114|>': 128125,\n",
       " '<|reserved_special_token_115|>': 128126,\n",
       " '<|reserved_special_token_116|>': 128127,\n",
       " '<|reserved_special_token_117|>': 128128,\n",
       " '<|reserved_special_token_118|>': 128129,\n",
       " '<|reserved_special_token_119|>': 128130,\n",
       " '<|reserved_special_token_120|>': 128131,\n",
       " '<|reserved_special_token_121|>': 128132,\n",
       " '<|reserved_special_token_122|>': 128133,\n",
       " '<|reserved_special_token_123|>': 128134,\n",
       " '<|reserved_special_token_124|>': 128135,\n",
       " '<|reserved_special_token_125|>': 128136,\n",
       " '<|reserved_special_token_126|>': 128137,\n",
       " '<|reserved_special_token_127|>': 128138,\n",
       " '<|reserved_special_token_128|>': 128139,\n",
       " '<|reserved_special_token_129|>': 128140,\n",
       " '<|reserved_special_token_130|>': 128141,\n",
       " '<|reserved_special_token_131|>': 128142,\n",
       " '<|reserved_special_token_132|>': 128143,\n",
       " '<|reserved_special_token_133|>': 128144,\n",
       " '<|reserved_special_token_134|>': 128145,\n",
       " '<|reserved_special_token_135|>': 128146,\n",
       " '<|reserved_special_token_136|>': 128147,\n",
       " '<|reserved_special_token_137|>': 128148,\n",
       " '<|reserved_special_token_138|>': 128149,\n",
       " '<|reserved_special_token_139|>': 128150,\n",
       " '<|reserved_special_token_140|>': 128151,\n",
       " '<|reserved_special_token_141|>': 128152,\n",
       " '<|reserved_special_token_142|>': 128153,\n",
       " '<|reserved_special_token_143|>': 128154,\n",
       " '<|reserved_special_token_144|>': 128155,\n",
       " '<|reserved_special_token_145|>': 128156,\n",
       " '<|reserved_special_token_146|>': 128157,\n",
       " '<|reserved_special_token_147|>': 128158,\n",
       " '<|reserved_special_token_148|>': 128159,\n",
       " '<|reserved_special_token_149|>': 128160,\n",
       " '<|reserved_special_token_150|>': 128161,\n",
       " '<|reserved_special_token_151|>': 128162,\n",
       " '<|reserved_special_token_152|>': 128163,\n",
       " '<|reserved_special_token_153|>': 128164,\n",
       " '<|reserved_special_token_154|>': 128165,\n",
       " '<|reserved_special_token_155|>': 128166,\n",
       " '<|reserved_special_token_156|>': 128167,\n",
       " '<|reserved_special_token_157|>': 128168,\n",
       " '<|reserved_special_token_158|>': 128169,\n",
       " '<|reserved_special_token_159|>': 128170,\n",
       " '<|reserved_special_token_160|>': 128171,\n",
       " '<|reserved_special_token_161|>': 128172,\n",
       " '<|reserved_special_token_162|>': 128173,\n",
       " '<|reserved_special_token_163|>': 128174,\n",
       " '<|reserved_special_token_164|>': 128175,\n",
       " '<|reserved_special_token_165|>': 128176,\n",
       " '<|reserved_special_token_166|>': 128177,\n",
       " '<|reserved_special_token_167|>': 128178,\n",
       " '<|reserved_special_token_168|>': 128179,\n",
       " '<|reserved_special_token_169|>': 128180,\n",
       " '<|reserved_special_token_170|>': 128181,\n",
       " '<|reserved_special_token_171|>': 128182,\n",
       " '<|reserved_special_token_172|>': 128183,\n",
       " '<|reserved_special_token_173|>': 128184,\n",
       " '<|reserved_special_token_174|>': 128185,\n",
       " '<|reserved_special_token_175|>': 128186,\n",
       " '<|reserved_special_token_176|>': 128187,\n",
       " '<|reserved_special_token_177|>': 128188,\n",
       " '<|reserved_special_token_178|>': 128189,\n",
       " '<|reserved_special_token_179|>': 128190,\n",
       " '<|reserved_special_token_180|>': 128191,\n",
       " '<|reserved_special_token_181|>': 128192,\n",
       " '<|reserved_special_token_182|>': 128193,\n",
       " '<|reserved_special_token_183|>': 128194,\n",
       " '<|reserved_special_token_184|>': 128195,\n",
       " '<|reserved_special_token_185|>': 128196,\n",
       " '<|reserved_special_token_186|>': 128197,\n",
       " '<|reserved_special_token_187|>': 128198,\n",
       " '<|reserved_special_token_188|>': 128199,\n",
       " '<|reserved_special_token_189|>': 128200,\n",
       " '<|reserved_special_token_190|>': 128201,\n",
       " '<|reserved_special_token_191|>': 128202,\n",
       " '<|reserved_special_token_192|>': 128203,\n",
       " '<|reserved_special_token_193|>': 128204,\n",
       " '<|reserved_special_token_194|>': 128205,\n",
       " '<|reserved_special_token_195|>': 128206,\n",
       " '<|reserved_special_token_196|>': 128207,\n",
       " '<|reserved_special_token_197|>': 128208,\n",
       " '<|reserved_special_token_198|>': 128209,\n",
       " '<|reserved_special_token_199|>': 128210,\n",
       " '<|reserved_special_token_200|>': 128211,\n",
       " '<|reserved_special_token_201|>': 128212,\n",
       " '<|reserved_special_token_202|>': 128213,\n",
       " '<|reserved_special_token_203|>': 128214,\n",
       " '<|reserved_special_token_204|>': 128215,\n",
       " '<|reserved_special_token_205|>': 128216,\n",
       " '<|reserved_special_token_206|>': 128217,\n",
       " '<|reserved_special_token_207|>': 128218,\n",
       " '<|reserved_special_token_208|>': 128219,\n",
       " '<|reserved_special_token_209|>': 128220,\n",
       " '<|reserved_special_token_210|>': 128221,\n",
       " '<|reserved_special_token_211|>': 128222,\n",
       " '<|reserved_special_token_212|>': 128223,\n",
       " '<|reserved_special_token_213|>': 128224,\n",
       " '<|reserved_special_token_214|>': 128225,\n",
       " '<|reserved_special_token_215|>': 128226,\n",
       " '<|reserved_special_token_216|>': 128227,\n",
       " '<|reserved_special_token_217|>': 128228,\n",
       " '<|reserved_special_token_218|>': 128229,\n",
       " '<|reserved_special_token_219|>': 128230,\n",
       " '<|reserved_special_token_220|>': 128231,\n",
       " '<|reserved_special_token_221|>': 128232,\n",
       " '<|reserved_special_token_222|>': 128233,\n",
       " '<|reserved_special_token_223|>': 128234,\n",
       " '<|reserved_special_token_224|>': 128235,\n",
       " '<|reserved_special_token_225|>': 128236,\n",
       " '<|reserved_special_token_226|>': 128237,\n",
       " '<|reserved_special_token_227|>': 128238,\n",
       " '<|reserved_special_token_228|>': 128239,\n",
       " '<|reserved_special_token_229|>': 128240,\n",
       " '<|reserved_special_token_230|>': 128241,\n",
       " '<|reserved_special_token_231|>': 128242,\n",
       " '<|reserved_special_token_232|>': 128243,\n",
       " '<|reserved_special_token_233|>': 128244,\n",
       " '<|reserved_special_token_234|>': 128245,\n",
       " '<|reserved_special_token_235|>': 128246,\n",
       " '<|reserved_special_token_236|>': 128247,\n",
       " '<|reserved_special_token_237|>': 128248,\n",
       " '<|reserved_special_token_238|>': 128249,\n",
       " '<|reserved_special_token_239|>': 128250,\n",
       " '<|reserved_special_token_240|>': 128251,\n",
       " '<|reserved_special_token_241|>': 128252,\n",
       " '<|reserved_special_token_242|>': 128253,\n",
       " '<|reserved_special_token_243|>': 128254,\n",
       " '<|reserved_special_token_244|>': 128255}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128001, 128009, 128008]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.stop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize_messages(\n",
    "                [\n",
    "                    Message(role=\"user\", content=\"Hello, world!\", masked=True),\n",
    "                    Message(role=\"assistant\", content=\"\", eot=False),\n",
    "                ],\n",
    "                add_end_tokens=False,\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|im_start|>userHello, world!<|im_end|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<|im_start|>assistantHello there, nice to meet you! How can I assist you today?<|im_end|><|reserved_special_token_1|>utterstock<|reserved_special_token_1|>\\n```\\n<|im_start|>userI am looking for a good way to learn Spanish. Can you recommend any apps or websites?<|im_end|>\\n```\\n\\n<|im_start|>assistantCertainly, I can help you with that!<|im_end|>Here are some popular language learning apps and websites that you might find helpful:\\n\\n1. Duolingo'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.generation import generate\n",
    "\n",
    "\n",
    "tokenizer.tokenize_messages\n",
    "\n",
    "tokens, logits = generate(\n",
    "    model=model,\n",
    "    prompt=torch.tensor(\n",
    "        [\n",
    "            tokenizer.tokenize_messages(\n",
    "                [\n",
    "                    Message(role=\"user\", content=\"Hello, world!\", masked=True),\n",
    "                ],\n",
    "                add_end_tokens=False,\n",
    "            )[0]\n",
    "        ]\n",
    "    ).to(\"cuda\"),\n",
    "    max_generated_tokens=100,\n",
    "    stop_tokens=tokenizer.stop_tokens,\n",
    ")\n",
    "tokenizer.decode(list(tokens[0]), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#\\n\\nI! I! Iitle'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.forward(torch.tensor([tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))]))\n",
    "\n",
    "tokenizer.decode(token_ids=response.argmax(dim=-1).squeeze().tolist(), skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
