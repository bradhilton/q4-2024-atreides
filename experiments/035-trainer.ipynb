{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.langfuse import langfuse\n",
    "# langfuse.enabled = False\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "import random\n",
    "import re\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class TemporalCluePuzzle(TypedDict):\n",
    "    num_clues: int\n",
    "    prompt: str\n",
    "    solution: dict[str, str]\n",
    "\n",
    "\n",
    "temporal_clue_puzzles: list[TemporalCluePuzzle] = json.load(\n",
    "    open(\"./data/temporal-clue-puzzles.json\")\n",
    ")\n",
    "random.seed(42)\n",
    "random.shuffle(temporal_clue_puzzles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_thought_examples: list[dict[str, str]] = json.load(\n",
    "    open(\"./data/chain-of-thought-examples.json\")\n",
    ")\n",
    "chain_of_thought_examples.pop(6)\n",
    "chain_of_thought_examples.pop(3)\n",
    "\n",
    "def get_episode(puzzle: TemporalCluePuzzle) -> Episode:\n",
    "\n",
    "    def validate(completion: EpisodeCompletion) -> None:\n",
    "        ...\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in completions:\n",
    "            content = completion.last_assistant_message.get(\"content\")\n",
    "            assert isinstance(content, str)\n",
    "            num_correct = 0\n",
    "            for key, value in puzzle[\"solution\"].items():\n",
    "                if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "                    match = matches[-1]\n",
    "                    if match.strip().lower() == value.lower():\n",
    "                        num_correct += 1\n",
    "            completion.commit(reward=num_correct / len(puzzle[\"solution\"]))\n",
    "            \n",
    "    example = random.choices(chain_of_thought_examples, k=1)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": puzzle[\"prompt\"]\n",
    "                .replace(\n",
    "                    \"Fill out your final answers in the following format:\",\n",
    "                    \"After verifiably finding all the correct answers, fill out your final answers in the following format:\",\n",
    "                )\n",
    "                ,\n",
    "            },\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": \"Let's think this through step by step...\",\n",
    "            # },\n",
    "        ],\n",
    "        on_sample=on_sample,\n",
    "        examples=[\n",
    "            {\"role\": \"user\", \"content\": example[0][\"prompt\"]},\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": example[0][\"chain_of_thought\"]\n",
    "                + (example[0][\"answer\"] and f\"\\n\\n---\\n\\n{example[0]['answer']}\"),\n",
    "            },\n",
    "            # {\"role\": \"user\", \"content\": example[1][\"prompt\"]},\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": example[1][\"chain_of_thought\"] \n",
    "            #     + (example[1][\"answer\"] and f\"\\n\\n---\\n\\n{example[1]['answer']}\"),\n",
    "            # },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "temporal_clue_episodes = [get_episode(puzzle) for puzzle in temporal_clue_puzzles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_clue_episodes[64:] = [\n",
    "    get_episode(puzzle)\n",
    "    for puzzle in json.load(open(\"./data/temporal-clue-puzzles-2.json\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "zebra_grid_questions = pl.read_parquet(\n",
    "    \"hf://datasets/allenai/ZebraLogicBench-private/grid_mode/test-00000-of-00001.parquet\"\n",
    ").to_dicts()\n",
    "random.shuffle(zebra_grid_questions)\n",
    "\n",
    "\n",
    "def get_episode(question: dict) -> Episode:\n",
    "    prompt = f\"\"\"{question[\"puzzle\"]}\n",
    "Fill in the grid with the correct values:\n",
    "\n",
    "| {' | '.join(question[\"solution\"][\"header\"])} |\n",
    "| {' | '.join([\"-\" * len(header) for header in question[\"solution\"][\"header\"]])} |\n",
    "\"\"\"\n",
    "\n",
    "    for _ in question[\"solution\"][\"rows\"]:\n",
    "        prompt += f\"| {' | '.join([\" \" * len(header) for header in question[\"solution\"][\"header\"]])} |\\n\"\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"\\| \" + r\"\\|\".join(r\"(.*?)\" for _ in question[\"solution\"][\"header\"]) + r\" \\|\"\n",
    "    )\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]):\n",
    "        for completion in completions:\n",
    "            assert \"content\" in completion.last_assistant_message and isinstance(\n",
    "                completion.last_assistant_message[\"content\"], str\n",
    "            )\n",
    "            num_cells = sum(len(row) for row in question[\"solution\"][\"rows\"])\n",
    "            num_correct = 0\n",
    "            for match, row in zip(\n",
    "                re.findall(pattern, completion.last_assistant_message[\"content\"])[\n",
    "                    -len(question[\"solution\"][\"rows\"]) :\n",
    "                ],\n",
    "                question[\"solution\"][\"rows\"],\n",
    "            ):\n",
    "                for cell, value in zip(match, row):\n",
    "                    if cell.strip().lower() == value.lower():\n",
    "                        num_correct += 1\n",
    "            completion.commit(reward=num_correct / num_cells)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "zebra_grid_episodes = [get_episode(question) for question in zebra_grid_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "math_questions = list(\n",
    "    load_dataset(\"lighteval/MATH\", \"all\")[\"train\"].to_iterable_dataset()  # type: ignore\n",
    ")\n",
    "random.shuffle(math_questions)\n",
    "\n",
    "\n",
    "question_solution = None\n",
    "pattern = re.compile(r\"\\\\boxed{([^}]+)}\")\n",
    "\n",
    "\n",
    "def get_episode(question: dict) -> Episode:\n",
    "    prompt = (\n",
    "        f\"{question['problem']}\\n\\n\"\n",
    "        \"Solve this math problem and show your work. Your final answer MUST be \"\n",
    "        \"formatted in a LaTeX box using \\\\boxed{{}}. For example: \"\n",
    "        \"$1+1=\\\\boxed{{2}}$\\n\\n\"\n",
    "        \"You can submit multiple attempts. Each attempt should end with a boxed \"\n",
    "        \"answer. Your last answer will be weighted the most, but you can get \"\n",
    "        \"partial credit if an earlier answer is correct. If after multiple \"\n",
    "        \"attempts you decide an earlier answer is the correct one, just submit \"\n",
    "        \"it again to get full credit.\"\n",
    "    )\n",
    "\n",
    "    global question_solution\n",
    "    question_solution = question[\"solution\"]\n",
    "    solution = re.search(pattern, question[\"solution\"])\n",
    "    assert solution is not None, question[\"solution\"]\n",
    "    solution = solution.group(1)\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]):\n",
    "        for completion in completions:\n",
    "            content = completion.last_assistant_message.get(\"content\")\n",
    "            assert isinstance(content, str)\n",
    "            solutions = [\n",
    "                match.group(1) for match in re.finditer(r\"\\\\boxed{([^}]+)}\", content)\n",
    "            ][::-1]\n",
    "            try:\n",
    "                reward = 0.9 ** solutions.index(solution)\n",
    "            except ValueError:\n",
    "                reward = 0\n",
    "            completion.commit(reward=reward)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "math_episodes = [\n",
    "    get_episode(question)\n",
    "    for question in math_questions[:2048]\n",
    "    if re.search(pattern, question[\"solution\"]) is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from lib.rl.completion import SplitMethod\n",
    "from lib.rl.completion_sampler import CompletionSampler, SamplingKwargs\n",
    "from lib.rl.trainer import ExploreImpl, ExploreOptions\n",
    "from lib.tokenizer import Tokenizer\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultExploreImpl(ExploreImpl):\n",
    "    explore_options: ExploreOptions\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        def done_callback(task: asyncio.Task[Episode]) -> None:\n",
    "            try:\n",
    "                done_episodes.put_nowait(task.result())\n",
    "            except BaseException as exception:\n",
    "                done_episodes.put_nowait(exception)\n",
    "\n",
    "        priority = 1\n",
    "        while episode := await ready_episodes.get():\n",
    "            asyncio.create_task(\n",
    "                self._explore_episode(\n",
    "                    completion_sampler, tokenizer, episode, update_progress, priority\n",
    "                )\n",
    "            ).add_done_callback(done_callback)\n",
    "            priority += 1\n",
    "\n",
    "    async def _explore_episode(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        episode: Episode,\n",
    "        update_progress: Callable[[float], None],\n",
    "        priority: int,\n",
    "    ) -> Episode:\n",
    "        for _ in range(self.explore_options.iterations):\n",
    "            await episode.sample_completions(\n",
    "                completion_sampler=completion_sampler,\n",
    "                tokenizer=tokenizer,\n",
    "                num_parents=self.explore_options.num_parents,\n",
    "                branch_factor=self.explore_options.branch_factor,\n",
    "                get_recovery_pattern=self.explore_options.get_recovery_pattern,\n",
    "                max_splits_per_completion=self.explore_options.max_split_points\n",
    "                or self.explore_options.num_parents,\n",
    "                priority=priority,\n",
    "                sample_probability_power=self.explore_options.get_sample_probability_power(),\n",
    "                sampling_kwargs=self.explore_options.sampling_kwargs,\n",
    "                split_by=self.explore_options.split_method,\n",
    "                split_separators=self.explore_options.split_separators,\n",
    "            )\n",
    "            update_progress(1 / self.explore_options.iterations)\n",
    "        return episode\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleExploreImpl(ExploreImpl):\n",
    "    num_samples: int\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        while episode := await ready_episodes.get():\n",
    "            task = asyncio.create_task(\n",
    "                episode.sample_completions(\n",
    "                    completion_sampler,\n",
    "                    tokenizer,\n",
    "                    num_parents=1,\n",
    "                    branch_factor=self.num_samples,\n",
    "                    sampling_kwargs=self.sampling_kwargs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            def done_callback(_: asyncio.Task[bool], episode=episode) -> None:\n",
    "                try:\n",
    "                    done_episodes.put_nowait(episode)\n",
    "                    update_progress(1)\n",
    "                except BaseException as e:\n",
    "                    done_episodes.put_nowait(e)\n",
    "\n",
    "            task.add_done_callback(done_callback)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TreeExploreImpl(ExploreImpl):\n",
    "    branch_factor: int\n",
    "    depth: int\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "    split_method: SplitMethod = \"count\"\n",
    "    split_separators: set[str] = field(default_factory=set)\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        model = await completion_sampler.get_model()\n",
    "\n",
    "        async def expand(episode: Episode, priority: int) -> None:\n",
    "            pending: set[asyncio.Task] = {\n",
    "                asyncio.create_task(\n",
    "                    episode.sample_completions(\n",
    "                        completion_sampler,\n",
    "                        tokenizer,\n",
    "                        num_parents=1,\n",
    "                        branch_factor=self.branch_factor,\n",
    "                        priority=priority,\n",
    "                        sampling_kwargs=self.sampling_kwargs,\n",
    "                        split_by=self.split_method,\n",
    "                        split_separators=self.split_separators,\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "\n",
    "            num_leaves = 0\n",
    "            while pending:\n",
    "                finished, pending = await asyncio.wait(\n",
    "                    pending, return_when=asyncio.FIRST_COMPLETED\n",
    "                )\n",
    "                for task in finished:\n",
    "                    try:\n",
    "                        task.result()\n",
    "                    except BaseException as e:\n",
    "                        await done_episodes.put(e)\n",
    "                        return\n",
    "                _num_leaves = 0\n",
    "                for leaf in episode.completion.leaves(model=model):\n",
    "                    _num_leaves += 1\n",
    "                    num_partitions = self.depth - leaf.depth() + 1\n",
    "                    if num_partitions > 1:\n",
    "                        parents = list(\n",
    "                            leaf.split(\n",
    "                                by=self.split_method,\n",
    "                                at=(\n",
    "                                    split / num_partitions\n",
    "                                    for split in range(1, num_partitions)\n",
    "                                ),\n",
    "                                separators=self.split_separators,\n",
    "                                cache=True,\n",
    "                            )\n",
    "                        )[:-1]\n",
    "                        for parent in parents:\n",
    "                            pending.add(\n",
    "                                asyncio.create_task(\n",
    "                                    episode._sample_completions(\n",
    "                                        parent=parent,\n",
    "                                        model=model,\n",
    "                                        completion_sampler=completion_sampler,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        branch_factor=self.branch_factor,\n",
    "                                        fork_decay=1.0,\n",
    "                                        recovery_pattern=None,\n",
    "                                        split_separators=self.split_separators,\n",
    "                                        sampling_kwargs=self.sampling_kwargs\n",
    "                                        or SamplingKwargs(),\n",
    "                                        priority=priority,\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                update_progress(\n",
    "                    (_num_leaves - num_leaves) / (self.branch_factor**self.depth)\n",
    "                )\n",
    "                num_leaves = _num_leaves\n",
    "\n",
    "            await done_episodes.put(episode)\n",
    "\n",
    "        priority = 0\n",
    "        while episode := await ready_episodes.get():\n",
    "            priority += 1\n",
    "            asyncio.create_task(expand(episode, priority))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IterativeVineExploreImpl(ExploreImpl):\n",
    "    branch_factor: int\n",
    "    depth: int\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "    split_method: SplitMethod = \"count\"\n",
    "    split_separators: set[str] = field(default_factory=set)\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        model = await completion_sampler.get_model()\n",
    "\n",
    "        async def iterate_vine(episode: Episode, priority: int) -> None:\n",
    "\n",
    "            for depth in range(self.depth):\n",
    "                if depth == 0:\n",
    "                    parent = episode.completion\n",
    "                else:\n",
    "                    parent = max(\n",
    "                        episode.completion.leaves(model=model),\n",
    "                        key=lambda leaf: leaf.reward,\n",
    "                    )\n",
    "                    parent = list(\n",
    "                        parent.split(\n",
    "                            by=self.split_method,\n",
    "                            at=[1 / (self.depth - depth + 1)],\n",
    "                            separators=self.split_separators,\n",
    "                            cache=True,\n",
    "                        )\n",
    "                    )[0]\n",
    "                await episode._sample_completions(\n",
    "                    parent=parent,\n",
    "                    model=model,\n",
    "                    completion_sampler=completion_sampler,\n",
    "                    tokenizer=tokenizer,\n",
    "                    branch_factor=self.branch_factor,\n",
    "                    fork_decay=1.0,\n",
    "                    recovery_pattern=None,\n",
    "                    split_separators=self.split_separators,\n",
    "                    sampling_kwargs=self.sampling_kwargs or SamplingKwargs(),\n",
    "                    priority=priority,\n",
    "                )\n",
    "                update_progress(1 / self.depth)\n",
    "\n",
    "            await done_episodes.put(episode)\n",
    "\n",
    "        priority = 0\n",
    "        while episode := await ready_episodes.get():\n",
    "            priority += 1\n",
    "            asyncio.create_task(iterate_vine(episode, priority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-01 20:00:47 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20250101_200048-rl76</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl76' target=\"_blank\">rl76</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl76' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl76</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aioitertools.helpers import maybe_await\n",
    "import asyncio\n",
    "import itertools as it\n",
    "from lib import clue\n",
    "from lib.rl.episode import Episode\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import Eval, ExploreOptions, Trainer, vLLMConfig\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import AsyncIterable\n",
    "\n",
    "\n",
    "episodes_per_iteration = 64 * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "async def train_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    pending: set[asyncio.Task[Episode | BaseException]] = set()\n",
    "    episodes = (\n",
    "        maybe_await(episode)\n",
    "        for episodes in zip(\n",
    "            # (clue.sample_random_episode() for _ in it.repeat(0)),\n",
    "            it.cycle(temporal_clue_episodes[64:]),\n",
    "            # it.cycle(zebra_grid_episodes[64:]),\n",
    "            # it.cycle(math_episodes[64:]),\n",
    "        )\n",
    "        for episode in episodes\n",
    "    )\n",
    "    while True:\n",
    "        pending.update(\n",
    "            asyncio.create_task(next(episodes))\n",
    "            for _ in range(episodes_per_iteration - len(pending))  # type: ignore\n",
    "        )\n",
    "        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "        for task in done:\n",
    "            try:\n",
    "                yield task.result()\n",
    "            except BaseException as e:\n",
    "                yield e\n",
    "\n",
    "\n",
    "async def val_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    for fut in asyncio.as_completed(clue.sample_random_episode() for _ in range(64)):\n",
    "        try:\n",
    "            yield await fut\n",
    "        except BaseException as e:\n",
    "            yield e\n",
    "\n",
    "\n",
    "explore_options = ExploreOptions(\n",
    "    iterations=1,\n",
    "    num_parents=6,\n",
    "    branch_factor=2,\n",
    "    patience=60,\n",
    "    advantage_max_weight=0.15,\n",
    "    sample_probability_power=None,\n",
    "    sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "    # split_method=\"prob\",\n",
    "    # split_point_std_deviation=0.5,\n",
    ")\n",
    "\n",
    "model_name = \"rl76\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=explore_options,\n",
    "    # explore_impl=DefaultExploreImpl(explore_options),\n",
    "    # explore_impl=SimpleExploreImpl(\n",
    "    #     num_samples=8, sampling_kwargs={\"max_tokens\": 4096}\n",
    "    # ),\n",
    "    explore_impl=TreeExploreImpl(\n",
    "        branch_factor=2,\n",
    "        depth=5,\n",
    "        sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "    ),\n",
    "    force_terminate_vllms=True,\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=episodes_per_iteration,\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    evals=[\n",
    "        # Eval(\n",
    "        #     name=\"variable_clue\",\n",
    "        #     episodes=val_episodes(),\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "        Eval(\n",
    "            name=\"temporal_clue\",\n",
    "            episodes=temporal_clue_episodes[:64],\n",
    "            samples_per_episode=3,\n",
    "            sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "        ),\n",
    "        # Eval(\n",
    "        #     name=\"zebra_grid\",\n",
    "        #     episodes=zebra_grid_episodes[:64],\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "        # Eval(\n",
    "        #     name=\"math\",\n",
    "        #     episodes=math_episodes[:64],\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "    ],\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        # max_steps_per_epoch=32,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=4e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=0.0,\n",
    "            clip_epsilon=0.2,\n",
    "            unclipped_policy_coef=0.0,\n",
    "            tanh_log_policy_coef=0.8,\n",
    "            value_coef=0.0,\n",
    "            entropy_coef=0.0,\n",
    "            entropy_target=0.6,\n",
    "            entropy_target_coef=0.05,\n",
    "            kl_coef=0.05,\n",
    "            weighted_entropy_coef=0.0,\n",
    "            weighted_kl_coef=0.0,\n",
    "            weighted_ce_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_chunked_prefill=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384,\n",
    "            preemption_mode=\"swap\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=100,\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        min_time_between_requests=0.0,\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3507106d107245cf9ed170daa98829ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f4c9514a44443994c3d8351dcfbbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping temporal_clue evaluation due to expired patience (2 remaining episodes x 60.0 patience per episode = 120.0 seconds)\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|31|Loss: 0.0095: 100%|██████████| 31/31 [09:16<00:00, 17.62s/it, advantage=0.0000, entropy=0.6108, entropy_target=0.0108, kl_div=0.1736, policy=0.0132, reinforce=0.0016, tanh_log_policy=0.0003, unclipped_policy=-0.0404, value=0.0000, weighted_ce=0.0016, weighted_entropy=-0.0153, weighted_kl_div=0.0024]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 1 model files to /home/ubuntu/atreides/experiments/models/rl76/0001\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0001 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0596c43f75884651aa985e0c9960544a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76183132326648e4871924bc98dcb4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|42|Loss: 0.0220: 100%|██████████| 42/42 [12:31<00:00, 17.65s/it, advantage=0.0000, entropy=0.7611, entropy_target=0.1611, kl_div=0.2431, policy=0.0198, reinforce=0.0029, tanh_log_policy=0.0023, unclipped_policy=-0.0169, value=0.0000, weighted_ce=0.0029, weighted_entropy=-0.0089, weighted_kl_div=-0.0026]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 2 model files to /home/ubuntu/atreides/experiments/models/rl76/0002\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0002 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85408c74a8c4ac09214e4a776fa69d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f1a2e42a3447e38151b5f9bc6ff22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|44|Loss: 0.0181: 100%|██████████| 44/44 [13:05<00:00, 17.63s/it, advantage=0.0000, entropy=0.7204, entropy_target=0.1204, kl_div=0.2114, policy=0.0410, reinforce=0.0002, tanh_log_policy=0.0019, unclipped_policy=0.0211, value=0.0000, weighted_ce=0.0002, weighted_entropy=-0.0029, weighted_kl_div=-0.0003]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 3 model files to /home/ubuntu/atreides/experiments/models/rl76/0003\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0003 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4822d36bf8d143d58395d6e7d729d3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dcfaf976614caa9b79f8d45011b9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|53|Loss: 0.0105: 100%|██████████| 53/53 [15:44<00:00, 17.59s/it, advantage=0.0000, entropy=0.5611, entropy_target=0.0389, kl_div=0.1292, policy=-0.0056, reinforce=0.0078, tanh_log_policy=0.0026, unclipped_policy=-0.0146, value=0.0000, weighted_ce=0.0078, weighted_entropy=-0.0174, weighted_kl_div=-0.0047] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 4 model files to /home/ubuntu/atreides/experiments/models/rl76/0004\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0004 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85bc79f217e49ae8db5f588aefe284b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b5c44a2c794103a1bf99d2bc98e151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|41|Loss: 0.0135: 100%|██████████| 41/41 [12:13<00:00, 17.66s/it, advantage=0.0000, entropy=0.4814, entropy_target=0.1186, kl_div=0.1355, policy=0.0680, reinforce=-0.0150, tanh_log_policy=0.0009, unclipped_policy=0.0463, value=0.0000, weighted_ce=-0.0150, weighted_entropy=0.0144, weighted_kl_div=0.0018]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 5 model files to /home/ubuntu/atreides/experiments/models/rl76/0005\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0005 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e6698657a84b9086ccde3bc6207e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9bbaf81395488f89b8b55213157171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|24|Loss: 0.0033: 100%|██████████| 24/24 [07:12<00:00, 17.59s/it, advantage=0.0000, entropy=0.5888, entropy_target=0.0112, kl_div=0.1710, policy=0.0247, reinforce=-0.0311, tanh_log_policy=-0.0073, unclipped_policy=-0.0125, value=0.0000, weighted_ce=-0.0311, weighted_entropy=0.0385, weighted_kl_div=0.0158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 6 model files to /home/ubuntu/atreides/experiments/models/rl76/0006\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0006 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d99adc5450493f97ab1ea7bfe608be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a73c1833374419a27c18cfb298d3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|12|Loss: 0.0229: 100%|██████████| 12/12 [03:40<00:00, 17.60s/it, advantage=0.0000, entropy=0.5725, entropy_target=0.0275, kl_div=0.2449, policy=1.1353, reinforce=-0.4091, tanh_log_policy=0.0115, unclipped_policy=0.8548, value=0.0000, weighted_ce=-0.4091, weighted_entropy=0.1947, weighted_kl_div=0.2202]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 7 model files to /home/ubuntu/atreides/experiments/models/rl76/0007\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0007 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badae69ce2614af5963147ed0b4c6a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71edffd5a19a44d893f0b6fc29b4db52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|8|Loss: 0.0112: 100%|██████████| 9/9 [02:48<00:00, 17.76s/it, advantage=0.0000, entropy=0.6081, entropy_target=0.0081, kl_div=0.1819, policy=0.0868, reinforce=-0.0033, tanh_log_policy=0.0021, unclipped_policy=-0.0528, value=0.0000, weighted_ce=-0.0033, weighted_entropy=-0.0454, weighted_kl_div=0.0226]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 8 model files to /home/ubuntu/atreides/experiments/models/rl76/0008\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0008 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a829f49c2f46e7a7b8535697c32aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd98be792bc48838b86ca3184423fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|10|Loss: -0.0350: 100%|██████████| 10/10 [03:05<00:00, 17.70s/it, advantage=0.0000, entropy=0.6044, entropy_target=0.0044, kl_div=0.1958, policy=0.4181, reinforce=-0.2267, tanh_log_policy=-0.0563, unclipped_policy=0.1547, value=0.0000, weighted_ce=-0.2267, weighted_entropy=0.2088, weighted_kl_div=0.0953] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 9 model files to /home/ubuntu/atreides/experiments/models/rl76/0009\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0009 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40609a2872d34c95a50bbc4e92107fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd4d9fe3bfd49bea01a5cd9300e0095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (1 remaining episodes x 60 patience per episode = 60 seconds)\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|21|Loss: 0.0123: 100%|██████████| 21/21 [06:20<00:00, 17.66s/it, advantage=0.0000, entropy=0.5380, entropy_target=0.0620, kl_div=0.1483, policy=0.0269, reinforce=0.0147, tanh_log_policy=0.0022, unclipped_policy=-0.0075, value=0.0000, weighted_ce=0.0147, weighted_entropy=-0.0226, weighted_kl_div=-0.0076] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 10 model files to /home/ubuntu/atreides/experiments/models/rl76/0010\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0010 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2155f25356e046b9958f20528144008a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ac124a5a704bb481fa714e8237831f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|19|Loss: -0.0132: 100%|██████████| 19/19 [05:45<00:00, 17.63s/it, advantage=0.0000, entropy=0.5073, entropy_target=0.0927, kl_div=0.1453, policy=0.1680, reinforce=-0.1130, tanh_log_policy=-0.0313, unclipped_policy=0.0602, value=0.0000, weighted_ce=-0.1130, weighted_entropy=0.1169, weighted_kl_div=0.0989] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 11 model files to /home/ubuntu/atreides/experiments/models/rl76/0011\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0011 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f006d56fb544072a1182d10a6aeeb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3b6c6760764cb19c822eba03f3f0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|15|Loss: 0.0045: 100%|██████████| 15/15 [04:34<00:00, 17.67s/it, advantage=0.0000, entropy=0.7260, entropy_target=0.1260, kl_div=0.1796, policy=0.0802, reinforce=-0.0441, tanh_log_policy=-0.0135, unclipped_policy=0.0093, value=0.0000, weighted_ce=-0.0441, weighted_entropy=0.0641, weighted_kl_div=0.0174]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 12 model files to /home/ubuntu/atreides/experiments/models/rl76/0012\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0012 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428edfd4d4a9422f840185003b4fb2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdc989890a3439986b534b3c435b27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|37|Loss: 0.0110: 100%|██████████| 37/37 [11:04<00:00, 17.65s/it, advantage=0.0000, entropy=0.6869, entropy_target=0.0869, kl_div=0.1447, policy=0.0311, reinforce=-0.0202, tanh_log_policy=-0.0008, unclipped_policy=0.0090, value=0.0000, weighted_ce=-0.0202, weighted_entropy=0.0196, weighted_kl_div=0.0127]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 13 model files to /home/ubuntu/atreides/experiments/models/rl76/0013\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0013 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b5c4f7964440ffb14901da4e0ab777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea49b73047a4349b58ddb4b510e2efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|18|Loss: 0.0144: 100%|██████████| 18/18 [05:27<00:00, 17.65s/it, advantage=0.0000, entropy=0.7027, entropy_target=0.1027, kl_div=0.1718, policy=0.0472, reinforce=-0.0326, tanh_log_policy=0.0008, unclipped_policy=0.0098, value=0.0000, weighted_ce=-0.0326, weighted_entropy=0.0516, weighted_kl_div=0.0115]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 14 model files to /home/ubuntu/atreides/experiments/models/rl76/0014\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0014 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf263a2fd7849a698e7d9a0ef13062f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f52c8434d3d43d68e4b50c679d6cb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl76/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|22|Loss: -0.0142: 100%|██████████| 22/22 [06:39<00:00, 17.47s/it, advantage=0.0000, entropy=0.8265, entropy_target=0.2265, kl_div=0.1662, policy=0.5463, reinforce=-0.1984, tanh_log_policy=-0.0422, unclipped_policy=0.5285, value=0.0000, weighted_ce=-0.1984, weighted_entropy=0.3950, weighted_kl_div=0.0864]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 15 model files to /home/ubuntu/atreides/experiments/models/rl76/0015\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl76/0015 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b75c61f28541f8ad7abd042fc00c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=15, verbosity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
