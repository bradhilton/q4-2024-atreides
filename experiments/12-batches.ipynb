{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.57it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.49it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.83it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.80it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.vllm import start_vllm_server, vllm_server_metrics\n",
    "\n",
    "model = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "shutdown_server, client = await start_vllm_server(\n",
    "    disable_log_requests=True,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.sampler import CompletionSampler\n",
    "\n",
    "completion_sampler = CompletionSampler(\n",
    "    client,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from lib.rl.completion import Completion\n",
    "from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam\n",
    "from typing import Callable, Coroutine, Optional\n",
    "\n",
    "\n",
    "class Episode:\n",
    "    def __init__(\n",
    "        self,\n",
    "        messages: list[ChatCompletionMessageParam],\n",
    "        on_sample: Callable[[list[Completion]], None | Coroutine[None, None, None]],\n",
    "        get_easier_episode: Optional[\n",
    "            tuple[float, Callable[[], \"Episode\" | Coroutine[None, None, \"Episode\"]]]\n",
    "        ] = None,\n",
    "        get_similar_episode: Optional[\n",
    "            Callable[[], \"Episode\" | Coroutine[None, None, \"Episode\"]]\n",
    "        ] = None,\n",
    "        get_harder_episode: Optional[\n",
    "            tuple[float, Callable[[], \"Episode\" | Coroutine[None, None, \"Episode\"]]]\n",
    "        ] = None,\n",
    "    ) -> None:\n",
    "        self.completion = Completion(messages=messages)  # type: ignore\n",
    "        self.on_sample = on_sample\n",
    "        self.min_value = (get_easier_episode or [None])[0]\n",
    "        self.max_value = (get_harder_episode or [None])[0]\n",
    "        self.get_easier_episode = (get_easier_episode or [None, None])[1]\n",
    "        self.get_similar_episode = get_similar_episode\n",
    "        self.get_harder_episode = (get_harder_episode or [None, None])[1]\n",
    "        self.weight = 1.0\n",
    "        self.task = asyncio.create_task(asyncio.sleep(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.clue import Clue, DeductiveSolver\n",
    "import re\n",
    "\n",
    "\n",
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": Clue.suspects[:3],\n",
    "            \"weapon\": Clue.weapons[:3],\n",
    "            \"room\": Clue.rooms[:3],\n",
    "            # \"motive\": Clue.motives[:6],\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.05,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt = game.get_prompt()\n",
    "    follow_up = \"Fill out your answer like this:\\n\" + \"\\n\".join(\n",
    "        f\"{element.capitalize()}: <#{element.upper()}#>\" for element in game.elements\n",
    "    )\n",
    "\n",
    "    async def reward_completion(completion: Completion) -> None:\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=completion.all_message_params()\n",
    "            + [\n",
    "                {\"role\": \"user\", \"content\": follow_up},\n",
    "            ],\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content\n",
    "        assert answer\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{element}: {solution}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for element, solution in game.solution.items()\n",
    "            ]\n",
    "        ) / len(game.solution)\n",
    "\n",
    "    async def on_sample(completions: list[Completion]) -> None:\n",
    "        await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        )\n",
    "        for completion in completions:\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class EpisodeSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample: Callable[[], Episode | Coroutine[None, None, Episode]],\n",
    "    ) -> None:\n",
    "        self.sample = sample\n",
    "        self.num_samples = 0\n",
    "        self.num_goldilocks = 0\n",
    "\n",
    "    def goldilocks_rate(self, prior: float, effective_sample_size: float) -> float:\n",
    "        return (self.num_goldilocks + prior * effective_sample_size) / (\n",
    "            self.num_samples + effective_sample_size\n",
    "        )\n",
    "\n",
    "\n",
    "branch_factor = 2\n",
    "min_requests = 5\n",
    "abs_buffer_size = 40\n",
    "weighted_buffer_size = 80\n",
    "buffer: list[Episode] = []\n",
    "min_random_episode_sample_probability_half_life = 80\n",
    "exploitation_factor = 1.0\n",
    "random_sampler = EpisodeSampler(sample_random_episode)\n",
    "other_samplers: list[EpisodeSampler] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-50428' coro=<sample_completions() done, defined at /tmp/ipykernel_11882/1222794176.py:15> exception=AssertionError('Merging assistant message params with absent or non-string content is not supported at this time.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_11882/1222794176.py\", line 39, in sample_completions\n",
      "    completions = await completion_sampler.sample_completions(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/sampler.py\", line 38, in sample_completions\n",
      "    messages=parent.all_message_params(),\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/completion.py\", line 218, in all_message_params\n",
      "    + joined_assistant_message_params(\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/completion.py\", line 399, in joined_assistant_message_params\n",
      "    assert isinstance(first_content, str) and isinstance(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: Merging assistant message params with absent or non-string content is not supported at this time.\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-57540' coro=<sample_completions() done, defined at /tmp/ipykernel_11882/1222794176.py:15> exception=AssertionError('Merging assistant message params with absent or non-string content is not supported at this time.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_11882/1222794176.py\", line 39, in sample_completions\n",
      "    completions = await completion_sampler.sample_completions(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/sampler.py\", line 38, in sample_completions\n",
      "    prefix = \"\"\n",
      "                \n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/completion.py\", line 218, in all_message_params\n",
      "    + joined_assistant_message_params(\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/completion.py\", line 399, in joined_assistant_message_params\n",
      "    assert isinstance(first_content, str) and isinstance(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: Merging assistant message params with absent or non-string content is not supported at this time.\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-61165' coro=<sample_completions() done, defined at /tmp/ipykernel_11882/1222794176.py:15> exception=AssertionError('Merging assistant message params with absent or non-string content is not supported at this time.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_11882/1222794176.py\", line 39, in sample_completions\n",
      "    completions = await completion_sampler.sample_completions(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/sampler.py\", line 38, in sample_completions\n",
      "    prefix = \"\"\n",
      "                \n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/completion.py\", line 218, in all_message_params\n",
      "    + joined_assistant_message_params(\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/rl/completion.py\", line 399, in joined_assistant_message_params\n",
      "    assert isinstance(first_content, str) and isinstance(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: Merging assistant message params with absent or non-string content is not supported at this time.\n"
     ]
    }
   ],
   "source": [
    "def goldilocks_rate_prior_and_effective_sample_size() -> tuple[float, float]:\n",
    "    num_goldilocks = random_sampler.num_goldilocks + sum(\n",
    "        s.num_goldilocks for s in other_samplers\n",
    "    )\n",
    "    num_samples = random_sampler.num_samples + sum(\n",
    "        s.num_samples for s in other_samplers\n",
    "    )\n",
    "    return (\n",
    "        num_goldilocks / num_samples\n",
    "        if num_goldilocks != 0 and num_samples != 0\n",
    "        else 1.0\n",
    "    ), max(num_samples / (len(other_samplers) + 1), 1)\n",
    "\n",
    "\n",
    "async def sample_completions(episode: Episode) -> None:\n",
    "    if episode.completion.children:\n",
    "        try:\n",
    "            leaf = max(\n",
    "                (\n",
    "                    completion\n",
    "                    for completion in episode.completion.leaves()\n",
    "                    if any(\n",
    "                        c.can_split() for c in completion.ancestors(including_self=True)\n",
    "                    )\n",
    "                ),\n",
    "                key=lambda c: c.all_abs_advantage() / c.all_token_count(tokenizer),\n",
    "            )\n",
    "            parent = max(\n",
    "                (c for c in leaf.ancestors(including_self=True) if c.can_split()),\n",
    "                key=lambda c: abs(c.advantage()) * c.token_count(tokenizer),\n",
    "            )\n",
    "            assert parent.split(by=\"count\"), \"Unable to split completion\"\n",
    "        except BaseException as e:\n",
    "            print(type(e), e)\n",
    "            episode.task = asyncio.create_task(asyncio.sleep(float(\"inf\")))\n",
    "            return\n",
    "        extra_body = dict(\n",
    "            add_generation_prompt=False,\n",
    "            continue_final_message=True,\n",
    "        )\n",
    "    else:\n",
    "        parent = episode.completion\n",
    "        extra_body = {}\n",
    "    completions = await completion_sampler.sample_completions(\n",
    "        parent,\n",
    "        n=branch_factor,\n",
    "        extra_body=extra_body,\n",
    "    )\n",
    "    on_sample = episode.on_sample(completions)\n",
    "    if isinstance(on_sample, Coroutine):\n",
    "        await on_sample\n",
    "\n",
    "\n",
    "async def get_episode() -> None:\n",
    "    if not other_samplers:\n",
    "        sampler = random_sampler\n",
    "    else:\n",
    "        prior, effective_sample_size = goldilocks_rate_prior_and_effective_sample_size()\n",
    "        min_random_goldilocks_rate = 1.0 * np.exp(\n",
    "            -np.log(2)\n",
    "            / min_random_episode_sample_probability_half_life\n",
    "            * random_sampler.num_samples\n",
    "        )\n",
    "        random_goldilocks_rate = max(\n",
    "            random_sampler.goldilocks_rate(prior, effective_sample_size),\n",
    "            min_random_goldilocks_rate,\n",
    "        )\n",
    "        other_goldilocks_rates = np.array(\n",
    "            [\n",
    "                sampler.goldilocks_rate(prior, effective_sample_size)\n",
    "                for sampler in other_samplers\n",
    "            ]\n",
    "        )\n",
    "        other_sampler_weights = other_goldilocks_rates**exploitation_factor\n",
    "        other_sampler_weights /= other_sampler_weights.sum()\n",
    "        other_expected_goldilocks_rate = other_goldilocks_rates @ other_sampler_weights\n",
    "        hierachical_weights = (\n",
    "            np.array([random_goldilocks_rate, other_expected_goldilocks_rate])\n",
    "            ** exploitation_factor\n",
    "        )\n",
    "        hierachical_weights /= hierachical_weights.sum()\n",
    "        if random.random() < hierachical_weights[0]:\n",
    "            sampler = random_sampler\n",
    "        else:\n",
    "            sampler = random.choices(other_samplers, weights=other_sampler_weights)[0]\n",
    "    episode = sampler.sample()\n",
    "    if isinstance(episode, Coroutine):\n",
    "        placeholder = Episode(\n",
    "            messages=[],\n",
    "            on_sample=lambda _: None,\n",
    "        )\n",
    "        buffer.append(\n",
    "            placeholder,\n",
    "        )\n",
    "        try:\n",
    "            episode = await episode\n",
    "        finally:\n",
    "            buffer.remove(placeholder)\n",
    "    episode.task = asyncio.create_task(sample_completions(episode))\n",
    "    buffer.append(episode)\n",
    "    try:\n",
    "        await episode.task\n",
    "    except BaseException as e:\n",
    "        buffer.remove(episode)\n",
    "        raise e\n",
    "    if not episode.completion.children:\n",
    "        return buffer.remove(episode)\n",
    "    sampler.num_samples += 1\n",
    "\n",
    "    if (\n",
    "        episode.get_easier_episode\n",
    "        and episode.min_value is not None\n",
    "        and episode.completion.value() <= episode.min_value\n",
    "    ):\n",
    "        other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_easier_episode,\n",
    "            )\n",
    "        )\n",
    "        return buffer.remove(episode)\n",
    "    elif (\n",
    "        episode.get_harder_episode\n",
    "        and episode.max_value is not None\n",
    "        and episode.completion.value() >= episode.max_value\n",
    "    ):\n",
    "        other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_harder_episode,\n",
    "            )\n",
    "        )\n",
    "        return buffer.remove(episode)\n",
    "    elif all(c.advantage() == 0 for c in episode.completion.children):\n",
    "        return buffer.remove(episode)\n",
    "    elif episode.get_similar_episode:\n",
    "        other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_similar_episode,\n",
    "            )\n",
    "        )\n",
    "    sampler.num_goldilocks += 1\n",
    "\n",
    "\n",
    "async def enrich_episode() -> None:\n",
    "    try:\n",
    "        episode = min(\n",
    "            (episode for episode in buffer if episode.task.done()),\n",
    "            key=lambda episode: len(list(episode.completion.descendants())),\n",
    "        )\n",
    "        episode.task = asyncio.create_task(sample_completions(episode))\n",
    "    except ValueError:\n",
    "        await get_episode()\n",
    "\n",
    "\n",
    "async def prepare_episodes() -> None:\n",
    "    while True:\n",
    "        await asyncio.sleep(5)\n",
    "        running, pending = vllm_server_metrics()\n",
    "        for _ in range(0, running - pending + min_requests, branch_factor * 2):\n",
    "            if (\n",
    "                len(buffer) < abs_buffer_size\n",
    "                or sum(e.weight for e in buffer) < weighted_buffer_size\n",
    "            ):\n",
    "                asyncio.create_task(get_episode())\n",
    "            else:\n",
    "                asyncio.create_task(enrich_episode())\n",
    "\n",
    "\n",
    "prepare_episodes_task = asyncio.create_task(prepare_episodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task pending name='Task-5' coro=<prepare_episodes() running at /tmp/ipykernel_11882/1222794176.py:150> wait_for=<Future pending cb=[Task.__wakeup()]>>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_episodes_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 15)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_server_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "                len(buffer) < abs_buffer_size\n",
    "                or sum(e.weight for e in buffer) < weighted_buffer_size\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.225"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(list(episode.completion.descendants())) for episode in buffer]) / len(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([episode for episode in buffer if episode.task.done()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "await prepare_episodes_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Trajectory:\n",
    "    episode: Episode\n",
    "    terminus: Completion\n",
    "    abs_advantage: float\n",
    "    token_count: int\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.episode.weight * self.abs_advantage / self.token_count\n",
    "\n",
    "\n",
    "def best_trajectory(episode: Episode) -> Trajectory:\n",
    "    return max(\n",
    "        (\n",
    "            Trajectory(\n",
    "                episode=episode,\n",
    "                terminus=completion,\n",
    "                abs_advantage=completion.all_abs_advantage(),\n",
    "                token_count=completion.all_token_count(tokenizer),\n",
    "            )\n",
    "            for completion in episode.completion.leaves()\n",
    "        ),\n",
    "        key=lambda t: t.abs_advantage / t.token_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = sorted(\n",
    "    (best_trajectory(episode) for episode in buffer),\n",
    "    key=lambda t: t.score(),\n",
    "    reverse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.2222222222222222,\n",
       " 0.48148148148148145,\n",
       " -0.09876543209876537,\n",
       " 0.2530864197530864,\n",
       " 0.0]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c.advantage() for c in trajectories[0].terminus.ancestors(including_self=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Request(BaseModel):\n",
    "    tokens_filename: str\n",
    "    advantages_filename: str\n",
    "    logprobs_filename: str\n",
    "    rows: int\n",
    "    seqlen: int\n",
    "    start: int\n",
    "    stop: int\n",
    "\n",
    "\n",
    "def handle_request(request: Request) -> None:\n",
    "    tokens = torch.from_file(\n",
    "        request.tokens_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.int64,\n",
    "    ).view(-1, request.seqlen)\n",
    "    advantages = torch.from_file(\n",
    "        request.advantages_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    logprobs = torch.from_file(\n",
    "        request.logprobs_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    trajectories = sorted(\n",
    "        (best_trajectory(episode) for episode in buffer),\n",
    "        key=lambda t: t.score(),\n",
    "        reverse=True,\n",
    "    )\n",
    "    for row in range(request.start, request.stop):\n",
    "        selected_trajectories: list[Trajectory] = []\n",
    "        for i in range(0, len(trajectories), -1):\n",
    "            if (\n",
    "                trajectories[i].token_count\n",
    "                + sum(t.token_count for t in selected_trajectories)\n",
    "                > request.seqlen\n",
    "            ):\n",
    "                continue\n",
    "            selected_trajectories.append(trajectories.pop(i))\n",
    "        tokens[row] = tokenizer.encode(\n",
    "            [\n",
    "                trajectory.terminus.all_message_params()\n",
    "                for trajectory in selected_trajectories\n",
    "            ],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
