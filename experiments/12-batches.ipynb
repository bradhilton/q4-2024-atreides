{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl import Completion, CompletionSampler, Episode, EpisodeSampler, EpisodeSamplerRouter\n",
    "from lib.tokenizer import Tokenizer\n",
    "from lib.vllm import start_vllm_server, vllm_server_metrics\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from typing import Callable, Coroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.55it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.69it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "shutdown_server, client = await start_vllm_server(\n",
    "    disable_log_requests=True,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutdown_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_sampler = CompletionSampler(\n",
    "    client,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": Clue.suspects[:3],\n",
    "            \"weapon\": Clue.weapons[:3],\n",
    "            \"room\": Clue.rooms[:3],\n",
    "            # \"motive\": Clue.motives[:6],\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.05,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt = game.get_prompt()\n",
    "    follow_up = \"Fill out your answer like this:\\n\" + \"\\n\".join(\n",
    "        f\"{element.capitalize()}: <#{element.upper()}#>\" for element in game.elements\n",
    "    )\n",
    "\n",
    "    async def reward_completion(completion: Completion) -> None:\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=completion.all_message_params()\n",
    "            + [\n",
    "                {\"role\": \"user\", \"content\": follow_up},\n",
    "            ],\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content\n",
    "        assert answer\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{element}: {solution}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for element, solution in game.solution.items()\n",
    "            ]\n",
    "        ) / len(game.solution)\n",
    "\n",
    "    async def on_sample(completions: list[Completion]) -> None:\n",
    "        await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        )\n",
    "        for completion in completions:\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_factor = 3\n",
    "abs_buffer_size = 20\n",
    "weighted_buffer_size = 40\n",
    "buffer: list[Episode] = []\n",
    "max_running = 100\n",
    "episode_sampler_router = EpisodeSamplerRouter(\n",
    "    EpisodeSampler(sample_random_episode),\n",
    "    exploitation_factor=1.0,\n",
    "    min_random_episode_sample_probability_half_life=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def sample_completions(episode: Episode) -> None:\n",
    "    if episode.completion.children:\n",
    "        try:\n",
    "            leaf = max(\n",
    "                (\n",
    "                    completion\n",
    "                    for completion in episode.completion.leaves()\n",
    "                    if any(\n",
    "                        c.can_split() for c in completion.ancestors(including_self=True)\n",
    "                    )\n",
    "                ),\n",
    "                key=lambda c: c.all_abs_advantage() / c.all_token_count(tokenizer),\n",
    "            )\n",
    "            parent = max(\n",
    "                (c for c in leaf.ancestors(including_self=True) if c.can_split()),\n",
    "                key=lambda c: abs(c.advantage()) * c.token_count(tokenizer),\n",
    "            )\n",
    "            assert parent.split(by=\"count\"), \"Unable to split completion\"\n",
    "        except BaseException as e:\n",
    "            print(type(e), e)\n",
    "            episode.task = asyncio.create_task(asyncio.sleep(float(\"inf\")))\n",
    "            return\n",
    "    else:\n",
    "        parent = episode.completion\n",
    "    completions = await completion_sampler.sample_completions(\n",
    "        parent,\n",
    "        n=branch_factor - len(parent.children),\n",
    "    )\n",
    "    on_sample = episode.on_sample(completions)\n",
    "    if isinstance(on_sample, Coroutine):\n",
    "        await on_sample\n",
    "\n",
    "\n",
    "async def get_episode() -> None:\n",
    "    sampler = episode_sampler_router.get_sampler()\n",
    "    episode = sampler.sample()\n",
    "    if isinstance(episode, Coroutine):\n",
    "        placeholder = Episode(\n",
    "            messages=[],\n",
    "            on_sample=lambda _: None,\n",
    "        )\n",
    "        buffer.append(placeholder)\n",
    "        try:\n",
    "            episode = await episode\n",
    "        finally:\n",
    "            buffer.remove(placeholder)\n",
    "    episode.task = asyncio.create_task(sample_completions(episode))\n",
    "    buffer.append(episode)\n",
    "    try:\n",
    "        await episode.task\n",
    "    except BaseException as e:\n",
    "        buffer.remove(episode)\n",
    "        raise e\n",
    "    if not episode.completion.children:\n",
    "        return buffer.remove(episode)\n",
    "    sampler.num_samples += 1\n",
    "\n",
    "    if (\n",
    "        episode.get_easier_episode\n",
    "        and episode.min_value is not None\n",
    "        and episode.completion.value() <= episode.min_value\n",
    "    ):\n",
    "        episode_sampler_router.other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_easier_episode,\n",
    "            )\n",
    "        )\n",
    "        return buffer.remove(episode)\n",
    "    elif (\n",
    "        episode.get_harder_episode\n",
    "        and episode.max_value is not None\n",
    "        and episode.completion.value() >= episode.max_value\n",
    "    ):\n",
    "        episode_sampler_router.other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_harder_episode,\n",
    "            )\n",
    "        )\n",
    "        return buffer.remove(episode)\n",
    "    elif all(c.advantage() == 0 for c in episode.completion.children):\n",
    "        return buffer.remove(episode)\n",
    "    elif episode.get_similar_episode:\n",
    "        episode_sampler_router.other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_similar_episode,\n",
    "            )\n",
    "        )\n",
    "    sampler.num_goldilocks += 1\n",
    "\n",
    "\n",
    "async def enrich_episode() -> None:\n",
    "    try:\n",
    "        episode = min(\n",
    "            (episode for episode in buffer if episode.task.done()),\n",
    "            key=lambda episode: episode.num_samples(),\n",
    "        )\n",
    "        episode.task = asyncio.create_task(sample_completions(episode))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "\n",
    "async def prepare_episodes() -> None:\n",
    "    while True:\n",
    "        await asyncio.sleep(2)\n",
    "        running, pending = vllm_server_metrics()\n",
    "        global max_running\n",
    "        max_running = max(max_running, running)\n",
    "        for _ in range(0, max_running - pending, branch_factor * 2):\n",
    "            if (\n",
    "                len(buffer) < abs_buffer_size\n",
    "                or sum(e.weight for e in buffer) < weighted_buffer_size\n",
    "            ):\n",
    "                asyncio.create_task(get_episode())\n",
    "            else:\n",
    "                asyncio.create_task(enrich_episode())\n",
    "\n",
    "\n",
    "prepare_episodes_task = asyncio.create_task(prepare_episodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_episodes_task.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 0)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_server_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buffer) < abs_buffer_size or sum(e.weight for e in buffer) < weighted_buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.65"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(list(episode.completion.descendants())) for episode in buffer]) / len(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([episode for episode in buffer if episode.task.done()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await prepare_episodes_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = next(buffer[0].completion.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = max(buffer[0].completion.leaves(), key=lambda c: c.all_abs_advantage() / c.all_token_count(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(completion.ancestors()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|im_start|>assistant\\n suspect in the Dining Room.'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(completion.parent.message_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "for completion in buffer[0].completion.descendants():\n",
    "    completion._token_count = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze the cards shown to each player:\n",
      "\n",
      "1. Blake shows the Lounge to Robert.\n",
      "2. Robert shows the Knife to Blake.\n",
      "3. Joel shows the Knife to Blake.\n",
      "4. Blake shows the Candlestick to Robert.\n",
      "5. Joel shows a card to Blake (must be Mrs. White or the Lead Pipe).\n",
      "6. Blake shows a card to Joel (must be Mr. Green or the Dining Room).\n",
      "7. Joel shows the Dining Room to Robert.\n",
      "8. Joel shows a card to Blake (must be the Lead Pipe).\n",
      "9. Blake shows a card to Robert (must be Mr. Green).\n",
      "\n",
      "From the information above, we can determine the following:\n",
      "\n",
      "- Robert has Miss Scarlet and the Knife.\n",
      "- Joel has Mrs. White and the Dining Room.\n",
      "- Blake has Mr. Green and the Candlestick.\n",
      "\n",
      "Since Blake showed the Candlestick to Robert, and Robert has the Knife, we know that the Candlestick is not the Knife. Therefore, the Candlestick must be the weapon in the Lounge.\n",
      "\n",
      "Similarly, since Blake showed a card to Joel (the Lead Pipe), and Joel has the Dining Room, we know that the Lead Pipe is not the Dining Room. Therefore, the Lead Pipe must be the weapon in the Hall.\n",
      "\n",
      "The only remaining card is the Mr. Green in the Dining Room.\n",
      "\n",
      "So, the facedown cards in the middle of the table are:\n",
      "\n",
      "Suspect: Miss Scarlet\n",
      "Weapon: Candlestick\n",
      "Room: Lounge\n",
      "\n",
      "The other two cards, Mr. Green and the Lead Pipe, were revealed during the game. The remaining card, Mrs. White, must be the suspect in the Dining Room.\n"
     ]
    }
   ],
   "source": [
    "print(completion.all_message_params()[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Trajectory:\n",
    "    episode: Episode\n",
    "    terminus: Completion\n",
    "    abs_advantage: float\n",
    "    token_count: int\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.episode.weight * self.abs_advantage / self.token_count\n",
    "\n",
    "\n",
    "def best_trajectory(episode: Episode) -> Trajectory:\n",
    "    return max(\n",
    "        (\n",
    "            Trajectory(\n",
    "                episode=episode,\n",
    "                terminus=completion,\n",
    "                abs_advantage=completion.all_abs_advantage(),\n",
    "                token_count=completion.all_token_count(tokenizer),\n",
    "            )\n",
    "            for completion in episode.completion.leaves()\n",
    "        ),\n",
    "        key=lambda t: t.abs_advantage / t.token_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 8192\n",
    "rows = 64\n",
    "\n",
    "tokens_file = torch.empty(rows * seqlen, dtype=torch.int64)\n",
    "tokens_file.numpy().tofile(\"/tmp/tokens.bin\")\n",
    "\n",
    "advantages_file = torch.empty(rows * seqlen, dtype=torch.float32)\n",
    "advantages_file.numpy().tofile(\"/tmp/advantages.bin\")\n",
    "\n",
    "logprobs_file = torch.empty(rows * seqlen, dtype=torch.float32)\n",
    "logprobs_file.numpy().tofile(\"/tmp/logprobs.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 76\u001b[0m\n\u001b[1;32m     67\u001b[0m         logprobs[row] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull_like(mask, fill_value\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnan, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     68\u001b[0m         logprobs[row][mask] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m     70\u001b[0m                 advantage\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m             )[:mask_size]\n\u001b[1;32m     74\u001b[0m         )\n\u001b[0;32m---> 76\u001b[0m \u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/tokens.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43madvantages_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/advantages.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/logprobs.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[157], line 44\u001b[0m, in \u001b[0;36mhandle_request\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     selected_trajectories\u001b[38;5;241m.\u001b[39mappend(trajectories\u001b[38;5;241m.\u001b[39mpop(i))\n\u001b[0;32m---> 44\u001b[0m tokens[row] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_message_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mselected_trajectories\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m replacement_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|reserved_special_token_250|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m mask \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m     54\u001b[0m     [trajectory\u001b[38;5;241m.\u001b[39mterminus\u001b[38;5;241m.\u001b[39mall_message_params(replacement_token\u001b[38;5;241m=\u001b[39mreplacement_token) \u001b[38;5;28;01mfor\u001b[39;00m trajectory \u001b[38;5;129;01min\u001b[39;00m selected_trajectories],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     concatenate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m     seqlen\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mseqlen,\n\u001b[1;32m     57\u001b[0m ) \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mget_token_id(replacement_token)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/tokenizer.py:50\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[0;34m(self, messages, add_generation_prompt, continue_final_message, concatenate, seqlen)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate \u001b[38;5;241m=\u001b[39m generate  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     49\u001b[0m pad_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(token_ids[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m concatenate:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "class Request(BaseModel):\n",
    "    tokens_filename: str\n",
    "    advantages_filename: str\n",
    "    logprobs_filename: str\n",
    "    rows: int\n",
    "    seqlen: int\n",
    "    start: int\n",
    "    stop: int\n",
    "\n",
    "\n",
    "def handle_request(request: Request) -> None:\n",
    "    tokens = torch.from_file(\n",
    "        request.tokens_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.int64,\n",
    "    ).view(-1, request.seqlen)\n",
    "    advantages = torch.from_file(\n",
    "        request.advantages_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    logprobs = torch.from_file(\n",
    "        request.logprobs_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    trajectories = sorted(\n",
    "        (best_trajectory(episode) for episode in buffer),\n",
    "        key=lambda t: t.score(),\n",
    "    )\n",
    "    for row in range(request.start, request.stop):\n",
    "        selected_trajectories: list[Trajectory] = []\n",
    "        for i in range(0, len(trajectories), -1):\n",
    "            if (\n",
    "                trajectories[i].token_count\n",
    "                + sum(t.token_count for t in selected_trajectories)\n",
    "                > request.seqlen\n",
    "            ):\n",
    "                continue\n",
    "            selected_trajectories.append(trajectories.pop(i))\n",
    "        tokens[row] = tokenizer.encode(\n",
    "            [\n",
    "                trajectory.terminus.all_message_params()\n",
    "                for trajectory in selected_trajectories\n",
    "            ],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        )\n",
    "        replacement_token = \"<|reserved_special_token_250|>\"\n",
    "        mask = tokenizer.encode(\n",
    "            [trajectory.terminus.all_message_params(replacement_token=replacement_token) for trajectory in selected_trajectories],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        ) == tokenizer.get_token_id(replacement_token)\n",
    "        mask_size = mask.sum()\n",
    "        advantages[row] = torch.full_like(mask, fill_value=torch.nan, dtype=torch.float32)\n",
    "        advantages[row][mask] = torch.tensor(\n",
    "            list(\n",
    "                advantage\n",
    "                for trajectory in selected_trajectories\n",
    "                for advantage in trajectory.terminus.all_token_advantages()\n",
    "            )[:mask_size]\n",
    "        )\n",
    "        logprobs[row] = torch.full_like(mask, fill_value=torch.nan, dtype=torch.float32)\n",
    "        logprobs[row][mask] = torch.tensor(\n",
    "            list(\n",
    "                advantage\n",
    "                for trajectory in selected_trajectories\n",
    "                for advantage in trajectory.terminus.all_logprobs()\n",
    "            )[:mask_size]\n",
    "        )\n",
    "\n",
    "handle_request(\n",
    "    Request(\n",
    "        tokens_filename=\"/tmp/tokens.bin\",\n",
    "        advantages_filename=\"/tmp/advantages.bin\",\n",
    "        logprobs_filename=\"/tmp/logprobs.bin\",\n",
    "        rows=64,\n",
    "        seqlen=8192,\n",
    "        start=0,\n",
    "        stop=1,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
