{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.56it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.69it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.56it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.54it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.vllm import start_vllm_server, vllm_server_metrics\n",
    "\n",
    "model = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "shutdown_server, client = await start_vllm_server(\n",
    "    disable_log_requests=True,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.sampler import CompletionSampler\n",
    "\n",
    "completion_sampler = CompletionSampler(\n",
    "    client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from lib.rl.completion import Completion\n",
    "from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam\n",
    "from typing import Callable, Coroutine, Optional\n",
    "\n",
    "\n",
    "class Episode:\n",
    "    def __init__(\n",
    "        self,\n",
    "        messages: list[ChatCompletionMessageParam],\n",
    "        on_sample: Callable[[list[Completion]], None | Coroutine[None, None, None]],\n",
    "        get_easier_episode: Optional[\n",
    "            tuple[float, Callable[[], \"Episode\" | Coroutine[None, None, \"Episode\"]]]\n",
    "        ] = None,\n",
    "        get_similar_episode: Optional[\n",
    "            Callable[[], \"Episode\" | Coroutine[None, None, \"Episode\"]]\n",
    "        ] = None,\n",
    "        get_harder_episode: Optional[\n",
    "            tuple[float, Callable[[], \"Episode\" | Coroutine[None, None, \"Episode\"]]]\n",
    "        ] = None,\n",
    "    ) -> None:\n",
    "        self.completion = Completion(messages=messages)  # type: ignore\n",
    "        self.on_sample = on_sample\n",
    "        self.min_value = (get_easier_episode or [None])[0]\n",
    "        self.max_value = (get_harder_episode or [None])[0]\n",
    "        self.get_easier_episode = (get_easier_episode or [None, None])[1]\n",
    "        self.get_similar_episode = get_similar_episode\n",
    "        self.get_harder_episode = (get_harder_episode or [None, None])[1]\n",
    "        self.weight = 1.0\n",
    "        self.task = asyncio.create_task(asyncio.sleep(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def on_sample(completions) -> None: ...\n",
    "\n",
    "\n",
    "episode = Episode(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you doing today?\"},\n",
    "    ],\n",
    "    on_sample=on_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def sample_random_episode() -> Episode:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13405539, 0.13405539, 0.13405539, 0.13405539, 0.13405539,\n",
       "       0.32972305])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class EpisodeSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample: Callable[[], Episode | Coroutine[None, None, Episode]],\n",
    "    ) -> None:\n",
    "        self.sample = sample\n",
    "        self.num_samples = 0\n",
    "        self.num_goldilocks = 0\n",
    "\n",
    "    def goldilocks_rate(self, prior: float, effective_sample_size: float) -> float:\n",
    "        return (self.num_goldilocks + prior * effective_sample_size) / (\n",
    "            self.num_samples + effective_sample_size\n",
    "        )\n",
    "\n",
    "\n",
    "branch_factor = 2\n",
    "min_requests = 10\n",
    "abs_buffer_size = 100\n",
    "weighted_buffer_size = 200\n",
    "buffer: list[Episode] = []\n",
    "min_random_episode_sample_probability_half_life = 100\n",
    "exploitation_factor = 1.0\n",
    "random_sampler = EpisodeSampler(sample_random_episode)\n",
    "other_samplers: list[EpisodeSampler] = []\n",
    "\n",
    "\n",
    "def goldilocks_rate_prior_and_effective_sample_size() -> tuple[float, float]:\n",
    "    num_goldilocks = random_sampler.num_goldilocks + sum(\n",
    "        s.num_goldilocks for s in other_samplers\n",
    "    )\n",
    "    num_samples = random_sampler.num_samples + sum(\n",
    "        s.num_samples for s in other_samplers\n",
    "    )\n",
    "    return (\n",
    "        num_goldilocks / num_samples\n",
    "        if num_goldilocks != 0 and num_samples != 0\n",
    "        else 1.0\n",
    "    ), max(num_samples / (len(other_samplers) + 1), 1)\n",
    "\n",
    "\n",
    "async def sample_completions(episode: Episode) -> None:\n",
    "    if episode.completion.children:\n",
    "        parent = episode.completion\n",
    "    else:\n",
    "        try:\n",
    "            leaf = max(\n",
    "                (\n",
    "                    completion\n",
    "                    for completion in episode.completion.leaves()\n",
    "                    if any(\n",
    "                        c.can_split() for c in completion.ancestors(including_self=True)\n",
    "                    )\n",
    "                ),\n",
    "                key=lambda c: c.all_abs_advantage() / c.all_token_count(tokenizer),\n",
    "            )\n",
    "            parent = max(\n",
    "                (c for c in leaf.ancestors(including_self=True) if c.can_split()),\n",
    "                key=lambda c: abs(c.advantage()) * c.token_count(tokenizer),\n",
    "            )\n",
    "            assert parent.split(by=\"count\")\n",
    "        except:\n",
    "            episode.task = asyncio.create_task(asyncio.sleep(float(\"inf\")))\n",
    "            return\n",
    "    completions = await completion_sampler.sample_completions(\n",
    "        parent,\n",
    "        n=branch_factor,\n",
    "    )\n",
    "    on_sample = episode.on_sample(completions)\n",
    "    if isinstance(on_sample, Coroutine):\n",
    "        await on_sample\n",
    "\n",
    "\n",
    "async def get_episode() -> None:\n",
    "    if not other_samplers:\n",
    "        sampler = random_sampler\n",
    "    else:\n",
    "        prior, effective_sample_size = goldilocks_rate_prior_and_effective_sample_size()\n",
    "        min_random_goldilocks_rate = 1.0 * np.exp(\n",
    "            -np.log(2)\n",
    "            / min_random_episode_sample_probability_half_life\n",
    "            * random_sampler.num_samples\n",
    "        )\n",
    "        random_goldilocks_rate = max(\n",
    "            random_sampler.goldilocks_rate(prior, effective_sample_size),\n",
    "            min_random_goldilocks_rate,\n",
    "        )\n",
    "        other_goldilocks_rates = np.array(\n",
    "            [\n",
    "                sampler.goldilocks_rate(prior, effective_sample_size)\n",
    "                for sampler in other_samplers\n",
    "            ]\n",
    "        )\n",
    "        other_sampler_weights = other_goldilocks_rates**exploitation_factor\n",
    "        other_sampler_weights /= other_sampler_weights.sum()\n",
    "        other_expected_goldilocks_rate = other_goldilocks_rates @ other_sampler_weights\n",
    "        hierachical_weights = (\n",
    "            np.array([random_goldilocks_rate, other_expected_goldilocks_rate])\n",
    "            ** exploitation_factor\n",
    "        )\n",
    "        hierachical_weights /= hierachical_weights.sum()\n",
    "        if random.random() < hierachical_weights[0]:\n",
    "            sampler = random_sampler\n",
    "        else:\n",
    "            sampler = random.choices(other_samplers, weights=other_sampler_weights)[0]\n",
    "    episode = sampler.sample()\n",
    "    if isinstance(episode, Coroutine):\n",
    "        placeholder = Episode(\n",
    "            messages=[],\n",
    "            on_sample=lambda _: None,\n",
    "        )\n",
    "        buffer.append(\n",
    "            placeholder,\n",
    "        )\n",
    "        try:\n",
    "            episode = await episode\n",
    "        finally:\n",
    "            buffer.remove(placeholder)\n",
    "    episode.task = asyncio.create_task(sample_completions(episode))\n",
    "    buffer.append(episode)\n",
    "    try:\n",
    "        await episode.task\n",
    "    except BaseException as e:\n",
    "        buffer.remove(episode)\n",
    "        raise e\n",
    "    if not episode.completion.children:\n",
    "        return buffer.remove(episode)\n",
    "    sampler.num_samples += 1\n",
    "\n",
    "    if (\n",
    "        episode.get_easier_episode\n",
    "        and episode.min_value is not None\n",
    "        and episode.completion.value() <= episode.min_value\n",
    "    ):\n",
    "        other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_easier_episode,\n",
    "            )\n",
    "        )\n",
    "        return buffer.remove(episode)\n",
    "    elif (\n",
    "        episode.get_harder_episode\n",
    "        and episode.max_value is not None\n",
    "        and episode.completion.value() >= episode.max_value\n",
    "    ):\n",
    "        other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_harder_episode,\n",
    "            )\n",
    "        )\n",
    "        return buffer.remove(episode)\n",
    "    elif all(c.advantage() == 0 for c in episode.completion.children):\n",
    "        return buffer.remove(episode)\n",
    "    elif episode.get_similar_episode:\n",
    "        other_samplers.append(\n",
    "            EpisodeSampler(\n",
    "                episode.get_similar_episode,\n",
    "            )\n",
    "        )\n",
    "    sampler.num_goldilocks += 1\n",
    "\n",
    "\n",
    "async def enrich_episode() -> None:\n",
    "    try:\n",
    "        episode = min(\n",
    "            (episode for episode in buffer if episode.task.done()),\n",
    "            key=lambda episode: len(list(episode.completion.descendants())),\n",
    "        )\n",
    "        episode.task = asyncio.create_task(sample_completions(episode))\n",
    "    except ValueError:\n",
    "        await get_episode()\n",
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "\n",
    "\n",
    "class Request(BaseModel):\n",
    "    tokens_filename: str\n",
    "    advantages_filename: str\n",
    "    logprobs_filename: str\n",
    "    rows: int\n",
    "    seqlen: int\n",
    "    start: int\n",
    "    stop: int\n",
    "\n",
    "\n",
    "requests: list[Request] = []\n",
    "\n",
    "\n",
    "class Trajectory(BaseModel):\n",
    "    episode: Episode\n",
    "    terminus: Completion\n",
    "    abs_advantage: float\n",
    "    token_count: int\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.episode.weight * self.abs_advantage / self.token_count\n",
    "\n",
    "\n",
    "def best_trajectory(episode: Episode) -> Trajectory:\n",
    "    return max(\n",
    "        (\n",
    "            Trajectory(\n",
    "                episode=episode,\n",
    "                terminus=completion,\n",
    "                abs_advantage=completion.all_abs_advantage(),\n",
    "                token_count=completion.all_token_count(tokenizer),\n",
    "            )\n",
    "            for completion in episode.completion.leaves()\n",
    "        ),\n",
    "        key=lambda t: t.abs_advantage / t.token_count,\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_request(request: Request) -> None:\n",
    "    tokens = torch.from_file(\n",
    "        request.tokens_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.int64,\n",
    "    ).view(-1, request.seqlen)\n",
    "    advantages = torch.from_file(\n",
    "        request.advantages_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    logprobs = torch.from_file(\n",
    "        request.logprobs_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    trajectories = sorted(\n",
    "        (best_trajectory(episode) for episode in buffer),\n",
    "        key=lambda t: t.score(),\n",
    "        reverse=True,\n",
    "    )\n",
    "    for row in range(request.start, request.stop):\n",
    "        selected_trajectories: list[Trajectory] = []\n",
    "        for i in range(0, len(trajectories), -1):\n",
    "            if (\n",
    "                trajectories[i].token_count\n",
    "                + sum(t.token_count for t in selected_trajectories)\n",
    "                > request.seqlen\n",
    "            ):\n",
    "                continue\n",
    "            selected_trajectories.append(trajectories.pop(i))\n",
    "        tokens[row] = tokenizer.encode(\n",
    "            [\n",
    "                trajectory.terminus.all_message_params()\n",
    "                for trajectory in selected_trajectories\n",
    "            ],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        )\n",
    "\n",
    "\n",
    "while True:\n",
    "    await asyncio.sleep(1)\n",
    "    for request in requests:\n",
    "        handle_request(request)\n",
    "    requests = []\n",
    "    running, pending = vllm_server_metrics()\n",
    "    for _ in range(0, running - pending + min_requests, branch_factor * 2):\n",
    "        if (\n",
    "            len(buffer) < abs_buffer_size\n",
    "            or sum(e.weight for e in buffer) < weighted_buffer_size\n",
    "        ):\n",
    "            asyncio.create_task(get_episode())\n",
    "        else:\n",
    "            asyncio.create_task(enrich_episode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
