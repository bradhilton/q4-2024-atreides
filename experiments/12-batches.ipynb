{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl import (\n",
    "    Completion,\n",
    "    CompletionSampler,\n",
    "    Episode,\n",
    "    EpisodeBuffer,\n",
    "    EpisodeSampler,\n",
    "    EpisodeSamplerRouter,\n",
    ")\n",
    "from lib.tokenizer import Tokenizer\n",
    "import re\n",
    "from typing import Coroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-07 23:14:09 api_server.py:528] vLLM API server version dev\n",
      "INFO 11-07 23:14:09 api_server.py:529] args: Namespace(host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='vllm-95f9a82b4ab473334457340fdc8b00d7', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False)\n",
      "INFO 11-07 23:14:09 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/2126b70a-5168-4786-b2fd-f432b2e56ba3 for IPC Path.\n",
      "INFO 11-07 23:14:09 api_server.py:179] Started engine process with PID 8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.54it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.29it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.69it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-07 23:14:37 api_server.py:232] vLLM to use /tmp/tmp_6vicj8y as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 11-07 23:14:37 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 11-07 23:14:37 launcher.py:19] Available routes are:\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 11-07 23:14:37 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    }
   ],
   "source": [
    "from lib.vllm import start_vllm_server, vllm_server_metrics\n",
    "\n",
    "model = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "shutdown_server, client = await start_vllm_server(\n",
    "    disable_log_requests=True,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_sampler = CompletionSampler(\n",
    "    client,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": Clue.suspects[:3],\n",
    "            \"weapon\": Clue.weapons[:3],\n",
    "            \"room\": Clue.rooms[:3],\n",
    "            # \"motive\": Clue.motives[:6],\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.05,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt = game.get_prompt()\n",
    "    follow_up = \"Fill out your answer like this:\\n\" + \"\\n\".join(\n",
    "        f\"{element.capitalize()}: <#{element.upper()}#>\" for element in game.elements\n",
    "    )\n",
    "\n",
    "    async def reward_completion(completion: Completion) -> None:\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=completion.all_message_params()\n",
    "            + [\n",
    "                {\"role\": \"user\", \"content\": follow_up},\n",
    "            ],\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content\n",
    "        assert answer\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{element}: {solution}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for element, solution in game.solution.items()\n",
    "            ]\n",
    "        ) / len(game.solution)\n",
    "\n",
    "    async def on_sample(completions: list[Completion]) -> None:\n",
    "        await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        )\n",
    "        for completion in completions:\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-07 23:14:39 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_buffer = EpisodeBuffer(\n",
    "    episode_sampler_router=EpisodeSamplerRouter(\n",
    "        EpisodeSampler(sample_random_episode),\n",
    "        exploitation_factor=1.0,\n",
    "        min_random_episode_sample_probability_half_life=40,\n",
    "    ),\n",
    "    completion_sampler=completion_sampler,\n",
    "    tokenizer=tokenizer,\n",
    "    branch_factor=2,\n",
    "    split_method=\"count\",\n",
    "    size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pydantic_core._pydantic_core.ValidationError'> "
     ]
    }
   ],
   "source": [
    "episode_buffer.start_buffering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running/Pending vLLM Requests: 47/0\n",
      "Number of Episodes: 64\n",
      "Pending Tasks: 37\n",
      "Sampled Completions: 2006\n"
     ]
    }
   ],
   "source": [
    "running, pending = vllm_server_metrics()\n",
    "print(f\"Running/Pending vLLM Requests: {running}/{pending}\")\n",
    "print(\"Number of Episodes:\", len(episode_buffer.episodes))\n",
    "print(\n",
    "    \"Pending Tasks:\",\n",
    "    sum(task._state == \"PENDING\" for task in episode_buffer.tasks.values()),\n",
    ")\n",
    "print(\n",
    "    \"Sampled Completions:\",\n",
    "    sum(episode.num_samples() for episode in episode_buffer.episodes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in episode_buffer.episodes:\n",
    "    episode.completion._cached_value = None\n",
    "    for descendent in episode.completion.descendants():\n",
    "        descendent._cached_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "for episode in episode_buffer.episodes:\n",
    "    episode.completion._cached_value = None\n",
    "    for descendent in episode.completion.descendants():\n",
    "        descendent._cached_value = None\n",
    "best_leaves = [\n",
    "    episode.best_leaf(tokenizer) for episode in episode_buffer.episodes\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [leaf.all_abs_advantage_per_token(tokenizer, cache=True) for leaf in best_leaves]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Absolute Advantage Per Token: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Average Absolute Advantage Per Token:\",\n",
    "    sum(\n",
    "        len(episode.best_leaf(tokenizer).children)#.all_abs_advantage_per_token(tokenizer)\n",
    "        for episode in episode_buffer.episodes\n",
    "    )\n",
    "    / len(episode_buffer.episodes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for _ in episode_buffer.episodes[0].completion.descendants())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00029788501638367595"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf = episode_buffer.episodes[0].best_leaf(tokenizer)\n",
    "leaf.all_abs_advantage() / leaf.all_token_count(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze the cards shown to each player:\n",
      "\n",
      "1. Blake shows the Lounge to Robert.\n",
      "2. Robert shows the Knife to Blake.\n",
      "3. Joel shows the Knife to Blake.\n",
      "4. Blake shows the Candlestick to Robert.\n",
      "5. Joel shows a card to Blake (must be Mrs. White or the Lead Pipe).\n",
      "6. Blake shows a card to Joel (must be Mr. Green or the Dining Room).\n",
      "7. Joel shows the Dining Room to Robert.\n",
      "8. Joel shows a card to Blake (must be the Lead Pipe).\n",
      "9. Blake shows a card to Robert (must be Mr. Green).\n",
      "\n",
      "From the information above, we can determine the following:\n",
      "\n",
      "- Robert has Miss Scarlet and the Knife.\n",
      "- Joel has Mrs. White and the Dining Room.\n",
      "- Blake has Mr. Green and the Candlestick.\n",
      "\n",
      "Since Blake showed the Candlestick to Robert, and Robert has the Knife, we know that the Candlestick is not the Knife. Therefore, the Candlestick must be the weapon in the Lounge.\n",
      "\n",
      "Similarly, since Blake showed a card to Joel (the Lead Pipe), and Joel has the Dining Room, we know that the Lead Pipe is not the Dining Room. Therefore, the Lead Pipe must be the weapon in the Hall.\n",
      "\n",
      "The only remaining card is the Mr. Green in the Dining Room.\n",
      "\n",
      "So, the facedown cards in the middle of the table are:\n",
      "\n",
      "Suspect: Miss Scarlet\n",
      "Weapon: Candlestick\n",
      "Room: Lounge\n",
      "\n",
      "The other two cards, Mr. Green and the Lead Pipe, were revealed during the game. The remaining card, Mrs. White, must be the suspect in the Dining Room.\n"
     ]
    }
   ],
   "source": [
    "print(completion.all_message_params()[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Trajectory:\n",
    "    episode: Episode\n",
    "    terminus: Completion\n",
    "    abs_advantage: float\n",
    "    token_count: int\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.episode.weight * self.abs_advantage / self.token_count\n",
    "\n",
    "\n",
    "def best_trajectory(episode: Episode) -> Trajectory:\n",
    "    return max(\n",
    "        (\n",
    "            Trajectory(\n",
    "                episode=episode,\n",
    "                terminus=completion,\n",
    "                abs_advantage=completion.all_abs_advantage(),\n",
    "                token_count=completion.all_token_count(tokenizer),\n",
    "            )\n",
    "            for completion in episode.completion.leaves()\n",
    "        ),\n",
    "        key=lambda t: t.abs_advantage / t.token_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 8192\n",
    "rows = 64\n",
    "\n",
    "tokens_file = torch.empty(rows * seqlen, dtype=torch.int64)\n",
    "tokens_file.numpy().tofile(\"/tmp/tokens.bin\")\n",
    "\n",
    "advantages_file = torch.empty(rows * seqlen, dtype=torch.float32)\n",
    "advantages_file.numpy().tofile(\"/tmp/advantages.bin\")\n",
    "\n",
    "logprobs_file = torch.empty(rows * seqlen, dtype=torch.float32)\n",
    "logprobs_file.numpy().tofile(\"/tmp/logprobs.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 76\u001b[0m\n\u001b[1;32m     67\u001b[0m         logprobs[row] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull_like(mask, fill_value\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnan, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     68\u001b[0m         logprobs[row][mask] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m     70\u001b[0m                 advantage\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m             )[:mask_size]\n\u001b[1;32m     74\u001b[0m         )\n\u001b[0;32m---> 76\u001b[0m \u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/tokens.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43madvantages_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/advantages.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/logprobs.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[157], line 44\u001b[0m, in \u001b[0;36mhandle_request\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     selected_trajectories\u001b[38;5;241m.\u001b[39mappend(trajectories\u001b[38;5;241m.\u001b[39mpop(i))\n\u001b[0;32m---> 44\u001b[0m tokens[row] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_message_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mselected_trajectories\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m replacement_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|reserved_special_token_250|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m mask \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m     54\u001b[0m     [trajectory\u001b[38;5;241m.\u001b[39mterminus\u001b[38;5;241m.\u001b[39mall_message_params(replacement_token\u001b[38;5;241m=\u001b[39mreplacement_token) \u001b[38;5;28;01mfor\u001b[39;00m trajectory \u001b[38;5;129;01min\u001b[39;00m selected_trajectories],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     concatenate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m     seqlen\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mseqlen,\n\u001b[1;32m     57\u001b[0m ) \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mget_token_id(replacement_token)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/tokenizer.py:50\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[0;34m(self, messages, add_generation_prompt, continue_final_message, concatenate, seqlen)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate \u001b[38;5;241m=\u001b[39m generate  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     49\u001b[0m pad_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(token_ids[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m concatenate:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "class Request(BaseModel):\n",
    "    tokens_filename: str\n",
    "    advantages_filename: str\n",
    "    logprobs_filename: str\n",
    "    rows: int\n",
    "    seqlen: int\n",
    "    start: int\n",
    "    stop: int\n",
    "\n",
    "\n",
    "def handle_request(request: Request) -> None:\n",
    "    tokens = torch.from_file(\n",
    "        request.tokens_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.int64,\n",
    "    ).view(-1, request.seqlen)\n",
    "    advantages = torch.from_file(\n",
    "        request.advantages_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    logprobs = torch.from_file(\n",
    "        request.logprobs_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    trajectories = sorted(\n",
    "        (best_trajectory(episode) for episode in buffer),\n",
    "        key=lambda t: t.score(),\n",
    "    )\n",
    "    for row in range(request.start, request.stop):\n",
    "        selected_trajectories: list[Trajectory] = []\n",
    "        for i in range(0, len(trajectories), -1):\n",
    "            if (\n",
    "                trajectories[i].token_count\n",
    "                + sum(t.token_count for t in selected_trajectories)\n",
    "                > request.seqlen\n",
    "            ):\n",
    "                continue\n",
    "            selected_trajectories.append(trajectories.pop(i))\n",
    "        tokens[row] = tokenizer.encode(\n",
    "            [\n",
    "                trajectory.terminus.all_message_params()\n",
    "                for trajectory in selected_trajectories\n",
    "            ],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        )\n",
    "        replacement_token = \"<|reserved_special_token_250|>\"\n",
    "        mask = tokenizer.encode(\n",
    "            [trajectory.terminus.all_message_params(replacement_token=replacement_token) for trajectory in selected_trajectories],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        ) == tokenizer.get_token_id(replacement_token)\n",
    "        mask_size = mask.sum()\n",
    "        advantages[row] = torch.full_like(mask, fill_value=torch.nan, dtype=torch.float32)\n",
    "        advantages[row][mask] = torch.tensor(\n",
    "            list(\n",
    "                advantage\n",
    "                for trajectory in selected_trajectories\n",
    "                for advantage in trajectory.terminus.all_token_advantages()\n",
    "            )[:mask_size]\n",
    "        )\n",
    "        logprobs[row] = torch.full_like(mask, fill_value=torch.nan, dtype=torch.float32)\n",
    "        logprobs[row][mask] = torch.tensor(\n",
    "            list(\n",
    "                advantage\n",
    "                for trajectory in selected_trajectories\n",
    "                for advantage in trajectory.terminus.all_logprobs()\n",
    "            )[:mask_size]\n",
    "        )\n",
    "\n",
    "handle_request(\n",
    "    Request(\n",
    "        tokens_filename=\"/tmp/tokens.bin\",\n",
    "        advantages_filename=\"/tmp/advantages.bin\",\n",
    "        logprobs_filename=\"/tmp/logprobs.bin\",\n",
    "        rows=64,\n",
    "        seqlen=8192,\n",
    "        start=0,\n",
    "        stop=1,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
