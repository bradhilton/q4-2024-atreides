{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.57it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.70it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.vllm import start_vllm_server, vllm_server_metrics\n",
    "\n",
    "model = \"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    "\n",
    "shutdown_server, client = await start_vllm_server(\n",
    "    disable_log_requests=True,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl import (\n",
    "    Completion,\n",
    "    CompletionSampler,\n",
    "    Episode,\n",
    "    EpisodeBuffer,\n",
    "    EpisodeSampler,\n",
    "    EpisodeSamplerRouter,\n",
    ")\n",
    "from lib.tokenizer import Tokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_episode() -> Episode:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": Clue.suspects[:3],\n",
    "            \"weapon\": Clue.weapons[:3],\n",
    "            \"room\": Clue.rooms[:3],\n",
    "            # \"motive\": Clue.motives[:6],\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.05,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        print_playthrough=False,\n",
    "    )\n",
    "    prompt = game.get_prompt()\n",
    "    follow_up = \"Fill out your answer like this:\\n\" + \"\\n\".join(\n",
    "        f\"{element.capitalize()}: <#{element.upper()}#>\" for element in game.elements\n",
    "    )\n",
    "\n",
    "    async def reward_completion(completion: Completion) -> None:\n",
    "        chat_completion = await client.chat.completions.create(\n",
    "            messages=completion.all_message_params()\n",
    "            + [\n",
    "                {\"role\": \"user\", \"content\": follow_up},\n",
    "            ],\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content\n",
    "        assert answer\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    re.search(\n",
    "                        f\"{element}: {solution}\",\n",
    "                        answer,\n",
    "                        re.IGNORECASE,\n",
    "                    )\n",
    "                )\n",
    "                for element, solution in game.solution.items()\n",
    "            ]\n",
    "        ) / len(game.solution)\n",
    "\n",
    "    async def on_sample(completions: list[Completion]) -> None:\n",
    "        await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        )\n",
    "        for completion in completions:\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_sampler = CompletionSampler(\n",
    "    client,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(model)\n",
    "\n",
    "episode_buffer = EpisodeBuffer(\n",
    "    episode_sampler_router=EpisodeSamplerRouter(\n",
    "        EpisodeSampler(sample_random_episode),\n",
    "        exploitation_factor=1.0,\n",
    "        min_random_episode_sample_probability_half_life=40,\n",
    "    ),\n",
    "    completion_sampler=completion_sampler,\n",
    "    tokenizer=tokenizer,\n",
    "    branch_factor=2,\n",
    "    split_method=\"count\",\n",
    "    size=512,\n",
    "    episode_decay=0.7,\n",
    "    completion_decay=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_buffer.size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_buffer.stop_buffering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running/Pending/HWM vLLM Requests: 0/0/162\n",
      "Number of Episodes: 512\n",
      "Pending Tasks: 0\n",
      "Sampled Completions: 7713\n",
      "Average Absolute Advantage Per Token: 0.0015422665585660134\n"
     ]
    }
   ],
   "source": [
    "running, pending = vllm_server_metrics()\n",
    "print(f\"Running/Pending/HWM vLLM Requests: {running}/{pending}/{episode_buffer.max_running_requests}\")\n",
    "print(\"Number of Episodes:\", len(episode_buffer.episodes))\n",
    "print(\n",
    "    \"Pending Tasks:\",\n",
    "    sum(task._state == \"PENDING\" for task in episode_buffer.tasks.values()),\n",
    ")\n",
    "print(\n",
    "    \"Sampled Completions:\",\n",
    "    sum(episode.num_samples() for episode in episode_buffer.episodes),\n",
    ")\n",
    "print(\n",
    "    \"Average Absolute Advantage Per Token:\",\n",
    "    sum(\n",
    "        episode.best_leaf(tokenizer).all_abs_advantage_per_token(tokenizer)\n",
    "        for episode in episode_buffer.episodes\n",
    "    )\n",
    "    / len(episode_buffer.episodes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "class Request(BaseModel):\n",
    "    tokens_filename: str\n",
    "    advantages_filename: str\n",
    "    logprobs_filename: str\n",
    "    rows: int\n",
    "    seqlen: int\n",
    "    start: int\n",
    "    stop: int\n",
    "\n",
    "\n",
    "class Trajectory:\n",
    "    episode: Episode\n",
    "    terminus: Completion\n",
    "    abs_advantage: float\n",
    "    token_count: int\n",
    "\n",
    "    def __init__(self, episode: Episode, tokenizer: Tokenizer) -> None:\n",
    "        self.episode = episode\n",
    "        self.terminus = episode.best_leaf(tokenizer)\n",
    "        self.abs_advantage = self.terminus.all_abs_advantage()\n",
    "        self.token_count = self.terminus.all_token_count(tokenizer)\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.episode.weight * self.abs_advantage / self.token_count\n",
    "\n",
    "\n",
    "@app.post(\"/write-trajectories\")\n",
    "def write_trajectories(request: Request) -> None:\n",
    "    tokens = torch.from_file(\n",
    "        request.tokens_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.int64,\n",
    "    ).view(-1, request.seqlen)\n",
    "    advantages = torch.from_file(\n",
    "        request.advantages_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    logprobs = torch.from_file(\n",
    "        request.logprobs_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    trajectories: list[Trajectory] = []\n",
    "    for row in range(request.start, request.stop):\n",
    "        if not trajectories:\n",
    "            trajectories = sorted(\n",
    "                (\n",
    "                    Trajectory(episode=episode, tokenizer=tokenizer)\n",
    "                    for episode in episode_buffer.episodes\n",
    "                ),\n",
    "                key=lambda trajectory: trajectory.score(),\n",
    "            )\n",
    "        selected_trajectories: list[Trajectory] = []\n",
    "        for i in range(0, len(trajectories), -1):\n",
    "            if (\n",
    "                trajectories[i].token_count\n",
    "                + sum(t.token_count for t in selected_trajectories)\n",
    "                > request.seqlen\n",
    "            ):\n",
    "                continue\n",
    "            selected_trajectories.append(trajectories.pop(i))\n",
    "        for trajectory in selected_trajectories:\n",
    "            trajectory.episode.weight *= episode_buffer.episode_decay\n",
    "            for completion in trajectory.terminus.ancestors(including_self=True):\n",
    "                completion.weight *= episode_buffer.completion_decay\n",
    "        tokens[row] = tokenizer.encode(\n",
    "            [\n",
    "                trajectory.terminus.all_message_params()\n",
    "                for trajectory in selected_trajectories\n",
    "            ],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        )\n",
    "        replacement_token = \"<|reserved_special_token_250|>\"\n",
    "        mask = tokenizer.encode(\n",
    "            [trajectory.terminus.all_message_params(replacement_token=replacement_token) for trajectory in selected_trajectories],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        ) == tokenizer.get_token_id(replacement_token)\n",
    "        mask_size = mask.sum()\n",
    "        advantages[row] = torch.full_like(\n",
    "            mask, fill_value=torch.nan, dtype=torch.float32\n",
    "        )\n",
    "        advantages[row][mask] = torch.tensor(\n",
    "            list(\n",
    "                advantage\n",
    "                for trajectory in selected_trajectories\n",
    "                for advantage in trajectory.terminus.all_token_advantages()\n",
    "            )[:mask_size]\n",
    "        )\n",
    "        logprobs[row] = torch.full_like(mask, fill_value=torch.nan, dtype=torch.float32)\n",
    "        logprobs[row][mask] = torch.tensor(\n",
    "            list(\n",
    "                advantage\n",
    "                for trajectory in selected_trajectories\n",
    "                for advantage in trajectory.terminus.all_logprobs()\n",
    "            )[:mask_size]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in episode_buffer.episodes:\n",
    "    for descendent in episode.completion.descendants(including_self=True):\n",
    "        descendent._cached_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(\n",
    "    uncached is cached\n",
    "    for uncached, cached in zip(\n",
    "        *(\n",
    "            [\n",
    "                episode.best_leaf(tokenizer, cache=cache)\n",
    "                for episode in episode_buffer.episodes\n",
    "            ]\n",
    "            for cache in [False, True]\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "best_leaves = [episode.best_leaf(tokenizer, cache=True) for episode in episode_buffer.episodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [leaf.all_abs_advantage_per_token(tokenizer, cache=True) for leaf in best_leaves]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Absolute Advantage Per Token: 0.0043192790392456125\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Average Absolute Advantage Per Token:\",\n",
    "    sum(\n",
    "        episode.best_leaf(tokenizer).all_abs_advantage_per_token(tokenizer)\n",
    "        for episode in episode_buffer.episodes\n",
    "    )\n",
    "    / len(episode_buffer.episodes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for _ in episode_buffer.episodes[0].completion.descendants())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00029788501638367595"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf = episode_buffer.episodes[0].best_leaf(tokenizer)\n",
    "leaf.all_abs_advantage() / leaf.all_token_count(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze the cards shown to each player:\n",
      "\n",
      "1. Blake shows the Lounge to Robert.\n",
      "2. Robert shows the Knife to Blake.\n",
      "3. Joel shows the Knife to Blake.\n",
      "4. Blake shows the Candlestick to Robert.\n",
      "5. Joel shows a card to Blake (must be Mrs. White or the Lead Pipe).\n",
      "6. Blake shows a card to Joel (must be Mr. Green or the Dining Room).\n",
      "7. Joel shows the Dining Room to Robert.\n",
      "8. Joel shows a card to Blake (must be the Lead Pipe).\n",
      "9. Blake shows a card to Robert (must be Mr. Green).\n",
      "\n",
      "From the information above, we can determine the following:\n",
      "\n",
      "- Robert has Miss Scarlet and the Knife.\n",
      "- Joel has Mrs. White and the Dining Room.\n",
      "- Blake has Mr. Green and the Candlestick.\n",
      "\n",
      "Since Blake showed the Candlestick to Robert, and Robert has the Knife, we know that the Candlestick is not the Knife. Therefore, the Candlestick must be the weapon in the Lounge.\n",
      "\n",
      "Similarly, since Blake showed a card to Joel (the Lead Pipe), and Joel has the Dining Room, we know that the Lead Pipe is not the Dining Room. Therefore, the Lead Pipe must be the weapon in the Hall.\n",
      "\n",
      "The only remaining card is the Mr. Green in the Dining Room.\n",
      "\n",
      "So, the facedown cards in the middle of the table are:\n",
      "\n",
      "Suspect: Miss Scarlet\n",
      "Weapon: Candlestick\n",
      "Room: Lounge\n",
      "\n",
      "The other two cards, Mr. Green and the Lead Pipe, were revealed during the game. The remaining card, Mrs. White, must be the suspect in the Dining Room.\n"
     ]
    }
   ],
   "source": [
    "print(completion.all_message_params()[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Trajectory:\n",
    "    episode: Episode\n",
    "    terminus: Completion\n",
    "    abs_advantage: float\n",
    "    token_count: int\n",
    "\n",
    "    def score(self) -> float:\n",
    "        return self.episode.weight * self.abs_advantage / self.token_count\n",
    "\n",
    "\n",
    "def best_trajectory(episode: Episode) -> Trajectory:\n",
    "    return max(\n",
    "        (\n",
    "            Trajectory(\n",
    "                episode=episode,\n",
    "                terminus=completion,\n",
    "                abs_advantage=completion.all_abs_advantage(),\n",
    "                token_count=completion.all_token_count(tokenizer),\n",
    "            )\n",
    "            for completion in episode.completion.leaves()\n",
    "        ),\n",
    "        key=lambda t: t.abs_advantage / t.token_count,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 8192\n",
    "rows = 64\n",
    "\n",
    "tokens_file = torch.empty(rows * seqlen, dtype=torch.int64)\n",
    "tokens_file.numpy().tofile(\"/tmp/tokens.bin\")\n",
    "\n",
    "advantages_file = torch.empty(rows * seqlen, dtype=torch.float32)\n",
    "advantages_file.numpy().tofile(\"/tmp/advantages.bin\")\n",
    "\n",
    "logprobs_file = torch.empty(rows * seqlen, dtype=torch.float32)\n",
    "logprobs_file.numpy().tofile(\"/tmp/logprobs.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 76\u001b[0m\n\u001b[1;32m     67\u001b[0m         logprobs[row] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull_like(mask, fill_value\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnan, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     68\u001b[0m         logprobs[row][mask] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m     70\u001b[0m                 advantage\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m             )[:mask_size]\n\u001b[1;32m     74\u001b[0m         )\n\u001b[0;32m---> 76\u001b[0m \u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/tokens.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43madvantages_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/advantages.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/logprobs.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[157], line 44\u001b[0m, in \u001b[0;36mhandle_request\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     selected_trajectories\u001b[38;5;241m.\u001b[39mappend(trajectories\u001b[38;5;241m.\u001b[39mpop(i))\n\u001b[0;32m---> 44\u001b[0m tokens[row] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_message_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mselected_trajectories\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m replacement_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|reserved_special_token_250|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m mask \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m     54\u001b[0m     [trajectory\u001b[38;5;241m.\u001b[39mterminus\u001b[38;5;241m.\u001b[39mall_message_params(replacement_token\u001b[38;5;241m=\u001b[39mreplacement_token) \u001b[38;5;28;01mfor\u001b[39;00m trajectory \u001b[38;5;129;01min\u001b[39;00m selected_trajectories],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     concatenate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m     seqlen\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mseqlen,\n\u001b[1;32m     57\u001b[0m ) \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mget_token_id(replacement_token)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/tokenizer.py:50\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[0;34m(self, messages, add_generation_prompt, continue_final_message, concatenate, seqlen)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate \u001b[38;5;241m=\u001b[39m generate  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     49\u001b[0m pad_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(token_ids[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m concatenate:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "class Request(BaseModel):\n",
    "    tokens_filename: str\n",
    "    advantages_filename: str\n",
    "    logprobs_filename: str\n",
    "    rows: int\n",
    "    seqlen: int\n",
    "    start: int\n",
    "    stop: int\n",
    "\n",
    "\n",
    "def handle_request(request: Request) -> None:\n",
    "    tokens = torch.from_file(\n",
    "        request.tokens_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.int64,\n",
    "    ).view(-1, request.seqlen)\n",
    "    advantages = torch.from_file(\n",
    "        request.advantages_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    logprobs = torch.from_file(\n",
    "        request.logprobs_filename,\n",
    "        shared=True,\n",
    "        size=request.rows * request.seqlen,\n",
    "        dtype=torch.float32,\n",
    "    ).view(-1, request.seqlen)\n",
    "    trajectories = sorted(\n",
    "        (best_trajectory(episode) for episode in buffer),\n",
    "        key=lambda t: t.score(),\n",
    "    )\n",
    "    for row in range(request.start, request.stop):\n",
    "        selected_trajectories: list[Trajectory] = []\n",
    "        for i in range(0, len(trajectories), -1):\n",
    "            if (\n",
    "                trajectories[i].token_count\n",
    "                + sum(t.token_count for t in selected_trajectories)\n",
    "                > request.seqlen\n",
    "            ):\n",
    "                continue\n",
    "            selected_trajectories.append(trajectories.pop(i))\n",
    "        tokens[row] = tokenizer.encode(\n",
    "            [\n",
    "                trajectory.terminus.all_message_params()\n",
    "                for trajectory in selected_trajectories\n",
    "            ],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        )\n",
    "        replacement_token = \"<|reserved_special_token_250|>\"\n",
    "        mask = tokenizer.encode(\n",
    "            [trajectory.terminus.all_message_params(replacement_token=replacement_token) for trajectory in selected_trajectories],  # type: ignore\n",
    "            concatenate=True,\n",
    "            seqlen=request.seqlen,\n",
    "        ) == tokenizer.get_token_id(replacement_token)\n",
    "        mask_size = mask.sum()\n",
    "        advantages[row] = torch.full_like(mask, fill_value=torch.nan, dtype=torch.float32)\n",
    "        advantages[row][mask] = torch.tensor(\n",
    "            list(\n",
    "                advantage\n",
    "                for trajectory in selected_trajectories\n",
    "                for advantage in trajectory.terminus.all_token_advantages()\n",
    "            )[:mask_size]\n",
    "        )\n",
    "        logprobs[row] = torch.full_like(mask, fill_value=torch.nan, dtype=torch.float32)\n",
    "        logprobs[row][mask] = torch.tensor(\n",
    "            list(\n",
    "                advantage\n",
    "                for trajectory in selected_trajectories\n",
    "                for advantage in trajectory.terminus.all_logprobs()\n",
    "            )[:mask_size]\n",
    "        )\n",
    "\n",
    "handle_request(\n",
    "    Request(\n",
    "        tokens_filename=\"/tmp/tokens.bin\",\n",
    "        advantages_filename=\"/tmp/advantages.bin\",\n",
    "        logprobs_filename=\"/tmp/logprobs.bin\",\n",
    "        rows=64,\n",
    "        seqlen=8192,\n",
    "        start=0,\n",
    "        stop=1,\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
