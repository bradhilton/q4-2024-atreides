{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(\n",
      "    dim=512,\n",
      "    n_layers=4,\n",
      "    n_heads=8,\n",
      "    n_kv_heads=2,\n",
      "    vocab_size=128256,\n",
      "    multiple_of=256,\n",
      "    ffn_dim_multiplier=1.5,\n",
      "    norm_eps=1e-05,\n",
      "    rope_theta=500000.0,\n",
      "    use_scaled_rope=True,\n",
      "    max_batch_size=16,\n",
      "    max_seq_len=512,\n",
      "    vision_chunk_size=-1,\n",
      "    vision_max_num_chunks=4,\n",
      "    vision_num_cross_attention_layers=-1,\n",
      ")\n",
      "Model Device: cuda:0\n",
      "Trainable Parameters: 146,543,104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216dd057835f4837af288eadbf35b565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ed0022cf11451ebde9dc3dbcb3634b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2930862080 [00:00<?, ?token/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1612: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "1405it [02:10, 10.80it/s]/2930862080 [02:10<6:12:35, 130588.59token/s, loss=8.1, val_loss=8.43, lr=0.000677] \n",
      "  0%|          | 11509760/2930862080 [02:10<9:09:58, 88469.39token/s, loss=8.1, val_loss=8.43, lr=0.000677] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 129\u001b[0m\n\u001b[1;32m    125\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mpretrain_tokens, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m--> 129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfineweb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_token_threshold\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupdate_val_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "Cell \u001b[0;32mIn[2], line 70\u001b[0m, in \u001b[0;36mbatches\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     68\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m (model_args\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m model_args\u001b[38;5;241m.\u001b[39mmax_batch_size\n\u001b[1;32m     69\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 70\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/datasets/iterable_dataset.py:2012\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2009\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   2010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2012\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;49;00m\n\u001b[1;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;49;00m\n\u001b[1;32m   2016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_apply_feature_types_on_example\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_per_repo_id\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/datasets/iterable_dataset.py:1203\u001b[0m, in \u001b[0;36mFilteredExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m key, formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1203\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter()\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/datasets/iterable_dataset.py:1261\u001b[0m, in \u001b[0;36mFilteredExamplesIterable._iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;66;03m# If not batched, we can apply the filtering function direcly\u001b[39;00m\n\u001b[0;32m-> 1261\u001b[0m         example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m format_dict(example) \u001b[38;5;28;01mif\u001b[39;00m format_dict \u001b[38;5;28;01melse\u001b[39;00m example\n\u001b[1;32m   1263\u001b[0m         function_args \u001b[38;5;241m=\u001b[39m [inputs] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [inputs[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_columns]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from contextlib import nullcontext\n",
    "import datasets\n",
    "from lib.llama3.reference_impl.model import ModelArgs, Transformer\n",
    "from lib.utils import black_print\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "from llama_models.sku_list import resolve_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.adamw import AdamW\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "llama3_2_1B = resolve_model(\"Llama3.2-1B\")\n",
    "assert llama3_2_1B is not None\n",
    "params = llama3_2_1B.arch_args\n",
    "params[\"dim\"] //= 4\n",
    "params[\"n_heads\"] //= 4\n",
    "params[\"n_kv_heads\"] //= 4\n",
    "params[\"n_layers\"] //= 4\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    max_seq_len=512,\n",
    "    max_batch_size=16,\n",
    "    **params,\n",
    ")\n",
    "micro_step_tokens = model_args.max_seq_len * model_args.max_batch_size\n",
    "black_print(model_args)\n",
    "\n",
    "model = Transformer(model_args)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "    model.compile()\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    context = (\n",
    "        torch.autocast(\n",
    "            device_type=\"cuda\",\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        if torch.cuda.is_bf16_supported()\n",
    "        else nullcontext()\n",
    "    )\n",
    "elif torch.backends.mps.is_available():\n",
    "    model = model.to(\"mps\")\n",
    "    context = nullcontext()\n",
    "else:\n",
    "    model = model.to(\"cpu\")\n",
    "    context = nullcontext()\n",
    "model_device = next(model.parameters()).device\n",
    "print(f\"Model Device: {model_device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable Parameters: {total_params:,}\")\n",
    "pretrain_tokens = int(total_params * 20)  # Chinchilla-optimal\n",
    "\n",
    "tokenizer = Tokenizer.get_instance()\n",
    "\n",
    "fineweb: datasets.IterableDataset = datasets.load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=True\n",
    ")  # type: ignore\n",
    "fineweb_ultra_mini: datasets.IterableDataset = datasets.load_dataset(\n",
    "    \"reflex-ai/fineweb-ultra-mini\", split=\"train\", streaming=True\n",
    ")  # type: ignore\n",
    "\n",
    "\n",
    "def batches(\n",
    "    dataset: datasets.IterableDataset,\n",
    ") -> Iterable[tuple[torch.Tensor, torch.Tensor]]:\n",
    "    max_tokens = (model_args.max_seq_len + 1) * model_args.max_batch_size\n",
    "    tokens = []\n",
    "    for document in dataset:\n",
    "        tokens += tokenizer.encode(document[\"text\"], bos=True, eos=True)\n",
    "        if len(tokens) >= max_tokens:\n",
    "            batch = torch.tensor(\n",
    "                tokens[:max_tokens], dtype=torch.long, device=model_device\n",
    "            ).reshape(model_args.max_batch_size, -1)\n",
    "            yield batch[:, :-1], batch[:, 1:]\n",
    "            tokens = tokens[max_tokens:]\n",
    "    if tokens:\n",
    "        pad_length = max_tokens - len(tokens)\n",
    "        tokens += [tokenizer.pad_id] * pad_length\n",
    "        batch = torch.tensor(tokens, dtype=torch.long, device=model_device).reshape(\n",
    "            model_args.max_batch_size, -1\n",
    "        )\n",
    "        yield batch[:, :-1], batch[:, 1:]\n",
    "\n",
    "\n",
    "peak_lr = 6e-4 / ((total_params * 1e-9) ** (1 / 3))\n",
    "optimizer = AdamW(model.parameters(), lr=peak_lr)\n",
    "step_tokens = 2**19\n",
    "cosine_annealing_steps = pretrain_tokens // step_tokens\n",
    "warmup_steps = cosine_annealing_steps // 150\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    [\n",
    "        torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, lambda x: min((x + 1) / warmup_steps, 1)\n",
    "        ),\n",
    "        torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_annealing_steps, eta_min=peak_lr * 0.01),  # type: ignore\n",
    "    ],\n",
    "    milestones=[warmup_steps],\n",
    ")\n",
    "grad_accum_steps = step_tokens // micro_step_tokens\n",
    "grad_accum_threshold = step_tokens\n",
    "\n",
    "val_size = 1000\n",
    "val_ids = {document[\"id\"] for document in fineweb_ultra_mini.take(val_size)}\n",
    "val_token_threshold = 0\n",
    "val_loss = 12.0\n",
    "\n",
    "\n",
    "def update_val_loss() -> None:\n",
    "    global val_token_threshold, val_loss\n",
    "    val_token_threshold += 10_000_000\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        num_batches = 0\n",
    "        for val_x, val_y in batches(fineweb_ultra_mini.take(val_size)):\n",
    "            val_logits = model(val_x, 0)\n",
    "            loss += F.cross_entropy(\n",
    "                val_logits.view(-1, model.vocab_size), val_y.flatten()\n",
    "            ).item()\n",
    "            num_batches += 1\n",
    "        val_loss = loss / num_batches\n",
    "    model.train()\n",
    "\n",
    "\n",
    "with tqdm(total=pretrain_tokens, unit=\"token\") as pbar:\n",
    "    for x, y in tqdm(batches(fineweb.filter(lambda x: x[\"id\"] not in val_ids))):\n",
    "        if pbar.n >= val_token_threshold:\n",
    "            update_val_loss()\n",
    "\n",
    "        with context:\n",
    "            logits = model(x, 0)\n",
    "            loss = F.cross_entropy(logits.view(-1, model.vocab_size), y.flatten())\n",
    "        train_loss = loss.item()\n",
    "        loss /= grad_accum_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if pbar.n >= grad_accum_threshold:\n",
    "            grad_accum_threshold += step_tokens\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        pbar.update(x.numel())\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 7.03 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.llama3.reference_impl.generation import Llama\n",
    "import os\n",
    "\n",
    "llama3_2_1B_ckpt_dir = os.path.expanduser(\"~/.llama/checkpoints/Llama3.2-1B/original/\")\n",
    "tokenizer_path = llama3_2_1B_ckpt_dir + \"tokenizer.model\"\n",
    "\n",
    "llama = Llama.build(\n",
    "    ckpt_dir=llama3_2_1B_ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=512,\n",
    "    max_batch_size=1,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "next(llama.model.parameters()).device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I have no idea, but I think I can'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.text_completion(\"What is the meaning of life?\", max_gen_len=10).generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of Shakespeare's text:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "\n",
      "Total characters: 1115394\n",
      "Total lines: 40000\n"
     ]
    }
   ],
   "source": [
    "shakespeare_text = open(\"./data/tinyshakespeare.txt\", \"r\").read()\n",
    "\n",
    "# Display the first few lines\n",
    "print(\"First few lines of Shakespeare's text:\")\n",
    "print(shakespeare_text[:500])\n",
    "\n",
    "# Get some statistics\n",
    "total_chars = len(shakespeare_text)\n",
    "total_lines = shakespeare_text.count(\"\\n\")\n",
    "\n",
    "print(f\"\\nTotal characters: {total_chars}\")\n",
    "print(f\"Total lines: {total_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "Vocabulary Size: 128256\n",
      "Number of Layers: 4\n",
      "Embedding Dimension: 512\n",
      "Number of Attention Heads: 8\n",
      "Max Sequence Length: 512\n",
      "Feedforward Dimension: 2048\n",
      "\n",
      "Parameter Initialization:\n",
      "tok_embeddings.weight: Initialized\n",
      "layers.0.attention.wq.weight: Initialized\n",
      "layers.0.attention.wk.weight: Initialized\n",
      "layers.0.attention.wv.weight: Initialized\n",
      "layers.0.attention.wo.weight: Initialized\n",
      "layers.0.feed_forward.w1.weight: Initialized\n",
      "layers.0.feed_forward.w2.weight: Initialized\n",
      "layers.0.feed_forward.w3.weight: Initialized\n",
      "layers.0.attention_norm.weight: Initialized\n",
      "layers.0.ffn_norm.weight: Initialized\n",
      "layers.1.attention.wq.weight: Initialized\n",
      "layers.1.attention.wk.weight: Initialized\n",
      "layers.1.attention.wv.weight: Initialized\n",
      "layers.1.attention.wo.weight: Initialized\n",
      "layers.1.feed_forward.w1.weight: Initialized\n",
      "layers.1.feed_forward.w2.weight: Initialized\n",
      "layers.1.feed_forward.w3.weight: Initialized\n",
      "layers.1.attention_norm.weight: Initialized\n",
      "layers.1.ffn_norm.weight: Initialized\n",
      "layers.2.attention.wq.weight: Initialized\n",
      "layers.2.attention.wk.weight: Initialized\n",
      "layers.2.attention.wv.weight: Initialized\n",
      "layers.2.attention.wo.weight: Initialized\n",
      "layers.2.feed_forward.w1.weight: Initialized\n",
      "layers.2.feed_forward.w2.weight: Initialized\n",
      "layers.2.feed_forward.w3.weight: Initialized\n",
      "layers.2.attention_norm.weight: Initialized\n",
      "layers.2.ffn_norm.weight: Initialized\n",
      "layers.3.attention.wq.weight: Initialized\n",
      "layers.3.attention.wk.weight: Initialized\n",
      "layers.3.attention.wv.weight: Initialized\n",
      "layers.3.attention.wo.weight: Initialized\n",
      "layers.3.feed_forward.w1.weight: Initialized\n",
      "layers.3.feed_forward.w2.weight: Initialized\n",
      "layers.3.feed_forward.w3.weight: Initialized\n",
      "layers.3.attention_norm.weight: Initialized\n",
      "layers.3.ffn_norm.weight: Initialized\n",
      "norm.weight: Initialized\n",
      "output.weight: Initialized\n",
      "\n",
      "Key Component Shapes:\n",
      "Token Embeddings: torch.Size([128256, 512])\n",
      "Output Layer: torch.Size([128256, 512])\n",
      "First Layer Query Weight: torch.Size([512, 512])\n",
      "First Layer Key Weight: torch.Size([128, 512])\n",
      "First Layer Value Weight: torch.Size([128, 512])\n",
      "\n",
      "NaN/Inf Check:\n",
      "tok_embeddings.weight: OK\n",
      "layers.0.attention.wq.weight: OK\n",
      "layers.0.attention.wk.weight: OK\n",
      "layers.0.attention.wv.weight: OK\n",
      "layers.0.attention.wo.weight: OK\n",
      "layers.0.feed_forward.w1.weight: OK\n",
      "layers.0.feed_forward.w2.weight: OK\n",
      "layers.0.feed_forward.w3.weight: OK\n",
      "layers.0.attention_norm.weight: OK\n",
      "layers.0.ffn_norm.weight: OK\n",
      "layers.1.attention.wq.weight: OK\n",
      "layers.1.attention.wk.weight: OK\n",
      "layers.1.attention.wv.weight: OK\n",
      "layers.1.attention.wo.weight: OK\n",
      "layers.1.feed_forward.w1.weight: OK\n",
      "layers.1.feed_forward.w2.weight: OK\n",
      "layers.1.feed_forward.w3.weight: OK\n",
      "layers.1.attention_norm.weight: OK\n",
      "layers.1.ffn_norm.weight: OK\n",
      "layers.2.attention.wq.weight: OK\n",
      "layers.2.attention.wk.weight: OK\n",
      "layers.2.attention.wv.weight: OK\n",
      "layers.2.attention.wo.weight: OK\n",
      "layers.2.feed_forward.w1.weight: OK\n",
      "layers.2.feed_forward.w2.weight: OK\n",
      "layers.2.feed_forward.w3.weight: OK\n",
      "layers.2.attention_norm.weight: OK\n",
      "layers.2.ffn_norm.weight: OK\n",
      "layers.3.attention.wq.weight: OK\n",
      "layers.3.attention.wk.weight: OK\n",
      "layers.3.attention.wv.weight: OK\n",
      "layers.3.attention.wo.weight: OK\n",
      "layers.3.feed_forward.w1.weight: OK\n",
      "layers.3.feed_forward.w2.weight: OK\n",
      "layers.3.feed_forward.w3.weight: OK\n",
      "layers.3.attention_norm.weight: OK\n",
      "layers.3.ffn_norm.weight: OK\n",
      "norm.weight: OK\n",
      "output.weight: OK\n"
     ]
    }
   ],
   "source": [
    "# Check model parameters\n",
    "print(\"Model Parameters:\")\n",
    "print(f\"Vocabulary Size: {model.vocab_size}\")\n",
    "print(f\"Number of Layers: {model.n_layers}\")\n",
    "print(f\"Embedding Dimension: {model.params.dim}\")\n",
    "print(f\"Number of Attention Heads: {model.params.n_heads}\")\n",
    "print(f\"Max Sequence Length: {model.params.max_seq_len}\")\n",
    "print(f\"Feedforward Dimension: {model.layers[0].feed_forward.w1.out_features}\")\n",
    "\n",
    "# Check if parameters are initialized\n",
    "print(\"\\nParameter Initialization:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {'Initialized' if param.sum().item() != 0 else 'Not initialized'}\")\n",
    "\n",
    "# Verify shapes of key components\n",
    "print(\"\\nKey Component Shapes:\")\n",
    "print(f\"Token Embeddings: {model.tok_embeddings.weight.shape}\")\n",
    "print(f\"Output Layer: {model.output.weight.shape}\")\n",
    "print(f\"First Layer Query Weight: {model.layers[0].attention.wq.weight.shape}\")\n",
    "print(f\"First Layer Key Weight: {model.layers[0].attention.wk.weight.shape}\")\n",
    "print(f\"First Layer Value Weight: {model.layers[0].attention.wv.weight.shape}\")\n",
    "\n",
    "# Check for NaNs or infinities\n",
    "print(\"\\nNaN/Inf Check:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "        print(f\"Warning: {name} contains NaN or Inf values\")\n",
    "    else:\n",
    "        print(f\"{name}: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Trainable Parameters: 146,543,104\n",
      "Approximate Model Size: 0.55 GB\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate size in gigabytes (assuming float32 parameters)\n",
    "size_in_gb = total_params * 4 / (1024**3)  # 4 bytes per float32 parameter\n",
    "print(f\"Approximate Model Size: {size_in_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to checkpoints/llama3_model_checkpoint.pth\n",
      "Model arguments saved to checkpoints/llama3_model_args.json\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint the model\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints if it doesn't exist\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save the model state\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"llama3_model_checkpoint.pth\")\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Save the model arguments\n",
    "import json\n",
    "\n",
    "model_args_path = os.path.join(checkpoint_dir, \"llama3_model_args.json\")\n",
    "with open(model_args_path, 'w') as f:\n",
    "    json.dump(vars(model.params), f, indent=2)\n",
    "\n",
    "print(f\"Model arguments saved to {model_args_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, no will to die through love itself.\n",
      "Why, how to tread how do to honour newly your brother,\n",
      "But to have five thousand thanks too much to Clarence:\n",
      "I'll give my soul,\n",
      "To should our speech of gold and too:\n",
      "You are dear train, and father, poor brother,\n",
      "Ere further conference with a passing small.\n",
      "O Dorsetable.\n",
      "Your sense may beggarly the tomb,\n",
      "And bid me mistress sit dispatch: past the boy,\n",
      "And well lost with one thing just proportion,\n",
      "And over the board, under his liking!\n",
      "And all the watchful eye of dear faith,\n",
      "More fierce and an inditeous wrath!\n",
      "How well, lords, I befall, and lay,\n",
      "Is not forgot the tyrant, to fill the crown,\n",
      "And manage of your glorious sun: regent join'd!\n",
      "Yet would youravenousoddess, that went;\n",
      "And well we have heard of all run a needful';\n",
      "Anduile me with the root\n",
      "And buryWhat! myself become a tyrant\n",
      "Stands without the brat's king in Bosworth\n",
      "To leap upon a black tidings was;\n",
      "And in all my tumble down: great leaving me,\n",
      "'Twere a bloody axe to that makes your countenance.\n",
      "See how, command of the head brings a tyrant,\n",
      "Found that the fair volume of York, God,\n",
      "Vouches her career me that was in'd death.\n",
      "\n",
      "EDWARD:\n",
      "Let me be not; and take me no fight:\n",
      "I'll none but well-spAGUE:\n",
      "My lord, yea, fealty like you night;\n",
      "And your mind are fled; and so most I stay,\n",
      "Together with many gawdy; and welcome down\n",
      "'Tis'd by them-ch committed by envious,\n",
      "And ancient feast, like a cover to obey.\n",
      "But if my ghostly heart's sin,\n",
      "Hark you in this and left protectors,\n",
      "And chivalrous'd my common men's music of alike,\n",
      "Since knees with O, the book of Percy,\n",
      "This heavy chamber-potless of pestil\n",
      "That brings the white man? that must we mock thee.\n",
      "Therefore, what a greater thaniger in this cell,\n",
      "Is ignorant with a dyingadvisedly com,\n",
      "Then vial and kin and Tybroke,\n",
      "This music crept like the grave with the bloody fray.\n",
      "\n",
      "RICHARD:\n",
      "O Duke of Somerset, Richard, dull delay\n",
      "WillDie in the vice: little thinks not goodly, my lord,\n",
      "Come quickly\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(Llama(model, llama.tokenizer, model_args).text_completion(\"To be or not to be,\", temperature=1.0, max_gen_len=500, echo=True).generation.split(\"<|begin_of_text|>\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
