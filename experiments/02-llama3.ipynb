{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "magic"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/ender-research/atreides/e/AT-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20241011_232928-jasry94t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/jasry94t' target=\"_blank\">earthy-brook-29</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/jasry94t' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/jasry94t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(\n",
      "    dim=512,\n",
      "    n_layers=4,\n",
      "    n_heads=8,\n",
      "    n_kv_heads=2,\n",
      "    vocab_size=128256,\n",
      "    multiple_of=256,\n",
      "    ffn_dim_multiplier=1.5,\n",
      "    norm_eps=1e-05,\n",
      "    rope_theta=500000.0,\n",
      "    use_scaled_rope=True,\n",
      "    max_batch_size=4,\n",
      "    max_seq_len=512,\n",
      "    vision_chunk_size=-1,\n",
      "    vision_max_num_chunks=4,\n",
      "    vision_num_cross_attention_layers=-1,\n",
      ")\n",
      "Model Device: cuda:0\n",
      "Trainable Parameters: 146,543,104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cfc06db7e8405d9378201d4c5d0318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c31dcd35ecf42188ff23b0b0ef3ace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   1%|          | 19,398,656/2,930,862,080 [16:59<41:06:49, 19670.80token/s, train_loss=7.49, val_loss=8.32, lr=0.00111] /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "Training model:   1%|â–         | 37,279,744/2,930,862,080 [32:43<42:19:58, 18986.90token/s, train_loss=6.76, val_loss=7.1, lr=0.00114] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 303\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m (distributed_model\u001b[38;5;241m.\u001b[39mno_sync \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nullcontext)():\n\u001b[1;32m    299\u001b[0m                 loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    301\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[1;32m    302\u001b[0m             {\n\u001b[0;32m--> 303\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    305\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    306\u001b[0m             }\n\u001b[1;32m    307\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     neptune_run\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from contextlib import nullcontext\n",
    "import datasets\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from lib.llama3.reference_impl.model import ModelArgs, Transformer\n",
    "from lib.utils import black_print\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "from llama_models.sku_list import resolve_model\n",
    "import neptune\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.distributed\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.adamw import AdamW\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if torch.cuda.is_available() and int(os.environ.get(\"RANK\", -1)) != -1:\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    rank = torch.distributed.get_rank()\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    world_size = torch.distributed.get_world_size()\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "if world_size > 1:\n",
    "    print(f\"Rank: {rank} - Local Rank: {local_rank} - World Size: {world_size}\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Llama3 training script\")\n",
    "parser.add_argument(\"--name\", type=str, default=None, help=\"Run name\")\n",
    "args, _ = parser.parse_known_args()\n",
    "run_name = args.name\n",
    "run_path = f\"./runs/{run_name}\"\n",
    "gs_path = f\"gs://atreides/experiments/runs/{run_name}\"\n",
    "if run_name and not os.path.exists(run_path):\n",
    "    os.makedirs(run_path, exist_ok=True)\n",
    "    subprocess.Popen(\n",
    "        f\"gsutil -m rsync -r {gs_path} {run_path}\",\n",
    "        shell=True,\n",
    "    ).wait()\n",
    "params_path = f\"{run_path}/params.json\"\n",
    "model_path = f\"{run_path}/model.pth\"\n",
    "optimizer_path = f\"{run_path}/optimizer.pth\"\n",
    "scheduler_path = f\"{run_path}/scheduler.pth\"\n",
    "dataset_state_path = f\"{run_path}/dataset-state.json\"\n",
    "training_state_path = f\"{run_path}/training-state.json\"\n",
    "\n",
    "if rank == 0:\n",
    "    neptune_run = neptune.init_run(custom_run_id=run_name)\n",
    "    wandb_run = wandb.init(name=run_name, resume=\"allow\", id=run_name)\n",
    "\n",
    "llama3_2_1B = resolve_model(\"Llama3.2-1B\")\n",
    "assert llama3_2_1B is not None\n",
    "params = llama3_2_1B.arch_args\n",
    "params[\"dim\"] //= 4\n",
    "params[\"n_heads\"] //= 4\n",
    "params[\"n_kv_heads\"] //= 4\n",
    "params[\"n_layers\"] //= 4\n",
    "params[\"max_seq_len\"] = 512\n",
    "params[\"max_batch_size\"] = 4\n",
    "params[\"use_flash_attention\"] = True\n",
    "\n",
    "if os.path.exists(params_path):\n",
    "    params = json.load(open(params_path, \"r\"))\n",
    "elif run_name:\n",
    "    os.makedirs(run_path, exist_ok=True)\n",
    "    json.dump(params, open(params_path, \"w\"))\n",
    "\n",
    "model_args = ModelArgs(**params)\n",
    "if rank == 0:\n",
    "    black_print(model_args)\n",
    "micro_step_tokens = model_args.max_seq_len * model_args.max_batch_size\n",
    "\n",
    "model = Transformer(model_args)\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(f\"cuda:{local_rank}\")\n",
    "    # model.compile()\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    # context = (\n",
    "    #     torch.autocast(\n",
    "    #         device_type=\"cuda\",\n",
    "    #         dtype=torch.bfloat16,\n",
    "    #     )\n",
    "    #     if torch.cuda.is_bf16_supported()\n",
    "    #     else nullcontext()\n",
    "    # )\n",
    "    context = nullcontext()\n",
    "elif torch.backends.mps.is_available():\n",
    "    model = model.to(\"mps\")\n",
    "    context = nullcontext()\n",
    "else:\n",
    "    model = model.to(\"cpu\")\n",
    "    context = nullcontext()\n",
    "model_device = next(model.parameters()).device\n",
    "if rank == 0:\n",
    "    print(f\"Model Device: {model_device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "if rank == 0:\n",
    "    print(f\"Trainable Parameters: {total_params:,}\")\n",
    "pretrain_tokens = int(total_params * 20)  # Chinchilla-optimal\n",
    "tokenizer = Tokenizer.get_instance()\n",
    "if rank != 0:\n",
    "    datasets.disable_progress_bars()\n",
    "dataset: datasets.IterableDataset = datasets.load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=True\n",
    ")  # type: ignore\n",
    "val_dataset: datasets.IterableDataset = datasets.load_dataset(\n",
    "    \"reflex-ai/fineweb-ultra-mini\", split=\"train\", streaming=True\n",
    ")  # type: ignore\n",
    "\n",
    "\n",
    "def batches(\n",
    "    dataset: datasets.IterableDataset,\n",
    ") -> Iterable[tuple[torch.Tensor, torch.Tensor]]:\n",
    "    max_tokens = (model_args.max_seq_len + 1) * model_args.max_batch_size\n",
    "    tokens = []\n",
    "    for index, document in enumerate(dataset):\n",
    "        if index % world_size != rank:\n",
    "            continue\n",
    "        tokens += tokenizer.encode(document[\"text\"], bos=True, eos=True)\n",
    "        if len(tokens) >= max_tokens:\n",
    "            batch = torch.tensor(\n",
    "                tokens[:max_tokens], dtype=torch.long, device=model_device\n",
    "            ).reshape(model_args.max_batch_size, -1)\n",
    "            yield batch[:, :-1], batch[:, 1:]\n",
    "            tokens = tokens[max_tokens:]\n",
    "    if tokens:\n",
    "        pad_length = max_tokens - len(tokens)\n",
    "        tokens += [tokenizer.pad_id] * pad_length\n",
    "        batch = torch.tensor(\n",
    "            tokens[:max_tokens], dtype=torch.long, device=model_device\n",
    "        ).reshape(model_args.max_batch_size, -1)\n",
    "        yield batch[:, :-1], batch[:, 1:]\n",
    "\n",
    "\n",
    "peak_lr = 6e-4 / ((total_params * 1e-9) ** (1 / 3))\n",
    "optimizer = AdamW(model.parameters(), lr=peak_lr)\n",
    "if os.path.exists(optimizer_path):\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, weights_only=True))\n",
    "step_tokens = 2**19\n",
    "cosine_annealing_steps = pretrain_tokens // step_tokens\n",
    "warmup_steps = cosine_annealing_steps // 150\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    [\n",
    "        torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=1 / warmup_steps,\n",
    "            end_factor=1,\n",
    "            total_iters=warmup_steps,\n",
    "        ),\n",
    "        torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_annealing_steps, eta_min=peak_lr * 0.01),  # type: ignore\n",
    "    ],\n",
    "    milestones=[warmup_steps],\n",
    ")\n",
    "if os.path.exists(scheduler_path):\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, weights_only=True))\n",
    "grad_accum_steps = step_tokens // micro_step_tokens // world_size\n",
    "grad_accum_threshold = step_tokens\n",
    "\n",
    "if world_size > 1:\n",
    "    distributed_model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model, device_ids=[local_rank]\n",
    "    )\n",
    "else:\n",
    "    distributed_model = model\n",
    "\n",
    "\n",
    "training_state = (\n",
    "    json.loads(open(training_state_path, \"r\").read())\n",
    "    if os.path.exists(training_state_path)\n",
    "    else {}\n",
    ")\n",
    "\n",
    "train_loss = torch.tensor(training_state.get(\"train_loss\", 12.0)).to(model_device)\n",
    "next_train_loss = torch.tensor(0.0).to(model_device)\n",
    "\n",
    "val_size = 1000\n",
    "val_ids = {document[\"id\"] for document in val_dataset.take(val_size)}\n",
    "val_token_threshold = training_state.get(\"val_token_threshold\", 0)\n",
    "val_loss = torch.tensor(training_state.get(\"val_loss\", 12.0)).to(model_device)\n",
    "\n",
    "\n",
    "def update_val_loss() -> None:\n",
    "    global val_token_threshold, val_loss\n",
    "    val_token_threshold += 10_000_000\n",
    "    distributed_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss.zero_()\n",
    "        num_batches = 0\n",
    "        for val_x, val_y in batches(val_dataset.take(val_size)):\n",
    "            val_logits = distributed_model(val_x, 0)\n",
    "            val_loss += F.cross_entropy(\n",
    "                val_logits.view(-1, model.vocab_size), val_y.flatten()\n",
    "            )\n",
    "            num_batches += 1\n",
    "        val_loss /= num_batches\n",
    "        if world_size > 1:\n",
    "            torch.distributed.all_reduce(val_loss, op=torch.distributed.ReduceOp.AVG)\n",
    "    distributed_model.train()\n",
    "\n",
    "\n",
    "dataset = dataset.filter(lambda x: x[\"id\"] not in val_ids)\n",
    "if os.path.exists(dataset_state_path):\n",
    "    dataset.load_state_dict(json.load(open(dataset_state_path, \"r\")))\n",
    "save_frequecy = 10_000_000\n",
    "save_token_threshold = training_state.get(\"save_token_threshold\", save_frequecy)\n",
    "\n",
    "\n",
    "def save_state(pbar: tqdm) -> None:\n",
    "    if not run_name:\n",
    "        return\n",
    "    global save_token_threshold\n",
    "    save_token_threshold += save_frequecy\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "    torch.save(scheduler.state_dict(), scheduler_path)\n",
    "    json.dump(dataset.state_dict(), open(dataset_state_path, \"w\"))\n",
    "    json.dump(\n",
    "        {\n",
    "            \"val_loss\": val_loss.item(),\n",
    "            \"val_token_threshold\": val_token_threshold,\n",
    "            \"train_loss\": train_loss.item(),\n",
    "            \"save_token_threshold\": save_token_threshold,\n",
    "            \"progress\": pbar.n,\n",
    "        },\n",
    "        open(training_state_path, \"w\"),\n",
    "    )\n",
    "    subprocess.Popen(\n",
    "        f\"gsutil -m rsync -r {run_path} {gs_path}\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "\n",
    "with tqdm(\n",
    "    desc=\"Training model\",\n",
    "    total=pretrain_tokens,\n",
    "    disable=rank != 0,\n",
    "    unit=\"token\",\n",
    "    initial=training_state.get(\"progress\", 0),\n",
    "    bar_format=\"{l_bar}{bar}| {n:,}/{total:,} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\",\n",
    ") as pbar:\n",
    "    for x, y in batches(dataset):\n",
    "        with context:\n",
    "            logits = distributed_model(x, 0)\n",
    "            loss = F.cross_entropy(logits.view(-1, model.vocab_size), y.flatten())\n",
    "        loss /= grad_accum_steps\n",
    "        next_train_loss += loss\n",
    "\n",
    "        if rank == 0:\n",
    "            pbar.update(x.numel() * world_size)\n",
    "        else:\n",
    "            pbar.n += x.numel() * world_size\n",
    "\n",
    "        if pbar.n >= grad_accum_threshold:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            grad_accum_threshold += step_tokens\n",
    "            train_loss = next_train_loss\n",
    "            if world_size > 1:\n",
    "                torch.distributed.all_reduce(\n",
    "                    train_loss, op=torch.distributed.ReduceOp.AVG\n",
    "                )\n",
    "            next_train_loss = torch.tensor(0.0).to(model_device)\n",
    "            if rank == 0:\n",
    "                neptune_run[\"train/loss\"].append(train_loss.item(), step=pbar.n)\n",
    "                neptune_run[\"lr\"].append(scheduler.get_last_lr()[0], step=pbar.n)\n",
    "                wandb_run.log(\n",
    "                    {\n",
    "                        \"loss/train\": train_loss.item(),\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                    },\n",
    "                    step=pbar.n,\n",
    "                )\n",
    "            if pbar.n >= val_token_threshold:\n",
    "                update_val_loss()\n",
    "                if rank == 0:\n",
    "                    neptune_run[\"val/loss\"].append(val_loss.item(), step=pbar.n)\n",
    "                    wandb_run.log({\"loss/val\": val_loss.item()}, step=pbar.n)\n",
    "            if pbar.n >= save_token_threshold:\n",
    "                save_state(pbar)\n",
    "            if pbar.n >= pretrain_tokens:\n",
    "                break\n",
    "        else:\n",
    "            with (distributed_model.no_sync if world_size > 1 else nullcontext)():\n",
    "                loss.backward()\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"train_loss\": train_loss.item(),\n",
    "                \"val_loss\": val_loss.item(),\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "        )\n",
    "\n",
    "if rank == 0:\n",
    "    neptune_run.stop()\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex_iterable': {'shard_idx': 0, 'shard_example_idx': 3335},\n",
       " 'previous_state': None,\n",
       " 'num_examples_since_previous_state': 0,\n",
       " 'previous_state_example_idx': 3335}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex_iterable': {'shard_idx': 0, 'shard_example_idx': 177},\n",
       " 'previous_state': None,\n",
       " 'num_examples_since_previous_state': 0,\n",
       " 'previous_state_example_idx': 177}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.llama3.reference_impl.model import ModelArgs\n",
    "from llama_models.sku_list import resolve_model\n",
    "\n",
    "\n",
    "llama3_2_1B = resolve_model(\"Llama3.2-1B\")\n",
    "assert llama3_2_1B is not None\n",
    "params = llama3_2_1B.arch_args\n",
    "params[\"dim\"] //= 4\n",
    "params[\"n_heads\"] //= 4\n",
    "params[\"n_kv_heads\"] //= 4\n",
    "params[\"n_layers\"] //= 4\n",
    "\n",
    "class ExtendedModelArgs(ModelArgs):\n",
    "    flash_attention: bool = False\n",
    "\n",
    "ExtendedModelArgs(\n",
    "    max_seq_len=512,\n",
    "    max_batch_size=8,\n",
    "    flash_attention=False,\n",
    "    **params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 7.03 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.llama3.reference_impl.generation import Llama\n",
    "import os\n",
    "\n",
    "llama3_2_1B_ckpt_dir = os.path.expanduser(\"~/.llama/checkpoints/Llama3.2-1B/original/\")\n",
    "tokenizer_path = llama3_2_1B_ckpt_dir + \"tokenizer.model\"\n",
    "\n",
    "llama = Llama.build(\n",
    "    ckpt_dir=llama3_2_1B_ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=512,\n",
    "    max_batch_size=1,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "next(llama.model.parameters()).device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I have no idea, but I think I can'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.text_completion(\"What is the meaning of life?\", max_gen_len=10).generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of Shakespeare's text:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "\n",
      "Total characters: 1115394\n",
      "Total lines: 40000\n"
     ]
    }
   ],
   "source": [
    "shakespeare_text = open(\"./data/tinyshakespeare.txt\", \"r\").read()\n",
    "\n",
    "# Display the first few lines\n",
    "print(\"First few lines of Shakespeare's text:\")\n",
    "print(shakespeare_text[:500])\n",
    "\n",
    "# Get some statistics\n",
    "total_chars = len(shakespeare_text)\n",
    "total_lines = shakespeare_text.count(\"\\n\")\n",
    "\n",
    "print(f\"\\nTotal characters: {total_chars}\")\n",
    "print(f\"Total lines: {total_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "Vocabulary Size: 128256\n",
      "Number of Layers: 4\n",
      "Embedding Dimension: 512\n",
      "Number of Attention Heads: 8\n",
      "Max Sequence Length: 512\n",
      "Feedforward Dimension: 2048\n",
      "\n",
      "Parameter Initialization:\n",
      "tok_embeddings.weight: Initialized\n",
      "layers.0.attention.wq.weight: Initialized\n",
      "layers.0.attention.wk.weight: Initialized\n",
      "layers.0.attention.wv.weight: Initialized\n",
      "layers.0.attention.wo.weight: Initialized\n",
      "layers.0.feed_forward.w1.weight: Initialized\n",
      "layers.0.feed_forward.w2.weight: Initialized\n",
      "layers.0.feed_forward.w3.weight: Initialized\n",
      "layers.0.attention_norm.weight: Initialized\n",
      "layers.0.ffn_norm.weight: Initialized\n",
      "layers.1.attention.wq.weight: Initialized\n",
      "layers.1.attention.wk.weight: Initialized\n",
      "layers.1.attention.wv.weight: Initialized\n",
      "layers.1.attention.wo.weight: Initialized\n",
      "layers.1.feed_forward.w1.weight: Initialized\n",
      "layers.1.feed_forward.w2.weight: Initialized\n",
      "layers.1.feed_forward.w3.weight: Initialized\n",
      "layers.1.attention_norm.weight: Initialized\n",
      "layers.1.ffn_norm.weight: Initialized\n",
      "layers.2.attention.wq.weight: Initialized\n",
      "layers.2.attention.wk.weight: Initialized\n",
      "layers.2.attention.wv.weight: Initialized\n",
      "layers.2.attention.wo.weight: Initialized\n",
      "layers.2.feed_forward.w1.weight: Initialized\n",
      "layers.2.feed_forward.w2.weight: Initialized\n",
      "layers.2.feed_forward.w3.weight: Initialized\n",
      "layers.2.attention_norm.weight: Initialized\n",
      "layers.2.ffn_norm.weight: Initialized\n",
      "layers.3.attention.wq.weight: Initialized\n",
      "layers.3.attention.wk.weight: Initialized\n",
      "layers.3.attention.wv.weight: Initialized\n",
      "layers.3.attention.wo.weight: Initialized\n",
      "layers.3.feed_forward.w1.weight: Initialized\n",
      "layers.3.feed_forward.w2.weight: Initialized\n",
      "layers.3.feed_forward.w3.weight: Initialized\n",
      "layers.3.attention_norm.weight: Initialized\n",
      "layers.3.ffn_norm.weight: Initialized\n",
      "norm.weight: Initialized\n",
      "output.weight: Initialized\n",
      "\n",
      "Key Component Shapes:\n",
      "Token Embeddings: torch.Size([128256, 512])\n",
      "Output Layer: torch.Size([128256, 512])\n",
      "First Layer Query Weight: torch.Size([512, 512])\n",
      "First Layer Key Weight: torch.Size([128, 512])\n",
      "First Layer Value Weight: torch.Size([128, 512])\n",
      "\n",
      "NaN/Inf Check:\n",
      "tok_embeddings.weight: OK\n",
      "layers.0.attention.wq.weight: OK\n",
      "layers.0.attention.wk.weight: OK\n",
      "layers.0.attention.wv.weight: OK\n",
      "layers.0.attention.wo.weight: OK\n",
      "layers.0.feed_forward.w1.weight: OK\n",
      "layers.0.feed_forward.w2.weight: OK\n",
      "layers.0.feed_forward.w3.weight: OK\n",
      "layers.0.attention_norm.weight: OK\n",
      "layers.0.ffn_norm.weight: OK\n",
      "layers.1.attention.wq.weight: OK\n",
      "layers.1.attention.wk.weight: OK\n",
      "layers.1.attention.wv.weight: OK\n",
      "layers.1.attention.wo.weight: OK\n",
      "layers.1.feed_forward.w1.weight: OK\n",
      "layers.1.feed_forward.w2.weight: OK\n",
      "layers.1.feed_forward.w3.weight: OK\n",
      "layers.1.attention_norm.weight: OK\n",
      "layers.1.ffn_norm.weight: OK\n",
      "layers.2.attention.wq.weight: OK\n",
      "layers.2.attention.wk.weight: OK\n",
      "layers.2.attention.wv.weight: OK\n",
      "layers.2.attention.wo.weight: OK\n",
      "layers.2.feed_forward.w1.weight: OK\n",
      "layers.2.feed_forward.w2.weight: OK\n",
      "layers.2.feed_forward.w3.weight: OK\n",
      "layers.2.attention_norm.weight: OK\n",
      "layers.2.ffn_norm.weight: OK\n",
      "layers.3.attention.wq.weight: OK\n",
      "layers.3.attention.wk.weight: OK\n",
      "layers.3.attention.wv.weight: OK\n",
      "layers.3.attention.wo.weight: OK\n",
      "layers.3.feed_forward.w1.weight: OK\n",
      "layers.3.feed_forward.w2.weight: OK\n",
      "layers.3.feed_forward.w3.weight: OK\n",
      "layers.3.attention_norm.weight: OK\n",
      "layers.3.ffn_norm.weight: OK\n",
      "norm.weight: OK\n",
      "output.weight: OK\n"
     ]
    }
   ],
   "source": [
    "# Check model parameters\n",
    "print(\"Model Parameters:\")\n",
    "print(f\"Vocabulary Size: {model.vocab_size}\")\n",
    "print(f\"Number of Layers: {model.n_layers}\")\n",
    "print(f\"Embedding Dimension: {model.params.dim}\")\n",
    "print(f\"Number of Attention Heads: {model.params.n_heads}\")\n",
    "print(f\"Max Sequence Length: {model.params.max_seq_len}\")\n",
    "print(f\"Feedforward Dimension: {model.layers[0].feed_forward.w1.out_features}\")\n",
    "\n",
    "# Check if parameters are initialized\n",
    "print(\"\\nParameter Initialization:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {'Initialized' if param.sum().item() != 0 else 'Not initialized'}\")\n",
    "\n",
    "# Verify shapes of key components\n",
    "print(\"\\nKey Component Shapes:\")\n",
    "print(f\"Token Embeddings: {model.tok_embeddings.weight.shape}\")\n",
    "print(f\"Output Layer: {model.output.weight.shape}\")\n",
    "print(f\"First Layer Query Weight: {model.layers[0].attention.wq.weight.shape}\")\n",
    "print(f\"First Layer Key Weight: {model.layers[0].attention.wk.weight.shape}\")\n",
    "print(f\"First Layer Value Weight: {model.layers[0].attention.wv.weight.shape}\")\n",
    "\n",
    "# Check for NaNs or infinities\n",
    "print(\"\\nNaN/Inf Check:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "        print(f\"Warning: {name} contains NaN or Inf values\")\n",
    "    else:\n",
    "        print(f\"{name}: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Trainable Parameters: 146,543,104\n",
      "Approximate Model Size: 0.55 GB\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate size in gigabytes (assuming float32 parameters)\n",
    "size_in_gb = total_params * 4 / (1024**3)  # 4 bytes per float32 parameter\n",
    "print(f\"Approximate Model Size: {size_in_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to checkpoints/llama3_model_checkpoint.pth\n",
      "Model arguments saved to checkpoints/llama3_model_args.json\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint the model\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints if it doesn't exist\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save the model state\n",
    "model_path = os.path.join(checkpoint_dir, \"llama3_model_checkpoint.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model checkpoint saved to {model_path}\")\n",
    "\n",
    "# Save the model arguments\n",
    "import json\n",
    "\n",
    "model_args_path = os.path.join(checkpoint_dir, \"llama3_model_args.json\")\n",
    "with open(model_args_path, 'w') as f:\n",
    "    json.dump(vars(model.params), f, indent=2)\n",
    "\n",
    "print(f\"Model arguments saved to {model_args_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, no will to die through love itself.\n",
      "Why, how to tread how do to honour newly your brother,\n",
      "But to have five thousand thanks too much to Clarence:\n",
      "I'll give my soul,\n",
      "To should our speech of gold and too:\n",
      "You are dear train, and father, poor brother,\n",
      "Ere further conference with a passing small.\n",
      "O Dorsetable.\n",
      "Your sense may beggarly the tomb,\n",
      "And bid me mistress sit dispatch: past the boy,\n",
      "And well lost with one thing just proportion,\n",
      "And over the board, under his liking!\n",
      "And all the watchful eye of dear faith,\n",
      "More fierce and an inditeous wrath!\n",
      "How well, lords, I befall, and lay,\n",
      "Is not forgot the tyrant, to fill the crown,\n",
      "And manage of your glorious sun: regent join'd!\n",
      "Yet would youravenousoddess, that went;\n",
      "And well we have heard of all run a needful';\n",
      "Anduile me with the root\n",
      "And buryWhat! myself become a tyrant\n",
      "Stands without the brat's king in Bosworth\n",
      "To leap upon a black tidings was;\n",
      "And in all my tumble down: great leaving me,\n",
      "'Twere a bloody axe to that makes your countenance.\n",
      "See how, command of the head brings a tyrant,\n",
      "Found that the fair volume of York, God,\n",
      "Vouches her career me that was in'd death.\n",
      "\n",
      "EDWARD:\n",
      "Let me be not; and take me no fight:\n",
      "I'll none but well-spAGUE:\n",
      "My lord, yea, fealty like you night;\n",
      "And your mind are fled; and so most I stay,\n",
      "Together with many gawdy; and welcome down\n",
      "'Tis'd by them-ch committed by envious,\n",
      "And ancient feast, like a cover to obey.\n",
      "But if my ghostly heart's sin,\n",
      "Hark you in this and left protectors,\n",
      "And chivalrous'd my common men's music of alike,\n",
      "Since knees with O, the book of Percy,\n",
      "This heavy chamber-potless of pestil\n",
      "That brings the white man? that must we mock thee.\n",
      "Therefore, what a greater thaniger in this cell,\n",
      "Is ignorant with a dyingadvisedly com,\n",
      "Then vial and kin and Tybroke,\n",
      "This music crept like the grave with the bloody fray.\n",
      "\n",
      "RICHARD:\n",
      "O Duke of Somerset, Richard, dull delay\n",
      "WillDie in the vice: little thinks not goodly, my lord,\n",
      "Come quickly\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(Llama(model, llama.tokenizer, model_args).text_completion(\"To be or not to be,\", temperature=1.0, max_gen_len=500, echo=True).generation.split(\"<|begin_of_text|>\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
