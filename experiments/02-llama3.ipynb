{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "magic"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0 - Local Rank: 0 - World Size: 1\n",
      "ModelArgs(\n",
      "    dim=512,\n",
      "    n_layers=4,\n",
      "    n_heads=8,\n",
      "    n_kv_heads=2,\n",
      "    vocab_size=128256,\n",
      "    multiple_of=256,\n",
      "    ffn_dim_multiplier=1.5,\n",
      "    norm_eps=1e-05,\n",
      "    rope_theta=500000.0,\n",
      "    use_scaled_rope=True,\n",
      "    max_batch_size=4,\n",
      "    max_seq_len=512,\n",
      "    vision_chunk_size=-1,\n",
      "    vision_max_num_chunks=4,\n",
      "    vision_num_cross_attention_layers=-1,\n",
      ")\n",
      "Model Device: mps:0\n",
      "Trainable Parameters: 146,543,104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a97bceb8f644536bd522beb069d65dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8c929b8f4a430898f0d732413ef900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 110592/2930862080 [01:09<514:18:56, 1582.87token/s, train_loss=11.9, val_loss=11.9, lr=3.08e-5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 159\u001b[0m\n\u001b[1;32m    155\u001b[0m     distributed_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mpretrain_tokens, disable\u001b[38;5;241m=\u001b[39mrank \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m--> 159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfineweb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_token_threshold\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mupdate_val_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 90\u001b[0m, in \u001b[0;36mbatches\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m%\u001b[39m world_size \u001b[38;5;241m!=\u001b[39m rank:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_tokens:\n\u001b[1;32m     92\u001b[0m     batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     93\u001b[0m         tokens[:max_tokens], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mmodel_device\n\u001b[1;32m     94\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(model_args\u001b[38;5;241m.\u001b[39mmax_batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/github/bradhilton/atreides/.venv/lib/python3.12/site-packages/llama_models/llama3/api/tokenizer.py:167\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[0;34m(self, s, bos, eos, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    164\u001b[0m t: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m substr \u001b[38;5;129;01min\u001b[39;00m substrs:\n\u001b[1;32m    166\u001b[0m     t\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubstr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bos:\n\u001b[1;32m    174\u001b[0m     t\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_id)\n",
      "File \u001b[0;32m~/github/bradhilton/atreides/.venv/lib/python3.12/site-packages/tiktoken/core.py:124\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    121\u001b[0m     allowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(allowed_special)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core_bpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_special\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# BPE operates on bytes, but the regex operates on unicode. If we pass a str that is\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# invalid UTF-8 to Rust, it will rightfully complain. Here we do a quick and dirty\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# string, but given that this is input we want to support, maybe that's okay.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Also we use errors=\"replace\" to handle weird things like lone surrogates.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from contextlib import nullcontext\n",
    "import datasets\n",
    "import json\n",
    "from lib.llama3.reference_impl.model import ModelArgs, Transformer\n",
    "from lib.utils import black_print\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "from llama_models.sku_list import resolve_model\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.adamw import AdamW\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable\n",
    "\n",
    "if torch.cuda.is_available() and int(os.environ.get(\"RANK\", -1)) != -1:\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    rank = torch.distributed.get_rank()\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    world_size = torch.distributed.get_world_size()\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "if world_size > 1:\n",
    "    print(f\"Rank: {rank} - Local Rank: {local_rank} - World Size: {world_size}\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Llama3 training script\")\n",
    "parser.add_argument(\"--name\", type=str, default=None, help=\"Run name\")\n",
    "args, _ = parser.parse_known_args()\n",
    "run_name = args.name\n",
    "run_path = f\"./runs/{run_name}\"\n",
    "params_path = f\"{run_path}/params.json\"\n",
    "model_path = f\"{run_path}/model.pth\"\n",
    "optimizer_path = f\"{run_path}/optimizer.pth\"\n",
    "scheduler_path = f\"{run_path}/scheduler.pth\"\n",
    "dataset_state_path = f\"{run_path}/dataset-state.json\"\n",
    "training_state_path = f\"{run_path}/training-state.json\"\n",
    "\n",
    "llama3_2_1B = resolve_model(\"Llama3.2-1B\")\n",
    "assert llama3_2_1B is not None\n",
    "params = llama3_2_1B.arch_args\n",
    "params[\"dim\"] //= 4\n",
    "params[\"n_heads\"] //= 4\n",
    "params[\"n_kv_heads\"] //= 4\n",
    "params[\"n_layers\"] //= 4\n",
    "params[\"max_seq_len\"] = 512\n",
    "params[\"max_batch_size\"] = 4\n",
    "params[\"use_flash_attention\"] = True\n",
    "\n",
    "if os.path.exists(params_path):\n",
    "    params = json.load(open(params_path, \"r\"))\n",
    "elif run_name:\n",
    "    os.makedirs(run_path, exist_ok=True)\n",
    "    json.dump(params, open(params_path, \"w\"))\n",
    "\n",
    "model_args = ModelArgs(**params)\n",
    "if rank == 0:\n",
    "    black_print(model_args)\n",
    "micro_step_tokens = model_args.max_seq_len * model_args.max_batch_size\n",
    "\n",
    "model = Transformer(model_args)\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(f\"cuda:{local_rank}\")\n",
    "    # model.compile()\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    # context = (\n",
    "    #     torch.autocast(\n",
    "    #         device_type=\"cuda\",\n",
    "    #         dtype=torch.bfloat16,\n",
    "    #     )\n",
    "    #     if torch.cuda.is_bf16_supported()\n",
    "    #     else nullcontext()\n",
    "    # )\n",
    "    context = nullcontext()\n",
    "elif torch.backends.mps.is_available():\n",
    "    model = model.to(\"mps\")\n",
    "    context = nullcontext()\n",
    "else:\n",
    "    model = model.to(\"cpu\")\n",
    "    context = nullcontext()\n",
    "model_device = next(model.parameters()).device\n",
    "if rank == 0:\n",
    "    print(f\"Model Device: {model_device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "if rank == 0:\n",
    "    print(f\"Trainable Parameters: {total_params:,}\")\n",
    "pretrain_tokens = int(total_params * 20)  # Chinchilla-optimal\n",
    "tokenizer = Tokenizer.get_instance()\n",
    "if rank != 0:\n",
    "    datasets.disable_progress_bars()\n",
    "dataset: datasets.IterableDataset = datasets.load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=True\n",
    ")  # type: ignore\n",
    "val_dataset: datasets.IterableDataset = datasets.load_dataset(\n",
    "    \"reflex-ai/fineweb-ultra-mini\", split=\"train\", streaming=True\n",
    ")  # type: ignore\n",
    "\n",
    "\n",
    "def batches(\n",
    "    dataset: datasets.IterableDataset,\n",
    ") -> Iterable[tuple[torch.Tensor, torch.Tensor]]:\n",
    "    max_tokens = (model_args.max_seq_len + 1) * model_args.max_batch_size\n",
    "    tokens = []\n",
    "    for index, document in enumerate(dataset):\n",
    "        if index % world_size != rank:\n",
    "            continue\n",
    "        tokens += tokenizer.encode(document[\"text\"], bos=True, eos=True)\n",
    "        if len(tokens) >= max_tokens:\n",
    "            batch = torch.tensor(\n",
    "                tokens[:max_tokens], dtype=torch.long, device=model_device\n",
    "            ).reshape(model_args.max_batch_size, -1)\n",
    "            yield batch[:, :-1], batch[:, 1:]\n",
    "            tokens = tokens[max_tokens:]\n",
    "    if tokens:\n",
    "        pad_length = max_tokens - len(tokens)\n",
    "        tokens += [tokenizer.pad_id] * pad_length\n",
    "        batch = torch.tensor(\n",
    "            tokens[:max_tokens], dtype=torch.long, device=model_device\n",
    "        ).reshape(model_args.max_batch_size, -1)\n",
    "        yield batch[:, :-1], batch[:, 1:]\n",
    "\n",
    "\n",
    "peak_lr = 6e-4 / ((total_params * 1e-9) ** (1 / 3))\n",
    "optimizer = AdamW(model.parameters(), lr=peak_lr)\n",
    "if os.path.exists(optimizer_path):\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path))\n",
    "step_tokens = 2**19\n",
    "cosine_annealing_steps = pretrain_tokens // step_tokens\n",
    "warmup_steps = cosine_annealing_steps // 150\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    [\n",
    "        torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, lambda x: min((x + 1) / warmup_steps, 1)\n",
    "        ),\n",
    "        torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_annealing_steps, eta_min=peak_lr * 0.01),  # type: ignore\n",
    "    ],\n",
    "    milestones=[warmup_steps],\n",
    ")\n",
    "if os.path.exists(scheduler_path):\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path))\n",
    "grad_accum_steps = step_tokens // micro_step_tokens // world_size\n",
    "grad_accum_threshold = step_tokens\n",
    "\n",
    "if world_size > 1:\n",
    "    distributed_model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model, device_ids=[local_rank]\n",
    "    )\n",
    "else:\n",
    "    distributed_model = model\n",
    "\n",
    "\n",
    "training_state = (\n",
    "    json.loads(open(training_state_path, \"r\").read())\n",
    "    if os.path.exists(training_state_path)\n",
    "    else {}\n",
    ")\n",
    "\n",
    "val_size = 1000\n",
    "val_ids = {document[\"id\"] for document in val_dataset.take(val_size)}\n",
    "val_token_threshold = training_state.get(\"val_token_threshold\", 0)\n",
    "val_loss = torch.tensor(training_state.get(\"val_loss\", 12.0)).to(model_device)\n",
    "\n",
    "\n",
    "def update_val_loss() -> None:\n",
    "    global val_token_threshold, val_loss\n",
    "    val_token_threshold += 10_000_000\n",
    "    distributed_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss.zero_()\n",
    "        num_batches = 0\n",
    "        for val_x, val_y in batches(val_dataset.take(val_size)):\n",
    "            val_logits = distributed_model(val_x, 0)\n",
    "            val_loss += F.cross_entropy(\n",
    "                val_logits.view(-1, model.vocab_size), val_y.flatten()\n",
    "            )\n",
    "            num_batches += 1\n",
    "        val_loss /= num_batches\n",
    "        if world_size > 1:\n",
    "            torch.distributed.all_reduce(val_loss, op=torch.distributed.ReduceOp.AVG)\n",
    "    distributed_model.train()\n",
    "\n",
    "\n",
    "train_loss = torch.tensor(training_state.get(\"train_loss\", 12.0)).to(model_device)\n",
    "dataset = dataset.filter(lambda x: x[\"id\"] not in val_ids)\n",
    "if os.path.exists(dataset_state_path):\n",
    "    dataset.load_state_dict(json.load(open(dataset_state_path, \"r\")))\n",
    "save_token_threshold = training_state.get(\"save_token_threshold\", 100_000_000)\n",
    "\n",
    "\n",
    "def save_state(pbar: tqdm) -> None:\n",
    "    if not run_name:\n",
    "        return\n",
    "    global save_token_threshold\n",
    "    save_token_threshold += 100_000_000\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "    torch.save(scheduler.state_dict(), scheduler_path)\n",
    "    json.dump(dataset.state_dict(), open(dataset_state_path, \"w\"))\n",
    "    json.dump(\n",
    "        {\n",
    "            \"val_loss\": val_loss.item(),\n",
    "            \"val_token_threshold\": val_token_threshold,\n",
    "            \"train_loss\": train_loss.item(),\n",
    "            \"save_token_threshold\": save_token_threshold,\n",
    "            \"progress\": pbar.n,\n",
    "        },\n",
    "        open(training_state_path, \"w\"),\n",
    "    )\n",
    "\n",
    "\n",
    "with tqdm(\n",
    "    desc=\"Training model\",\n",
    "    total=pretrain_tokens,\n",
    "    disable=rank != 0,\n",
    "    unit=\"token\",\n",
    "    initial=training_state.get(\"progress\", 0),\n",
    ") as pbar:\n",
    "    for x, y in batches(dataset):\n",
    "        with context:\n",
    "            logits = distributed_model(x, 0)\n",
    "            loss = F.cross_entropy(logits.view(-1, model.vocab_size), y.flatten())\n",
    "        alpha = 0.05  # Smoothing factor, adjust as needed\n",
    "        train_loss = alpha * loss + (1 - alpha) * train_loss\n",
    "        loss /= grad_accum_steps\n",
    "\n",
    "        if pbar.n >= grad_accum_threshold:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            grad_accum_threshold += step_tokens\n",
    "            if pbar.n >= val_token_threshold:\n",
    "                update_val_loss()\n",
    "            if pbar.n >= save_token_threshold:\n",
    "                save_state(pbar)\n",
    "        else:\n",
    "            with (distributed_model.no_sync if world_size > 1 else nullcontext)():\n",
    "                loss.backward()\n",
    "\n",
    "        pbar.update(x.numel() * world_size)\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"train_loss\": train_loss.item(),\n",
    "                \"val_loss\": val_loss.item(),\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex_iterable': {'shard_idx': 0, 'shard_example_idx': 177},\n",
       " 'previous_state': None,\n",
       " 'num_examples_since_previous_state': 0,\n",
       " 'previous_state_example_idx': 177}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.llama3.reference_impl.model import ModelArgs\n",
    "from llama_models.sku_list import resolve_model\n",
    "\n",
    "\n",
    "llama3_2_1B = resolve_model(\"Llama3.2-1B\")\n",
    "assert llama3_2_1B is not None\n",
    "params = llama3_2_1B.arch_args\n",
    "params[\"dim\"] //= 4\n",
    "params[\"n_heads\"] //= 4\n",
    "params[\"n_kv_heads\"] //= 4\n",
    "params[\"n_layers\"] //= 4\n",
    "\n",
    "class ExtendedModelArgs(ModelArgs):\n",
    "    flash_attention: bool = False\n",
    "\n",
    "ExtendedModelArgs(\n",
    "    max_seq_len=512,\n",
    "    max_batch_size=8,\n",
    "    flash_attention=False,\n",
    "    **params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 7.03 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.llama3.reference_impl.generation import Llama\n",
    "import os\n",
    "\n",
    "llama3_2_1B_ckpt_dir = os.path.expanduser(\"~/.llama/checkpoints/Llama3.2-1B/original/\")\n",
    "tokenizer_path = llama3_2_1B_ckpt_dir + \"tokenizer.model\"\n",
    "\n",
    "llama = Llama.build(\n",
    "    ckpt_dir=llama3_2_1B_ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=512,\n",
    "    max_batch_size=1,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "next(llama.model.parameters()).device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I have no idea, but I think I can'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.text_completion(\"What is the meaning of life?\", max_gen_len=10).generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of Shakespeare's text:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "\n",
      "Total characters: 1115394\n",
      "Total lines: 40000\n"
     ]
    }
   ],
   "source": [
    "shakespeare_text = open(\"./data/tinyshakespeare.txt\", \"r\").read()\n",
    "\n",
    "# Display the first few lines\n",
    "print(\"First few lines of Shakespeare's text:\")\n",
    "print(shakespeare_text[:500])\n",
    "\n",
    "# Get some statistics\n",
    "total_chars = len(shakespeare_text)\n",
    "total_lines = shakespeare_text.count(\"\\n\")\n",
    "\n",
    "print(f\"\\nTotal characters: {total_chars}\")\n",
    "print(f\"Total lines: {total_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "Vocabulary Size: 128256\n",
      "Number of Layers: 4\n",
      "Embedding Dimension: 512\n",
      "Number of Attention Heads: 8\n",
      "Max Sequence Length: 512\n",
      "Feedforward Dimension: 2048\n",
      "\n",
      "Parameter Initialization:\n",
      "tok_embeddings.weight: Initialized\n",
      "layers.0.attention.wq.weight: Initialized\n",
      "layers.0.attention.wk.weight: Initialized\n",
      "layers.0.attention.wv.weight: Initialized\n",
      "layers.0.attention.wo.weight: Initialized\n",
      "layers.0.feed_forward.w1.weight: Initialized\n",
      "layers.0.feed_forward.w2.weight: Initialized\n",
      "layers.0.feed_forward.w3.weight: Initialized\n",
      "layers.0.attention_norm.weight: Initialized\n",
      "layers.0.ffn_norm.weight: Initialized\n",
      "layers.1.attention.wq.weight: Initialized\n",
      "layers.1.attention.wk.weight: Initialized\n",
      "layers.1.attention.wv.weight: Initialized\n",
      "layers.1.attention.wo.weight: Initialized\n",
      "layers.1.feed_forward.w1.weight: Initialized\n",
      "layers.1.feed_forward.w2.weight: Initialized\n",
      "layers.1.feed_forward.w3.weight: Initialized\n",
      "layers.1.attention_norm.weight: Initialized\n",
      "layers.1.ffn_norm.weight: Initialized\n",
      "layers.2.attention.wq.weight: Initialized\n",
      "layers.2.attention.wk.weight: Initialized\n",
      "layers.2.attention.wv.weight: Initialized\n",
      "layers.2.attention.wo.weight: Initialized\n",
      "layers.2.feed_forward.w1.weight: Initialized\n",
      "layers.2.feed_forward.w2.weight: Initialized\n",
      "layers.2.feed_forward.w3.weight: Initialized\n",
      "layers.2.attention_norm.weight: Initialized\n",
      "layers.2.ffn_norm.weight: Initialized\n",
      "layers.3.attention.wq.weight: Initialized\n",
      "layers.3.attention.wk.weight: Initialized\n",
      "layers.3.attention.wv.weight: Initialized\n",
      "layers.3.attention.wo.weight: Initialized\n",
      "layers.3.feed_forward.w1.weight: Initialized\n",
      "layers.3.feed_forward.w2.weight: Initialized\n",
      "layers.3.feed_forward.w3.weight: Initialized\n",
      "layers.3.attention_norm.weight: Initialized\n",
      "layers.3.ffn_norm.weight: Initialized\n",
      "norm.weight: Initialized\n",
      "output.weight: Initialized\n",
      "\n",
      "Key Component Shapes:\n",
      "Token Embeddings: torch.Size([128256, 512])\n",
      "Output Layer: torch.Size([128256, 512])\n",
      "First Layer Query Weight: torch.Size([512, 512])\n",
      "First Layer Key Weight: torch.Size([128, 512])\n",
      "First Layer Value Weight: torch.Size([128, 512])\n",
      "\n",
      "NaN/Inf Check:\n",
      "tok_embeddings.weight: OK\n",
      "layers.0.attention.wq.weight: OK\n",
      "layers.0.attention.wk.weight: OK\n",
      "layers.0.attention.wv.weight: OK\n",
      "layers.0.attention.wo.weight: OK\n",
      "layers.0.feed_forward.w1.weight: OK\n",
      "layers.0.feed_forward.w2.weight: OK\n",
      "layers.0.feed_forward.w3.weight: OK\n",
      "layers.0.attention_norm.weight: OK\n",
      "layers.0.ffn_norm.weight: OK\n",
      "layers.1.attention.wq.weight: OK\n",
      "layers.1.attention.wk.weight: OK\n",
      "layers.1.attention.wv.weight: OK\n",
      "layers.1.attention.wo.weight: OK\n",
      "layers.1.feed_forward.w1.weight: OK\n",
      "layers.1.feed_forward.w2.weight: OK\n",
      "layers.1.feed_forward.w3.weight: OK\n",
      "layers.1.attention_norm.weight: OK\n",
      "layers.1.ffn_norm.weight: OK\n",
      "layers.2.attention.wq.weight: OK\n",
      "layers.2.attention.wk.weight: OK\n",
      "layers.2.attention.wv.weight: OK\n",
      "layers.2.attention.wo.weight: OK\n",
      "layers.2.feed_forward.w1.weight: OK\n",
      "layers.2.feed_forward.w2.weight: OK\n",
      "layers.2.feed_forward.w3.weight: OK\n",
      "layers.2.attention_norm.weight: OK\n",
      "layers.2.ffn_norm.weight: OK\n",
      "layers.3.attention.wq.weight: OK\n",
      "layers.3.attention.wk.weight: OK\n",
      "layers.3.attention.wv.weight: OK\n",
      "layers.3.attention.wo.weight: OK\n",
      "layers.3.feed_forward.w1.weight: OK\n",
      "layers.3.feed_forward.w2.weight: OK\n",
      "layers.3.feed_forward.w3.weight: OK\n",
      "layers.3.attention_norm.weight: OK\n",
      "layers.3.ffn_norm.weight: OK\n",
      "norm.weight: OK\n",
      "output.weight: OK\n"
     ]
    }
   ],
   "source": [
    "# Check model parameters\n",
    "print(\"Model Parameters:\")\n",
    "print(f\"Vocabulary Size: {model.vocab_size}\")\n",
    "print(f\"Number of Layers: {model.n_layers}\")\n",
    "print(f\"Embedding Dimension: {model.params.dim}\")\n",
    "print(f\"Number of Attention Heads: {model.params.n_heads}\")\n",
    "print(f\"Max Sequence Length: {model.params.max_seq_len}\")\n",
    "print(f\"Feedforward Dimension: {model.layers[0].feed_forward.w1.out_features}\")\n",
    "\n",
    "# Check if parameters are initialized\n",
    "print(\"\\nParameter Initialization:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {'Initialized' if param.sum().item() != 0 else 'Not initialized'}\")\n",
    "\n",
    "# Verify shapes of key components\n",
    "print(\"\\nKey Component Shapes:\")\n",
    "print(f\"Token Embeddings: {model.tok_embeddings.weight.shape}\")\n",
    "print(f\"Output Layer: {model.output.weight.shape}\")\n",
    "print(f\"First Layer Query Weight: {model.layers[0].attention.wq.weight.shape}\")\n",
    "print(f\"First Layer Key Weight: {model.layers[0].attention.wk.weight.shape}\")\n",
    "print(f\"First Layer Value Weight: {model.layers[0].attention.wv.weight.shape}\")\n",
    "\n",
    "# Check for NaNs or infinities\n",
    "print(\"\\nNaN/Inf Check:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "        print(f\"Warning: {name} contains NaN or Inf values\")\n",
    "    else:\n",
    "        print(f\"{name}: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Trainable Parameters: 146,543,104\n",
      "Approximate Model Size: 0.55 GB\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate size in gigabytes (assuming float32 parameters)\n",
    "size_in_gb = total_params * 4 / (1024**3)  # 4 bytes per float32 parameter\n",
    "print(f\"Approximate Model Size: {size_in_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to checkpoints/llama3_model_checkpoint.pth\n",
      "Model arguments saved to checkpoints/llama3_model_args.json\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint the model\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints if it doesn't exist\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save the model state\n",
    "model_path = os.path.join(checkpoint_dir, \"llama3_model_checkpoint.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model checkpoint saved to {model_path}\")\n",
    "\n",
    "# Save the model arguments\n",
    "import json\n",
    "\n",
    "model_args_path = os.path.join(checkpoint_dir, \"llama3_model_args.json\")\n",
    "with open(model_args_path, 'w') as f:\n",
    "    json.dump(vars(model.params), f, indent=2)\n",
    "\n",
    "print(f\"Model arguments saved to {model_args_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, no will to die through love itself.\n",
      "Why, how to tread how do to honour newly your brother,\n",
      "But to have five thousand thanks too much to Clarence:\n",
      "I'll give my soul,\n",
      "To should our speech of gold and too:\n",
      "You are dear train, and father, poor brother,\n",
      "Ere further conference with a passing small.\n",
      "O Dorsetable.\n",
      "Your sense may beggarly the tomb,\n",
      "And bid me mistress sit dispatch: past the boy,\n",
      "And well lost with one thing just proportion,\n",
      "And over the board, under his liking!\n",
      "And all the watchful eye of dear faith,\n",
      "More fierce and an inditeous wrath!\n",
      "How well, lords, I befall, and lay,\n",
      "Is not forgot the tyrant, to fill the crown,\n",
      "And manage of your glorious sun: regent join'd!\n",
      "Yet would youravenousoddess, that went;\n",
      "And well we have heard of all run a needful';\n",
      "Anduile me with the root\n",
      "And buryWhat! myself become a tyrant\n",
      "Stands without the brat's king in Bosworth\n",
      "To leap upon a black tidings was;\n",
      "And in all my tumble down: great leaving me,\n",
      "'Twere a bloody axe to that makes your countenance.\n",
      "See how, command of the head brings a tyrant,\n",
      "Found that the fair volume of York, God,\n",
      "Vouches her career me that was in'd death.\n",
      "\n",
      "EDWARD:\n",
      "Let me be not; and take me no fight:\n",
      "I'll none but well-spAGUE:\n",
      "My lord, yea, fealty like you night;\n",
      "And your mind are fled; and so most I stay,\n",
      "Together with many gawdy; and welcome down\n",
      "'Tis'd by them-ch committed by envious,\n",
      "And ancient feast, like a cover to obey.\n",
      "But if my ghostly heart's sin,\n",
      "Hark you in this and left protectors,\n",
      "And chivalrous'd my common men's music of alike,\n",
      "Since knees with O, the book of Percy,\n",
      "This heavy chamber-potless of pestil\n",
      "That brings the white man? that must we mock thee.\n",
      "Therefore, what a greater thaniger in this cell,\n",
      "Is ignorant with a dyingadvisedly com,\n",
      "Then vial and kin and Tybroke,\n",
      "This music crept like the grave with the bloody fray.\n",
      "\n",
      "RICHARD:\n",
      "O Duke of Somerset, Richard, dull delay\n",
      "WillDie in the vice: little thinks not goodly, my lord,\n",
      "Come quickly\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(Llama(model, llama.tokenizer, model_args).text_completion(\"To be or not to be,\", temperature=1.0, max_gen_len=500, echo=True).generation.split(\"<|begin_of_text|>\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
