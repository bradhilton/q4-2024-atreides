{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "magic"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying gs://atreides/experiments/runs/001/optimizer.pth...\n",
      "Copying gs://atreides/experiments/runs/001/scheduler.pth...                     \n",
      "Copying gs://atreides/experiments/runs/001/dataset-state.json...                \n",
      "Copying gs://atreides/experiments/runs/001/params.json...                       \n",
      "Copying gs://atreides/experiments/runs/001/training-state.json...\n",
      "Copying gs://atreides/experiments/runs/001/model.pth...                         \n",
      "/ [6/6 files][  1.6 GiB/  1.6 GiB] 100% Done 107.0 MiB/s ETA 00:00:00           \n",
      "Operation completed over 6 objects/1.6 GiB.                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(\n",
      "    dim=512,\n",
      "    n_layers=4,\n",
      "    n_heads=8,\n",
      "    n_kv_heads=2,\n",
      "    vocab_size=128256,\n",
      "    multiple_of=256,\n",
      "    ffn_dim_multiplier=1.5,\n",
      "    norm_eps=1e-05,\n",
      "    rope_theta=500000.0,\n",
      "    use_scaled_rope=True,\n",
      "    max_batch_size=32,\n",
      "    max_seq_len=512,\n",
      "    vision_chunk_size=-1,\n",
      "    vision_max_num_chunks=4,\n",
      "    vision_num_cross_attention_layers=-1,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58562/3354254371.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Device: cuda:0\n",
      "Trainable Parameters: 146,543,104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb19f4c7d53643e8b498e70cf197ec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de73299757c947a6813e71c936cee584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model:   2%|▏         | 45,088,768/2,930,862,080 [00:00<?, ?token/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:1612: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "Training model:   2%|▏         | 71,270,400/2,930,862,080 [03:01<5:30:10, 144345.08token/s, train_loss=6.77, val_loss=6.93, lr=3.07e-5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 272\u001b[0m\n\u001b[1;32m    267\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    269\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(x\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m*\u001b[39m world_size)\n\u001b[1;32m    270\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[1;32m    271\u001b[0m     {\n\u001b[0;32m--> 272\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    275\u001b[0m     }\n\u001b[1;32m    276\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from contextlib import nullcontext\n",
    "import datasets\n",
    "import json\n",
    "from lib.llama3.reference_impl.model import ModelArgs, Transformer\n",
    "from lib.utils import black_print\n",
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "from llama_models.sku_list import resolve_model\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.distributed\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.adamw import AdamW\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable\n",
    "\n",
    "if torch.cuda.is_available() and int(os.environ.get(\"RANK\", -1)) != -1:\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    rank = torch.distributed.get_rank()\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    world_size = torch.distributed.get_world_size()\n",
    "else:\n",
    "    rank = 0\n",
    "    local_rank = 0\n",
    "    world_size = 1\n",
    "\n",
    "if world_size > 1:\n",
    "    print(f\"Rank: {rank} - Local Rank: {local_rank} - World Size: {world_size}\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Llama3 training script\")\n",
    "parser.add_argument(\"--name\", type=str, default=None, help=\"Run name\")\n",
    "args, _ = parser.parse_known_args()\n",
    "run_name = args.name\n",
    "run_path = f\"./runs/{run_name}\"\n",
    "gs_path = f\"gs://atreides/experiments/runs/{run_name}\"\n",
    "if run_name and not os.path.exists(run_path):\n",
    "    os.makedirs(run_path, exist_ok=True)\n",
    "    subprocess.Popen(\n",
    "        f\"gsutil -m rsync -r {gs_path} {run_path}\",\n",
    "        shell=True,\n",
    "    ).wait()\n",
    "params_path = f\"{run_path}/params.json\"\n",
    "model_path = f\"{run_path}/model.pth\"\n",
    "optimizer_path = f\"{run_path}/optimizer.pth\"\n",
    "scheduler_path = f\"{run_path}/scheduler.pth\"\n",
    "dataset_state_path = f\"{run_path}/dataset-state.json\"\n",
    "training_state_path = f\"{run_path}/training-state.json\"\n",
    "\n",
    "llama3_2_1B = resolve_model(\"Llama3.2-1B\")\n",
    "assert llama3_2_1B is not None\n",
    "params = llama3_2_1B.arch_args\n",
    "params[\"dim\"] //= 4\n",
    "params[\"n_heads\"] //= 4\n",
    "params[\"n_kv_heads\"] //= 4\n",
    "params[\"n_layers\"] //= 4\n",
    "params[\"max_seq_len\"] = 512\n",
    "params[\"max_batch_size\"] = 32\n",
    "params[\"use_flash_attention\"] = True\n",
    "\n",
    "if os.path.exists(params_path):\n",
    "    params = json.load(open(params_path, \"r\"))\n",
    "elif run_name:\n",
    "    os.makedirs(run_path, exist_ok=True)\n",
    "    json.dump(params, open(params_path, \"w\"))\n",
    "\n",
    "model_args = ModelArgs(**params)\n",
    "if rank == 0:\n",
    "    black_print(model_args)\n",
    "micro_step_tokens = model_args.max_seq_len * model_args.max_batch_size\n",
    "\n",
    "model = Transformer(model_args)\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(f\"cuda:{local_rank}\")\n",
    "    model.compile()\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    context = (\n",
    "        torch.autocast(\n",
    "            device_type=\"cuda\",\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        if torch.cuda.is_bf16_supported()\n",
    "        else nullcontext()\n",
    "    )\n",
    "    context = nullcontext()\n",
    "elif torch.backends.mps.is_available():\n",
    "    model = model.to(\"mps\")\n",
    "    context = nullcontext()\n",
    "else:\n",
    "    model = model.to(\"cpu\")\n",
    "    context = nullcontext()\n",
    "model_device = next(model.parameters()).device\n",
    "if rank == 0:\n",
    "    print(f\"Model Device: {model_device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "if rank == 0:\n",
    "    print(f\"Trainable Parameters: {total_params:,}\")\n",
    "pretrain_tokens = int(total_params * 20)  # Chinchilla-optimal\n",
    "tokenizer = Tokenizer.get_instance()\n",
    "if rank != 0:\n",
    "    datasets.disable_progress_bars()\n",
    "dataset: datasets.IterableDataset = datasets.load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\", name=\"sample-10BT\", split=\"train\", streaming=True\n",
    ")  # type: ignore\n",
    "val_dataset: datasets.IterableDataset = datasets.load_dataset(\n",
    "    \"reflex-ai/fineweb-ultra-mini\", split=\"train\", streaming=True\n",
    ")  # type: ignore\n",
    "\n",
    "\n",
    "def batches(\n",
    "    dataset: datasets.IterableDataset,\n",
    ") -> Iterable[tuple[torch.Tensor, torch.Tensor]]:\n",
    "    max_tokens = (model_args.max_seq_len + 1) * model_args.max_batch_size\n",
    "    tokens = []\n",
    "    for index, document in enumerate(dataset):\n",
    "        if index % world_size != rank:\n",
    "            continue\n",
    "        tokens += tokenizer.encode(document[\"text\"], bos=True, eos=True)\n",
    "        if len(tokens) >= max_tokens:\n",
    "            batch = torch.tensor(\n",
    "                tokens[:max_tokens], dtype=torch.long, device=model_device\n",
    "            ).reshape(model_args.max_batch_size, -1)\n",
    "            yield batch[:, :-1], batch[:, 1:]\n",
    "            tokens = tokens[max_tokens:]\n",
    "    if tokens:\n",
    "        pad_length = max_tokens - len(tokens)\n",
    "        tokens += [tokenizer.pad_id] * pad_length\n",
    "        batch = torch.tensor(\n",
    "            tokens[:max_tokens], dtype=torch.long, device=model_device\n",
    "        ).reshape(model_args.max_batch_size, -1)\n",
    "        yield batch[:, :-1], batch[:, 1:]\n",
    "\n",
    "\n",
    "peak_lr = 6e-4 / ((total_params * 1e-9) ** (1 / 3))\n",
    "optimizer = AdamW(model.parameters(), lr=peak_lr)\n",
    "if os.path.exists(optimizer_path):\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path, weights_only=True))\n",
    "step_tokens = 2**19\n",
    "cosine_annealing_steps = pretrain_tokens // step_tokens\n",
    "warmup_steps = cosine_annealing_steps // 150\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    [\n",
    "        torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=1 / warmup_steps,\n",
    "            end_factor=1,\n",
    "            total_iters=warmup_steps,\n",
    "        ),\n",
    "        torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cosine_annealing_steps, eta_min=peak_lr * 0.01),  # type: ignore\n",
    "    ],\n",
    "    milestones=[warmup_steps],\n",
    ")\n",
    "if os.path.exists(scheduler_path):\n",
    "    scheduler.load_state_dict(torch.load(scheduler_path, weights_only=True))\n",
    "grad_accum_steps = step_tokens // micro_step_tokens // world_size\n",
    "grad_accum_threshold = step_tokens\n",
    "\n",
    "if world_size > 1:\n",
    "    distributed_model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model, device_ids=[local_rank]\n",
    "    )\n",
    "else:\n",
    "    distributed_model = model\n",
    "\n",
    "\n",
    "training_state = (\n",
    "    json.loads(open(training_state_path, \"r\").read())\n",
    "    if os.path.exists(training_state_path)\n",
    "    else {}\n",
    ")\n",
    "\n",
    "val_size = 1000\n",
    "val_ids = {document[\"id\"] for document in val_dataset.take(val_size)}\n",
    "val_token_threshold = training_state.get(\"val_token_threshold\", 0)\n",
    "val_loss = torch.tensor(training_state.get(\"val_loss\", 12.0)).to(model_device)\n",
    "\n",
    "\n",
    "def update_val_loss() -> None:\n",
    "    global val_token_threshold, val_loss\n",
    "    val_token_threshold += 10_000_000\n",
    "    distributed_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss.zero_()\n",
    "        num_batches = 0\n",
    "        for val_x, val_y in batches(val_dataset.take(val_size)):\n",
    "            val_logits = distributed_model(val_x, 0)\n",
    "            val_loss += F.cross_entropy(\n",
    "                val_logits.view(-1, model.vocab_size), val_y.flatten()\n",
    "            )\n",
    "            num_batches += 1\n",
    "        val_loss /= num_batches\n",
    "        if world_size > 1:\n",
    "            torch.distributed.all_reduce(val_loss, op=torch.distributed.ReduceOp.AVG)\n",
    "    distributed_model.train()\n",
    "\n",
    "\n",
    "train_loss = torch.tensor(training_state.get(\"train_loss\", 12.0)).to(model_device)\n",
    "dataset = dataset.filter(lambda x: x[\"id\"] not in val_ids)\n",
    "if os.path.exists(dataset_state_path):\n",
    "    dataset.load_state_dict(json.load(open(dataset_state_path, \"r\")))\n",
    "save_frequecy = 10_000_000\n",
    "save_token_threshold = training_state.get(\"save_token_threshold\", save_frequecy)\n",
    "\n",
    "\n",
    "def save_state(pbar: tqdm) -> None:\n",
    "    if not run_name:\n",
    "        return\n",
    "    global save_token_threshold\n",
    "    save_token_threshold += save_frequecy\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "    torch.save(scheduler.state_dict(), scheduler_path)\n",
    "    json.dump(dataset.state_dict(), open(dataset_state_path, \"w\"))\n",
    "    json.dump(\n",
    "        {\n",
    "            \"val_loss\": val_loss.item(),\n",
    "            \"val_token_threshold\": val_token_threshold,\n",
    "            \"train_loss\": train_loss.item(),\n",
    "            \"save_token_threshold\": save_token_threshold,\n",
    "            \"progress\": pbar.n,\n",
    "        },\n",
    "        open(training_state_path, \"w\"),\n",
    "    )\n",
    "    subprocess.Popen(\n",
    "        f\"gsutil -m rsync -r {run_path} {gs_path}\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "    )\n",
    "\n",
    "\n",
    "with tqdm(\n",
    "    desc=\"Training model\",\n",
    "    total=pretrain_tokens,\n",
    "    disable=rank != 0,\n",
    "    unit=\"token\",\n",
    "    initial=training_state.get(\"progress\", 0),\n",
    "    bar_format=\"{l_bar}{bar}| {n:,}/{total:,} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\",\n",
    ") as pbar:\n",
    "    for x, y in batches(dataset):\n",
    "        with context:\n",
    "            logits = distributed_model(x, 0)\n",
    "            loss = F.cross_entropy(logits.view(-1, model.vocab_size), y.flatten())\n",
    "        alpha = 0.05  # Smoothing factor, adjust as needed\n",
    "        train_loss = alpha * loss + (1 - alpha) * train_loss\n",
    "        loss /= grad_accum_steps\n",
    "\n",
    "        if pbar.n >= grad_accum_threshold:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            grad_accum_threshold += step_tokens\n",
    "            if pbar.n >= val_token_threshold:\n",
    "                update_val_loss()\n",
    "            if pbar.n >= save_token_threshold:\n",
    "                save_state(pbar)\n",
    "            if pbar.n >= pretrain_tokens:\n",
    "                break\n",
    "        else:\n",
    "            with (distributed_model.no_sync if world_size > 1 else nullcontext)():\n",
    "                loss.backward()\n",
    "\n",
    "        pbar.update(x.numel() * world_size)\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"train_loss\": train_loss.item(),\n",
    "                \"val_loss\": val_loss.item(),\n",
    "                \"lr\": scheduler.get_last_lr()[0],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex_iterable': {'shard_idx': 0, 'shard_example_idx': 105935},\n",
       " 'previous_state': None,\n",
       " 'num_examples_since_previous_state': 0,\n",
       " 'previous_state_example_idx': 105935}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex_iterable': {'shard_idx': 0, 'shard_example_idx': 177},\n",
       " 'previous_state': None,\n",
       " 'num_examples_since_previous_state': 0,\n",
       " 'previous_state_example_idx': 177}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.llama3.reference_impl.model import ModelArgs\n",
    "from llama_models.sku_list import resolve_model\n",
    "\n",
    "\n",
    "llama3_2_1B = resolve_model(\"Llama3.2-1B\")\n",
    "assert llama3_2_1B is not None\n",
    "params = llama3_2_1B.arch_args\n",
    "params[\"dim\"] //= 4\n",
    "params[\"n_heads\"] //= 4\n",
    "params[\"n_kv_heads\"] //= 4\n",
    "params[\"n_layers\"] //= 4\n",
    "\n",
    "class ExtendedModelArgs(ModelArgs):\n",
    "    flash_attention: bool = False\n",
    "\n",
    "ExtendedModelArgs(\n",
    "    max_seq_len=512,\n",
    "    max_batch_size=8,\n",
    "    flash_attention=False,\n",
    "    **params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 7.03 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.llama3.reference_impl.generation import Llama\n",
    "import os\n",
    "\n",
    "llama3_2_1B_ckpt_dir = os.path.expanduser(\"~/.llama/checkpoints/Llama3.2-1B/original/\")\n",
    "tokenizer_path = llama3_2_1B_ckpt_dir + \"tokenizer.model\"\n",
    "\n",
    "llama = Llama.build(\n",
    "    ckpt_dir=llama3_2_1B_ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=512,\n",
    "    max_batch_size=1,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "next(llama.model.parameters()).device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I have no idea, but I think I can'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.text_completion(\"What is the meaning of life?\", max_gen_len=10).generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of Shakespeare's text:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "\n",
      "Total characters: 1115394\n",
      "Total lines: 40000\n"
     ]
    }
   ],
   "source": [
    "shakespeare_text = open(\"./data/tinyshakespeare.txt\", \"r\").read()\n",
    "\n",
    "# Display the first few lines\n",
    "print(\"First few lines of Shakespeare's text:\")\n",
    "print(shakespeare_text[:500])\n",
    "\n",
    "# Get some statistics\n",
    "total_chars = len(shakespeare_text)\n",
    "total_lines = shakespeare_text.count(\"\\n\")\n",
    "\n",
    "print(f\"\\nTotal characters: {total_chars}\")\n",
    "print(f\"Total lines: {total_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "Vocabulary Size: 128256\n",
      "Number of Layers: 4\n",
      "Embedding Dimension: 512\n",
      "Number of Attention Heads: 8\n",
      "Max Sequence Length: 512\n",
      "Feedforward Dimension: 2048\n",
      "\n",
      "Parameter Initialization:\n",
      "tok_embeddings.weight: Initialized\n",
      "layers.0.attention.wq.weight: Initialized\n",
      "layers.0.attention.wk.weight: Initialized\n",
      "layers.0.attention.wv.weight: Initialized\n",
      "layers.0.attention.wo.weight: Initialized\n",
      "layers.0.feed_forward.w1.weight: Initialized\n",
      "layers.0.feed_forward.w2.weight: Initialized\n",
      "layers.0.feed_forward.w3.weight: Initialized\n",
      "layers.0.attention_norm.weight: Initialized\n",
      "layers.0.ffn_norm.weight: Initialized\n",
      "layers.1.attention.wq.weight: Initialized\n",
      "layers.1.attention.wk.weight: Initialized\n",
      "layers.1.attention.wv.weight: Initialized\n",
      "layers.1.attention.wo.weight: Initialized\n",
      "layers.1.feed_forward.w1.weight: Initialized\n",
      "layers.1.feed_forward.w2.weight: Initialized\n",
      "layers.1.feed_forward.w3.weight: Initialized\n",
      "layers.1.attention_norm.weight: Initialized\n",
      "layers.1.ffn_norm.weight: Initialized\n",
      "layers.2.attention.wq.weight: Initialized\n",
      "layers.2.attention.wk.weight: Initialized\n",
      "layers.2.attention.wv.weight: Initialized\n",
      "layers.2.attention.wo.weight: Initialized\n",
      "layers.2.feed_forward.w1.weight: Initialized\n",
      "layers.2.feed_forward.w2.weight: Initialized\n",
      "layers.2.feed_forward.w3.weight: Initialized\n",
      "layers.2.attention_norm.weight: Initialized\n",
      "layers.2.ffn_norm.weight: Initialized\n",
      "layers.3.attention.wq.weight: Initialized\n",
      "layers.3.attention.wk.weight: Initialized\n",
      "layers.3.attention.wv.weight: Initialized\n",
      "layers.3.attention.wo.weight: Initialized\n",
      "layers.3.feed_forward.w1.weight: Initialized\n",
      "layers.3.feed_forward.w2.weight: Initialized\n",
      "layers.3.feed_forward.w3.weight: Initialized\n",
      "layers.3.attention_norm.weight: Initialized\n",
      "layers.3.ffn_norm.weight: Initialized\n",
      "norm.weight: Initialized\n",
      "output.weight: Initialized\n",
      "\n",
      "Key Component Shapes:\n",
      "Token Embeddings: torch.Size([128256, 512])\n",
      "Output Layer: torch.Size([128256, 512])\n",
      "First Layer Query Weight: torch.Size([512, 512])\n",
      "First Layer Key Weight: torch.Size([128, 512])\n",
      "First Layer Value Weight: torch.Size([128, 512])\n",
      "\n",
      "NaN/Inf Check:\n",
      "tok_embeddings.weight: OK\n",
      "layers.0.attention.wq.weight: OK\n",
      "layers.0.attention.wk.weight: OK\n",
      "layers.0.attention.wv.weight: OK\n",
      "layers.0.attention.wo.weight: OK\n",
      "layers.0.feed_forward.w1.weight: OK\n",
      "layers.0.feed_forward.w2.weight: OK\n",
      "layers.0.feed_forward.w3.weight: OK\n",
      "layers.0.attention_norm.weight: OK\n",
      "layers.0.ffn_norm.weight: OK\n",
      "layers.1.attention.wq.weight: OK\n",
      "layers.1.attention.wk.weight: OK\n",
      "layers.1.attention.wv.weight: OK\n",
      "layers.1.attention.wo.weight: OK\n",
      "layers.1.feed_forward.w1.weight: OK\n",
      "layers.1.feed_forward.w2.weight: OK\n",
      "layers.1.feed_forward.w3.weight: OK\n",
      "layers.1.attention_norm.weight: OK\n",
      "layers.1.ffn_norm.weight: OK\n",
      "layers.2.attention.wq.weight: OK\n",
      "layers.2.attention.wk.weight: OK\n",
      "layers.2.attention.wv.weight: OK\n",
      "layers.2.attention.wo.weight: OK\n",
      "layers.2.feed_forward.w1.weight: OK\n",
      "layers.2.feed_forward.w2.weight: OK\n",
      "layers.2.feed_forward.w3.weight: OK\n",
      "layers.2.attention_norm.weight: OK\n",
      "layers.2.ffn_norm.weight: OK\n",
      "layers.3.attention.wq.weight: OK\n",
      "layers.3.attention.wk.weight: OK\n",
      "layers.3.attention.wv.weight: OK\n",
      "layers.3.attention.wo.weight: OK\n",
      "layers.3.feed_forward.w1.weight: OK\n",
      "layers.3.feed_forward.w2.weight: OK\n",
      "layers.3.feed_forward.w3.weight: OK\n",
      "layers.3.attention_norm.weight: OK\n",
      "layers.3.ffn_norm.weight: OK\n",
      "norm.weight: OK\n",
      "output.weight: OK\n"
     ]
    }
   ],
   "source": [
    "# Check model parameters\n",
    "print(\"Model Parameters:\")\n",
    "print(f\"Vocabulary Size: {model.vocab_size}\")\n",
    "print(f\"Number of Layers: {model.n_layers}\")\n",
    "print(f\"Embedding Dimension: {model.params.dim}\")\n",
    "print(f\"Number of Attention Heads: {model.params.n_heads}\")\n",
    "print(f\"Max Sequence Length: {model.params.max_seq_len}\")\n",
    "print(f\"Feedforward Dimension: {model.layers[0].feed_forward.w1.out_features}\")\n",
    "\n",
    "# Check if parameters are initialized\n",
    "print(\"\\nParameter Initialization:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {'Initialized' if param.sum().item() != 0 else 'Not initialized'}\")\n",
    "\n",
    "# Verify shapes of key components\n",
    "print(\"\\nKey Component Shapes:\")\n",
    "print(f\"Token Embeddings: {model.tok_embeddings.weight.shape}\")\n",
    "print(f\"Output Layer: {model.output.weight.shape}\")\n",
    "print(f\"First Layer Query Weight: {model.layers[0].attention.wq.weight.shape}\")\n",
    "print(f\"First Layer Key Weight: {model.layers[0].attention.wk.weight.shape}\")\n",
    "print(f\"First Layer Value Weight: {model.layers[0].attention.wv.weight.shape}\")\n",
    "\n",
    "# Check for NaNs or infinities\n",
    "print(\"\\nNaN/Inf Check:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "        print(f\"Warning: {name} contains NaN or Inf values\")\n",
    "    else:\n",
    "        print(f\"{name}: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Trainable Parameters: 146,543,104\n",
      "Approximate Model Size: 0.55 GB\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate size in gigabytes (assuming float32 parameters)\n",
    "size_in_gb = total_params * 4 / (1024**3)  # 4 bytes per float32 parameter\n",
    "print(f\"Approximate Model Size: {size_in_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to checkpoints/llama3_model_checkpoint.pth\n",
      "Model arguments saved to checkpoints/llama3_model_args.json\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint the model\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints if it doesn't exist\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save the model state\n",
    "model_path = os.path.join(checkpoint_dir, \"llama3_model_checkpoint.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model checkpoint saved to {model_path}\")\n",
    "\n",
    "# Save the model arguments\n",
    "import json\n",
    "\n",
    "model_args_path = os.path.join(checkpoint_dir, \"llama3_model_args.json\")\n",
    "with open(model_args_path, 'w') as f:\n",
    "    json.dump(vars(model.params), f, indent=2)\n",
    "\n",
    "print(f\"Model arguments saved to {model_args_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, no will to die through love itself.\n",
      "Why, how to tread how do to honour newly your brother,\n",
      "But to have five thousand thanks too much to Clarence:\n",
      "I'll give my soul,\n",
      "To should our speech of gold and too:\n",
      "You are dear train, and father, poor brother,\n",
      "Ere further conference with a passing small.\n",
      "O Dorsetable.\n",
      "Your sense may beggarly the tomb,\n",
      "And bid me mistress sit dispatch: past the boy,\n",
      "And well lost with one thing just proportion,\n",
      "And over the board, under his liking!\n",
      "And all the watchful eye of dear faith,\n",
      "More fierce and an inditeous wrath!\n",
      "How well, lords, I befall, and lay,\n",
      "Is not forgot the tyrant, to fill the crown,\n",
      "And manage of your glorious sun: regent join'd!\n",
      "Yet would youravenousoddess, that went;\n",
      "And well we have heard of all run a needful';\n",
      "Anduile me with the root\n",
      "And buryWhat! myself become a tyrant\n",
      "Stands without the brat's king in Bosworth\n",
      "To leap upon a black tidings was;\n",
      "And in all my tumble down: great leaving me,\n",
      "'Twere a bloody axe to that makes your countenance.\n",
      "See how, command of the head brings a tyrant,\n",
      "Found that the fair volume of York, God,\n",
      "Vouches her career me that was in'd death.\n",
      "\n",
      "EDWARD:\n",
      "Let me be not; and take me no fight:\n",
      "I'll none but well-spAGUE:\n",
      "My lord, yea, fealty like you night;\n",
      "And your mind are fled; and so most I stay,\n",
      "Together with many gawdy; and welcome down\n",
      "'Tis'd by them-ch committed by envious,\n",
      "And ancient feast, like a cover to obey.\n",
      "But if my ghostly heart's sin,\n",
      "Hark you in this and left protectors,\n",
      "And chivalrous'd my common men's music of alike,\n",
      "Since knees with O, the book of Percy,\n",
      "This heavy chamber-potless of pestil\n",
      "That brings the white man? that must we mock thee.\n",
      "Therefore, what a greater thaniger in this cell,\n",
      "Is ignorant with a dyingadvisedly com,\n",
      "Then vial and kin and Tybroke,\n",
      "This music crept like the grave with the bloody fray.\n",
      "\n",
      "RICHARD:\n",
      "O Duke of Somerset, Richard, dull delay\n",
      "WillDie in the vice: little thinks not goodly, my lord,\n",
      "Come quickly\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(Llama(model, llama.tokenizer, model_args).text_completion(\"To be or not to be,\", temperature=1.0, max_gen_len=500, echo=True).generation.split(\"<|begin_of_text|>\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
