{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5548b69330849388349cd1e9c432927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/716 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 15:22:24 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6e152eb9ac4ee3b78676330d88f66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e82499046745458758c099615b80e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaa5e08db4241d683c8572c3163616a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfa4e4aef334278ae3d942142b521d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/169 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20241217_152226-rl38</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl38' target=\"_blank\">rl38</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl38' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl38</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import json\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import ExploreOptions, Trainer, vLLMConfig\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import Any, AsyncIterable, Literal, Optional\n",
    "\n",
    "with open(\"./data/chain-of-thought-examples.json\") as f:\n",
    "    chain_of_thought_examples: list[dict[str, str]] = json.load(f)\n",
    "\n",
    "client = httpx.AsyncClient(\n",
    "    timeout=httpx.Timeout(5.0, read=30.0),\n",
    "    limits=httpx.Limits(max_connections=512, max_keepalive_connections=512),\n",
    ")\n",
    "\n",
    "\n",
    "async def sample_random_episode(\n",
    "    difficulty: Literal[\"easy\", \"variable\"] = \"variable\",\n",
    "    example_probability: float = 0.0,\n",
    "    max_prompt_characters: int = 8192,\n",
    "    reward_follow_up_completion: bool = True,\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    "    length_penalty: float = 0.0,\n",
    ") -> Episode:\n",
    "    while True:\n",
    "        params: dict[str, Any] = {\n",
    "            \"difficulty\": difficulty,\n",
    "        }\n",
    "        if return_first_solver_as_winner is not None:\n",
    "            params[\"return_first_solver_as_winner\"] = return_first_solver_as_winner\n",
    "        try:\n",
    "            response = await client.get(\n",
    "                \"http://0.0.0.0:2218/new-episode-data\",\n",
    "                params=params,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "        except httpx.TimeoutException:\n",
    "            continue\n",
    "        result = response.json()\n",
    "        prompt = result[\"prompt\"]\n",
    "        follow_up = result[\"follow_up\"]\n",
    "        solution = result[\"solution\"]\n",
    "        if len(prompt) <= max_prompt_characters:\n",
    "            break\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ],\n",
    "                max_tokens=10\n",
    "                + len(\"\\n\".join(f\"{key}: {value}\" for key, value in solution.items()))\n",
    "                // 2,\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        if reward_follow_up_completion:\n",
    "            completion = follow_up_completion\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    # Find first match of key followed by colon and capture following text\n",
    "                    (\n",
    "                        match := re.search(\n",
    "                            rf\"{key}: ([A-Za-z \\.:-]+)\",\n",
    "                            answer,\n",
    "                            re.IGNORECASE,\n",
    "                        )\n",
    "                    )\n",
    "                    # Check if captured group matches expected value\n",
    "                    and match.group(1).strip().lower() == value.strip().lower()\n",
    "                )\n",
    "                for key, value in solution.items()\n",
    "            ]\n",
    "        ) / len(solution)\n",
    "        completion.reward -= (\n",
    "            completion.all_absent_stop_tokens\n",
    "            / (3 if reward_follow_up_completion else 2)\n",
    "            / len(solution)\n",
    "        )\n",
    "        completion.reward -= (\n",
    "            completion.completion_tokens\n",
    "            / (len(prompt) + len(solution) * 10)\n",
    "            * length_penalty\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    example = random.choice(chain_of_thought_examples)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        examples=lambda: (\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": example[\"chain_of_thought\"]\n",
    "                    + (example[\"answer\"] and f\"\\n\\n---\\n\\n{example['answer']}\"),\n",
    "                },\n",
    "            ]\n",
    "            if random.random() < example_probability\n",
    "            else []\n",
    "        ),\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "episodes_per_iteration = 64 * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "async def train_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    pending: set[asyncio.Task[Episode | BaseException]] = set()\n",
    "    while True:\n",
    "        pending.update(\n",
    "            asyncio.create_task(sample_random_episode())\n",
    "            for _ in range(episodes_per_iteration - len(pending))\n",
    "        )\n",
    "        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "        for task in done:\n",
    "            try:\n",
    "                yield task.result()\n",
    "            except BaseException as e:\n",
    "                yield e\n",
    "\n",
    "\n",
    "async def val_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    for fut in asyncio.as_completed(\n",
    "        sample_random_episode() for _ in range(64 * torch.cuda.device_count())\n",
    "    ):\n",
    "        try:\n",
    "            yield await fut\n",
    "        except BaseException as e:\n",
    "            yield e\n",
    "\n",
    "\n",
    "model_name = \"rl38\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=ExploreOptions(\n",
    "        iterations=7,\n",
    "        num_parents=6,\n",
    "        branch_factor=3,\n",
    "        patience=5,\n",
    "        sample_probability_power=None,\n",
    "        sampling_kwargs={\"max_tokens\": 2048},\n",
    "        # split_method=\"prob\",\n",
    "        # split_point_std_deviation=0.5,\n",
    "    ),\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=episodes_per_iteration,\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=val_episodes(),\n",
    "    val_patience=15,\n",
    "    val_samples_per_episode=3,\n",
    "    val_sampling_kwargs={\"max_tokens\": 2048},\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        # max_steps_per_epoch=32,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=4e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=0.0,\n",
    "            clip_epsilon=0.2,\n",
    "            unclipped_policy_coef=0.0,\n",
    "            tanh_log_policy_coef=0.8,\n",
    "            value_coef=0.0,\n",
    "            entropy_coef=0.0,\n",
    "            entropy_target=0.6,\n",
    "            entropy_target_coef=0.05,\n",
    "            kl_coef=0.05,\n",
    "            weighted_entropy_coef=0.2,\n",
    "            weighted_kl_coef=0.0,\n",
    "            weighted_ce_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_chunked_prefill=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384,\n",
    "            preemption_mode=\"swap\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=100,\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        min_time_between_requests=0.0,\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11748: 0.45,\n",
       " 14524: 0.45,\n",
       " 3868: 0.45,\n",
       " 81122: 0.4,\n",
       " 88601: 0.4,\n",
       " 93114: 0.4,\n",
       " 69487: 0.4,\n",
       " 66372: 0.35,\n",
       " 32576: 0.35,\n",
       " 8530: 0.35,\n",
       " 74128: 0.35,\n",
       " 53692: 0.35,\n",
       " 3604: 0.35,\n",
       " 74303: 0.35,\n",
       " 65002: 0.35,\n",
       " 4856: 0.35,\n",
       " 3983: 0.3,\n",
       " 5451: 0.3,\n",
       " 1176: 0.3,\n",
       " 68791: 0.3,\n",
       " 30293: 0.3,\n",
       " 55915: 0.3,\n",
       " 9093: 0.3,\n",
       " 65937: 0.3,\n",
       " 31887: 0.3,\n",
       " 4619: 0.3,\n",
       " 35970: 0.25,\n",
       " 30285: 0.25,\n",
       " 5076: 0.25,\n",
       " 63054: 0.25,\n",
       " 41568: 0.25,\n",
       " 15465: 0.25,\n",
       " 2201: 0.25,\n",
       " 2822: 0.25,\n",
       " 912: 0.25,\n",
       " 8248: 0.2,\n",
       " 4071: 0.2,\n",
       " 719: 0.2,\n",
       " 708: 0.2,\n",
       " 4516: 0.2,\n",
       " 779: 0.2,\n",
       " 34232: 0.2,\n",
       " 45600: 0.2,\n",
       " 8617: 0.2,\n",
       " 3409: 0.2,\n",
       " 7184: 0.2,\n",
       " 1457: 0.2,\n",
       " 43068: 0.2,\n",
       " 22818: 0.2,\n",
       " 2728: 0.2,\n",
       " 23289: 0.2,\n",
       " 2319: 0.15,\n",
       " 12174: 0.15,\n",
       " 14346: 0.15,\n",
       " 65662: 0.15,\n",
       " 56372: 0.15,\n",
       " 1315: 0.15,\n",
       " 6107: 0.15,\n",
       " 1314: 0.15}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens(string: str) -> list[int]:\n",
    "    tokens = trainer.tokenizer.llm.get_tokenizer().encode(\n",
    "        string, add_special_tokens=False  # type: ignore\n",
    "    )\n",
    "    if len(tokens) > 1:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "\n",
    "word_biases = [\n",
    "    # Critical thinking markers (highest importance)\n",
    "    (\"wait\", 0.9),  # Frequent correction initiator\n",
    "    (\"hmm\", 0.8),  # Common uncertainty marker\n",
    "    (\"alternatively\", 0.8),  # Key for exploring options\n",
    "    # Important correction/uncertainty markers\n",
    "    (\"perhaps\", 0.7),  # Common for tentative suggestions\n",
    "    (\"actually\", 0.7),  # Important for corrections\n",
    "    (\"rather\", 0.7),  # Refinement marker\n",
    "    # Analysis progression markers\n",
    "    (\"first\", 0.6),\n",
    "    (\"similarly\", 0.6),\n",
    "    (\"therefore\", 0.6),\n",
    "    (\"instead\", 0.6),\n",
    "    # Mistake acknowledgment\n",
    "    (\"wrong\", 0.5),\n",
    "    (\"incorrect\", 0.5),\n",
    "    (\"no\", 0.5),\n",
    "    # Supporting markers (lower importance)\n",
    "    (\"but\", 0.4),\n",
    "    (\"so\", 0.4),\n",
    "    (\"thus\", 0.4),\n",
    "    (\"now\", 0.4),\n",
    "    (\"given\", 0.4),\n",
    "    (\"suppose\", 0.4),\n",
    "    # Light correction markers\n",
    "    (\"oh\", 0.3),\n",
    "    (\"oops\", 0.3),\n",
    "    (\"right\", 0.3),\n",
    "]\n",
    "\n",
    "logit_bias = {\n",
    "    token: bias / 2\n",
    "    for word, bias in word_biases\n",
    "    for transform in [lambda x: x, str.capitalize, lambda x: \" \" + x]\n",
    "    for token in tokens(transform(word))\n",
    "}\n",
    "logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.explore_options.sampling_kwargs[\"logit_bias\"] = logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8adfb6f824e4d3b833d5181449786b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_result = await trainer.explore(verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await trainer.tune(trainer.explore_results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f54a64915d4b2cb9c218b390cb3c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1fc6327ee242d8b41a6e86baa2a5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (0 remaining episodes x 15 patience per episode = 0 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl35/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|19|Loss: 0.0137: 100%|██████████| 19/19 [05:51<00:00, 17.82s/it, entropy=0.4768, entropy_target=0.1232, kl_div=0.1285, policy=-0.0627, tanh_log_policy=0.0041, unclipped_policy=-0.1151, value=2.8588, weighted_ce=-0.1450, weighted_entropy=0.0109, weighted_kl_div=0.1807] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 39 model files to /home/ubuntu/atreides/experiments/models/rl35/0039\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl35/0039 --port=8009 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f991e24a6f493a90fb612b4eb5b97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82bae6c0b394b6ea122bf7576db748a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl35/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|20|Loss: -0.1178: 100%|██████████| 20/20 [06:05<00:00, 17.69s/it, entropy=0.9599, entropy_target=0.3599, kl_div=0.2557, policy=-0.6068, tanh_log_policy=-0.0675, unclipped_policy=-1.4084, value=6.7460, weighted_ce=-0.5565, weighted_entropy=0.4731, weighted_kl_div=0.0815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 40 model files to /home/ubuntu/atreides/experiments/models/rl35/0040\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl35/0040 --port=8009 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a5caa79b754987bedb3ae06503d458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f869b01b434a3dba087808ff608d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (0 remaining episodes x 15 patience per episode = 0 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl35/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|14|Loss: 0.0581: 100%|██████████| 14/14 [04:23<00:00, 17.65s/it, entropy=0.6196, entropy_target=0.0196, kl_div=0.1941, policy=-0.1284, tanh_log_policy=0.0604, unclipped_policy=-0.1790, value=2.4163, weighted_ce=0.0416, weighted_entropy=0.0047, weighted_kl_div=-0.1024]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 41 model files to /home/ubuntu/atreides/experiments/models/rl35/0041\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl35/0041 --port=8009 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52040d3f950a415cb09e63d31b4c2b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa411909a434222b4e5b23570dca6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl35/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|22|Loss: 0.0034: 100%|██████████| 22/22 [06:46<00:00, 17.93s/it, entropy=0.5840, entropy_target=0.0160, kl_div=0.1036, policy=0.0085, tanh_log_policy=-0.0008, unclipped_policy=0.0071, value=1.5228, weighted_ce=-0.0038, weighted_entropy=0.0098, weighted_kl_div=0.0029]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 42 model files to /home/ubuntu/atreides/experiments/models/rl35/0042\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl35/0042 --port=8009 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0a2092b8294fdbab46d0eaaa874ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7e6b658b944c1287a4aae511ee20db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (0 remaining episodes x 15 patience per episode = 0 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl35/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|16|Loss: 0.0051: 100%|██████████| 16/16 [04:57<00:00, 17.51s/it, entropy=0.4877, entropy_target=0.1123, kl_div=0.0803, policy=0.0129, tanh_log_policy=0.0012, unclipped_policy=0.0093, value=3.4596, weighted_ce=-0.0097, weighted_entropy=0.0276, weighted_kl_div=0.0043]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 43 model files to /home/ubuntu/atreides/experiments/models/rl35/0043\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl35/0043 --port=8009 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f557b0725f74fe480f305a3af877aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (0 remaining episodes x 15 patience per episode = 0 seconds)\n"
     ]
    }
   ],
   "source": [
    "await trainer.train(iterations=10, verbosity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
