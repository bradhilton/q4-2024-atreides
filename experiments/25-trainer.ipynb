{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 18:15:22 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20241217_181524-rl38</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl38' target=\"_blank\">rl38</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl38' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl38</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import json\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import ExploreOptions, Trainer, vLLMConfig\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import Any, AsyncIterable, Literal, Optional\n",
    "\n",
    "with open(\"./data/chain-of-thought-examples.json\") as f:\n",
    "    chain_of_thought_examples: list[dict[str, str]] = json.load(f)\n",
    "\n",
    "client = httpx.AsyncClient(\n",
    "    timeout=httpx.Timeout(5.0, read=30.0),\n",
    "    limits=httpx.Limits(max_connections=512, max_keepalive_connections=512),\n",
    ")\n",
    "\n",
    "\n",
    "async def sample_random_episode(\n",
    "    difficulty: Literal[\"easy\", \"variable\"] = \"variable\",\n",
    "    example_probability: float = 0.0,\n",
    "    max_prompt_characters: int = 8192,\n",
    "    reward_follow_up_completion: bool = True,\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    "    length_penalty: float = 0.0,\n",
    ") -> Episode:\n",
    "    while True:\n",
    "        params: dict[str, Any] = {\n",
    "            \"difficulty\": difficulty,\n",
    "        }\n",
    "        if return_first_solver_as_winner is not None:\n",
    "            params[\"return_first_solver_as_winner\"] = return_first_solver_as_winner\n",
    "        try:\n",
    "            response = await client.get(\n",
    "                \"http://0.0.0.0:2218/new-episode-data\",\n",
    "                params=params,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "        except httpx.TimeoutException:\n",
    "            continue\n",
    "        result = response.json()\n",
    "        prompt = result[\"prompt\"]\n",
    "        follow_up = result[\"follow_up\"]\n",
    "        solution = result[\"solution\"]\n",
    "        if len(prompt) <= max_prompt_characters:\n",
    "            break\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ],\n",
    "                max_tokens=10\n",
    "                + len(\"\\n\".join(f\"{key}: {value}\" for key, value in solution.items()))\n",
    "                // 2,\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        if reward_follow_up_completion:\n",
    "            completion = follow_up_completion\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    # Find first match of key followed by colon and capture following text\n",
    "                    (\n",
    "                        match := re.search(\n",
    "                            rf\"{key}: ([A-Za-z \\.:-]+)\",\n",
    "                            answer,\n",
    "                            re.IGNORECASE,\n",
    "                        )\n",
    "                    )\n",
    "                    # Check if captured group matches expected value\n",
    "                    and match.group(1).strip().lower() == value.strip().lower()\n",
    "                )\n",
    "                for key, value in solution.items()\n",
    "            ]\n",
    "        ) / len(solution)\n",
    "        completion.reward -= (\n",
    "            completion.all_absent_stop_tokens\n",
    "            / (3 if reward_follow_up_completion else 2)\n",
    "            / len(solution)\n",
    "        )\n",
    "        completion.reward -= (\n",
    "            completion.completion_tokens\n",
    "            / (len(prompt) + len(solution) * 10)\n",
    "            * length_penalty\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    example = random.choice(chain_of_thought_examples)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        examples=lambda: (\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": example[\"chain_of_thought\"]\n",
    "                    + (example[\"answer\"] and f\"\\n\\n---\\n\\n{example['answer']}\"),\n",
    "                },\n",
    "            ]\n",
    "            if random.random() < example_probability\n",
    "            else []\n",
    "        ),\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "episodes_per_iteration = 64 * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "async def train_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    pending: set[asyncio.Task[Episode | BaseException]] = set()\n",
    "    while True:\n",
    "        pending.update(\n",
    "            asyncio.create_task(sample_random_episode(example_probability=0.2))\n",
    "            for _ in range(episodes_per_iteration - len(pending))\n",
    "        )\n",
    "        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "        for task in done:\n",
    "            try:\n",
    "                yield task.result()\n",
    "            except BaseException as e:\n",
    "                yield e\n",
    "\n",
    "\n",
    "async def val_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    for fut in asyncio.as_completed(\n",
    "        sample_random_episode() for _ in range(64 * torch.cuda.device_count())\n",
    "    ):\n",
    "        try:\n",
    "            yield await fut\n",
    "        except BaseException as e:\n",
    "            yield e\n",
    "\n",
    "\n",
    "model_name = \"rl38\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=ExploreOptions(\n",
    "        iterations=7,\n",
    "        num_parents=6,\n",
    "        branch_factor=3,\n",
    "        patience=5,\n",
    "        sample_probability_power=None,\n",
    "        sampling_kwargs={\n",
    "            \"max_tokens\": 2048\n",
    "        },\n",
    "        # split_method=\"prob\",\n",
    "        # split_point_std_deviation=0.5,\n",
    "    ),\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=episodes_per_iteration,\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=val_episodes(),\n",
    "    val_patience=15,\n",
    "    val_samples_per_episode=3,\n",
    "    val_sampling_kwargs={\"max_tokens\": 2048},\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        # max_steps_per_epoch=32,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=4e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=0.0,\n",
    "            clip_epsilon=0.2,\n",
    "            unclipped_policy_coef=0.0,\n",
    "            tanh_log_policy_coef=0.8,\n",
    "            value_coef=0.0,\n",
    "            entropy_coef=0.0,\n",
    "            entropy_target=0.6,\n",
    "            entropy_target_coef=0.05,\n",
    "            kl_coef=0.05,\n",
    "            weighted_entropy_coef=0.2,\n",
    "            weighted_kl_coef=0.0,\n",
    "            weighted_ce_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_chunked_prefill=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384,\n",
    "            preemption_mode=\"swap\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=100,\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        min_time_between_requests=0.0,\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl38/0001 --port=8002 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n"
     ]
    }
   ],
   "source": [
    "await trainer.train(iterations=10, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8002 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7d9f4bd65a4189b4e160893f685547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await trainer.explore(verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.573509693145752,\n",
       " -1.5497195136049413e-06,\n",
       " -3.683499380713329e-05,\n",
       " -1.1517176628112793,\n",
       " -0.5885536670684814,\n",
       " -0.16787488758563995,\n",
       " -0.03347204998135567,\n",
       " -0.017174215987324715,\n",
       " -0.011563891544938087,\n",
       " -6.958192348480225,\n",
       " -0.04683351516723633,\n",
       " -0.35457366704940796,\n",
       " -0.2585309147834778,\n",
       " -0.7922614812850952,\n",
       " -2.3859448432922363,\n",
       " -0.3589121699333191,\n",
       " -9.238292841473594e-05,\n",
       " -0.0005824061809107661,\n",
       " -0.806515097618103,\n",
       " -0.009681769646704197,\n",
       " -0.47959402203559875,\n",
       " -1.8232364654541016,\n",
       " -0.23573361337184906,\n",
       " -0.056180961430072784,\n",
       " -2.094564914703369,\n",
       " -1.9626911878585815,\n",
       " -0.18298567831516266,\n",
       " -4.172316494077677e-06,\n",
       " -0.004867489915341139,\n",
       " 0.0,\n",
       " -0.5472233295440674,\n",
       " 0.0,\n",
       " -4.7205765440594405e-05,\n",
       " -0.00012206286191940308,\n",
       " -1.0607349872589111,\n",
       " -1.7881377516459906e-06,\n",
       " -0.6786861419677734,\n",
       " -6.198863957251888e-06,\n",
       " -0.001359015703201294,\n",
       " 0.0,\n",
       " -3.814689989667386e-06,\n",
       " 0.0,\n",
       " -0.26380497217178345,\n",
       " 0.0,\n",
       " -0.00022504181833937764,\n",
       " -9.65590606938349e-06,\n",
       " -0.000337305391440168,\n",
       " -0.42754217982292175,\n",
       " -3.4570634852570947e-06,\n",
       " -3.077521562576294,\n",
       " -2.0265373677830212e-05,\n",
       " -0.04113318771123886,\n",
       " -0.3722229599952698,\n",
       " -1.1349953413009644,\n",
       " -1.1520605087280273,\n",
       " -1.1920928244535389e-07,\n",
       " -0.0013404440833255649,\n",
       " -0.36089643836021423,\n",
       " -2.3841855067985307e-07,\n",
       " -9.226373367710039e-05,\n",
       " -0.0005559567362070084,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.22383806109428406,\n",
       " -0.10215232521295547,\n",
       " -0.007023526355624199,\n",
       " -0.0011148196645081043,\n",
       " -3.576278118089249e-07,\n",
       " -1.1801649634435307e-05,\n",
       " -3.798077344894409,\n",
       " -1.1920928244535389e-07,\n",
       " -6.318072337307967e-06,\n",
       " -0.07140098512172699,\n",
       " 0.0,\n",
       " -7.152555099310121e-07,\n",
       " 0.0,\n",
       " -3.748843193054199,\n",
       " 0.0,\n",
       " -0.000278195773717016,\n",
       " 0.0,\n",
       " -5.960462772236497e-07,\n",
       " 0.0,\n",
       " -2.248793125152588,\n",
       " -0.00389828416518867,\n",
       " -0.3781927824020386,\n",
       " -0.11068183183670044,\n",
       " -7.652943895664066e-05,\n",
       " -1.7647359371185303,\n",
       " -0.9903830289840698,\n",
       " -0.015589984133839607,\n",
       " -0.013384255580604076,\n",
       " -0.06411369144916534,\n",
       " 0.0,\n",
       " -8.344646857949556e-07,\n",
       " -0.2699594795703888,\n",
       " 0.0,\n",
       " -0.00015698630886618048,\n",
       " -0.00435748603194952,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.4005649983882904,\n",
       " -0.01041701715439558,\n",
       " -0.09852757304906845,\n",
       " -3.6954811548639555e-06,\n",
       " -0.07094069570302963,\n",
       " -0.09048697352409363,\n",
       " -4.768370445162873e-07,\n",
       " -4.768370445162873e-07,\n",
       " -0.27920112013816833,\n",
       " -0.2948276698589325,\n",
       " 0.0,\n",
       " -2.3841855067985307e-07,\n",
       " -0.07074012607336044,\n",
       " -0.00032658010604791343,\n",
       " -0.008119314908981323,\n",
       " -0.4312279224395752,\n",
       " -8.821448318485636e-06,\n",
       " 0.0,\n",
       " -0.000386640167562291,\n",
       " 0.0,\n",
       " -1.1920858014491387e-05,\n",
       " 0.0,\n",
       " -3.576278118089249e-07,\n",
       " -0.03927535191178322,\n",
       " 0.0,\n",
       " -0.010311543010175228,\n",
       " -0.0003216941258870065,\n",
       " 0.0,\n",
       " -1.1920858014491387e-05,\n",
       " -8.344646857949556e-07,\n",
       " 0.0,\n",
       " -0.0038433035369962454,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.00027366707217879593,\n",
       " -1.1920928244535389e-07,\n",
       " 0.0,\n",
       " -0.0012831796193495393,\n",
       " 0.0,\n",
       " -0.05138698220252991,\n",
       " 0.0,\n",
       " -7.986990567587782e-06,\n",
       " -0.00011050090688513592,\n",
       " -1.4305104514278355e-06,\n",
       " -8.702239938429557e-06,\n",
       " -4.768370445162873e-07,\n",
       " -3.576278118089249e-07,\n",
       " -2.8729025871143676e-05,\n",
       " -3.576272320060525e-06,\n",
       " -0.0032961820252239704,\n",
       " -0.0019342063460499048,\n",
       " -4.017272294731811e-05,\n",
       " 0.0,\n",
       " -0.18083783984184265,\n",
       " -3.317779541015625,\n",
       " -0.04266207292675972,\n",
       " -0.9281906485557556,\n",
       " -0.030622873455286026,\n",
       " -0.03378039225935936,\n",
       " -0.5274379253387451,\n",
       " -0.01551827136427164,\n",
       " -2.145764938177308e-06,\n",
       " -0.023769890889525414,\n",
       " -0.18765488266944885,\n",
       " -0.0007884969236329198,\n",
       " -0.6181596517562866,\n",
       " 0.0,\n",
       " -0.003307113191112876,\n",
       " -0.1790870875120163,\n",
       " -2.3841830625315197e-06,\n",
       " -1.1920928244535389e-07,\n",
       " -0.0024732735473662615,\n",
       " -0.5045567750930786,\n",
       " -0.0010024051880463958,\n",
       " -1.253943920135498,\n",
       " 0.0,\n",
       " -0.0039575365372002125,\n",
       " -0.6501988768577576,\n",
       " 0.0,\n",
       " -2.4175031185150146,\n",
       " -9.298280929215252e-06,\n",
       " 0.0,\n",
       " -1.6324084997177124,\n",
       " -0.08312667906284332,\n",
       " -2.1934269170742482e-05,\n",
       " -0.5085783004760742,\n",
       " -1.1920928244535389e-07,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -1.597391747054644e-05,\n",
       " -1.4233137369155884,\n",
       " 0.0,\n",
       " -0.00032205163734033704,\n",
       " -1.7011021375656128,\n",
       " -0.11069271713495255,\n",
       " -0.002374093746766448,\n",
       " 0.0,\n",
       " -3.4570634852570947e-06,\n",
       " -0.05378159135580063,\n",
       " 0.0,\n",
       " -0.002894024597480893,\n",
       " -0.012516752816736698,\n",
       " -4.279521817807108e-05,\n",
       " -0.0037949702236801386,\n",
       " -0.0009528625523671508,\n",
       " -0.2616764307022095,\n",
       " -1.0728830375228426e-06,\n",
       " -0.004246504046022892,\n",
       " -3.933898824470816e-06,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.9607318639755249,\n",
       " -0.5458126664161682,\n",
       " -0.00010084597306558862,\n",
       " -3.1294379234313965,\n",
       " 0.0,\n",
       " -2.3245540432981215e-05,\n",
       " -1.6030824184417725,\n",
       " -1.311301275563892e-06,\n",
       " -0.00026854246971197426,\n",
       " -1.0055991411209106,\n",
       " 0.0,\n",
       " -4.875540980719961e-05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.9028669595718384,\n",
       " 0.0,\n",
       " -6.556489552167477e-06,\n",
       " -0.35641130805015564,\n",
       " 0.0,\n",
       " -1.1920928244535389e-07,\n",
       " -0.23976340889930725,\n",
       " 0.0,\n",
       " -2.622600959512056e-06,\n",
       " -0.013648170046508312,\n",
       " -0.06736809760332108,\n",
       " -0.04153627157211304,\n",
       " 0.0,\n",
       " -0.00013124081306159496,\n",
       " -2.7431087493896484,\n",
       " -1.5616295058862306e-05,\n",
       " -0.026500144973397255,\n",
       " -2.622600959512056e-06,\n",
       " 0.0,\n",
       " -2.90866428258596e-05,\n",
       " 0.0,\n",
       " -3.576278118089249e-07,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -4.768370445162873e-07,\n",
       " 0.0,\n",
       " -1.1920928244535389e-07,\n",
       " -0.21605096757411957]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\n",
    "    token_logprob.logprob\n",
    "    for token_logprob in list(result.episodes[23].completion.leaves())[\n",
    "        0\n",
    "    ].all_token_logprobs()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(989145)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~torch.isnan(result.tensors()[\"values\"])).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.loss.normalize_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl38/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:torchtune.utils._logging:Training is not distributed. If you want to train on multiple GPUs and are using the tune CLI, specify --nnodes 1 and --nproc_per_node [num_gpus]\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl38\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl38/tensors\n",
      "  num_sequences: 75\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.6\n",
      "  entropy_target_coef: 0.05\n",
      "  kl_coef: 0.05\n",
      "  normalize_advantages: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  tanh_log_policy_coef: 0.8\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.2\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl38\n",
      "  name: rl38\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl38\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: true\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20241217_190023-rl38\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl38\n",
      "wandb: â­ï¸ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: ðŸš€ View run at https://wandb.ai/bradhilton/torchtune/runs/rl38\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 2.96 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.02 GiB\n",
      "\tGPU peak memory reserved: 15.14 GiB\n",
      "\tGPU peak memory active: 15.02 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/75 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|75|Loss: 0.0478: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [22:14<00:00, 17.56s/it, entropy=0.1772, entropy_target=0.4228, kl_div=0.0615, policy=-0.5102, tanh_log_policy=0.0163, unclipped_policy=-0.5196, value=0.0000, weighted_ce=0.0537, weighted_entropy=-0.0524, weighted_kl_div=-0.0293]   INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 0.00 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl38/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl38/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl38/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl38/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 21.04 secs\n",
      "1|75|Loss: 0.0478: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [22:35<00:00, 18.08s/it, entropy=0.1772, entropy_target=0.4228, kl_div=0.0615, policy=-0.5102, tanh_log_policy=0.0163, unclipped_policy=-0.5196, value=0.0000, weighted_ce=0.0537, weighted_entropy=-0.0524, weighted_kl_div=-0.0293]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                   entropy â–â–ˆâ–‡â–ƒâ–ƒâ–„â–„â–ƒâ–…â–‚â–„â–ƒâ–„â–‡â–„â–ˆâ–ƒâ–„â–…â–…â–…â–„â–…â–…â–†â–ƒâ–…â–…â–‡â–…â–‡â–ƒâ–…â–†â–…â–…â–„â–†â–†â–\n",
      "wandb:            entropy_target â–‚â–ƒâ–„â–„â–„â–ƒâ–„â–‚â–ƒâ–…â–ƒâ–„â–ƒâ–â–ˆâ–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–â–ƒâ–‚â–â–‚â–ƒâ–„â–â–‚â–ƒâ–ƒâ–‚â–‚â–â–†\n",
      "wandb:               global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "wandb:                    kl_div â–â–†â–†â–‚â–…â–ƒâ–ƒâ–„â–„â–‚â–„â–…â–„â–…â–…â–…â–†â–ˆâ–„â–„â–†â–„â–„â–‚â–„â–ƒâ–„â–„â–‡â–„â–„â–†â–‚â–ƒâ–‡â–„â–†â–…â–â–„\n",
      "wandb:                      loss â–…â–…â–…â–…â–…â–…â–…â–…â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–‚â–â–…â–…â–…â–…â–…\n",
      "wandb:                        lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "wandb:        peak_memory_active â–â–â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–â–„â–‚â–‚â–ƒâ–‚â–…â–ƒâ–…â–‚â–ƒâ–…â–ƒâ–„â–†â–†â–†â–„â–†â–…â–‡â–‡â–…â–‡â–„â–‡â–†â–†â–…â–‡â–ˆ\n",
      "wandb:         peak_memory_alloc â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "wandb:      peak_memory_reserved â–â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…\n",
      "wandb:                    policy â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–‡\n",
      "wandb:           tanh_log_policy â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–â–ˆâ–ˆâ–ˆ\n",
      "wandb: tokens_per_second_per_gpu â–ƒâ–ƒâ–‡â–ˆâ–‡â–„â–‚â–‚â–ƒâ–‡â–‚â–‡â–‚â–‚â–ƒâ–ˆâ–‡â–‡â–‚â–‡â–‡â–‚â–‡â–‡â–ˆâ–ˆâ–‡â–ƒâ–ˆâ–‡â–„â–ˆâ–‚â–‡â–â–‡â–â–†â–ƒâ–\n",
      "wandb:          unclipped_policy â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "wandb:                     value â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "wandb:               weighted_ce â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–â–ˆâ–ˆâ–ˆ\n",
      "wandb:          weighted_entropy â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡\n",
      "wandb:           weighted_kl_div â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                   entropy 0.17721\n",
      "wandb:            entropy_target 0.42279\n",
      "wandb:               global_step 75\n",
      "wandb:                    kl_div 0.06146\n",
      "wandb:                      loss 0.04777\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 68.34124\n",
      "wandb:         peak_memory_alloc 68.34124\n",
      "wandb:      peak_memory_reserved 70.27734\n",
      "wandb:                    policy -0.51022\n",
      "wandb:           tanh_log_policy 0.01634\n",
      "wandb: tokens_per_second_per_gpu 26.79681\n",
      "wandb:          unclipped_policy -0.51958\n",
      "wandb:                     value 0\n",
      "wandb:               weighted_ce 0.05366\n",
      "wandb:          weighted_entropy -0.05241\n",
      "wandb:           weighted_kl_div -0.0293\n",
      "wandb: \n",
      "wandb: ðŸš€ View run rl38 at: https://wandb.ai/bradhilton/torchtune/runs/rl38\n",
      "wandb: â­ï¸ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20241217_190023-rl38/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 1 model files to /home/ubuntu/atreides/experiments/models/rl38/0001\n"
     ]
    }
   ],
   "source": [
    "await trainer.tune(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11748': 6.0,\n",
       " '14524': 6.0,\n",
       " '3868': 6.0,\n",
       " '81122': 10.8,\n",
       " '88601': 10.8,\n",
       " '93114': 8.399999999999999,\n",
       " '69487': 8.399999999999999,\n",
       " '66372': 7.199999999999999,\n",
       " '32576': 7.199999999999999,\n",
       " '8530': 7.199999999999999,\n",
       " '74128': 7.199999999999999,\n",
       " '53692': 7.199999999999999,\n",
       " '3604': 7.199999999999999,\n",
       " '74303': 6.0,\n",
       " '65002': 6.0,\n",
       " '4856': 6.0,\n",
       " '98936': 3.5999999999999996,\n",
       " '11458': 3.5999999999999996,\n",
       " '4869': 3.5999999999999996,\n",
       " '3983': 3.5999999999999996,\n",
       " '5451': 3.5999999999999996,\n",
       " '1176': 3.5999999999999996,\n",
       " '68791': 2.4000000000000004,\n",
       " '30293': 2.4000000000000004,\n",
       " '55915': 2.4000000000000004,\n",
       " '9093': 2.4000000000000004,\n",
       " '65937': 3.5999999999999996,\n",
       " '31887': 3.5999999999999996,\n",
       " '4619': 3.5999999999999996,\n",
       " '35970': 6.0,\n",
       " '30285': 6.0,\n",
       " '5076': 6.0,\n",
       " '63054': 6.0,\n",
       " '41568': 6.0,\n",
       " '15465': 6.0,\n",
       " '2201': 1.2000000000000002,\n",
       " '2822': 1.2000000000000002,\n",
       " '912': 1.2000000000000002,\n",
       " '8248': 1.2000000000000002,\n",
       " '4071': 1.2000000000000002,\n",
       " '719': 1.2000000000000002,\n",
       " '708': 0.6000000000000001,\n",
       " '4516': 0.6000000000000001,\n",
       " '779': 0.6000000000000001,\n",
       " '34232': 2.4000000000000004,\n",
       " '45600': 2.4000000000000004,\n",
       " '8617': 2.4000000000000004,\n",
       " '3409': 0.6000000000000001,\n",
       " '7184': 0.6000000000000001,\n",
       " '1457': 0.6000000000000001,\n",
       " '43068': 2.4000000000000004,\n",
       " '22818': 2.4000000000000004,\n",
       " '2728': 2.4000000000000004,\n",
       " '23289': 2.4000000000000004,\n",
       " '2319': 6.0,\n",
       " '12174': 6.0,\n",
       " '14346': 6.0,\n",
       " '65662': 6.0,\n",
       " '56372': 6.0,\n",
       " '1315': 1.7999999999999998,\n",
       " '6107': 1.7999999999999998,\n",
       " '1314': 1.7999999999999998}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens(string: str) -> list[int]:\n",
    "    tokens = trainer.tokenizer.llm.get_tokenizer().encode(\n",
    "        string, add_special_tokens=False  # type: ignore\n",
    "    )\n",
    "    if len(tokens) > 1:\n",
    "        return []\n",
    "    return tokens\n",
    "\n",
    "\n",
    "word_biases = [\n",
    "    # Critical thinking markers (highest importance)\n",
    "    (\"wait\", 1.0),  # Frequent correction initiator\n",
    "    (\"hmm\", 1.8),  # Common uncertainty marker\n",
    "    (\"alternatively\", 1.4),  # Key for exploring options\n",
    "    # Important correction/uncertainty markers\n",
    "    (\"perhaps\", 1.2),  # Common for tentative suggestions\n",
    "    (\"actually\", 1.2),  # Important for corrections\n",
    "    (\"rather\", 1.0),  # Refinement marker\n",
    "    (\"however\", 0.6),  # Common for corrections\n",
    "    # Analysis progression markers\n",
    "    (\"first\", 0.6),\n",
    "    (\"similarly\", 0.4),\n",
    "    (\"therefore\", 0.4),\n",
    "    (\"instead\", 0.6),\n",
    "    # Mistake acknowledgment\n",
    "    (\"wrong\", 1.0),\n",
    "    (\"incorrect\", 1.0),\n",
    "    (\"no\", 0.2),\n",
    "    # Supporting markers (lower importance)\n",
    "    (\"but\", 0.2),\n",
    "    (\"so\", 0.1),\n",
    "    (\"thus\", 0.4),\n",
    "    (\"now\", 0.1),\n",
    "    (\"given\", 0.4),\n",
    "    (\"suppose\", 0.4),\n",
    "    # Light correction markers\n",
    "    (\"oh\", 1.0),\n",
    "    (\"oops\", 1.0),\n",
    "    (\"right\", 0.3),\n",
    "]\n",
    "\n",
    "logit_bias = {\n",
    "    str(token): bias * 6\n",
    "    for word, bias in word_biases\n",
    "    for transform in [lambda x: x, str.capitalize, lambda x: \" \" + x]\n",
    "    for token in tokens(transform(word))\n",
    "}\n",
    "logit_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.explore_options.sampling_kwargs[\"logit_bias\"] = logit_bias\n",
    "trainer.explore_options.sampling_kwargs[\"frequency_penalty\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.explore_options.iterations = 1\n",
    "trainer.explore_options.num_parents = 1\n",
    "trainer.explore_options.branch_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.explore_options.patience = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3da58bf13d4f519b64ad82a450c003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_result = await trainer.explore(verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(explore_result.episodes[0].completion.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's first analyze the given information:\n",
      "\n",
      "1. Skylar has Madame Rose and Candlestick.\n",
      "2. Leslie showed Monsieur Brunette to Skylar.\n",
      "3. Leslie showed Miss Scarlet to Skylar.\n",
      "4. Leslie showed a card to Kayla (unknown which one).\n",
      "5. Kayla showed Leslie a card (unknown which one).\n",
      "6. Kayla showed Sophie a card (unknown which one).\n",
      "7. Kayla showed Sophie another card (unknown which one).\n",
      "8. Sophie showed Skylar the Carriage House.\n",
      "9. Skylar showed Leslie the Candlestick.\n",
      "10. Skylar showed Kayla the Candlestick.\n",
      "\n",
      "From point 9, we can determine that the facedown card in the center of the table for \"Weapon\" was actually already revealed, so there was no \"Candlestick\" in play.\n",
      "\n",
      "Now, let's look at point 7 again: \"Kayla showed Sophie another card (unknown which one).\" Since we know that Kayla has 3 cards and she has already shown two cards to other players, we can deduce that this third card must be the remaining weapon, Rope.\n",
      "\n",
      "Similarly, from point 10: \"Skylar showed Kayla the Candlestick.\" Since Candlestick is actually an incorrect reveal already mentioned in point 9, this actually means that Skylar lied about showing a card to Kayla.\n",
      "\n",
      "Now let's think about Sophie's first question: \"if anyone had Miss Scarlet or the Rope or the Lounge.\" Since Skylar actually lied about showing a card to Kayla and Sophie didn't have any of these cards either, this implies that actually Skylar had Miss Scarlet and Rope among her wrongfully shown cards.\n",
      "\n",
      "Lastly, from point 8: \"Sophie showed Skylar the Carriage House.\" This actually means that there was no Carriage House in play for Room either.\n",
      "\n",
      "So, putting all these incorrect reveals together:\n",
      "\n",
      "1. Facedown center table cards were actually incorrect reveals:\n",
      "\t* Weapon: Lead Pipe instead of Candlestick\n",
      "\t* Room: Gazebo instead of Carriage House\n",
      "\t* Suspect: Madam Rose instead of Miss Scarlet\n",
      "\n",
      "Therefore, after correcting all incorrect reveals:\n",
      "\n",
      "1. The actually facedown center table cards were:\n",
      "\t* Weapon: Lead Pipe\n",
      "\t* Room: Gazebo\n",
      "\t* Suspect: Madam Rose<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(list(explore_result.episodes[3].completion.leaves())[0].all_message_params()[1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await trainer.tune(trainer.explore_results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9414212ada494d80b7446dafc059406c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val: 0episode [00:00, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e20e4b9ff5b4b57910ac47e1d021871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=10, verbosity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
