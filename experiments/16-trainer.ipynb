{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI, Timeout\n",
    "\n",
    "reference_client = AsyncOpenAI(\n",
    "    api_key=\"default\",\n",
    "    base_url=\"http://209.20.157.218:8000/v1\",\n",
    "    timeout=Timeout(600, connect=60),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.00798675324767828),\n",
       " TopLogprob(token='Hi', bytes=[72, 105], logprob=-4.834799289703369),\n",
       " TopLogprob(token='Hey', bytes=[72, 101, 121], logprob=-12.700722694396973),\n",
       " TopLogprob(token='Greetings', bytes=[71, 114, 101, 101, 116, 105, 110, 103, 115], logprob=-12.879494667053223),\n",
       " TopLogprob(token='Welcome', bytes=[87, 101, 108, 99, 111, 109, 101], logprob=-14.667202949523926),\n",
       " TopLogprob(token='Hallo', bytes=[72, 97, 108, 108, 111], logprob=-14.845974922180176),\n",
       " TopLogprob(token=' Hello', bytes=[32, 72, 101, 108, 108, 111], logprob=-15.203516960144043),\n",
       " TopLogprob(token='Hola', bytes=[72, 111, 108, 97], logprob=-16.633684158325195),\n",
       " TopLogprob(token='hello', bytes=[104, 101, 108, 108, 111], logprob=-18.60016441345215),\n",
       " TopLogprob(token='Bonjour', bytes=[66, 111, 110, 106, 111, 117, 114], logprob=-19.136476516723633),\n",
       " TopLogprob(token='Nice', bytes=[78, 105, 99, 101], logprob=-19.4940185546875),\n",
       " TopLogprob(token='Good', bytes=[71, 111, 111, 100], logprob=-20.030330657958984),\n",
       " TopLogprob(token='Thank', bytes=[84, 104, 97, 110, 107], logprob=-20.38787269592285),\n",
       " TopLogprob(token='How', bytes=[72, 111, 119], logprob=-20.38787269592285),\n",
       " TopLogprob(token='Sal', bytes=[83, 97, 108], logprob=-20.5666446685791),\n",
       " TopLogprob(token='HI', bytes=[72, 73], logprob=-20.92418670654297),\n",
       " TopLogprob(token='Hel', bytes=[72, 101, 108], logprob=-21.102956771850586),\n",
       " TopLogprob(token='G', bytes=[71], logprob=-21.19234275817871),\n",
       " TopLogprob(token='„Åì„Çì„Å´„Å°„ÅØ', bytes=[227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175], logprob=-21.19234275817871),\n",
       " TopLogprob(token='He', bytes=[72, 101], logprob=-21.281728744506836)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion = await reference_client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    logprobs=True,\n",
    "    top_logprobs=20,\n",
    ")\n",
    "chat_completion.choices[0].logprobs.content[0].top_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.6440e-02, 4.0069e-02, 6.9702e-01, 2.8915e-02, 6.7287e-06, 6.7746e-01,\n",
       "        2.9725e-04, 9.6658e-04, 1.3310e-04, 1.3671e+00, 2.9643e-05, 3.3668e-02,\n",
       "        5.3873e-02, 1.5532e-04, 1.2505e-01, 2.8845e-01, 9.9986e-07, 1.2106e-07,\n",
       "        1.0927e-05, 1.2103e+00, 4.1731e-01, 2.5629e-02, 1.4311e-03, 4.3575e-05,\n",
       "        1.8644e-01, 4.9029e-03, 8.1870e-01, 1.0115e-02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "logprobs = [[top_logprob.logprob for top_logprob in logprob.top_logprobs] for logprob in chat_completion.choices[0].logprobs.content]\n",
    "entropy = torch.distributions.Categorical(probs=torch.exp(torch.tensor(logprobs))).entropy()\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from /home/ubuntu/atreides/experiments/models/rl5/0002\n",
      "INFO 12-06 20:12:09 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20241206_201211-rl5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl5' target=\"_blank\">rl5</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl5' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import Trainer\n",
    "from lib.utils import return_exception\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "def get_variable_difficulty_game(\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    ") -> Clue:\n",
    "    num_players = random.randint(3, 6)\n",
    "    num_weapons = max(\n",
    "        3,\n",
    "        min(\n",
    "            num_players + random.randint(-1, 5),\n",
    "            len(Clue.weapons),\n",
    "        ),\n",
    "    )\n",
    "    num_suspects = min(\n",
    "        num_weapons + random.randint(0, num_weapons - 1), len(Clue.suspects)\n",
    "    )\n",
    "    num_rooms = min(num_suspects + random.randint(0, num_suspects - 2), len(Clue.rooms))\n",
    "    elements = {\n",
    "        \"suspect\": random.sample(Clue.suspects, k=num_suspects),\n",
    "        \"weapon\": random.sample(Clue.weapons, k=num_weapons),\n",
    "        \"room\": random.sample(Clue.rooms, k=num_rooms),\n",
    "    }\n",
    "    if random.random() < 0.1:\n",
    "        elements[\"motive\"] = random.sample(\n",
    "            Clue.motives,\n",
    "            k=max(3, min(num_weapons + random.randint(-1, 3), len(Clue.motives))),\n",
    "        )\n",
    "    if random.random() < 0.1:\n",
    "        frequency = random.choice([0.25, 0.5, 1.0])\n",
    "        start = 24.0 - frequency\n",
    "        end = 0.0\n",
    "        for _ in range(random.randint(1, num_weapons + 1)):\n",
    "            if random.randint(0, 1):\n",
    "                end += frequency\n",
    "            else:\n",
    "                start -= frequency\n",
    "\n",
    "        def format_time(time: float) -> str:\n",
    "            return f\"{int(time):02d}:{int(60 * (time - int(time))):02d}\"\n",
    "\n",
    "        elements[\"time\"] = Clue.get_times(\n",
    "            format_time(start), format_time(end), f\"{int(frequency * 60)}min\"\n",
    "        )\n",
    "    game = Clue(\n",
    "        num_players=num_players,\n",
    "        elements=elements,\n",
    "    )\n",
    "    difficulty_level = num_players + random.randint(-2, 3)\n",
    "    # print(f\"Players: {num_players}\")\n",
    "    # for element in elements:\n",
    "    #     print(f\"{element.capitalize()}: {len(elements[element])}\")\n",
    "    # print(f\"Difficulty level: {difficulty_level}\")\n",
    "    return game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=difficulty_level > 1,\n",
    "            check_one_of_constraints=difficulty_level > 2,\n",
    "            check_inverse_one_of_constraints=difficulty_level > 3,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=difficulty_level > 4,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        return_first_solver_as_winner=(\n",
    "            bool(random.randint(0, 1))\n",
    "            if return_first_solver_as_winner is None\n",
    "            else return_first_solver_as_winner\n",
    "        ),\n",
    "        print_playthrough=False,\n",
    "        max_turns=100,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_easy_game(return_first_solver_as_winner: Optional[bool] = None) -> Clue:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": random.sample(Clue.suspects, k=3),\n",
    "            \"weapon\": random.sample(Clue.weapons, k=3),\n",
    "            \"room\": random.sample(Clue.rooms, k=3),\n",
    "            # \"motive\": random.sample(Clue.motives, k=3),\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        return_first_solver_as_winner=return_first_solver_as_winner or False,\n",
    "        print_playthrough=False,\n",
    "        max_turns=100,\n",
    "    )\n",
    "    return game\n",
    "\n",
    "\n",
    "@return_exception\n",
    "def sample_random_episode(\n",
    "    difficulty: Literal[\"easy\", \"variable\"] = \"variable\",\n",
    "    max_prompt_characters: int = 8192,\n",
    "    reward_follow_up_completion: bool = True,\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    ") -> Episode:\n",
    "    while True:\n",
    "        try:\n",
    "            game = (get_easy_game if difficulty == \"easy\" else get_variable_difficulty_game)(\n",
    "                return_first_solver_as_winner=return_first_solver_as_winner\n",
    "            )\n",
    "            prompt, follow_up, solution = game.get_prompt_and_follow_up_and_solution()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if len(prompt) <= max_prompt_characters:\n",
    "            break\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        if reward_follow_up_completion:\n",
    "            completion = follow_up_completion\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    # Find first match of key followed by colon and capture following text\n",
    "                    (\n",
    "                        match := re.search(\n",
    "                            rf\"{key}: ([A-Za-z \\.:-]+)\",\n",
    "                            answer,\n",
    "                            re.IGNORECASE,\n",
    "                        )\n",
    "                    )\n",
    "                    # Check if captured group matches expected value\n",
    "                    and match.group(1).strip().lower() == value.strip().lower()\n",
    "                )\n",
    "                for key, value in solution.items()\n",
    "            ]\n",
    "        ) / len(solution)\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_episodes():\n",
    "    while True:\n",
    "        yield sample_random_episode()\n",
    "\n",
    "\n",
    "model_name = \"rl5\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    samples_per_episode=81,\n",
    "    branch_factor=3,\n",
    "    reference_clients_and_model=(\n",
    "        [reference_client],\n",
    "        \"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    ),\n",
    "    sample_probability_power=0,\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=64 * torch.cuda.device_count(),\n",
    "    patience_per_episode=5,\n",
    "    patience_per_val_sample=10,\n",
    "    sampling_kwargs={\n",
    "        \"max_tokens\": 1024,\n",
    "    },\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=(\n",
    "        sample_random_episode() for _ in range(64 * torch.cuda.device_count())\n",
    "    ),\n",
    "    val_samples_per_episode=3,\n",
    "    torchrun_kwargs=dict(nnodes=1, nproc_per_node=torch.cuda.device_count()),\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=5e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=0.0,\n",
    "            clip_epsilon=0.2,\n",
    "            unclipped_policy_coef=0.0,\n",
    "            tanh_log_policy_coef=0.8,\n",
    "            value_coef=0.0,\n",
    "            entropy_coef=0.0,\n",
    "            entropy_target=0.6,\n",
    "            entropy_target_coef=0.05,\n",
    "            kl_coef=0.05,\n",
    "            weighted_entropy_coef=0.2,\n",
    "            weighted_kl_coef=0.0,\n",
    "            weighted_ce_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "    vllm_kwargs=dict(\n",
    "        block_size=32,\n",
    "        disable_log_requests=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enforce_eager=True,\n",
    "        gpu_memory_utilization=0.95,\n",
    "        max_model_len=16384,\n",
    "        max_num_seqs=512,\n",
    "        max_num_batched_tokens=16384 * 4,\n",
    "        return_tokens_as_token_ids=True,\n",
    "        swap_space=8,\n",
    "        # scheduling_policy=\"priority\",\n",
    "        # tensor_parallel_size=torch.cuda.device_count() // 8,\n",
    "    ),\n",
    "    vllm_max_concurrent_samples=512 * torch.cuda.device_count(),\n",
    "    vllm_min_time_between_requests=0.0,\n",
    "    vllm_num=torch.cuda.device_count(),\n",
    "    vllm_timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl5/0002 --port=8001 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=8 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a10564322f4743b6e0ef64de33ebf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val: 0episode [00:00, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2b7d26382849cea78364083c05b1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=2, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_exceptions[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05518f44b164e79a8a68100990c4721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.18576218313537646, [])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await trainer.eval(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trainer.eval_episodes[\"val\"][0].completion.leaves())[0].value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"On a cool spring day Audrey, Samantha, and Noah sat down to play a casual deduction game.\\n\\nThey gathered 3 decks of cards, each for a different type of data composed of the following:\\n\\nSuspect:\\n- Madame Rose\\n- Colonel Mustard\\n- Sgt. Gray\\n\\nWeapon:\\n- Rope\\n- Knife\\n- Poison\\n\\nRoom:\\n- Kitchen\\n- Study\\n- Fountain\\n\\nAfter randomly (and blindly) choosing one card from each group and placing them in the center of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\\n\\n- Audrey: 2 cards (Poison and Study)\\n- Samantha: 2 cards\\n- Noah: 2 cards\\n\\nThe game proceeded as follows:\\n\\n1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\\n2. The player directed this question to the other players in clockwise order, starting with the player to their left.\\n3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\\n4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\\n5. This continued until either:\\na) A player showed a card to the asking player, or\\nb) All the queried players had stated they didn't have any of the asked-about cards.\\n6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\\n\\nHere is how the game played out:\\n\\nAudrey asked if anyone had Madame Rose or the Knife or the Study:\\n- Samantha showed Audrey the Knife\\n\\nSamantha asked if anyone had Sgt. Gray or the Knife or the Kitchen:\\n- Noah showed Samantha a card\\n\\nNoah asked if anyone had Sgt. Gray or the Poison or the Fountain:\\n- Audrey showed Noah the Poison\\n\\nAudrey asked if anyone had Colonel Mustard or the Rope or the Study:\\n- Samantha did not have any of the cards\\n- Noah did not have any of the cards\\n\\nSamantha asked if anyone had Colonel Mustard or the Rope or the Fountain:\\n- Noah showed Samantha a card\\n\\nNoah asked if anyone had Madame Rose or the Poison or the Kitchen:\\n- Audrey showed Noah the Poison\\n\\nAudrey asked if anyone had Madame Rose or the Rope or the Study:\\n- Samantha showed Audrey Madame Rose\\n\\nSamantha asked if anyone had Colonel Mustard or the Knife or the Study:\\n- Noah did not have any of the cards\\n- Audrey showed Samantha the Study\\n\\nNoah asked if anyone had Sgt. Gray or the Knife or the Fountain:\\n- Audrey did not have any of the cards\\n- Samantha showed Noah a card\\n\\nAudrey asked if anyone had Colonel Mustard or the Rope or the Fountain:\\n- Samantha did not have any of the cards\\n- Noah showed Audrey the Fountain\\n\\nSamantha asked if anyone had Madame Rose or the Knife or the Kitchen:\\n- Noah did not have any of the cards\\n- Audrey did not have any of the cards\\n\\nAt this point Audrey was able to correctly infer the answer.\\n\\nWhat were the facedown cards in the center of the table? And where were the other cards?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Let's analyze the game step by step:\\n\\n1. Audrey asked if anyone had Madame Rose or the Knife or the Study. Samantha showed Audrey the Knife.\\n   - So, Samantha has the Knife and at least one other card.\\n2. Samantha asked if anyone had Sgt. Gray or the Knife or the Kitchen. Noah showed Samantha a card.\\n   - Since Samantha already had the Knife, Noah must have shown her a card other than the Knife. Let's say Noah showed Samantha the Sgt. Gray.\\n   - So, Noah has Sgt. Gray and at least one other card.\\n3. Noah asked if anyone had Sgt. Gray or the Poison or the Fountain. Audrey showed Noah the Poison.\\n   - So, Audrey has the Poison and at least one other card.\\n4. Audrey asked if anyone had Colonel Mustard or the Rope or the Study. Samantha did not have any of the cards, and Noah did not have any of the cards.\\n   - This means Audrey must have the Study.\\n5. Samantha asked if anyone had Colonel Mustard or the Rope or the Fountain. Noah showed Samantha a card.\\n   - Let's say Noah showed Samantha the Rope. So, Noah has the Rope and at least one other card.\\n6. Noah asked if anyone had Madame Rose or the Poison or the Kitchen. Audrey showed Noah the Poison.\\n   - This confirms that Audrey had the Poison and showed it to Noah twice.\\n7. Audrey asked if anyone had Colonel Mustard or the Rope or the Fountain. Samantha did not have any of the cards, and Noah showed Audrey the Fountain.\\n   - So, Noah has the Fountain and at least one other card.\\n8. Samantha asked if anyone had Madame Rose or the Knife or the Kitchen. Noah did not have any of the cards, and Audrey did not have any of the cards.\\n   - This means Samantha must have Madame Rose and the Kitchen.\\n9. Audrey asked if anyone had Colonel Mustard or the Rope or the Fountain. Samantha did not have any of the cards, and Noah showed Audrey the Fountain.\\n   - This confirms that Noah had the Fountain and showed it to Audrey twice.\\n10. Samantha asked if anyone had Madame Rose or the Knife or the Kitchen. Noah did not have any of the cards, and Audrey did not have any of the cards.\\n    - This confirms that Samantha had Madame Rose and the Kitchen.\\n\\nNow, let's summarize the cards:\\n\\n- Audrey: Poison, Study\\n- Samantha: Madame Rose, Knife, Kitchen\\n- Noah: Sgt. Gray, Rope\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Fill out your answer like this:\\nMadame Rose: <#LOCATION#>\\nColonel Mustard: <#LOCATION#>\\nSgt. Gray: <#LOCATION#>\\nRope: <#LOCATION#>\\nKnife: <#LOCATION#>\\nPoison: <#LOCATION#>\\nKitchen: <#LOCATION#>\\nStudy: <#LOCATION#>\\nFountain: <#LOCATION#>\\nWhere valid locations are Audrey, Samantha, Noah, or Solution.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Here's the completed answer:\\n\\nMadame Rose: Samantha\\nColonel Mustard: Solution\\nSgt. Gray: Noah\\nRope: Noah\\nKnife: Samantha\\nPoison: Audrey\\nKitchen: Samantha\\nStudy: Audrey\\nFountain: Noah<|im_end|>\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trainer.eval_episodes[\"val\"][0].completion.leaves())[0].all_message_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "await trainer.stop_vllms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5833027ecc8c499db5c2095ff854adb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_result = await trainer.explore(verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b6e5965b9b4e7ba753ea1f3ae64e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explore_result = await trainer.explore(verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[openai.APIConnectionError('Connection error.'),\n",
       " openai.APIConnectionError('Connection error.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_result.exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.metric_logger.name = f\"{model_name}_tune\"\n",
    "trainer.tune_recipe_config.metric_logger.id = f\"{model_name}_tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 16384])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_result.tensors()['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run --nnodes=1 --nproc-per-node=1 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl3/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl3/0001\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl3/0001/hf_model_0003_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl3/0001/hf_model_0004_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl3/0001/hf_model_0001_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl3/0001/hf_model_0002_0.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl3\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl3/tensors\n",
      "  num_sequences: 74\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.5\n",
      "  entropy_target_coef: 0.1\n",
      "  kl_coef: 0.05\n",
      "  normalize_advantages: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  tanh_log_policy_coef: 1.0\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.0\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl3\n",
      "  name: rl3\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20241205_204921-rl3\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl3\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: üöÄ View run at https://wandb.ai/bradhilton/torchtune/runs/rl3\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/atreides/experiments/models/rl3/0001/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 4.64 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 16.00 GiB\n",
      "\tGPU peak memory reserved: 16.21 GiB\n",
      "\tGPU peak memory active: 16.00 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/74 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|74|Loss: 0.0139: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [14:20<00:00, 11.50s/it, entropy=0.4362, entropy_target=0.0638, kl_div=0.0925, policy=0.0071, tanh_log_policy_to_log=0.0029, unclipped_policy=-0.0051, value=2.3676, weighted_ce=0.0112, weighted_entropy=-0.0180, weighted_kl_div=-0.0010]   INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 8.51 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl3/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl3/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl3/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl3/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 14.03 secs\n",
      "1|74|Loss: 0.0139: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [14:43<00:00, 11.93s/it, entropy=0.4362, entropy_target=0.0638, kl_div=0.0925, policy=0.0071, tanh_log_policy_to_log=0.0029, unclipped_policy=-0.0051, value=2.3676, weighted_ce=0.0112, weighted_entropy=-0.0180, weighted_kl_div=-0.0010]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                   entropy ‚ñá‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñá‚ñà‚ñá‚ñÑ\n",
      "wandb:            entropy_target ‚ñá‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÅ\n",
      "wandb:               global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:                    kl_div ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá\n",
      "wandb:                      loss ‚ñá‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñÜ\n",
      "wandb:                        lr ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "wandb:        peak_memory_active ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:         peak_memory_alloc ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:      peak_memory_reserved ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "wandb:                    policy ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñà‚ñÜ‚ñÇ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÉ\n",
      "wandb:           tanh_log_policy ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ\n",
      "wandb: tokens_per_second_per_gpu ‚ñá‚ñÖ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ\n",
      "wandb:          unclipped_policy ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà\n",
      "wandb:                     value ‚ñÜ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÖ\n",
      "wandb:               weighted_ce ‚ñÖ‚ñÉ‚ñà‚ñá‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÅ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ\n",
      "wandb:          weighted_entropy ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñà‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñà‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ\n",
      "wandb:           weighted_kl_div ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                   entropy 0.43621\n",
      "wandb:            entropy_target 0.06379\n",
      "wandb:               global_step 74\n",
      "wandb:                    kl_div 0.09255\n",
      "wandb:                      loss 0.01389\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 69.32\n",
      "wandb:         peak_memory_alloc 69.32\n",
      "wandb:      peak_memory_reserved 78.02539\n",
      "wandb:                    policy 0.00705\n",
      "wandb:           tanh_log_policy 0.00289\n",
      "wandb: tokens_per_second_per_gpu 1171.56946\n",
      "wandb:          unclipped_policy -0.0051\n",
      "wandb:                     value 2.36761\n",
      "wandb:               weighted_ce 0.01121\n",
      "wandb:          weighted_entropy -0.01803\n",
      "wandb:           weighted_kl_div -0.00098\n",
      "wandb: \n",
      "wandb: üöÄ View run rl3 at: https://wandb.ai/bradhilton/torchtune/runs/rl3\n",
      "wandb: ‚≠êÔ∏è View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20241205_204921-rl3/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 2 model files to /home/ubuntu/atreides/experiments/models/rl3/0002\n"
     ]
    }
   ],
   "source": [
    "await trainer.tune(explore_result, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.loss.entropy_target_coef = 0.1\n",
    "trainer.tune_recipe_config.loss.kl_coef = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03e22d5174443a1b512f7cf37e4ca98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63a25570d5a418a88efae7862daee45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function vLLM.__del__ at 0x7b46653f8720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run --nnodes=1 --nproc-per-node=1 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl3/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|22|Loss: 0.0106: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [04:21<00:00, 11.49s/it, entropy=0.5910, entropy_target=0.0910, kl_div=0.0428, policy=0.0069, tanh_log_policy_to_log=-0.0006, unclipped_policy=-0.0006, value=1.3990, weighted_ce=-0.0050, weighted_entropy=0.0047, weighted_kl_div=0.0011]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 15 model files to /home/ubuntu/atreides/experiments/models/rl3/0015\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl3/0015 --port=8006 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=8 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f537bf35c64f406da0c59797986b4770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9247cbdbdd3d4cf79f5e95bb7878cbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run --nnodes=1 --nproc-per-node=1 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl3/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|20|Loss: 0.0076: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:57<00:00, 11.48s/it, entropy=0.5419, entropy_target=0.0419, kl_div=0.0327, policy=0.1791, tanh_log_policy_to_log=0.0018, unclipped_policy=0.1565, value=2.0940, weighted_ce=-0.0292, weighted_entropy=-0.0079, weighted_kl_div=-0.0013]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 16 model files to /home/ubuntu/atreides/experiments/models/rl3/0016\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl3/0016 --port=8006 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=8 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eae5dadf2e74df1b9e54e87f1bfd0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4ea8f248164a90ae9c2245f54f426d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run --nnodes=1 --nproc-per-node=1 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl3/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|25|Loss: 0.0030: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [05:08<00:00, 11.55s/it, entropy=0.5285, entropy_target=0.0285, kl_div=0.0293, policy=0.0086, tanh_log_policy_to_log=-0.0013, unclipped_policy=-0.0057, value=2.1253, weighted_ce=-0.0212, weighted_entropy=0.0078, weighted_kl_div=-0.0007] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 17 model files to /home/ubuntu/atreides/experiments/models/rl3/0017\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl3/0017 --port=8006 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=8 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba6e777d45d4fe78f0e5ba437c39d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cec2d459f2d4b0cb48b0d9b647794bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function vLLM.__del__ at 0x7b46653f8720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/experiments/lib/vllm.py\", line 101, in __del__\n",
      "    self.process.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/subprocess.py\", line 143, in terminate\n",
      "    self._transport.terminate()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 149, in terminate\n",
      "    self._check_proc()\n",
      "  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/base_subprocess.py\", line 142, in _check_proc\n",
      "    raise ProcessLookupError()\n",
      "ProcessLookupError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (0 remaining samples x 10 patience per sample = 0 seconds)\n",
      "$ tune run --nnodes=1 --nproc-per-node=1 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl3/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|19|Loss: 0.0085: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [04:08<00:00, 11.54s/it, entropy=0.4671, entropy_target=0.0329, kl_div=0.0323, policy=0.0063, tanh_log_policy_to_log=0.0036, unclipped_policy=-0.0249, value=1.4129, weighted_ce=0.0180, weighted_entropy=-0.0283, weighted_kl_div=-0.0014]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 18 model files to /home/ubuntu/atreides/experiments/models/rl3/0018\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl3/0018 --port=8006 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=8 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1fb458095d4d839d67be25645305fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=4, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n",
      "User:\n",
      "On a warm autumn morning Conner, Arianna, Kaleb, and Daisy sat down to play a casual deduction game.\n",
      "\n",
      "They assembled 3 stacks of cards, each for a different type of data composed of the following:\n",
      "\n",
      "Suspect:\n",
      "- Madame Rose\n",
      "- Mr. Green\n",
      "- Colonel Mustard\n",
      "- Sgt. Gray\n",
      "- Monsieur Brunette\n",
      "- Professor Plum\n",
      "\n",
      "Weapon:\n",
      "- Knife\n",
      "- Revolver\n",
      "- Candlestick\n",
      "- Lead Pipe\n",
      "- Poison\n",
      "\n",
      "Room:\n",
      "- Billiard Room\n",
      "- Cloak Room\n",
      "- Courtyard\n",
      "- Hall\n",
      "- Dining Room\n",
      "- Studio\n",
      "- Library\n",
      "- Fountain\n",
      "- Kitchen\n",
      "\n",
      "After randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\n",
      "\n",
      "- Conner: 4 cards (Billiard Room, Revolver, Studio, and Candlestick)\n",
      "- Arianna: 4 cards\n",
      "- Kaleb: 4 cards\n",
      "- Daisy: 5 cards\n",
      "\n",
      "The game proceeded as follows:\n",
      "\n",
      "1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\n",
      "2. The player directed this question to the other players in clockwise order, starting with the player to their left.\n",
      "3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\n",
      "4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\n",
      "5. This continued until either:\n",
      "a) A player showed a card to the asking player, or\n",
      "b) All the queried players had stated they didn't have any of the asked-about cards.\n",
      "6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\n",
      "\n",
      "Here is how the game played out:\n",
      "\n",
      "Conner asked if anyone had Monsieur Brunette or the Knife or the Billiard Room:\n",
      "- Arianna showed Conner the Knife\n",
      "\n",
      "Arianna asked if anyone had Monsieur Brunette or the Poison or the Courtyard:\n",
      "- Kaleb did not have any of the cards\n",
      "- Daisy showed Arianna a card\n",
      "\n",
      "Kaleb asked if anyone had Sgt. Gray or the Lead Pipe or the Courtyard:\n",
      "- Daisy showed Kaleb a card\n",
      "\n",
      "Daisy asked if anyone had Monsieur Brunette or the Revolver or the Courtyard:\n",
      "- Conner showed Daisy the Revolver\n",
      "\n",
      "Conner asked if anyone had Sgt. Gray or the Revolver or the Kitchen:\n",
      "- Arianna showed Conner the Kitchen\n",
      "\n",
      "Arianna asked if anyone had Sgt. Gray or the Knife or the Fountain:\n",
      "- Kaleb did not have any of the cards\n",
      "- Daisy showed Arianna a card\n",
      "\n",
      "Kaleb asked if anyone had Mr. Green or the Knife or the Courtyard:\n",
      "- Daisy showed Kaleb a card\n",
      "\n",
      "Daisy asked if anyone had Madame Rose or the Poison or the Studio:\n",
      "- Conner showed Daisy the Studio\n",
      "\n",
      "Conner asked if anyone had Monsieur Brunette or the Lead Pipe or the Cloak Room:\n",
      "- Arianna showed Conner the Cloak Room\n",
      "\n",
      "Arianna asked if anyone had Mr. Green or the Knife or the Kitchen:\n",
      "- Kaleb showed Arianna a card\n",
      "\n",
      "Kaleb asked if anyone had Monsieur Brunette or the Candlestick or the Billiard Room:\n",
      "- Daisy did not have any of the cards\n",
      "- Conner showed Kaleb the Billiard Room\n",
      "\n",
      "Daisy asked if anyone had Sgt. Gray or the Lead Pipe or the Billiard Room:\n",
      "- Conner showed Daisy the Billiard Room\n",
      "\n",
      "Conner asked if anyone had Colonel Mustard or the Poison or the Dining Room:\n",
      "- Arianna showed Conner the Dining Room\n",
      "\n",
      "Arianna asked if anyone had Mr. Green or the Knife or the Hall:\n",
      "- Kaleb showed Arianna a card\n",
      "\n",
      "Kaleb asked if anyone had Monsieur Brunette or the Poison or the Dining Room:\n",
      "- Daisy showed Kaleb a card\n",
      "\n",
      "Daisy asked if anyone had Professor Plum or the Poison or the Courtyard:\n",
      "- Conner did not have any of the cards\n",
      "- Arianna did not have any of the cards\n",
      "- Kaleb showed Daisy a card\n",
      "\n",
      "Conner asked if anyone had Monsieur Brunette or the Lead Pipe or the Hall:\n",
      "- Arianna did not have any of the cards\n",
      "- Kaleb did not have any of the cards\n",
      "- Daisy showed Conner the Hall\n",
      "\n",
      "Arianna asked if anyone had Professor Plum or the Knife or the Kitchen:\n",
      "- Kaleb showed Arianna a card\n",
      "\n",
      "Kaleb asked if anyone had Monsieur Brunette or the Revolver or the Fountain:\n",
      "- Daisy showed Kaleb a card\n",
      "\n",
      "At this point Conner was able to correctly deduce the answer.\n",
      "\n",
      "What were the facedown cards in the center of the table? And where were the other cards?\n",
      "\n",
      "Assistant:\n",
      "Based on the information given, the facedown cards in the center of the table are:\n",
      "\n",
      "Suspect: Monsieur Brunette\n",
      "Weapon: Poison\n",
      "Room: Fountain\n",
      "\n",
      "And the other cards are distributed among the players as follows:\n",
      "\n",
      "Conner: Billiard Room, Revolver, Studio, Candlestick\n",
      "Arianna: 3 unknown cards, 1 known card (Cloak Room from the Room category)\n",
      "Kaleb: Lead Pipe, Courtyard, Fountain, Candlestick\n",
      "Daisy: 5 unknown cards, 1 known card (Hall from the Room category)<|im_end|>\n",
      "\n",
      "User:\n",
      "Fill out your answer like this:\n",
      "Madame Rose: <#LOCATION#>\n",
      "Mr. Green: <#LOCATION#>\n",
      "Colonel Mustard: <#LOCATION#>\n",
      "Sgt. Gray: <#LOCATION#>\n",
      "Monsieur Brunette: <#LOCATION#>\n",
      "Professor Plum: <#LOCATION#>\n",
      "Knife: <#LOCATION#>\n",
      "Revolver: <#LOCATION#>\n",
      "Candlestick: <#LOCATION#>\n",
      "Lead Pipe: <#LOCATION#>\n",
      "Poison: <#LOCATION#>\n",
      "Billiard Room: <#LOCATION#>\n",
      "Cloak Room: <#LOCATION#>\n",
      "Courtyard: <#LOCATION#>\n",
      "Hall: <#LOCATION#>\n",
      "Dining Room: <#LOCATION#>\n",
      "Studio: <#LOCATION#>\n",
      "Library: <#LOCATION#>\n",
      "Fountain: <#LOCATION#>\n",
      "Kitchen: <#LOCATION#>\n",
      "Where valid locations are Conner, Arianna, Kaleb, Daisy, or Solution.\n",
      "\n",
      "Assistant:\n",
      "Madame Rose: Solution\n",
      "Mr. Green: Solution\n",
      "Colonel Mustard: Solution\n",
      "Sgt. Gray: Solution\n",
      "Monsieur Brunette: Fountain\n",
      "Professor Plum: Solution\n",
      "Knife: Arianna\n",
      "Revolver: Conner\n",
      "Candlestick: Conner\n",
      "Lead Pipe: Kaleb\n",
      "Poison: Solution\n",
      "Billiard Room: Conner\n",
      "Cloak Room: Arianna\n",
      "Courtyard: Daisy\n",
      "Hall: Daisy\n",
      "Dining Room: Arianna\n",
      "Studio: Conner\n",
      "Library: Solution\n",
      "Fountain: Kaleb\n",
      "Kitchen: Arianna\n",
      "Where valid locations are Suspect, Weapon, Room, Conner, Arianna, Kaleb, Daisy, or Solution.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terminus = list(trainer.eval_episodes[\"val\"][10].completion.leaves(model='/home/ubuntu/atreides/experiments/models/rl3/0018'))[2]\n",
    "print(terminus.value())\n",
    "for message in terminus.all_message_params():\n",
    "    print(message[\"role\"].capitalize() + \":\")\n",
    "    print(message[\"content\"], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c315c1c3492c470a89a1c653a132e0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/128 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (36.11519999998988 remaining samples x 0.037037037037037035 patience per sample = 1.3375999999996253 seconds)\n"
     ]
    }
   ],
   "source": [
    "explore_result = await trainer.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run --nnodes=1 --nproc-per-node=1 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl2/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl2\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl2/tensors\n",
      "  num_sequences: 90\n",
      "  sequence_length: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.02\n",
      "  kl_coef: 0.0\n",
      "  normalize_advantages: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 1.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.0\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 5.0e-06\n",
      "optimizer_in_bwd: false\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20241203_224601-pgehveiv\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run good-voice-28\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: üöÄ View run at https://wandb.ai/bradhilton/torchtune/runs/pgehveiv\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 3.55 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.89 GiB\n",
      "\tGPU peak memory reserved: 15.99 GiB\n",
      "\tGPU peak memory active: 15.89 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/45 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|8|Loss: 0.0100:  18%|‚ñà‚ñä        | 8/45 [01:07<04:50,  7.84s/it, entropy=0.2795, kl_div=0.0446, policy=0.0156, value=2.0344, weighted_ce=-0.0114, weighted_entropy=-0.0001, weighted_kl_div=0.0091]"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtune(explore_result)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:598\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, result, verbosity)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m process\u001b[38;5;241m.\u001b[39mstderr:\n\u001b[1;32m    595\u001b[0m         tasks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    596\u001b[0m             asyncio\u001b[38;5;241m.\u001b[39mcreate_task(log_output(process\u001b[38;5;241m.\u001b[39mstderr, sys\u001b[38;5;241m.\u001b[39mstderr))\n\u001b[1;32m    597\u001b[0m         )\n\u001b[0;32m--> 598\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     cleanup_before_training()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:385\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:316\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    314\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:569\u001b[0m, in \u001b[0;36mTrainer.tune.<locals>.log_output\u001b[0;34m(stream, io)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[1;32m    571\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/streams.py:713\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(blocks)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# This will work right even if buffer is less than n bytes\u001b[39;00m\n\u001b[1;32m    716\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(\u001b[38;5;28mmemoryview\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)[:n])\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/streams.py:545\u001b[0m, in \u001b[0;36mStreamReader._wait_for_data\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_future()\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py:291\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:385\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py:198\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m _CANCELLED:\n\u001b[1;32m    197\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidStateError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult is not ready.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "await trainer.tune(explore_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "completion_sampler = await trainer.get_completion_sampler()\n",
    "client: AsyncOpenAI = completion_sampler.samplers[0].client  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm.client.chat.completions.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "episode = explore_result.episodes[0]\n",
    "completion = next(iter(episode.completion.leaves()))\n",
    "tokens = completion.all_tokens(trainer.tokenizer, cache=True).tolist()\n",
    "plain_completion = await vllm.client.completions.create(\n",
    "    model=trainer.model,\n",
    "    prompt=tokens,\n",
    "    max_tokens=1,\n",
    "    extra_body={\n",
    "        \"prompt_logprobs\": True,\n",
    "    },\n",
    ")\n",
    "prompt_logprobs: list[dict[str, dict[str, Any]]] = plain_completion.choices[0].prompt_logprobs  # type: ignore\n",
    "prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(prompt_logprobs) == len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_logprobs = [\n",
    "    prompt_logprob[str(token)][\"logprob\"] if prompt_logprob else torch.nan\n",
    "    for token, prompt_logprob in zip(tokens, prompt_logprobs)\n",
    "]\n",
    "for c in completion.ancestors(including_self=True, reverse=True):\n",
    "    count = c.token_count(trainer.tokenizer, cache=True)\n",
    "    c.reference_logprobs, reference_logprobs = (\n",
    "        torch.tensor(reference_logprobs[:count]),\n",
    "        reference_logprobs[count:],\n",
    "    )\n",
    "\n",
    "completion.reference_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(\n",
    "    [\n",
    "        prompt_logprob[str(token)][\"logprob\"] if prompt_logprob else torch.nan\n",
    "        for token, prompt_logprob in zip(tokens, prompt_logprobs)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt_logprobs), len(completion.all_tokens(trainer.tokenizer, cache=True).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/tmp/err_execute_model_input_20241201-005319.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(plain_completion, \"prompt_logprobs\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_completion.choices[0].prompt_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.all_tokens(trainer.tokenizer, cache=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.patience_per_val_sample = 1.0\n",
    "trainer.patience_per_test_sample = 1.0\n",
    "trainer.tune_recipe_config.optimizer.lr = 8e-6\n",
    "trainer.tune_recipe_config.loss.clip_epsilon = 0.1\n",
    "trainer.tune_recipe_config.loss.weighted_ce_coef = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await trainer.train(iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.pack import packed_tensors_from_dir\n",
    "\n",
    "tensors = packed_tensors_from_dir(\n",
    "    dir=\"./models/rl/tensors\", num_sequences=50, sequence_length=16384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors[\"mask\"][0][2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.max_mask_sequence_batch_size = 16\n",
    "# (eval_score, eval_exceptions),\n",
    "(result,) = await asyncio.gather(\n",
    "    # trainer.eval(\"val\", 0, return_exceptions=True),\n",
    "    trainer.explore(1, return_exceptions=True),\n",
    ")\n",
    "# print(f\"Eval score: {eval_score:.2%}\")\n",
    "print(\n",
    "    f\"Generated {sum(completion.num_token_logprobs() for episode in result.episodes for completion in episode.completion.descendants()):,} tokens\"\n",
    ")\n",
    "tensors = trainer.tensors(result.episodes)\n",
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = trainer.tensors(result.episodes)\n",
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tensors[\"advantages\"].shape).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "i = 1\n",
    "_tensors = tensors\n",
    "key = \"input_pos\"\n",
    "\n",
    "show(\n",
    "    _tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        _tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(_tensors[key][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(_tensors[key][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tensors()[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensors[\"mask\"] == result.tensors()[\"mask\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(127):\n",
    "    for j in range(127):\n",
    "        if (tensors[\"mask\"][i] == result.tensors()[\"mask\"][j]).all():\n",
    "            print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"advantages\"\n",
    "torch.isclose(\n",
    "    tensors[key], result.tensors()[key], rtol=1e-5, atol=1e-8, equal_nan=True\n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all((tensors[\"weights\"] == result.tensors()[\"weights\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise result.exceptions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_033_717 / 4.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_064_056 / 6.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_064_056 / 10.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_071_601 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2_071_601 / 11.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4_119_041 / 16.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await trainer.train(iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_score, episodes = await asyncio.gather(trainer.eval(\"val\", 0), trainer.explore(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from torchtune.training import cleanup_before_training\n",
    "from torchtune.training.metric_logging import DiskLogger\n",
    "from typing import Any\n",
    "\n",
    "from lib.recipes.rl import ComponentConfig, RLConfig, RLRecipe\n",
    "from lib.rl.pack import PackedDataset, packed_tensors_to_dir\n",
    "from lib.rl.ppo import PPOLoss\n",
    "\n",
    "\n",
    "tensors, checkpoint_dir, checkpoint_files = await trainer.tune_resources(episodes)\n",
    "\n",
    "PLACEHOLDER: Any = None\n",
    "\n",
    "config = RLConfig(\n",
    "    # Dataset\n",
    "    dataset=ComponentConfig(\n",
    "        PackedDataset, **packed_tensors_to_dir(tensors, trainer.output_dir + \"/tensors\")\n",
    "    ),\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    # Model\n",
    "    model=ComponentConfig(llama3_1_8b),\n",
    "    num_output_chunks=4,\n",
    "    # Checkpointer\n",
    "    checkpointer=ComponentConfig(\n",
    "        \"torchtune.training.FullModelHFCheckpointer\",\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        checkpoint_files=checkpoint_files,\n",
    "        recipe_checkpoint=None,\n",
    "        output_dir=trainer.output_dir,\n",
    "        model_type=\"LLAMA3\",\n",
    "    ),\n",
    "    resume_from_checkpoint=False,\n",
    "    # Fine-tuning arguments\n",
    "    batch_size=4,\n",
    "    epochs=1,\n",
    "    optimizer=ComponentConfig(\n",
    "        \"torch.optim.AdamW\",\n",
    "        # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "        # \"bitsandbytes.optim.AdamW\",\n",
    "        # params=PLACEHOLDER,\n",
    "        lr=5e-6,\n",
    "        fused=True,\n",
    "    ),\n",
    "    loss=ComponentConfig(\n",
    "        PPOLoss,\n",
    "        # clip_epsilon=0.3,\n",
    "        # entropy_coef=0.0,\n",
    "        # kl_coef=0.0,\n",
    "        clip_epsilon=0.3,\n",
    "        entropy_coef=0.025,\n",
    "        kl_coef=0.025,\n",
    "        normalize_advantages=False,\n",
    "    ),\n",
    "    max_steps_per_epoch=None,\n",
    "    compile=False,\n",
    "    optimizer_in_bwd=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    # Training env\n",
    "    device=\"cuda\",\n",
    "    # Memory management\n",
    "    enable_activation_checkpointing=True,\n",
    "    enable_activation_offloading=False,\n",
    "    custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "    # Reduced precision\n",
    "    dtype=\"bf16\",\n",
    "    # Logging\n",
    "    metric_logger=ComponentConfig(\n",
    "        DiskLogger, log_dir=\"/home/ubuntu/atreides/experiments/logs\"\n",
    "    ),\n",
    "    log_every_n_steps=1,\n",
    "    log_peak_memory_stats=True,\n",
    ")\n",
    "\n",
    "# recipe = RLRecipe(config)\n",
    "# recipe.setup(config)\n",
    "# recipe.train()\n",
    "# recipe.cleanup()\n",
    "# del tensors, recipe\n",
    "# cleanup_before_training()\n",
    "# trainer.save(base_checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "dict_config = config.dict_config()\n",
    "OmegaConf.save(dict_config, trainer.output_dir + \"/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import IO\n",
    "\n",
    "torchrun_kwargs = {\"nnodes\": 1, \"nproc_per_node\": 2}\n",
    "kwargs = {}\n",
    "env = {\"CUDA_LAUNCH_BLOCKING\": \"1\"}\n",
    "\n",
    "args = [\n",
    "    \"tune\",\n",
    "    \"run\",\n",
    "    *[\n",
    "        f\"--{key.replace('_', '-')}{f'={value}' if value is not True else ''}\"\n",
    "        for key, value in torchrun_kwargs.items()\n",
    "    ],\n",
    "    \"lib.recipes.rl.RLRecipe\",\n",
    "    \"--config\",\n",
    "    trainer.output_dir + \"/config.yaml\",\n",
    "    *[\n",
    "        f\"--{key.replace('_', '-')}{f'={value}' if value != True else ''}\"\n",
    "        for key, value in kwargs.items()\n",
    "    ],\n",
    "]\n",
    "print(f\"$ {' '.join(args)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = await asyncio.create_subprocess_exec(\n",
    "    *args,\n",
    "    stdout=asyncio.subprocess.PIPE,\n",
    "    stderr=asyncio.subprocess.PIPE,\n",
    "    env={\n",
    "        **os.environ,\n",
    "        **(env or {}),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "async def log_output(stream: asyncio.StreamReader, io: IO[str]) -> None:\n",
    "    while True:\n",
    "        line = await stream.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        decoded_line = line.decode()\n",
    "        io.write(decoded_line)\n",
    "        io.flush()\n",
    "\n",
    "\n",
    "tasks = []\n",
    "if process.stdout:\n",
    "    tasks.append(asyncio.create_task(log_output(process.stdout, sys.stdout)))\n",
    "if process.stderr:\n",
    "    tasks.append(asyncio.create_task(log_output(process.stderr, sys.stderr)))\n",
    "_ = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.recipes.rl import recipe_main\n",
    "import os\n",
    "from torch import distributed as dist\n",
    "from torchtune.training import is_distributed\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "\n",
    "\n",
    "recipe_main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "dict_config = config.dict_config()\n",
    "OmegaConf.save(dict_config, trainer.output_dir + \"/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl.completion import Completion\n",
    "\n",
    "\n",
    "OmegaConf.create(\n",
    "    OmegaConf.to_yaml(\n",
    "        DictConfig(dict(name=f\"{Completion.__module__}.{Completion.__name__}\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import sys\n",
    "\n",
    "traceback.clear_frames(sys.exc_info()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_before_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(base_checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show(mask: torch.Tensor) -> None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(mask, cmap=\"inferno\")\n",
    "    plt.colorbar(label=\"Relative Position\")\n",
    "    plt.title(\"Relative Position Attention Mask\")\n",
    "    plt.xlabel(\"Target Position\")\n",
    "    plt.ylabel(\"Source Position\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "i = 1\n",
    "\n",
    "show(\n",
    "    tensors[\"mask\"][i].cumsum(dim=1)\n",
    "    * (\n",
    "        tensors[\"mask\"][i]\n",
    "        & (\n",
    "            ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(0)\n",
    "            & ~torch.isnan(tensors[\"advantages\"][i]).unsqueeze(1)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    f'<div style=\"white-space: pre-wrap\">{list(episodes[2].completion.leaves())[0].html(30.0)}</div>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_pos_ids(\n",
    "    ids: torch.Tensor, parent_ids: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Creates an attention mask and position IDs for hierarchical attention based on node IDs and their parent IDs.\n",
    "\n",
    "    Args:\n",
    "        ids: A tensor of shape (batch_size, sequence_length) containing node IDs\n",
    "        parent_ids: A tensor of shape (batch_size, sequence_length) containing parent IDs for each node\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - mask: A boolean tensor of shape (batch_size, sequence_length, sequence_length) where True indicates\n",
    "          allowed attention connections. Each position can attend to itself and any of its ancestors\n",
    "          in the hierarchy, but only for previous positions (due to causal masking).\n",
    "        - pos_ids: A tensor of shape (batch_size, sequence_length, sequence_length) containing relative\n",
    "          position IDs for each allowed attention connection, with -1 for masked positions.\n",
    "    \"\"\"\n",
    "    mask = ids.unsqueeze(1) == ids.unsqueeze(2)\n",
    "    _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    while torch.any(mask != _mask):\n",
    "        parent_ids = parent_ids.gather(\n",
    "            1, torch.argmax((parent_ids.unsqueeze(2) == ids.unsqueeze(1)).int(), dim=2)\n",
    "        )\n",
    "        mask = _mask\n",
    "        _mask = mask | (ids.unsqueeze(1) == parent_ids.unsqueeze(2))\n",
    "    mask &= torch.tril(torch.ones_like(mask, dtype=torch.bool, device=ids.device))\n",
    "    # mask = torch.linalg.matrix_power(mask.float(), mask.size(1) - 1) > 0\n",
    "    pos_ids = (torch.where(mask, mask.cumsum(2), 0) - 1).max(1).values\n",
    "    return mask, pos_ids\n",
    "\n",
    "\n",
    "def test_mask_and_pos_ids(\n",
    "    ids: list[int],\n",
    "    parent_ids: list[int],\n",
    "    expected_mask: list[list[int]],\n",
    "    expected_pos_ids: list[int],\n",
    "):\n",
    "    mask, pos_ids = mask_and_pos_ids(\n",
    "        ids=torch.tensor([ids]), parent_ids=torch.tensor([parent_ids])\n",
    "    )\n",
    "    assert torch.all(mask.int() == torch.tensor([expected_mask])), f\"\\n{mask.int()[0]}\"\n",
    "    assert torch.all(\n",
    "        pos_ids == torch.tensor([expected_pos_ids])\n",
    "    ), f\"{pos_ids[0].tolist()}\"\n",
    "\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1],\n",
    "    parent_ids=[0, 1],\n",
    "    expected_mask=[[1, 0], [0, 1]],\n",
    "    expected_pos_ids=[0, 0],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 1],\n",
    "    parent_ids=[0, 0, 0],\n",
    "    expected_mask=[[1, 0, 0], [1, 1, 0], [1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 0, 1, 2],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 2, 3],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 0, 1, 1],\n",
    "    parent_ids=[0, 0, 1, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]],\n",
    "    expected_pos_ids=[0, 1, 0, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3],\n",
    "    parent_ids=[0, 1, 0, 1],\n",
    "    expected_mask=[[1, 0, 0, 0], [0, 1, 0, 0], [1, 0, 1, 0], [0, 1, 0, 1]],\n",
    "    expected_pos_ids=[0, 0, 1, 1],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 2, 3, 3],\n",
    "    parent_ids=[0, 1, 0, 0, 1, 1],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 0, 1, 2, 1, 2],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[0, 1, 2, 3, 4, 4, 5, 5],\n",
    "    parent_ids=[0, 0, 1, 1, 2, 2, 3, 3],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 1, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 2, 2, 3, 4, 3, 4],\n",
    ")\n",
    "\n",
    "test_mask_and_pos_ids(\n",
    "    ids=[2, 1, 0],\n",
    "    parent_ids=[2, 2, 0],\n",
    "    expected_mask=[\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 0, 1],\n",
    "    ],\n",
    "    expected_pos_ids=[0, 1, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
