{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import AsyncOpenAI, Timeout\n",
    "\n",
    "# reference_client = AsyncOpenAI(\n",
    "#     api_key=\"default\",\n",
    "#     base_url=\"http://209.20.157.218:8000/v1\",\n",
    "#     timeout=Timeout(600, connect=60),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await reference_client.chat.completions.create(\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "#     model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from /home/ubuntu/atreides/experiments/models/rl17/0007\n",
      "INFO 12-11 19:52:33 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20241211_195235-rl17</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl17' target=\"_blank\">rl17</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl17' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl17</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from lib.clue import Clue, DeductiveSolver\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import ExploreOptions, Trainer, vLLMConfig\n",
    "from lib.utils import return_exception\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import Literal, Optional\n",
    "\n",
    "with open(\"./data/chain-of-thought-examples.json\") as f:\n",
    "    chain_of_thought_examples: list[dict[str, str]] = json.load(f)\n",
    "\n",
    "\n",
    "def get_variable_difficulty_game(\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    ") -> Clue:\n",
    "    num_players = random.randint(3, 6)\n",
    "    num_weapons = max(\n",
    "        3,\n",
    "        min(\n",
    "            num_players + random.randint(-1, 5),\n",
    "            len(Clue.weapons),\n",
    "        ),\n",
    "    )\n",
    "    num_suspects = min(\n",
    "        num_weapons + random.randint(0, num_weapons - 1), len(Clue.suspects)\n",
    "    )\n",
    "    num_rooms = min(num_suspects + random.randint(0, num_suspects - 2), len(Clue.rooms))\n",
    "    elements = {\n",
    "        \"suspect\": random.sample(Clue.suspects, k=num_suspects),\n",
    "        \"weapon\": random.sample(Clue.weapons, k=num_weapons),\n",
    "        \"room\": random.sample(Clue.rooms, k=num_rooms),\n",
    "    }\n",
    "    if random.random() < 0.1:\n",
    "        elements[\"motive\"] = random.sample(\n",
    "            Clue.motives,\n",
    "            k=max(3, min(num_weapons + random.randint(-1, 3), len(Clue.motives))),\n",
    "        )\n",
    "    if random.random() < 0.1:\n",
    "        frequency = random.choice([0.25, 0.5, 1.0])\n",
    "        start = 24.0 - frequency\n",
    "        end = 0.0\n",
    "        for _ in range(random.randint(1, num_weapons + 1)):\n",
    "            if random.randint(0, 1):\n",
    "                end += frequency\n",
    "            else:\n",
    "                start -= frequency\n",
    "\n",
    "        def format_time(time: float) -> str:\n",
    "            return f\"{int(time):02d}:{int(60 * (time - int(time))):02d}\"\n",
    "\n",
    "        elements[\"time\"] = Clue.get_times(\n",
    "            format_time(start), format_time(end), f\"{int(frequency * 60)}min\"\n",
    "        )\n",
    "    game = Clue(\n",
    "        num_players=num_players,\n",
    "        elements=elements,\n",
    "    )\n",
    "    difficulty_level = num_players + random.randint(-2, 3)\n",
    "    # print(f\"Players: {num_players}\")\n",
    "    # for element in elements:\n",
    "    #     print(f\"{element.capitalize()}: {len(elements[element])}\")\n",
    "    # print(f\"Difficulty level: {difficulty_level}\")\n",
    "    return game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=difficulty_level > 1,\n",
    "            check_one_of_constraints=difficulty_level > 2,\n",
    "            check_inverse_one_of_constraints=difficulty_level > 3,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=difficulty_level > 4,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        return_first_solver_as_winner=(\n",
    "            bool(random.randint(0, 1))\n",
    "            if return_first_solver_as_winner is None\n",
    "            else return_first_solver_as_winner\n",
    "        ),\n",
    "        print_playthrough=False,\n",
    "        max_turns=100,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_easy_game(return_first_solver_as_winner: Optional[bool] = None) -> Clue:\n",
    "    game = Clue(\n",
    "        num_players=3,\n",
    "        elements={\n",
    "            \"suspect\": random.sample(Clue.suspects, k=3),\n",
    "            \"weapon\": random.sample(Clue.weapons, k=3),\n",
    "            \"room\": random.sample(Clue.rooms, k=3),\n",
    "            # \"motive\": random.sample(Clue.motives, k=3),\n",
    "            # \"time\": Clue.get_times(\"21:00\", \"03:00\", \"1h\"),\n",
    "        },\n",
    "    )\n",
    "    game.play(\n",
    "        deductive_solver=DeductiveSolver(\n",
    "            # note_cards_in_hand=False,\n",
    "            # note_responses_to_suggestions=False,\n",
    "            # note_cards_that_players_do_not_have=False,\n",
    "            # check_unique_card_placement_constraints=False,\n",
    "            # check_player_hand_size_constraints=False,\n",
    "            check_solution_has_one_and_only_one_card_per_element=False,\n",
    "            check_one_of_constraints=False,\n",
    "            check_inverse_one_of_constraints=False,\n",
    "            merge_and_check_disjoint_inverse_one_of_constraints=False,\n",
    "            exhaustively_test_possible_assignments=False,\n",
    "        ),\n",
    "        cp_solver_max_solve_time_per_turn=0.01,\n",
    "        check_cp_solver_grid=False,\n",
    "        check_if_deductive_solver_and_cp_solver_grids_match=False,\n",
    "        return_first_solver_as_winner=return_first_solver_as_winner or False,\n",
    "        print_playthrough=False,\n",
    "        max_turns=100,\n",
    "    )\n",
    "    return game\n",
    "\n",
    "\n",
    "@return_exception\n",
    "def sample_random_episode(\n",
    "    difficulty: Literal[\"easy\", \"variable\"] = \"variable\",\n",
    "    example_probability: float = 0.0,\n",
    "    max_prompt_characters: int = 8192,\n",
    "    reward_follow_up_completion: bool = True,\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    ") -> Episode:\n",
    "    while True:\n",
    "        try:\n",
    "            game = (\n",
    "                get_easy_game if difficulty == \"easy\" else get_variable_difficulty_game\n",
    "            )(return_first_solver_as_winner=return_first_solver_as_winner)\n",
    "            prompt, follow_up, solution = game.get_prompt_and_follow_up_and_solution()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if len(prompt) <= max_prompt_characters:\n",
    "            break\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        if reward_follow_up_completion:\n",
    "            completion = follow_up_completion\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    # Find first match of key followed by colon and capture following text\n",
    "                    (\n",
    "                        match := re.search(\n",
    "                            rf\"{key}: ([A-Za-z \\.:-]+)\",\n",
    "                            answer,\n",
    "                            re.IGNORECASE,\n",
    "                        )\n",
    "                    )\n",
    "                    # Check if captured group matches expected value\n",
    "                    and match.group(1).strip().lower() == value.strip().lower()\n",
    "                )\n",
    "                for key, value in solution.items()\n",
    "            ]\n",
    "        ) / len(solution)\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    example = (\n",
    "        random.choice(chain_of_thought_examples)\n",
    "        if random.random() < example_probability\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        examples=(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": example[\"chain_of_thought\"]\n",
    "                    + (example[\"answer\"] and f\"\\n\\n---\\n\\n{example['answer']}\"),\n",
    "                },\n",
    "            ]\n",
    "            if example\n",
    "            else []\n",
    "        ),\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_episodes():\n",
    "    while True:\n",
    "        yield sample_random_episode()\n",
    "\n",
    "\n",
    "model_name = \"rl17\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=ExploreOptions(\n",
    "        iterations=4,\n",
    "        num_parents=6,\n",
    "        branch_factor=3,\n",
    "        patience=5,\n",
    "        sample_probability_power=None,\n",
    "        sampling_kwargs={\n",
    "            \"max_tokens\": 1024,\n",
    "        },\n",
    "    ),\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=64 * torch.cuda.device_count(),\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=(\n",
    "        sample_random_episode() for _ in range(64 * torch.cuda.device_count())\n",
    "    ),\n",
    "    val_patience=15,\n",
    "    val_samples_per_episode=3,\n",
    "    val_sampling_kwargs={\"max_tokens\": 1024},\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=4e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=0.0,\n",
    "            clip_epsilon=0.2,\n",
    "            unclipped_policy_coef=0.0,\n",
    "            tanh_log_policy_coef=0.8,\n",
    "            value_coef=0.0,\n",
    "            entropy_coef=0.0,\n",
    "            entropy_target=0.75,\n",
    "            entropy_target_coef=0.1,\n",
    "            kl_coef=0.1,\n",
    "            weighted_entropy_coef=0.2,\n",
    "            weighted_kl_coef=0.0,\n",
    "            weighted_ce_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384 * 4,\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=32,\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        # min_time_between_requests=15 / (64 * 24),\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl17/0007 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --api-key=default\n",
      "INFO 12-11 19:52:41 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 12-11 19:52:41 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl17/0007', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl17/0007', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=32.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x79afa9fa4fe0>)\n",
      "INFO 12-11 19:52:41 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/03db53ca-a5d5-4a50-9e61-4a893cf08ed3 for IPC Path.\n",
      "INFO 12-11 19:52:41 api_server.py:179] Started engine process with PID 54865\n",
      "WARNING 12-11 19:52:41 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-11 19:52:46 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-11 19:52:46 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-11 19:52:46 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-11 19:52:51 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-11 19:52:51 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-11 19:52:51 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl17/0007', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl17/0007', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl17/0007, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 12-11 19:52:53 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl17/0007...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.20s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:05<00:04,  2.29s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.90s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.94s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:11<00:00,  2.95s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 19:53:05 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 12-11 19:53:07 gpu_executor.py:122] # GPU blocks: 12296, # CPU blocks: 8192\n",
      "INFO 12-11 19:53:07 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 24.02x\n",
      "INFO 12-11 19:53:22 api_server.py:232] vLLM to use /tmp/tmpwrfvwt1s as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 12-11 19:53:22 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 12-11 19:53:22 launcher.py:19] Available routes are:\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 12-11 19:53:22 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [54797]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57540 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea49677edf74c54a26bacf78783e7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val: 0episode [00:00, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4b1d3f7cc941bb9d0f78f056952e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl17/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:torchtune.utils._logging:Training is not distributed. If you want to train on multiple GPUs and are using the tune CLI, specify --nnodes 1 and --nproc_per_node [num_gpus]\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl17/0007\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0007/hf_model_0003_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0007/hf_model_0004_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0007/hf_model_0001_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0007/hf_model_0002_0.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl17\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl17/tensors\n",
      "  num_sequences: 35\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.75\n",
      "  entropy_target_coef: 0.1\n",
      "  kl_coef: 0.1\n",
      "  normalize_advantages: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  tanh_log_policy_coef: 0.8\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.2\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl17\n",
      "  name: rl17\n",
      "  reinit: true\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl17\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20241211_195941-rl17\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl17\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl17\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/atreides/experiments/models/rl17/0007/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 2.97 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.02 GiB\n",
      "\tGPU peak memory reserved: 15.14 GiB\n",
      "\tGPU peak memory active: 15.02 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|34|Loss: 0.0356:  97%|█████████▋| 34/35 [10:11<00:17, 17.57s/it, entropy=0.6818, entropy_target=0.0682, kl_div=0.1487, policy=-0.0055, tanh_log_policy_to_log=0.0060, unclipped_policy=-0.0157, value=1.8538, weighted_ce=0.0268, weighted_entropy=-0.0454, weighted_kl_div=-0.0092]/home/ubuntu/atreides/experiments/lib/rl/ppo.py:373: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  value_preds.std() + 1e-8\n",
      "1|35|Loss: -0.0125: 100%|██████████| 35/35 [10:28<00:00, 17.45s/it, entropy=0.6969, entropy_target=0.0531, kl_div=0.1480, policy=0.1399, tanh_log_policy_to_log=-0.0100, unclipped_policy=0.1287, value=1.9980, weighted_ce=-0.0712, weighted_entropy=0.1232, weighted_kl_div=0.0310]  INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 0.00 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 21.53 secs\n",
      "1|35|Loss: -0.0125: 100%|██████████| 35/35 [10:49<00:00, 18.57s/it, entropy=0.6969, entropy_target=0.0531, kl_div=0.1480, policy=0.1399, tanh_log_policy_to_log=-0.0100, unclipped_policy=0.1287, value=1.9980, weighted_ce=-0.0712, weighted_entropy=0.1232, weighted_kl_div=0.0310]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                   entropy ▆▂▂▁▂▂▃▅▃▅▃▆▃▄▆█▆▇▅▅▇▅▅▇▅▅▆▇▅▆▆▅▅▅▅\n",
      "wandb:            entropy_target ▁▇▇█▇▇▅▂▅▁▅▂▄▄▁▅▂▃▁▂▂▂▂▃▁▂▁▃▁▂▁▁▂▂▂\n",
      "wandb:               global_step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "wandb:                    kl_div ▅▂▂▁▂▄▄▇▄▅▄▆▃▄▆█▆▇▄▄▆▄▅▆▅▄▅▆▄▆▅▅▄▄▄\n",
      "wandb:                      loss ▅▇████▇▂▇▄▇▅▆▆▆▇▅▆▄▅▅▅▅▅▄▅▅▅▅▅▄▅▅▇▁\n",
      "wandb:                        lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:        peak_memory_active ▁██████████████████████████████████\n",
      "wandb:         peak_memory_alloc ▁██████████████████████████████████\n",
      "wandb:      peak_memory_reserved ▁█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "wandb:                    policy ▂▂▂▂▂▁▂▄▂▂▂▂▂▂▁▄▂▂▂▂▂▂▂▃▂▂▁▂▂▄▂▂▁▁█\n",
      "wandb:           tanh_log_policy ▇▇▆▆▆▇▇▁▇▆▇▇▇▇█▆▆▆▆▆▇▇▇▆▆▇▇▆▇▇▅▆▇█▄\n",
      "wandb: tokens_per_second_per_gpu ▃▇▆▅▆▇▆▆▇▇████▆▅▇▇██▆▇█▇▇▇▇▇█▆▇▇▇▇▁\n",
      "wandb:          unclipped_policy ▂▂▂▂▃▁▁▂▂▂▂▂▂▂▂▂▃▂▂▁▂▂▂▁▂▂▂▂▂▃▂▁▂▁█\n",
      "wandb:                     value ▂▃▅▆▃▃▄▅▂▂▂▂▂▁▅█▃█▂▃▁▁▂▃▂▁▄▂▁▄▄▃▁▃▃\n",
      "wandb:               weighted_ce ▆▆▆▆▆▇▆▂▆▅▆▆▆▆█▅▅▅▆▆▆▆▆▆▆▆▆▅▆▅▅▆▆█▁\n",
      "wandb:          weighted_entropy ▃▃▂▂▃▂▂▅▃▃▃▃▃▃▁▄▃▃▃▃▃▃▃▄▃▃▃▄▃▄▃▃▃▁█\n",
      "wandb:           weighted_kl_div ▃▃▃▂▃▂▃█▃▃▃▂▃▃▁▆▄▄▃▄▃▃▃▄▃▃▃▄▃▄▅▃▃▂▇\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                   entropy 0.69689\n",
      "wandb:            entropy_target 0.05311\n",
      "wandb:               global_step 35\n",
      "wandb:                    kl_div 0.14797\n",
      "wandb:                      loss -0.01251\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 69.3195\n",
      "wandb:         peak_memory_alloc 69.3195\n",
      "wandb:      peak_memory_reserved 71.23828\n",
      "wandb:                    policy 0.13987\n",
      "wandb:           tanh_log_policy -0.00997\n",
      "wandb: tokens_per_second_per_gpu 186.5192\n",
      "wandb:          unclipped_policy 0.12865\n",
      "wandb:                     value 1.99802\n",
      "wandb:               weighted_ce -0.0712\n",
      "wandb:          weighted_entropy 0.12321\n",
      "wandb:           weighted_kl_div 0.03099\n",
      "wandb: \n",
      "wandb: 🚀 View run rl17 at: https://wandb.ai/bradhilton/torchtune/runs/rl17\n",
      "wandb: ⭐️ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20241211_195941-rl17/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 8 model files to /home/ubuntu/atreides/experiments/models/rl17/0008\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl17/0008 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --api-key=default\n",
      "INFO 12-11 20:10:47 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 12-11 20:10:47 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl17/0008', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl17/0008', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=32.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7021218a8fe0>)\n",
      "INFO 12-11 20:10:47 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/2499e799-c64c-4717-ad1a-90b39f657ca9 for IPC Path.\n",
      "INFO 12-11 20:10:47 api_server.py:179] Started engine process with PID 61683\n",
      "WARNING 12-11 20:10:47 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-11 20:10:53 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-11 20:10:53 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-11 20:10:53 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-11 20:10:58 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-11 20:10:58 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-11 20:10:58 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl17/0008', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl17/0008', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl17/0008, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 12-11 20:11:00 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl17/0008...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.14s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:05<00:04,  2.28s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.15s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.49s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.33s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 20:11:13 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 12-11 20:11:16 gpu_executor.py:122] # GPU blocks: 12296, # CPU blocks: 8192\n",
      "INFO 12-11 20:11:16 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 24.02x\n",
      "INFO 12-11 20:11:29 api_server.py:232] vLLM to use /tmp/tmp7_k1gu4y as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 12-11 20:11:29 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 12-11 20:11:29 launcher.py:19] Available routes are:\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 12-11 20:11:29 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [61619]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:45376 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afc48d2b15445e6976d2953e622a088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157751bd4f1644869ac2ef015671fb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (1 remaining episodes x 15 patience per episode = 15 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl17/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:torchtune.utils._logging:Training is not distributed. If you want to train on multiple GPUs and are using the tune CLI, specify --nnodes 1 and --nproc_per_node [num_gpus]\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl17/0008\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0008/hf_model_0003_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0008/hf_model_0004_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0008/hf_model_0001_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0008/hf_model_0002_0.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl17\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl17/tensors\n",
      "  num_sequences: 28\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.75\n",
      "  entropy_target_coef: 0.1\n",
      "  kl_coef: 0.1\n",
      "  normalize_advantages: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  tanh_log_policy_coef: 0.8\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.2\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl17\n",
      "  name: rl17\n",
      "  reinit: true\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl17\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20241211_201657-rl17\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl17\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl17\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/atreides/experiments/models/rl17/0008/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 2.97 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.02 GiB\n",
      "\tGPU peak memory reserved: 15.14 GiB\n",
      "\tGPU peak memory active: 15.02 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/28 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|27|Loss: 0.0347:  96%|█████████▋| 27/28 [08:07<00:17, 17.53s/it, entropy=0.5191, entropy_target=0.2309, kl_div=0.1060, policy=0.0042, tanh_log_policy_to_log=0.0000, unclipped_policy=-0.0012, value=3.2135, weighted_ce=0.0023, weighted_entropy=-0.0047, weighted_kl_div=-0.0001]/home/ubuntu/atreides/experiments/lib/rl/ppo.py:373: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  value_preds.std() + 1e-8\n",
      "1|28|Loss: 0.0137: 100%|██████████| 28/28 [08:24<00:00, 17.44s/it, entropy=0.7503, entropy_target=0.0003, kl_div=0.1513, policy=0.0043, tanh_log_policy_to_log=-0.0009, unclipped_policy=-0.0037, value=2.0424, weighted_ce=-0.0028, weighted_entropy=0.0039, weighted_kl_div=0.0032]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 0.00 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 19.59 secs\n",
      "1|28|Loss: 0.0137: 100%|██████████| 28/28 [08:44<00:00, 18.73s/it, entropy=0.7503, entropy_target=0.0003, kl_div=0.1513, policy=0.0043, tanh_log_policy_to_log=-0.0009, unclipped_policy=-0.0037, value=2.0424, weighted_ce=-0.0028, weighted_entropy=0.0039, weighted_kl_div=0.0032]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                   entropy ▁▇▆▇▃▅▄▃▇▆▄▆▇▆█▇▆▆▅▇▆█▄▅▆▇▂▆\n",
      "wandb:            entropy_target █▂▁▂▅▂▄▅▂▂▄▁▂▁▄▂▁▁▃▂▁▃▃▃▁▂▆▁\n",
      "wandb:               global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
      "wandb:                    kl_div ▁▆▆█▄▄▄▅▇▇▆▄▇▆▇▆▅▇▄▆▇▇▄▆▆▆▂▅\n",
      "wandb:                      loss ▇▄▃▁▆▄▇▃▄▅▄▃▅▄▆▄▄▄▄▅▄▆▅▅▃▅█▃\n",
      "wandb:                        lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:        peak_memory_active ▁███████████████████████████\n",
      "wandb:         peak_memory_alloc ▁███████████████████████████\n",
      "wandb:      peak_memory_reserved ▁█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "wandb:                    policy ▂▃▆█▆▁▂▄▂▂▂▁▁▂▂▁▅▂▂▁▂▁▂▁▂▁▁▁\n",
      "wandb:           tanh_log_policy ▆▄▅▁▆▆█▁▆▇▂▆▆▇▆▆▇▆▅▇▆▆▆▇▆▇▇▆\n",
      "wandb: tokens_per_second_per_gpu ▂▄▆▄▄█▇▅▇▆▄█▇▆▆▇▆▆▆▇▇▅▇▇▇▇█▁\n",
      "wandb:          unclipped_policy ▅▁▆▄▅▃▅▂▄▅▂▄▄▄▄▄█▅▄▄▄▄▅▄▄▄▄▄\n",
      "wandb:                     value ▃▄▅▆▆▅▆█▄▂█▁▂▃▂▁▄▁▅▄▄▃▃▂▁▂█▄\n",
      "wandb:               weighted_ce ▆▇▅▁▆▇█▃▆▆▃▆▇▇▆▇▆▆▆▇▇▇▇▇▆▇▇▇\n",
      "wandb:          weighted_entropy ▄▁▃█▄▂▁▇▂▂▄▃▂▂▃▃▂▃▃▂▂▃▃▂▃▂▂▂\n",
      "wandb:           weighted_kl_div ▅▂▆▆▃▃▁█▃▃█▃▃▃▄▃▃▃▄▃▃▃▃▃▃▃▃▃\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                   entropy 0.75029\n",
      "wandb:            entropy_target 0.00029\n",
      "wandb:               global_step 28\n",
      "wandb:                    kl_div 0.15134\n",
      "wandb:                      loss 0.01365\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 69.31947\n",
      "wandb:         peak_memory_alloc 69.31947\n",
      "wandb:      peak_memory_reserved 71.23828\n",
      "wandb:                    policy 0.00434\n",
      "wandb:           tanh_log_policy -0.0009\n",
      "wandb: tokens_per_second_per_gpu 217.67841\n",
      "wandb:          unclipped_policy -0.0037\n",
      "wandb:                     value 2.04242\n",
      "wandb:               weighted_ce -0.00279\n",
      "wandb:          weighted_entropy 0.00394\n",
      "wandb:           weighted_kl_div 0.00319\n",
      "wandb: \n",
      "wandb: 🚀 View run rl17 at: https://wandb.ai/bradhilton/torchtune/runs/rl17\n",
      "wandb: ⭐️ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20241211_201657-rl17/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 9 model files to /home/ubuntu/atreides/experiments/models/rl17/0009\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl17/0009 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --api-key=default\n",
      "INFO 12-11 20:25:58 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 12-11 20:25:58 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl17/0009', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl17/0009', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=32.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7a5be3aa4fe0>)\n",
      "INFO 12-11 20:25:58 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/6836fee9-6920-47a1-a3fb-c6c0e5bdb057 for IPC Path.\n",
      "INFO 12-11 20:25:58 api_server.py:179] Started engine process with PID 67271\n",
      "WARNING 12-11 20:25:58 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-11 20:26:02 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-11 20:26:02 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-11 20:26:03 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-11 20:26:07 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-11 20:26:07 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-11 20:26:07 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl17/0009', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl17/0009', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl17/0009, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 12-11 20:26:08 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl17/0009...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.21s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:05<00:04,  2.30s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.13s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.39s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.27s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 20:26:21 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 12-11 20:26:24 gpu_executor.py:122] # GPU blocks: 12296, # CPU blocks: 8192\n",
      "INFO 12-11 20:26:24 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 24.02x\n",
      "INFO 12-11 20:26:39 api_server.py:232] vLLM to use /tmp/tmp4cghg_tk as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 12-11 20:26:39 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 12-11 20:26:39 launcher.py:19] Available routes are:\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 12-11 20:26:39 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [67204]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42082 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494e6726883944c5819df8d96b734091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6ddcefb5494451bcf47ec4ab215ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (1 remaining episodes x 15 patience per episode = 15 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl17/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:torchtune.utils._logging:Training is not distributed. If you want to train on multiple GPUs and are using the tune CLI, specify --nnodes 1 and --nproc_per_node [num_gpus]\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl17/0009\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0009/hf_model_0003_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0009/hf_model_0004_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0009/hf_model_0001_0.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl17/0009/hf_model_0002_0.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl17\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl17/tensors\n",
      "  num_sequences: 35\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.75\n",
      "  entropy_target_coef: 0.1\n",
      "  kl_coef: 0.1\n",
      "  normalize_advantages: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  tanh_log_policy_coef: 0.8\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.2\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl17\n",
      "  name: rl17\n",
      "  reinit: true\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl17\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: false\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20241211_203300-rl17\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl17\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl17\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/atreides/experiments/models/rl17/0009/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 2.93 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.02 GiB\n",
      "\tGPU peak memory reserved: 15.14 GiB\n",
      "\tGPU peak memory active: 15.02 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|34|Loss: 0.0152:  97%|█████████▋| 34/35 [10:10<00:17, 17.50s/it, entropy=0.6690, entropy_target=0.0810, kl_div=0.1121, policy=0.0058, tanh_log_policy_to_log=-0.0016, unclipped_policy=0.0002, value=2.3648, weighted_ce=-0.0117, weighted_entropy=0.0142, weighted_kl_div=0.0046]  /home/ubuntu/atreides/experiments/lib/rl/ppo.py:373: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1808.)\n",
      "  value_preds.std() + 1e-8\n",
      "1|35|Loss: 0.0254: 100%|██████████| 35/35 [10:27<00:00, 17.43s/it, entropy=0.8437, entropy_target=0.0937, kl_div=0.1560, policy=0.0012, tanh_log_policy_to_log=0.0002, unclipped_policy=-0.0021, value=1.0084, weighted_ce=0.0016, weighted_entropy=-0.0015, weighted_kl_div=0.0001]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 0.00 secs\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl17/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 20.61 secs\n",
      "1|35|Loss: 0.0254: 100%|██████████| 35/35 [10:48<00:00, 18.53s/it, entropy=0.8437, entropy_target=0.0937, kl_div=0.1560, policy=0.0012, tanh_log_policy_to_log=0.0002, unclipped_policy=-0.0021, value=1.0084, weighted_ce=0.0016, weighted_entropy=-0.0015, weighted_kl_div=0.0001]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                   entropy ▂█▅▃▃▂▂▃▄▃▂▂▂▂▂▃▅▄▅▅▃▄▄▃▃▄▄▁▃▂▃▂▂▂▄\n",
      "wandb:            entropy_target ▃█▄▁▁▃▃▂▁▂▃▃▂▂▃▂▃▁▃▃▂▂▂▁▂▁▂▄▂▂▁▃▄▂▂\n",
      "wandb:               global_step ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "wandb:                    kl_div ▃█▇▇▆▅▅▅▇▆▂▄▄▄▄▅▇▅▇▇▄▅▅▄▅▆▄▁▄▃▅▂▃▂▆\n",
      "wandb:                      loss ▄█▅▅▁▅▄▅▃▁▃▄▃▄▃▄▅▄▄▄▃▃▄▃▃▃▄▅▄▃▃▃▄▃▄\n",
      "wandb:                        lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:        peak_memory_active ▁██████████████████████████████████\n",
      "wandb:         peak_memory_alloc ▁██████████████████████████████████\n",
      "wandb:      peak_memory_reserved ▁█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "wandb:                    policy ▂▂▁▂█▂▃▂▁▂▂▁▁▁▁▄▁▁▂▁▁▂▁▂▁▁▂▁▂▁▁▁▁▁▁\n",
      "wandb:           tanh_log_policy ▅▅▅█▃▆▄▆▅▁▃▅▅▅▄▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "wandb: tokens_per_second_per_gpu ▂▇▇▄▄▅▄▇▅▃█▇███▇▇▇▇▆▇▇▇▇▇█▇▇▇▇▇▇█▇▁\n",
      "wandb:          unclipped_policy ▂▃▂▂█▂▂▃▂▁▂▂▂▂▂▅▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂\n",
      "wandb:                     value ▃▂▂▄█▂▄▄▂▄▃▂▂▃▃▂▂▂▅▃▂▄▁▃▂▂▂▂▂▂▂▂▂▃▁\n",
      "wandb:               weighted_ce ▅▅▅▇▁▆▃█▄▁▄▅▅▅▄▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅\n",
      "wandb:          weighted_entropy ▄▃▄▁█▂▃▁▄▇▆▃▃▃▅▃▃▂▅▅▃▃▂▃▃▄▂▃▃▄▃▃▃▄▃\n",
      "wandb:           weighted_kl_div ▄▄▄▂▇▄▆▁▄█▅▂▃▃▄▄▄▃▄▃▃▄▃▃▄▄▃▃▃▄▃▄▃▄▃\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                   entropy 0.84366\n",
      "wandb:            entropy_target 0.09366\n",
      "wandb:               global_step 35\n",
      "wandb:                    kl_div 0.15598\n",
      "wandb:                      loss 0.02539\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 69.31959\n",
      "wandb:         peak_memory_alloc 69.31959\n",
      "wandb:      peak_memory_reserved 71.23828\n",
      "wandb:                    policy 0.00115\n",
      "wandb:           tanh_log_policy 0.00015\n",
      "wandb: tokens_per_second_per_gpu 293.09937\n",
      "wandb:          unclipped_policy -0.00209\n",
      "wandb:                     value 1.00844\n",
      "wandb:               weighted_ce 0.00161\n",
      "wandb:          weighted_entropy -0.00155\n",
      "wandb:           weighted_kl_div 0.00014\n",
      "wandb: \n",
      "wandb: 🚀 View run rl17 at: https://wandb.ai/bradhilton/torchtune/runs/rl17\n",
      "wandb: ⭐️ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20241211_203300-rl17/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 10 model files to /home/ubuntu/atreides/experiments/models/rl17/0010\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl17/0010 --port=8000 --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --api-key=default\n",
      "INFO 12-11 20:44:11 api_server.py:528] vLLM API server version 0.6.3.post1\n",
      "INFO 12-11 20:44:11 api_server.py:529] args: Namespace(subparser='serve', model_tag='/home/ubuntu/atreides/experiments/models/rl17/0010', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/home/ubuntu/atreides/experiments/models/rl17/0010', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=32.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x754cd3da4fe0>)\n",
      "INFO 12-11 20:44:11 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/9c7638fe-948b-4474-9d07-64bb59672465 for IPC Path.\n",
      "INFO 12-11 20:44:11 api_server.py:179] Started engine process with PID 74075\n",
      "WARNING 12-11 20:44:11 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-11 20:44:17 config.py:1786] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-11 20:44:17 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-11 20:44:17 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-11 20:44:22 arg_utils.py:1019] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.\n",
      "WARNING 12-11 20:44:22 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-11 20:44:22 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/home/ubuntu/atreides/experiments/models/rl17/0010', speculative_config=None, tokenizer='/home/ubuntu/atreides/experiments/models/rl17/0010', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/ubuntu/atreides/experiments/models/rl17/0010, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=True, mm_processor_kwargs=None)\n",
      "INFO 12-11 20:44:24 model_runner.py:1056] Starting to load model /home/ubuntu/atreides/experiments/models/rl17/0010...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py:425: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n",
      "Loading pt checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.25s/it]\n",
      "Loading pt checkpoint shards:  50% Completed | 2/4 [00:05<00:04,  2.32s/it]\n",
      "Loading pt checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.17s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.54s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 4/4 [00:13<00:00,  3.37s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 20:44:38 model_runner.py:1067] Loading model weights took 14.9595 GB\n",
      "INFO 12-11 20:44:41 gpu_executor.py:122] # GPU blocks: 12296, # CPU blocks: 8192\n",
      "INFO 12-11 20:44:41 gpu_executor.py:126] Maximum concurrency for 16384 tokens per request: 24.02x\n",
      "INFO 12-11 20:44:58 api_server.py:232] vLLM to use /tmp/tmpmk2v773c as PROMETHEUS_MULTIPROC_DIR\n",
      "WARNING 12-11 20:44:58 serving_embedding.py:199] embedding_mode is False. Embedding API will not work.\n",
      "INFO 12-11 20:44:58 launcher.py:19] Available routes are:\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 12-11 20:44:58 launcher.py:27] Route: /v1/embeddings, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [73989]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on socket ('0.0.0.0', 8000) (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:49378 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1218ee23c69f4241a3d57f50cc14f468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
