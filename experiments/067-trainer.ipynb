{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.langfuse import langfuse\n",
    "langfuse.enabled = False\n",
    "# langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "import random\n",
    "import re\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class TemporalCluePuzzle(TypedDict):\n",
    "    num_clues: int\n",
    "    prompt: str\n",
    "    solution: dict[str, str]\n",
    "\n",
    "\n",
    "temporal_clue_puzzles: list[TemporalCluePuzzle] = json.load(\n",
    "    open(\"./data/temporal-clue-puzzles.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "chain_of_thought_examples: list[dict[str, str]] = json.load(\n",
    "    open(\"./data/chain-of-thought-examples.json\")\n",
    ")\n",
    "chain_of_thought_examples.pop(6)\n",
    "chain_of_thought_examples.pop(3)\n",
    "\n",
    "\n",
    "def get_episode(puzzle: TemporalCluePuzzle, example: dict[str, str]) -> Episode:\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in completions:\n",
    "            content = completion.last_assistant_message.get(\"content\")\n",
    "            assert isinstance(content, str)\n",
    "            num_correct = 0\n",
    "            for key, value in puzzle[\"solution\"].items():\n",
    "                if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "                    match = matches[-1]\n",
    "                    if match.strip().lower() == value.lower():\n",
    "                        num_correct += 1\n",
    "            completion.commit(reward=num_correct / len(puzzle[\"solution\"]))\n",
    "\n",
    "    return Episode(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": puzzle[\"prompt\"].replace(\n",
    "                    \"Fill out your final answers in the following format:\",\n",
    "                    \"After verifiably finding all the correct answers, fill out your final answers in the following format:\",\n",
    "                ),\n",
    "            },\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": \"Let's think this through step by step...\",\n",
    "            # },\n",
    "        ],\n",
    "        on_sample=on_sample,\n",
    "        examples=[\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example[\"chain_of_thought\"]\n",
    "                + (example[\"answer\"] and f\"\\n\\n---\\n\\n{example['answer']}\"),\n",
    "            },\n",
    "            # {\"role\": \"user\", \"content\": example[1][\"prompt\"]},\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": example[1][\"chain_of_thought\"]\n",
    "            #     + (example[1][\"answer\"] and f\"\\n\\n---\\n\\n{example[1]['answer']}\"),\n",
    "            # },\n",
    "        ],\n",
    "        # logprobs_mask=Clue.get_logprobs_mask(),\n",
    "    )\n",
    "\n",
    "\n",
    "temporal_clue_episodes = [\n",
    "    get_episode(puzzle, example)\n",
    "    for puzzle, example in zip(temporal_clue_puzzles, cycle(chain_of_thought_examples))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "zebra_grid_questions = pl.read_parquet(\n",
    "    \"hf://datasets/allenai/ZebraLogicBench-private/grid_mode/test-00000-of-00001.parquet\"\n",
    ").to_dicts()\n",
    "random.shuffle(zebra_grid_questions)\n",
    "\n",
    "\n",
    "def get_episode(question: dict) -> Episode:\n",
    "    prompt = f\"\"\"{question[\"puzzle\"]}\n",
    "Fill in the grid with the correct values:\n",
    "\n",
    "| {' | '.join(question[\"solution\"][\"header\"])} |\n",
    "| {' | '.join([\"-\" * len(header) for header in question[\"solution\"][\"header\"]])} |\n",
    "\"\"\"\n",
    "\n",
    "    for _ in question[\"solution\"][\"rows\"]:\n",
    "        prompt += f\"| {' | '.join([\" \" * len(header) for header in question[\"solution\"][\"header\"]])} |\\n\"\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"\\| \" + r\"\\|\".join(r\"(.*?)\" for _ in question[\"solution\"][\"header\"]) + r\" \\|\"\n",
    "    )\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]):\n",
    "        for completion in completions:\n",
    "            assert \"content\" in completion.last_assistant_message and isinstance(\n",
    "                completion.last_assistant_message[\"content\"], str\n",
    "            )\n",
    "            num_cells = sum(len(row) for row in question[\"solution\"][\"rows\"])\n",
    "            num_correct = 0\n",
    "            for match, row in zip(\n",
    "                re.findall(pattern, completion.last_assistant_message[\"content\"])[\n",
    "                    -len(question[\"solution\"][\"rows\"]) :\n",
    "                ],\n",
    "                question[\"solution\"][\"rows\"],\n",
    "            ):\n",
    "                for cell, value in zip(match, row):\n",
    "                    if cell.strip().lower() == value.lower():\n",
    "                        num_correct += 1\n",
    "            completion.commit(reward=num_correct / num_cells)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "zebra_grid_episodes = [get_episode(question) for question in zebra_grid_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "math_questions = list(\n",
    "    load_dataset(\"lighteval/MATH\", \"all\")[\"train\"].to_iterable_dataset()  # type: ignore\n",
    ")\n",
    "random.shuffle(math_questions)\n",
    "\n",
    "\n",
    "question_solution = None\n",
    "pattern = re.compile(r\"\\\\boxed{([^}]+)}\")\n",
    "\n",
    "\n",
    "def get_episode(question: dict) -> Episode:\n",
    "    prompt = (\n",
    "        f\"{question['problem']}\\n\\n\"\n",
    "        \"Solve this math problem and show your work. Your final answer MUST be \"\n",
    "        \"formatted in a LaTeX box using \\\\boxed{{}}. For example: \"\n",
    "        \"$1+1=\\\\boxed{{2}}$\\n\\n\"\n",
    "        \"You can submit multiple attempts. Each attempt should end with a boxed \"\n",
    "        \"answer. Your last answer will be weighted the most, but you can get \"\n",
    "        \"partial credit if an earlier answer is correct. If after multiple \"\n",
    "        \"attempts you decide an earlier answer is the correct one, just submit \"\n",
    "        \"it again to get full credit.\"\n",
    "    )\n",
    "\n",
    "    global question_solution\n",
    "    question_solution = question[\"solution\"]\n",
    "    solution = re.search(pattern, question[\"solution\"])\n",
    "    assert solution is not None, question[\"solution\"]\n",
    "    solution = solution.group(1)\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]):\n",
    "        for completion in completions:\n",
    "            content = completion.last_assistant_message.get(\"content\")\n",
    "            assert isinstance(content, str)\n",
    "            solutions = [\n",
    "                match.group(1) for match in re.finditer(r\"\\\\boxed{([^}]+)}\", content)\n",
    "            ][::-1]\n",
    "            try:\n",
    "                reward = 0.9 ** solutions.index(solution)\n",
    "            except ValueError:\n",
    "                reward = 0\n",
    "            completion.commit(reward=reward)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "math_episodes = [\n",
    "    get_episode(question)\n",
    "    for question in math_questions[:2048]\n",
    "    if re.search(pattern, question[\"solution\"]) is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import cycle\n",
    "from lib.rl.completion import SplitMethod\n",
    "from lib.rl.completion_sampler import (\n",
    "    CompletionSampler,\n",
    "    SamplingKwargs,\n",
    "    CompletionSamplerPool,\n",
    ")\n",
    "from lib.rl.trainer import ExploreImpl, ExploreOptions\n",
    "from lib.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultExploreImpl(ExploreImpl):\n",
    "    explore_options: ExploreOptions\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        def done_callback(task: asyncio.Task[Episode]) -> None:\n",
    "            try:\n",
    "                done_episodes.put_nowait(task.result())\n",
    "            except BaseException as exception:\n",
    "                done_episodes.put_nowait(exception)\n",
    "\n",
    "        priority = 1\n",
    "        while episode := await ready_episodes.get():\n",
    "            asyncio.create_task(\n",
    "                self._explore_episode(\n",
    "                    completion_sampler, tokenizer, episode, update_progress, priority\n",
    "                )\n",
    "            ).add_done_callback(done_callback)\n",
    "            priority += 1\n",
    "\n",
    "    async def _explore_episode(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        episode: Episode,\n",
    "        update_progress: Callable[[float], None],\n",
    "        priority: int,\n",
    "    ) -> Episode:\n",
    "        for _ in range(self.explore_options.iterations):\n",
    "            await episode.sample_completions(\n",
    "                completion_sampler=completion_sampler,\n",
    "                tokenizer=tokenizer,\n",
    "                num_parents=self.explore_options.num_parents,\n",
    "                branch_factor=self.explore_options.branch_factor,\n",
    "                get_recovery_pattern=self.explore_options.get_recovery_pattern,\n",
    "                max_splits_per_completion=self.explore_options.max_split_points\n",
    "                or self.explore_options.num_parents,\n",
    "                priority=priority,\n",
    "                sample_probability_power=self.explore_options.get_sample_probability_power(),\n",
    "                sampling_kwargs=self.explore_options.sampling_kwargs,\n",
    "                split_by=self.explore_options.split_method,\n",
    "                split_separators=self.explore_options.split_separators,\n",
    "            )\n",
    "            update_progress(1 / self.explore_options.iterations)\n",
    "        return episode\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleExploreImpl(ExploreImpl):\n",
    "    num_samples: int\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        while episode := await ready_episodes.get():\n",
    "            task = asyncio.create_task(\n",
    "                episode.sample_completions(\n",
    "                    completion_sampler,\n",
    "                    tokenizer,\n",
    "                    num_parents=1,\n",
    "                    branch_factor=self.num_samples,\n",
    "                    sampling_kwargs=self.sampling_kwargs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            def done_callback(_: asyncio.Task[bool], episode=episode) -> None:\n",
    "                try:\n",
    "                    done_episodes.put_nowait(episode)\n",
    "                    update_progress(1)\n",
    "                except BaseException as e:\n",
    "                    done_episodes.put_nowait(e)\n",
    "\n",
    "            task.add_done_callback(done_callback)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TreeExploreImpl(ExploreImpl):\n",
    "    branch_factor: int\n",
    "    depth: int\n",
    "    num_roots: int | None = None\n",
    "    best_leaf_sampling_temperature: float = 0.01\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "    split_method: SplitMethod = \"count\"\n",
    "    split_separators: set[str] = field(default_factory=set)\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler_pool: CompletionSamplerPool,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        async def expand(\n",
    "            episode: Episode, priority: int, completion_sampler: CompletionSampler\n",
    "        ) -> None:\n",
    "            model = await completion_sampler.get_model()\n",
    "            num_roots = self.num_roots or self.branch_factor\n",
    "\n",
    "            # If there are existing trajectories, we'll sample one\n",
    "            # of the best ones to stabilize and improve training.\n",
    "            leaves = list(episode.completion.leaves(models=None))\n",
    "            if leaves:\n",
    "                best_leaf = random.choices(\n",
    "                    leaves,\n",
    "                    weights=[\n",
    "                        np.exp(\n",
    "                            leaf.cumulative_reward()\n",
    "                            / self.best_leaf_sampling_temperature\n",
    "                        )\n",
    "                        for leaf in leaves\n",
    "                    ],\n",
    "                    k=1,\n",
    "                )[0]\n",
    "                best_leaf = best_leaf.recursive_copy(model=model)\n",
    "                best_leaf.commit()\n",
    "                while best_leaf.parent and best_leaf.parent.parent is None:\n",
    "                    best_leaf = best_leaf.merge()\n",
    "            else:\n",
    "                best_leaf = None\n",
    "\n",
    "            pending: set[asyncio.Task] = {\n",
    "                asyncio.create_task(\n",
    "                    episode.sample_completions(\n",
    "                        completion_sampler,\n",
    "                        tokenizer,\n",
    "                        num_parents=1,\n",
    "                        branch_factor=num_roots - 1 if best_leaf else num_roots,\n",
    "                        priority=priority,\n",
    "                        sampling_kwargs=self.sampling_kwargs,\n",
    "                        split_by=self.split_method,\n",
    "                        split_separators=self.split_separators,\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "\n",
    "            num_leaves = 0\n",
    "            while pending:\n",
    "                finished, pending = await asyncio.wait(\n",
    "                    pending, return_when=asyncio.FIRST_COMPLETED\n",
    "                )\n",
    "                for task in finished:\n",
    "                    try:\n",
    "                        task.result()\n",
    "                    except BaseException as e:\n",
    "                        await done_episodes.put(e)\n",
    "                        return\n",
    "                _num_leaves = 0\n",
    "                for leaf in episode.completion.leaves(models={model}):\n",
    "                    _num_leaves += 1\n",
    "                    num_partitions = self.depth - leaf.depth() + 1\n",
    "                    if num_partitions > 1:\n",
    "                        parents = list(\n",
    "                            leaf.split(\n",
    "                                by=self.split_method,\n",
    "                                at=(\n",
    "                                    split / num_partitions\n",
    "                                    for split in range(1, num_partitions)\n",
    "                                ),\n",
    "                                separators=self.split_separators,\n",
    "                                cache=True,\n",
    "                            )\n",
    "                        )[:-1]\n",
    "                        for parent in parents:\n",
    "                            pending.add(\n",
    "                                asyncio.create_task(\n",
    "                                    episode._sample_completions(\n",
    "                                        parent=parent,\n",
    "                                        model=model,\n",
    "                                        completion_sampler=completion_sampler,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        branch_factor=self.branch_factor,\n",
    "                                        fork_decay=1.0,\n",
    "                                        recovery_pattern=None,\n",
    "                                        split_separators=self.split_separators,\n",
    "                                        sampling_kwargs=self.sampling_kwargs\n",
    "                                        or SamplingKwargs(),\n",
    "                                        priority=priority,\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                update_progress(\n",
    "                    (_num_leaves - num_leaves)\n",
    "                    / (num_roots * (self.branch_factor ** (self.depth - 1)))\n",
    "                )\n",
    "                num_leaves = _num_leaves\n",
    "\n",
    "            await done_episodes.put(episode)\n",
    "\n",
    "        completion_samplers = cycle(completion_sampler_pool.samplers)\n",
    "        priority = 0\n",
    "        while episode := await ready_episodes.get():\n",
    "            priority += 1\n",
    "            asyncio.create_task(expand(episode, priority, next(completion_samplers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from ['/home/ubuntu/atreides/experiments/models/rl123/0001']\n",
      "INFO 01-07 18:42:43 config.py:510] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 01-07 18:42:43 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-07 18:42:44 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n",
      "INFO 01-07 18:42:44 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20250107_184245-rl123</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl123' target=\"_blank\">rl123</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl123' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl123</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aioitertools.helpers import maybe_await\n",
    "import asyncio\n",
    "from collections import Counter\n",
    "import itertools as it\n",
    "from lib import clue\n",
    "from lib.rl.episode import Episode\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import Eval, ExploreOptions, Trainer, vLLMConfig\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import AsyncIterable\n",
    "\n",
    "\n",
    "episodes_per_iteration = 32 * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "async def train_episodes(\n",
    "    revisit_frequency: float = 0.0,\n",
    ") -> AsyncIterable[Episode | BaseException]:\n",
    "    pending: set[asyncio.Task[Episode | BaseException]] = set()\n",
    "    episodes = (\n",
    "        maybe_await(episode)\n",
    "        for episodes in zip(\n",
    "            # (clue.sample_random_episode() for _ in it.repeat(0)),\n",
    "            it.cycle(temporal_clue_episodes[64:]),\n",
    "            # it.cycle(zebra_grid_episodes[64:]),\n",
    "            # it.cycle(math_episodes[64:]),\n",
    "        )\n",
    "        for episode in episodes\n",
    "    )\n",
    "    visited_episodes = Counter[Episode]()\n",
    "    while True:\n",
    "        pending.update(\n",
    "            asyncio.create_task(next(episodes))\n",
    "            for _ in range(episodes_per_iteration - len(pending))  # type: ignore\n",
    "        )\n",
    "        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "        if len(visited_episodes) > episodes_per_iteration:\n",
    "            while random.random() < revisit_frequency:\n",
    "                episode = min(visited_episodes, key=lambda e: visited_episodes[e])\n",
    "                visited_episodes[episode] += 1\n",
    "                yield episode\n",
    "        for task in done:\n",
    "            try:\n",
    "                result = task.result()\n",
    "                if isinstance(result, Episode):\n",
    "                    visited_episodes[result] += 1\n",
    "                yield result\n",
    "            except BaseException as e:\n",
    "                yield e\n",
    "\n",
    "\n",
    "async def val_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    for fut in asyncio.as_completed(clue.sample_random_episode() for _ in range(64)):\n",
    "        try:\n",
    "            yield await fut\n",
    "        except BaseException as e:\n",
    "            yield e\n",
    "\n",
    "\n",
    "explore_options = ExploreOptions(\n",
    "    iterations=1,\n",
    "    num_parents=6,\n",
    "    branch_factor=2,\n",
    "    patience=60,\n",
    "    advantage_max_weight=0.15,\n",
    "    sample_probability_power=None,\n",
    "    sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "    # split_method=\"prob\",\n",
    "    # split_point_std_deviation=0.5,\n",
    ")\n",
    "\n",
    "model_name = \"rl123\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=explore_options,\n",
    "    # explore_impl=DefaultExploreImpl(explore_options),\n",
    "    # explore_impl=SimpleExploreImpl(\n",
    "    #     num_samples=8, sampling_kwargs={\"max_tokens\": 4096}\n",
    "    # ),\n",
    "    explore_impl=TreeExploreImpl(\n",
    "        branch_factor=2,\n",
    "        depth=5,\n",
    "        num_roots=4,\n",
    "        # best_leaf_sampling_temperature=0.05,\n",
    "        sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "    ),\n",
    "    force_terminate_vllms=True,\n",
    "    train_episodes=train_episodes(revisit_frequency=0.5),\n",
    "    episodes_per_iteration=episodes_per_iteration,\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    evals=[\n",
    "        # Eval(\n",
    "        #     name=\"variable_clue\",\n",
    "        #     episodes=val_episodes(),\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "        Eval(\n",
    "            name=\"temporal_clue\",\n",
    "            episodes=temporal_clue_episodes[:64],\n",
    "            samples_per_episode=3,\n",
    "            sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "        ),\n",
    "        # Eval(\n",
    "        #     name=\"zebra_grid\",\n",
    "        #     episodes=zebra_grid_episodes[:64],\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "        # Eval(\n",
    "        #     name=\"math\",\n",
    "        #     episodes=math_episodes[:64],\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "    ],\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_configs=[\n",
    "        TuneRecipeConfig(\n",
    "            shuffle=True,\n",
    "            num_output_chunks=4,\n",
    "            resume_from_checkpoint=False,\n",
    "            batch_size=1,\n",
    "            epochs=1,\n",
    "            max_steps_per_epoch=32,\n",
    "            optimizer=ComponentConfig(\n",
    "                \"torch.optim.AdamW\",\n",
    "                # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "                # \"bitsandbytes.optim.AdamW\",\n",
    "                # params=PLACEHOLDER,\n",
    "                lr=lr,\n",
    "                fused=True,\n",
    "            ),\n",
    "            loss=ComponentConfig(\n",
    "                PPOLoss,\n",
    "                policy_coef=0.0,\n",
    "                clip_epsilon=0.2,\n",
    "                tanh_log_policy_coef=0.8,\n",
    "                advantage_prediction_coef=0.0,\n",
    "                predicted_advantage_weight=0.0,\n",
    "                entropy_coef=0.0,\n",
    "                entropy_target=0.6,\n",
    "                entropy_target_coef=0.05,\n",
    "                kl_coef=0.05,\n",
    "                self_kl_coef=0.01,\n",
    "                peer_kl_coef=-0.01,\n",
    "                normalize_values=False,\n",
    "                normalize_value_predictions=False,\n",
    "                normalize_advantages=False,\n",
    "            ),\n",
    "            compile=False,\n",
    "            optimizer_in_bwd=False,\n",
    "            gradient_accumulation_steps=1,\n",
    "            enable_activation_checkpointing=True,\n",
    "            enable_activation_offloading=False,\n",
    "            custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "            log_every_n_steps=1,\n",
    "            log_peak_memory_stats=True,\n",
    "        )\n",
    "        for lr in (4e-6, 2e-6)\n",
    "    ],\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_chunked_prefill=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384,\n",
    "            preemption_mode=\"swap\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=100,\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        min_time_between_requests=0.0,\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0001 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5501f57ad484f7da9f9308a69f29198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7ee9aa084140638b201a60797252c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping temporal_clue evaluation due to expired patience (1 remaining episodes x 60.0 patience per episode = 60.0 seconds)\n",
      "Early stopping exploration due to expired patience (1 remaining episodes x 60 patience per episode = 60 seconds)\n",
      "Tuning model on 18 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2620382d99ef42d28198f436882e2e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 2 model files to /home/ubuntu/atreides/experiments/models/rl123/0002\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0002 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b629c2d483423096e977ae67efc352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d53adde74540c7ad2bb42bcd0734b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 7 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621b6c2a97f6446496991627bc011601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 3 model files to /home/ubuntu/atreides/experiments/models/rl123/0003\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0003 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ae60eb7ade4897bc21a21b64689b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b08f08b9b04e1191fd19cb842cfb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 8 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58391938fe0c4f78a4ec9428e56e2dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 4 model files to /home/ubuntu/atreides/experiments/models/rl123/0004\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0004 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28aa6f2692e64adebfc4c02bdfa60758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd14124fdfee4e76b2c4214567ad9161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 4 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fd5da29bdf4a15bebc676ce7c5219f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 5 model files to /home/ubuntu/atreides/experiments/models/rl123/0005\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0005 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722a9d1058c64a218e878d4dec521b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcb9878c8b54ab7a87d9938782e0c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 7 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd30054cdae447c4a9884a44b48c5f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 6 model files to /home/ubuntu/atreides/experiments/models/rl123/0006\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0006 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bf4c11a19544ed8de857a37e5be246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dc8e6d1c4b47219ada2797bab90bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 9 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eadeaa6c8474e95aeb7bec54a79a87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 7 model files to /home/ubuntu/atreides/experiments/models/rl123/0007\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0007 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49445b00dd654535830a9ce58bc31b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb72444149494914a872550a3373da39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 9 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37af55b4ffa4a0b8d9051cf56f6c362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 8 model files to /home/ubuntu/atreides/experiments/models/rl123/0008\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0008 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a3e735cc4a4d9cb5584658eb47bc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd364b9e5074431699fac2c66e135efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 27 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21351b6ac1a44f26be071ab06e6bb1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 9 model files to /home/ubuntu/atreides/experiments/models/rl123/0009\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0009 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2702538add674711aceff3c5219f1372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c700e2126944499d05a13d7fd212a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping temporal_clue evaluation due to expired patience (1 remaining episodes x 60.0 patience per episode = 60.0 seconds)\n",
      "Tuning model on 28 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236294af26ad4e16874ddd7edd672a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 10 model files to /home/ubuntu/atreides/experiments/models/rl123/0010\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0010 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be4e4ca5f844da093745382c3a5e046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ff5c88bc8b49e0814bb74ded6fe2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 40 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8272157050b4a29b2678fadea20ac9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 11 model files to /home/ubuntu/atreides/experiments/models/rl123/0011\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0011 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f558faff6bc54c03a3949045843aba8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cc0e7929734feb879fd4d0dc5fce69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 36 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bdbf5dbd8b44cdbe5188b7a1c60823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 12 model files to /home/ubuntu/atreides/experiments/models/rl123/0012\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0012 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e50d8f431fc4e68a1deb7f8b0e1347a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd75d0f5664845669da54ec0dd440ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 18 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a890e101da53460bab8b857c47b95ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 13 model files to /home/ubuntu/atreides/experiments/models/rl123/0013\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0013 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0865563c844d2492c6cf88781d8a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546a1c78db0f406190ff314fc05c536b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (1 remaining episodes x 60 patience per episode = 60 seconds)\n",
      "Tuning model on 10 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c657c1df799049abbc3f7f635265ed9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 14 model files to /home/ubuntu/atreides/experiments/models/rl123/0014\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0014 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2462e140df8c426db50e742d0d1f6eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb19cf3ac6e4ddebfd347eac85f6618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 11 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864f4e5c82404e2abd8451699da66163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 15 model files to /home/ubuntu/atreides/experiments/models/rl123/0015\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0015 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ac719f48634571b6696b4275654254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb1577d6fee4fe2b506d4c2f025152b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 10 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9308a2ff8e8e4d2ea79428d7e2efefd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 16 model files to /home/ubuntu/atreides/experiments/models/rl123/0016\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0016 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d8b4bdb0214002a9acf8d46aca4348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b1a1250d384d36899e76fca02f3de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 6 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a96ae2f4cf4f0888bdcae848184961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 17 model files to /home/ubuntu/atreides/experiments/models/rl123/0017\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0017 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1fe1774f60421e8492472da5a0aa6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbe5e7451af4916acfadf2a999a9eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 5 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fedba9df704ca898c1c7f0364b96ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 18 model files to /home/ubuntu/atreides/experiments/models/rl123/0018\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0018 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dbd27ea0f34a9990979241ec9c4fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e512ba7fb24f4484a85756f83edd023a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 5 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3710be4e4d45809ba7f44d4c0275fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 19 model files to /home/ubuntu/atreides/experiments/models/rl123/0019\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0019 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464187069d6440ad8f11e1268b93091c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0214d734faa4a2ea433f0633172d423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 8 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586b0c152c6f40599a0fb247c3fb3882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 20 model files to /home/ubuntu/atreides/experiments/models/rl123/0020\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0020 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b6d92b51b449e487548e1f978ad18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4b9c91b787468e9fd08c5ca3a9e740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 8 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d825365acb340dd88a2b8cb77846b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 21 model files to /home/ubuntu/atreides/experiments/models/rl123/0021\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0021 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dedea5f8fb94661a2330c3c74830cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd819ac847674fc8b98ac26e4f470481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 8 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d474f0cc03b44ffa8fdd9f912494aa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 22 model files to /home/ubuntu/atreides/experiments/models/rl123/0022\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0022 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a9705deb7846f4ac14eba1c8827241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b347dda1c1b6492d8b70ee36549088a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 13 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f2b45b1f82422893f43a355bcc4ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 23 model files to /home/ubuntu/atreides/experiments/models/rl123/0023\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0023 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a11e9e874b841cda2e34d59a1c1be09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe86cd87eac4cacb55684b1103d650e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 10 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7908c62e7b45ab92add40194f1a6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 24 model files to /home/ubuntu/atreides/experiments/models/rl123/0024\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0024 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392053d86399462bbba20e50f8e88073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c962dbe1d5440818ef2bf12499ac655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 11 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af08f6ba364742feadcc51e02cd158b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 25 model files to /home/ubuntu/atreides/experiments/models/rl123/0025\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0025 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b168d592c0664ad69486845befc8afd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2233fa289d47e0baec4d46159e46d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 11 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454c09e8e3774df8a2f9f5eb302843d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 26 model files to /home/ubuntu/atreides/experiments/models/rl123/0026\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0026 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8298283851654c538f84cc03309bc8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0484780d8f524b05a4c9b091583c48d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 11 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ab78e74703498ebf5b8a173a8fe3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 27 model files to /home/ubuntu/atreides/experiments/models/rl123/0027\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0027 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d442dbf6dc4a4e109563ebb77ca31d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da156bfa58cf4f05975f887f1870d64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 9 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af490f80dc647b18f63eacd5649d0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 28 model files to /home/ubuntu/atreides/experiments/models/rl123/0028\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0028 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59849d4f8a944982beb7632c7b7e1ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f6b5737c094b5f8960b25e894afe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 6 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f9aab5935b46ed8f6739c3ea41cecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 29 model files to /home/ubuntu/atreides/experiments/models/rl123/0029\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0029 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c426a35a4742488403dab995aa71f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48742be37bbb4bc6820de8bcdb51bcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 11 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302808541b054ccc90b4fa0a7159bbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 30 model files to /home/ubuntu/atreides/experiments/models/rl123/0030\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0030 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb36f4123ac947069486c727ec0ad19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4ee6a63c2c4729927a3dc1309bce53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping temporal_clue evaluation due to expired patience (1 remaining episodes x 60.0 patience per episode = 60.0 seconds)\n",
      "Tuning model on 9 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f325dc22ac1c4e7fbbb124803570b4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 31 model files to /home/ubuntu/atreides/experiments/models/rl123/0031\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0031 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a689228b5064b26b29aeca6e8b3b52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f216775a501d4424a0306390caa46c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 15 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4fc4e6cea34715802cb15ca8d69663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 32 model files to /home/ubuntu/atreides/experiments/models/rl123/0032\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0032 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ef9c51c540448c91f1c8836d30a454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f2f8ed15584d46824bca54eb53de71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 7 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4968f97a5c4dafbd0d89657acbe58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 33 model files to /home/ubuntu/atreides/experiments/models/rl123/0033\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0033 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c440ad96a664478921fac0249d985a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abc6808318e4bb0894ad772842e7934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping temporal_clue evaluation due to expired patience (1 remaining episodes x 60.0 patience per episode = 60.0 seconds)\n",
      "Tuning model on 14 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e74ff5740540a5b9e7058c56092a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 34 model files to /home/ubuntu/atreides/experiments/models/rl123/0034\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0034 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b5a9f0f38f46008546b3982f4c01c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aa38e020264b0aa21ec2af629b7e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 6 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3ce7baa10640a38c403046338e4514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 35 model files to /home/ubuntu/atreides/experiments/models/rl123/0035\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0035 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7824c630d343afae91bf96863a995c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd491dd57e345da938637cff7f638c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (1 remaining episodes x 60 patience per episode = 60 seconds)\n",
      "Tuning model on 9 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c5064387e140a9a9c6c1a05710b6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 36 model files to /home/ubuntu/atreides/experiments/models/rl123/0036\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0036 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0423a20f8a2048a7936e62da739e0f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e2ed8441ce43f99a300e2769de425e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 14 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e891957109241fdb627ae785a99edde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 37 model files to /home/ubuntu/atreides/experiments/models/rl123/0037\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0037 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28cc92857ae40a3b841a5697feee25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a690c9b5c54923836e3c3861ff4074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping temporal_clue evaluation due to expired patience (1 remaining episodes x 60.0 patience per episode = 60.0 seconds)\n",
      "Tuning model on 6 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817644b572bf4469bd06fd5924f05603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 38 model files to /home/ubuntu/atreides/experiments/models/rl123/0038\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0038 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9abbb5f0e644ce5be032cdb0496d386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ebc528220d4ef78191b23b1c76bac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 8 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead87a60adc346e3bc9f967189299089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 39 model files to /home/ubuntu/atreides/experiments/models/rl123/0039\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0039 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5261214b5c404ef590e3b5e56c5f1779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587f32ae2aac4c6e8f258a9e7dacb6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 7 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a093b2eed34d459d573f2ad8891039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 40 model files to /home/ubuntu/atreides/experiments/models/rl123/0040\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0040 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c476396dd84ad3a3e2a75dd053e09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30dbe04e78c466d9dd1b61e2750ba89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 12 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6311b497c8ec450782bc39672087867d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 41 model files to /home/ubuntu/atreides/experiments/models/rl123/0041\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0041 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f5c67515a24b7fb294d9e1ac97bd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb6c7fb15e44a6d9d4374ed48ea891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 17 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0cc4ad3e7a42879e5e8a1a577682e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 42 model files to /home/ubuntu/atreides/experiments/models/rl123/0042\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0042 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d857769f2a54f1598fa6226f40afc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d95e19e952b4256981c96c4271b04d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 17 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7886de21134b729146958f8cd2be35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 43 model files to /home/ubuntu/atreides/experiments/models/rl123/0043\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0043 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287b366b729248c5970e4e4dac8f868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7526662dda46b696d92a5388d8092f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 14 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b13e0a4a2b4949822ce9734b8f8c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 44 model files to /home/ubuntu/atreides/experiments/models/rl123/0044\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0044 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061ed9d8b1e84659966e07f1bc673d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0ca03696224392bacf0f2c2ee296d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 15 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97c12b1362d4c99909a9a25407d4dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 45 model files to /home/ubuntu/atreides/experiments/models/rl123/0045\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0045 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5637c904fd314dc29f6db3b2c2463fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a001dc1b610341a1b3f547c0cfd2d676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 23 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c57487b3e643588dedf9190d5a3da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 46 model files to /home/ubuntu/atreides/experiments/models/rl123/0046\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0046 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7202656bca8446fa82b526b41977c7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60669e176c04cf193538e7f9fa536a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 16 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd658b7878e42ee90bd32ce0b1b5916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 47 model files to /home/ubuntu/atreides/experiments/models/rl123/0047\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0047 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9192dc1b864b8ab98afd68cac8bf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee0b0a1b9154b4b96e9f563757d9471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 15 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfc67f0d78a43bca23a284c870612ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 48 model files to /home/ubuntu/atreides/experiments/models/rl123/0048\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0048 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e6d918eae24ed58d8164c03827e99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fdef6725de459eb65fb22587c81fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 7 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755064b9639f48adbe428de80b364df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 49 model files to /home/ubuntu/atreides/experiments/models/rl123/0049\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0049 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a5842958aa4022801476721d405cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f2ac320a484e0cae3e56c7a50e91c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 5 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8497c8d8384e9aa6e12700dd91d1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 50 model files to /home/ubuntu/atreides/experiments/models/rl123/0050\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0050 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff147afa74cb4462b08ab4325d6ed3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98d6962a90b48e992ccbd07239d1d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 4 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9df397a3ec44376bb3f56f3463487e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 51 model files to /home/ubuntu/atreides/experiments/models/rl123/0051\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0051 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e3d691cbe6477fb5511620667b42f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=50, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d114aa990172485b9ef01f0786d38041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0f5a3b43ef47d6b790a20e3eca7540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping temporal_clue evaluation due to expired patience (0 remaining episodes x 60.0 patience per episode = 0.0 seconds)\n",
      "Early stopping exploration due to expired patience (1 remaining episodes x 60 patience per episode = 60 seconds)\n",
      "Tuning model on 4 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a0c76a952e47c399db74f24e72e83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 52 model files to /home/ubuntu/atreides/experiments/models/rl123/0052\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0052 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8530eff021402a8239af2d65a852a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffd06d5264747399cce3641619b58e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 4 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a52b1e7a5f4ad2bc5a56447ddfe720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 53 model files to /home/ubuntu/atreides/experiments/models/rl123/0053\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0053 --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16663950ace84ffc872ee900407748c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a1e6d2d01147f5b02cd251197622d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 4 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be87b96a97144e449448d4499adda2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 54 model files to /home/ubuntu/atreides/experiments/models/rl123/0054\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0054 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa40ee8f45d44fd7982773516b12b58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c831551dd3ae41a4b85754a070f0c88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 5 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9fbb5c2300483096f00d1a360566a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 55 model files to /home/ubuntu/atreides/experiments/models/rl123/0055\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0055 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2be7f0f73f04be987e13235a577dd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47eab56a0569461193ae8fcdb6770e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 4 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c179798d2d8944909de9729671bd5499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 56 model files to /home/ubuntu/atreides/experiments/models/rl123/0056\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0056 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ddc27a32344ff3b86da1236440cdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f03b9f287eb41299837c4bd14075c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 7 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d75c5ee7ef446a9d61c6f0a9c63281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 57 model files to /home/ubuntu/atreides/experiments/models/rl123/0057\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0057 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc1580e030a4b8c933d2442425907cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59a13e02f3d4972b5cec2c42204f2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 5 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c40c8bb3d048e0868e6d66127f26c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 58 model files to /home/ubuntu/atreides/experiments/models/rl123/0058\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl123/0058 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a2d76c635346138217c12cb7a4089f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd75f41a6ec4354a2a40ae4d5832a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 7 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl123/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3721d58c8d174e7db537af77901f3fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=50, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 45 sequences\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl121/cuda:1/config.yaml\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl121/cuda:0/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:torchtune.utils._logging:Training is not distributed. If you want to train on multiple GPUs and are using the tune CLI, specify --nnodes 1 and --nproc_per_node [num_gpus]\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: lib.rl.mlp_head_checkpointer.MLPHeadCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl121/cuda:1\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl121/tensors\n",
      "  num_sequences: 44\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  advantage_prediction_coef: 0.0\n",
      "  clip_epsilon: 0.2\n",
      "  convergence_coef: 0.0001\n",
      "  divergence_coef: 0.0001\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.6\n",
      "  entropy_target_coef: 0.05\n",
      "  kl_coef: 0.05\n",
      "  model_id: 0\n",
      "  normalize_advantages: false\n",
      "  normalize_value_predictions: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  predicted_advantage_weight: 0.0\n",
      "  tanh_log_policy_coef: 0.8\n",
      "max_steps_per_epoch: 32\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl121\n",
      "  name: rl121\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 2.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl121\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 3851462596\n",
      "shuffle: true\n",
      "\n",
      "DEBUG:torchtune.utils._logging:Training is not distributed. If you want to train on multiple GPUs and are using the tune CLI, specify --nnodes 1 and --nproc_per_node [num_gpus]\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: lib.rl.mlp_head_checkpointer.MLPHeadCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl121/cuda:0\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl121/tensors\n",
      "  num_sequences: 44\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  advantage_prediction_coef: 0.0\n",
      "  clip_epsilon: 0.2\n",
      "  convergence_coef: 0.0001\n",
      "  divergence_coef: 0.0001\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.6\n",
      "  entropy_target_coef: 0.05\n",
      "  kl_coef: 0.05\n",
      "  model_id: 0\n",
      "  normalize_advantages: false\n",
      "  normalize_value_predictions: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  predicted_advantage_weight: 0.0\n",
      "  tanh_log_policy_coef: 0.8\n",
      "max_steps_per_epoch: 32\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl121\n",
      "  name: rl121\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl121\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 4035555144\n",
      "shuffle: true\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 4035555144. Local seed is seed + rank = 4035555144 + 0\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 3851462596. Local seed is seed + rank = 3851462596 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20250107_164333-rl121\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl121\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl121\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20250107_164333-rl121\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl121\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl121\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:No MLP head weights found, will initialize from scratch\n",
      "INFO:torchtune.utils._logging:No MLP head weights found, will initialize from scratch\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 3.18 secs\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 3.18 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.03 GiB\n",
      "\tGPU peak memory reserved: 15.16 GiB\n",
      "\tGPU peak memory active: 15.03 GiB\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.03 GiB\n",
      "\tGPU peak memory reserved: 15.16 GiB\n",
      "\tGPU peak memory active: 15.03 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|32|Loss: 0.0227: 100%|██████████| 32/32 [06:50<00:00, 12.46s/it, conv=0.2585, div=0.0000, entropy=0.3321, entropy_target=0.2679, kl_div=0.1435, tanh_log_policy=0.0027] INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 0.00 secs\n",
      "INFO:torchtune.utils._logging:MLP head checkpoint saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:1/mlp_head_0.pt.ignore\n",
      "1|32|Loss: 0.0123: 100%|██████████| 32/32 [06:50<00:00, 12.47s/it, conv=0.6669, div=0.0000, entropy=0.6659, entropy_target=0.0659, kl_div=0.2278, tanh_log_policy=-0.0031]INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 0.00 secs\n",
      "INFO:torchtune.utils._logging:MLP head checkpoint saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:0/mlp_head_0.pt.ignore\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:1/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:0/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:1/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:0/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:1/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:0/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:1/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 22.18 secs\n",
      "1|32|Loss: 0.0227: 100%|██████████| 32/32 [07:13<00:00, 13.54s/it, conv=0.2585, div=0.0000, entropy=0.3321, entropy_target=0.2679, kl_div=0.1435, tanh_log_policy=0.0027]\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl121/cuda:0/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 22.10 secs\n",
      "1|32|Loss: 0.0123: 100%|██████████| 32/32 [07:13<00:00, 13.53s/it, conv=0.6669, div=0.0000, entropy=0.6659, entropy_target=0.0659, kl_div=0.2278, tanh_log_policy=-0.0031]\n",
      "wandb:                                                                                \n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                 advantage ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:      advantage_prediction ▆▁▂▃▃▂▁▁▃▂▁▂▂█▁▂▂▁▁▂▂▁▁▃▁▆▁▂▇▁▁▁\n",
      "wandb:                        ce ▃▃▁▁▁▄█▃▄▃▃▄▇▄▁▃▅▅▂▄▅▃▄▃▂▅▂▃▄▄▃▂\n",
      "wandb:               convergence ▃▂▂▃▃▃█▃▄▃▃▅█▃▁▂▅▅▂▅▄▃▃▄▃▅▃▃▄▃▃▁\n",
      "wandb:                divergence ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:                   entropy ▂▂▂▂▃▃█▄▅▄▃▅█▃▂▂▆▆▃▆▅▄▅▄▄▆▄▄▅▄▄▁\n",
      "wandb:            entropy_target ▅▇▆▅▄▄▇▂▁▂▄▂▇▄▇▅▄▄▅▄▁▃▁▁▃▄▂▂▂▂▂█\n",
      "wandb:               exploration ▃▂▂▃▃▃█▃▄▃▃▅█▃▁▂▅▅▂▅▄▃▃▄▃▅▃▃▄▃▃▁\n",
      "wandb:               global_step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "wandb:                    kl_div ▄▃▃▃▄▄▅▅▆▅▄▆▆▄▁▂██▃▇▅▅▅▅▅█▄▄▇▅▅▁\n",
      "wandb:                      loss ██▇▆▆▃▇▃▁▂▅▂▇▁█▄▅█▃▄▃▃▃▁▅▂▂▂▃▄▂█\n",
      "wandb:                        lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:        peak_memory_active ▁███████████████████████████████\n",
      "wandb:         peak_memory_alloc ▁███████████████████████████████\n",
      "wandb:      peak_memory_reserved ▁█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "wandb:                    policy ▅▂▂▄▃▃▃▂▆▄▂▂▃█▁▄▄▂▄▅▃▂▂▄▁▄▃▄▃▃▃▂\n",
      "wandb:                 reinforce ▃▆▆▇▄▄▂▆▂▄▆▆▇▁█▄▅█▄▃▆▆▇▂█▂▆▆▆▆▅▇\n",
      "wandb:           tanh_log_policy █▇▇▇█▄▄▅▃▃▆▃▄▁█▃▃▇▃▃▆▅▇▄▇▁▄▄▃▇▄▇\n",
      "wandb: tokens_per_second_per_gpu ▁▆█▇██▆▆▄▆▇▆▆▇▇▇▄▇▆▇▆▇▅▆▇▇▇▇▆▆▇▇\n",
      "wandb:          unclipped_policy ▅▂▂▄▄▃▃▂▆▄▁▂▃█▁▄▄▂▄▄▃▂▂▄▁▄▃▃▂▃▃▂\n",
      "wandb:                     value ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:               weighted_ce ▃▆▆▇▄▄▂▆▂▄▆▆▇▁█▄▅█▄▃▆▆▇▂█▂▆▆▆▆▅▇\n",
      "wandb:          weighted_entropy ▅▃▃▃▃▅▆▃█▆▃▃▃█▂▅▄▁▅▆▃▃▂▇▁▇▄▃▂▃▄▃\n",
      "wandb:           weighted_kl_div ▅▃▃▂▅▄▃▃▇▅▃▄▃█▁▄▄▂▄▄▄▃▂▅▁▇▃▃▅▂▃▂\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                 advantage 0\n",
      "wandb:      advantage_prediction 0.18909\n",
      "wandb:                        ce 0.83709\n",
      "wandb:               convergence 0.25847\n",
      "wandb:                divergence 0\n",
      "wandb:                   entropy 0.33211\n",
      "wandb:            entropy_target 0.26789\n",
      "wandb:               exploration 0.25847\n",
      "wandb:               global_step 32\n",
      "wandb:                    kl_div 0.14347\n",
      "wandb:                      loss 0.02274\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 68.34943\n",
      "wandb:         peak_memory_alloc 68.34943\n",
      "wandb:      peak_memory_reserved 70.20312\n",
      "wandb:                    policy -0.02692\n",
      "wandb:                 reinforce 0.00636\n",
      "wandb:           tanh_log_policy 0.00268\n",
      "wandb: tokens_per_second_per_gpu 1068.96777\n",
      "wandb:          unclipped_policy -0.04097\n",
      "wandb:                     value 0\n",
      "wandb:               weighted_ce 0.00636\n",
      "wandb:          weighted_entropy -0.01251\n",
      "wandb:           weighted_kl_div -0.00539\n",
      "wandb: \n",
      "wandb: 🚀 View run rl121 at: https://wandb.ai/bradhilton/torchtune/runs/rl121\n",
      "wandb: ⭐️ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20250107_164333-rl121/logs\n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                 advantage ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:      advantage_prediction ▁▁▂▂▁▁▂▆▁▁▁▂▃▇▂▇█▄▂▃▁▂▃▁▁▄▁▂▁▂▃▂\n",
      "wandb:                        ce ▄▃▃▂▃▂▂▄▂▃▅▅▃▃▁▄▃▄▄██▂▃▂▄▄▂▅▃▂▁▅\n",
      "wandb:               convergence ▂▃▄▃▃▂▂▅▁▂▅▃▃▂▁▃▄▄▄██▄▂▃▄▁▂▄▃▁▃▆\n",
      "wandb:                divergence ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:                   entropy ▂▄█▆▆▄▃▆▁▂▆▃▃▂▁▃▄▄▆██▄▃▃▄▁▂▅▃▁▃▅\n",
      "wandb:            entropy_target ▅▁▇▄▄▁▂▃▆▅▄▃▂▅▆▂▁▂▄██▁▄▃▁▆▄▃▃▇▃▃\n",
      "wandb:               exploration ▂▃▄▃▃▂▂▅▁▂▅▃▃▂▁▃▄▄▄██▄▂▃▄▁▂▄▃▁▃▆\n",
      "wandb:               global_step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "wandb:                    kl_div ▄▅█▆▆▄▄█▁▂█▄▅▃▂▆▇▇▇▆▅▅▄▅▅▂▃█▄▁▃▆\n",
      "wandb:                      loss ▇▄▇▄▅▂▄▅▅▆▇▅▃▇▇▁▆▁▅█▇▃▄▅▃▃▄▅▄▅▆▄\n",
      "wandb:                        lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:        peak_memory_active ▁███████████████████████████████\n",
      "wandb:         peak_memory_alloc ▁███████████████████████████████\n",
      "wandb:      peak_memory_reserved ▁█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "wandb:                    policy ▃▃▆▅▄▃▅▅▄▂▂▄▄▇▃█▅█▆▄▃▃▅▁▃▅▃▅▂▅▄▅\n",
      "wandb:                 reinforce ▇▆▅▅▆▆▇▃▅▆▇▆▅▄▆▁█▃▄▇▄█▅▇▇▅▅▅▆▅▅▇\n",
      "wandb:           tanh_log_policy ▆▅▄▃▅▄▅▃▃▆▆▆▄▇▆▁█▁▃▄▃▄▄▆▅▁▄▄▄▄▇▄\n",
      "wandb: tokens_per_second_per_gpu ▁▆▇▆▇▇▇▇▆▇▇▆▇▇▇▆▆▅▇▆▆▆█▇▇▆▇▄▇▇█▆\n",
      "wandb:          unclipped_policy ▄▃▆▆▄▄▅▅▅▂▂▄▄█▄█▅█▆▄▄▃▅▁▃▅▄▅▃▆▅▅\n",
      "wandb:                     value ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:               weighted_ce ▇▆▅▅▆▆▇▃▅▆▇▆▅▄▆▁█▃▄▇▄█▅▇▇▅▅▅▆▅▅▇\n",
      "wandb:          weighted_entropy ▂▃▅▅▃▃▃▆▄▂▁▃▄▅▂█▂▆▅▃▅▁▄▁▂▄▄▃▃▄▃▂\n",
      "wandb:           weighted_kl_div ▂▃▄▄▃▃▃▆▃▂▂▃▅▄▃█▂▇▄▃▃▂▄▁▂▄▄▄▃▄▄▃\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                 advantage 0\n",
      "wandb:      advantage_prediction 0.71384\n",
      "wandb:                        ce 1.07111\n",
      "wandb:               convergence 0.66694\n",
      "wandb:                divergence 0\n",
      "wandb:                   entropy 0.66587\n",
      "wandb:            entropy_target 0.06587\n",
      "wandb:               exploration 0.66694\n",
      "wandb:               global_step 32\n",
      "wandb:                    kl_div 0.22777\n",
      "wandb:                      loss 0.01227\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 68.34936\n",
      "wandb:         peak_memory_alloc 68.34936\n",
      "wandb:      peak_memory_reserved 70.20312\n",
      "wandb:                    policy 0.07711\n",
      "wandb:                 reinforce 0.00517\n",
      "wandb:           tanh_log_policy -0.0031\n",
      "wandb: tokens_per_second_per_gpu 1047.85278\n",
      "wandb:          unclipped_policy 0.02108\n",
      "wandb:                     value 0\n",
      "wandb:               weighted_ce 0.00517\n",
      "wandb:          weighted_entropy -0.0127\n",
      "wandb:           weighted_kl_div 0.00376\n",
      "wandb: \n",
      "wandb: 🚀 View run rl121 at: https://wandb.ai/bradhilton/torchtune/runs/rl121\n",
      "wandb: ⭐️ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20250107_164333-rl121/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 1 model files to /home/ubuntu/atreides/experiments/models/rl121/0001\n",
      "Saved iteration 2 model files to /home/ubuntu/atreides/experiments/models/rl121/0002\n"
     ]
    }
   ],
   "source": [
    "await trainer.tune(trainer.explore_results[-1], verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NousResearch/Hermes-2-Theta-Llama-3-8B',\n",
       " 'NousResearch/Hermes-2-Theta-Llama-3-8B']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = await trainer.get_completion_sampler_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n"
     ]
    }
   ],
   "source": [
    "await trainer.stop_vllms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NousResearch/Hermes-2-Theta-Llama-3-8B'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.latest_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<lib.rl.completion_sampler.CompletionSampler at 0x76aecd92a8a0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.samplers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
