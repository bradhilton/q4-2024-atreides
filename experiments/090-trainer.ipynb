{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.langfuse import langfuse\n",
    "# langfuse.enabled = False\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "import random\n",
    "import re\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class TemporalCluePuzzle(TypedDict):\n",
    "    num_clues: int\n",
    "    prompt: str\n",
    "    solution: dict[str, str]\n",
    "\n",
    "\n",
    "temporal_clue_puzzles: list[TemporalCluePuzzle] = json.load(\n",
    "    open(\"./data/temporal-clue-puzzles.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "chain_of_thought_examples: list[dict[str, str]] = json.load(\n",
    "    open(\"./data/chain-of-thought-examples.json\")\n",
    ")\n",
    "chain_of_thought_examples.pop(6)\n",
    "chain_of_thought_examples.pop(3)\n",
    "\n",
    "\n",
    "def get_episode(puzzle: TemporalCluePuzzle, example: dict[str, str]) -> Episode:\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in completions:\n",
    "            content = completion.last_assistant_message.get(\"content\")\n",
    "            assert isinstance(content, str)\n",
    "            num_correct = 0\n",
    "            for key, value in puzzle[\"solution\"].items():\n",
    "                if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "                    match = matches[-1]\n",
    "                    if match.strip().lower() == value.lower():\n",
    "                        num_correct += 1\n",
    "            completion.commit(reward=num_correct / len(puzzle[\"solution\"]))\n",
    "\n",
    "    return Episode(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": puzzle[\"prompt\"].replace(\n",
    "                    \"Fill out your final answers in the following format:\",\n",
    "                    \"After verifiably finding all the correct answers, fill out your final answers in the following format:\",\n",
    "                ),\n",
    "            },\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": \"Let's think this through step by step...\",\n",
    "            # },\n",
    "        ],\n",
    "        on_sample=on_sample,\n",
    "        examples=[\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example[\"chain_of_thought\"]\n",
    "                + (example[\"answer\"] and f\"\\n\\n---\\n\\n{example['answer']}\"),\n",
    "            },\n",
    "            # {\"role\": \"user\", \"content\": example[1][\"prompt\"]},\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": example[1][\"chain_of_thought\"]\n",
    "            #     + (example[1][\"answer\"] and f\"\\n\\n---\\n\\n{example[1]['answer']}\"),\n",
    "            # },\n",
    "        ],\n",
    "        # logprobs_mask=Clue.get_logprobs_mask(),\n",
    "    )\n",
    "\n",
    "\n",
    "temporal_clue_episodes = [\n",
    "    get_episode(puzzle, example)\n",
    "    for puzzle, example in zip(temporal_clue_puzzles, cycle(chain_of_thought_examples))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "zebra_grid_questions = pl.read_parquet(\n",
    "    \"hf://datasets/allenai/ZebraLogicBench-private/grid_mode/test-00000-of-00001.parquet\"\n",
    ").to_dicts()\n",
    "random.shuffle(zebra_grid_questions)\n",
    "\n",
    "\n",
    "def get_episode(question: dict) -> Episode:\n",
    "    prompt = f\"\"\"{question[\"puzzle\"]}\n",
    "Fill in the grid with the correct values:\n",
    "\n",
    "| {' | '.join(question[\"solution\"][\"header\"])} |\n",
    "| {' | '.join([\"-\" * len(header) for header in question[\"solution\"][\"header\"]])} |\n",
    "\"\"\"\n",
    "\n",
    "    for _ in question[\"solution\"][\"rows\"]:\n",
    "        prompt += f\"| {' | '.join([\" \" * len(header) for header in question[\"solution\"][\"header\"]])} |\\n\"\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"\\| \" + r\"\\|\".join(r\"(.*?)\" for _ in question[\"solution\"][\"header\"]) + r\" \\|\"\n",
    "    )\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]):\n",
    "        for completion in completions:\n",
    "            assert \"content\" in completion.last_assistant_message and isinstance(\n",
    "                completion.last_assistant_message[\"content\"], str\n",
    "            )\n",
    "            num_cells = sum(len(row) for row in question[\"solution\"][\"rows\"])\n",
    "            num_correct = 0\n",
    "            for match, row in zip(\n",
    "                re.findall(pattern, completion.last_assistant_message[\"content\"])[\n",
    "                    -len(question[\"solution\"][\"rows\"]) :\n",
    "                ],\n",
    "                question[\"solution\"][\"rows\"],\n",
    "            ):\n",
    "                for cell, value in zip(match, row):\n",
    "                    if cell.strip().lower() == value.lower():\n",
    "                        num_correct += 1\n",
    "            completion.commit(reward=num_correct / num_cells)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "zebra_grid_episodes = [get_episode(question) for question in zebra_grid_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "math_questions = list(\n",
    "    load_dataset(\"lighteval/MATH\", \"all\")[\"train\"].to_iterable_dataset()  # type: ignore\n",
    ")\n",
    "random.shuffle(math_questions)\n",
    "\n",
    "\n",
    "question_solution = None\n",
    "pattern = re.compile(r\"\\\\boxed{([^}]+)}\")\n",
    "\n",
    "\n",
    "def get_episode(question: dict) -> Episode:\n",
    "    prompt = (\n",
    "        f\"{question['problem']}\\n\\n\"\n",
    "        \"Solve this math problem and show your work. Your final answer MUST be \"\n",
    "        \"formatted in a LaTeX box using \\\\boxed{{}}. For example: \"\n",
    "        \"$1+1=\\\\boxed{{2}}$\\n\\n\"\n",
    "        \"You can submit multiple attempts. Each attempt should end with a boxed \"\n",
    "        \"answer. Your last answer will be weighted the most, but you can get \"\n",
    "        \"partial credit if an earlier answer is correct. If after multiple \"\n",
    "        \"attempts you decide an earlier answer is the correct one, just submit \"\n",
    "        \"it again to get full credit.\"\n",
    "    )\n",
    "\n",
    "    global question_solution\n",
    "    question_solution = question[\"solution\"]\n",
    "    solution = re.search(pattern, question[\"solution\"])\n",
    "    assert solution is not None, question[\"solution\"]\n",
    "    solution = solution.group(1)\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]):\n",
    "        for completion in completions:\n",
    "            content = completion.last_assistant_message.get(\"content\")\n",
    "            assert isinstance(content, str)\n",
    "            solutions = [\n",
    "                match.group(1) for match in re.finditer(r\"\\\\boxed{([^}]+)}\", content)\n",
    "            ][::-1]\n",
    "            try:\n",
    "                reward = 0.9 ** solutions.index(solution)\n",
    "            except ValueError:\n",
    "                reward = 0\n",
    "            completion.commit(reward=reward)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "math_episodes = [\n",
    "    get_episode(question)\n",
    "    for question in math_questions[:2048]\n",
    "    if re.search(pattern, question[\"solution\"]) is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import cycle\n",
    "from lib.rl.completion import SplitMethod\n",
    "from lib.rl.completion_sampler import (\n",
    "    CompletionSampler,\n",
    "    SamplingKwargs,\n",
    "    CompletionSamplerPool,\n",
    ")\n",
    "from lib.rl.trainer import ExploreImpl, ExploreOptions\n",
    "from lib.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultExploreImpl(ExploreImpl):\n",
    "    explore_options: ExploreOptions\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        def done_callback(task: asyncio.Task[Episode]) -> None:\n",
    "            try:\n",
    "                done_episodes.put_nowait(task.result())\n",
    "            except BaseException as exception:\n",
    "                done_episodes.put_nowait(exception)\n",
    "\n",
    "        priority = 1\n",
    "        while episode := await ready_episodes.get():\n",
    "            asyncio.create_task(\n",
    "                self._explore_episode(\n",
    "                    completion_sampler, tokenizer, episode, update_progress, priority\n",
    "                )\n",
    "            ).add_done_callback(done_callback)\n",
    "            priority += 1\n",
    "\n",
    "    async def _explore_episode(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        episode: Episode,\n",
    "        update_progress: Callable[[float], None],\n",
    "        priority: int,\n",
    "    ) -> Episode:\n",
    "        for _ in range(self.explore_options.iterations):\n",
    "            await episode.sample_completions(\n",
    "                completion_sampler=completion_sampler,\n",
    "                tokenizer=tokenizer,\n",
    "                num_parents=self.explore_options.num_parents,\n",
    "                branch_factor=self.explore_options.branch_factor,\n",
    "                get_recovery_pattern=self.explore_options.get_recovery_pattern,\n",
    "                max_splits_per_completion=self.explore_options.max_split_points\n",
    "                or self.explore_options.num_parents,\n",
    "                priority=priority,\n",
    "                sample_probability_power=self.explore_options.get_sample_probability_power(),\n",
    "                sampling_kwargs=self.explore_options.sampling_kwargs,\n",
    "                split_by=self.explore_options.split_method,\n",
    "                split_separators=self.explore_options.split_separators,\n",
    "            )\n",
    "            update_progress(1 / self.explore_options.iterations)\n",
    "        return episode\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleExploreImpl(ExploreImpl):\n",
    "    num_samples: int\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        while episode := await ready_episodes.get():\n",
    "            task = asyncio.create_task(\n",
    "                episode.sample_completions(\n",
    "                    completion_sampler,\n",
    "                    tokenizer,\n",
    "                    num_parents=1,\n",
    "                    branch_factor=self.num_samples,\n",
    "                    sampling_kwargs=self.sampling_kwargs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            def done_callback(_: asyncio.Task[bool], episode=episode) -> None:\n",
    "                try:\n",
    "                    done_episodes.put_nowait(episode)\n",
    "                    update_progress(1)\n",
    "                except BaseException as e:\n",
    "                    done_episodes.put_nowait(e)\n",
    "\n",
    "            task.add_done_callback(done_callback)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TreeExploreImpl(ExploreImpl):\n",
    "    branch_factor: int\n",
    "    depth: int\n",
    "    num_roots: int | None = None\n",
    "    best_leaf_sampling_temperature: float = 0.01\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "    split_method: SplitMethod = \"count\"\n",
    "    split_separators: set[str] = field(default_factory=set)\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler_pool: CompletionSamplerPool,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        async def expand(\n",
    "            episode: Episode, priority: int, completion_sampler: CompletionSampler\n",
    "        ) -> None:\n",
    "            model = await completion_sampler.get_model()\n",
    "            num_roots = self.num_roots or self.branch_factor\n",
    "\n",
    "            # If there are existing trajectories, we'll sample one\n",
    "            # of the best ones to stabilize and improve training.\n",
    "            leaves = list(episode.completion.leaves(models=None))\n",
    "            if leaves:\n",
    "                best_leaf = random.choices(\n",
    "                    leaves,\n",
    "                    weights=[\n",
    "                        np.exp(\n",
    "                            leaf.cumulative_reward()\n",
    "                            / self.best_leaf_sampling_temperature\n",
    "                        )\n",
    "                        for leaf in leaves\n",
    "                    ],\n",
    "                    k=1,\n",
    "                )[0]\n",
    "                best_leaf = best_leaf.recursive_copy(model=model)\n",
    "                best_leaf.commit()\n",
    "                while best_leaf.parent and best_leaf.parent.parent is None:\n",
    "                    best_leaf = best_leaf.merge()\n",
    "            else:\n",
    "                best_leaf = None\n",
    "\n",
    "            pending: set[asyncio.Task] = {\n",
    "                asyncio.create_task(\n",
    "                    episode.sample_completions(\n",
    "                        completion_sampler,\n",
    "                        tokenizer,\n",
    "                        num_parents=1,\n",
    "                        branch_factor=num_roots - 1 if best_leaf else num_roots,\n",
    "                        priority=priority,\n",
    "                        sampling_kwargs=self.sampling_kwargs,\n",
    "                        split_by=self.split_method,\n",
    "                        split_separators=self.split_separators,\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "\n",
    "            num_leaves = 0\n",
    "            while pending:\n",
    "                finished, pending = await asyncio.wait(\n",
    "                    pending, return_when=asyncio.FIRST_COMPLETED\n",
    "                )\n",
    "                for task in finished:\n",
    "                    try:\n",
    "                        task.result()\n",
    "                    except BaseException as e:\n",
    "                        await done_episodes.put(e)\n",
    "                        return\n",
    "                _num_leaves = 0\n",
    "                for leaf in episode.completion.leaves(models={model}):\n",
    "                    _num_leaves += 1\n",
    "                    num_partitions = self.depth - leaf.depth() + 1\n",
    "                    if num_partitions > 1:\n",
    "                        parents = list(\n",
    "                            leaf.split(\n",
    "                                by=self.split_method,\n",
    "                                at=(\n",
    "                                    split / num_partitions\n",
    "                                    for split in range(1, num_partitions)\n",
    "                                ),\n",
    "                                separators=self.split_separators,\n",
    "                                cache=True,\n",
    "                            )\n",
    "                        )[:-1]\n",
    "                        for parent in parents:\n",
    "                            pending.add(\n",
    "                                asyncio.create_task(\n",
    "                                    episode._sample_completions(\n",
    "                                        parent=parent,\n",
    "                                        model=model,\n",
    "                                        completion_sampler=completion_sampler,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        branch_factor=self.branch_factor,\n",
    "                                        fork_decay=1.0,\n",
    "                                        recovery_pattern=None,\n",
    "                                        split_separators=self.split_separators,\n",
    "                                        sampling_kwargs=self.sampling_kwargs\n",
    "                                        or SamplingKwargs(),\n",
    "                                        priority=priority,\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                update_progress(\n",
    "                    (_num_leaves - num_leaves)\n",
    "                    / (num_roots * (self.branch_factor ** (self.depth - 1)))\n",
    "                )\n",
    "                num_leaves = _num_leaves\n",
    "\n",
    "            await done_episodes.put(episode)\n",
    "\n",
    "        completion_samplers = cycle(completion_sampler_pool.samplers)\n",
    "        priority = 0\n",
    "        while episode := await ready_episodes.get():\n",
    "            priority += 1\n",
    "            asyncio.create_task(expand(episode, priority, next(completion_samplers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from ['/home/ubuntu/atreides/experiments/models/rl90/0065']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61f4e6c7b8d4113a74c2460bbcb082e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/716 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-09 16:58:30 config.py:510] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 01-09 16:58:30 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1848d977ff4b49c182e247a7e8a2ced3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/56.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318208e7984e487680881bd6aef0b55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762cdcae53e245c28767c210e4d5611f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeb6c42354b4dfdbc1eaddfe08c79bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/169 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-09 16:58:33 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n",
      "INFO 01-09 16:58:33 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20250109_165833-rl90</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl90' target=\"_blank\">rl90</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl90' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl90</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aioitertools.helpers import maybe_await\n",
    "import asyncio\n",
    "from collections import Counter\n",
    "import itertools as it\n",
    "from lib import clue\n",
    "from lib.rl.episode import Episode\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import Eval, ExploreOptions, Trainer, vLLMConfig\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import AsyncIterable\n",
    "\n",
    "\n",
    "episodes_per_iteration = 32 * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "async def train_episodes(\n",
    "    revisit_frequency: float = 0.0,\n",
    ") -> AsyncIterable[Episode | BaseException]:\n",
    "    pending: set[asyncio.Task[Episode | BaseException]] = set()\n",
    "    episodes = (\n",
    "        maybe_await(episode)\n",
    "        for episodes in zip(\n",
    "            # (clue.sample_random_episode() for _ in it.repeat(0)),\n",
    "            it.cycle(temporal_clue_episodes[64:]),\n",
    "            # it.cycle(zebra_grid_episodes[64:]),\n",
    "            # it.cycle(math_episodes[64:]),\n",
    "        )\n",
    "        for episode in episodes\n",
    "    )\n",
    "    visited_episodes = Counter[Episode]()\n",
    "    while True:\n",
    "        pending.update(\n",
    "            asyncio.create_task(next(episodes))\n",
    "            for _ in range(episodes_per_iteration - len(pending))  # type: ignore\n",
    "        )\n",
    "        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "        if len(visited_episodes) > episodes_per_iteration:\n",
    "            while random.random() < revisit_frequency:\n",
    "                episode = min(visited_episodes, key=lambda e: visited_episodes[e])\n",
    "                visited_episodes[episode] += 1\n",
    "                yield episode\n",
    "        for task in done:\n",
    "            try:\n",
    "                result = task.result()\n",
    "                if isinstance(result, Episode):\n",
    "                    visited_episodes[result] += 1\n",
    "                yield result\n",
    "            except BaseException as e:\n",
    "                yield e\n",
    "\n",
    "\n",
    "async def val_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    for fut in asyncio.as_completed(clue.sample_random_episode() for _ in range(64)):\n",
    "        try:\n",
    "            yield await fut\n",
    "        except BaseException as e:\n",
    "            yield e\n",
    "\n",
    "\n",
    "explore_options = ExploreOptions(\n",
    "    iterations=1,\n",
    "    num_parents=6,\n",
    "    branch_factor=2,\n",
    "    patience=60,\n",
    "    advantage_max_weight=0.15,\n",
    "    sample_probability_power=None,\n",
    "    sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "    # split_method=\"prob\",\n",
    "    # split_point_std_deviation=0.5,\n",
    ")\n",
    "\n",
    "model_name = \"rl90\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=explore_options,\n",
    "    # explore_impl=DefaultExploreImpl(explore_options),\n",
    "    # explore_impl=SimpleExploreImpl(\n",
    "    #     num_samples=8, sampling_kwargs={\"max_tokens\": 4096}\n",
    "    # ),\n",
    "    explore_impl=TreeExploreImpl(\n",
    "        branch_factor=2,\n",
    "        depth=5,\n",
    "        num_roots=4,\n",
    "        # best_leaf_sampling_temperature=0.05,\n",
    "        sampling_kwargs={\n",
    "            \"max_tokens\": 4096,\n",
    "            \"stop\": [\"://\", \"<|end_of_text|>\"],\n",
    "            \"name\": \"explore\",\n",
    "            \"tags\": [model_name],\n",
    "        },  # type: ignore\n",
    "    ),\n",
    "    force_terminate_vllms=True,\n",
    "    train_episodes=train_episodes(revisit_frequency=0.5),\n",
    "    episodes_per_iteration=episodes_per_iteration,\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    evals=[\n",
    "        # Eval(\n",
    "        #     name=\"variable_clue\",\n",
    "        #     episodes=val_episodes(),\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "        Eval(\n",
    "            name=\"temporal_clue\",\n",
    "            episodes=temporal_clue_episodes[:64],\n",
    "            samples_per_episode=3,\n",
    "            sampling_kwargs={\n",
    "                \"max_tokens\": 4096,\n",
    "                \"stop\": [\"://\", \"<|end_of_text|>\"],\n",
    "                \"name\": \"eval\",\n",
    "                \"tags\": [model_name, \"temporal-clue\"],\n",
    "            },  # type: ignore\n",
    "        ),\n",
    "        # Eval(\n",
    "        #     name=\"zebra_grid\",\n",
    "        #     episodes=zebra_grid_episodes[:64],\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "        # Eval(\n",
    "        #     name=\"math\",\n",
    "        #     episodes=math_episodes[:64],\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "    ],\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_configs=[\n",
    "        TuneRecipeConfig(\n",
    "            shuffle=True,\n",
    "            num_output_chunks=4,\n",
    "            resume_from_checkpoint=False,\n",
    "            batch_size=1,\n",
    "            epochs=1,\n",
    "            max_steps_per_epoch=32,\n",
    "            optimizer=ComponentConfig(\n",
    "                \"torch.optim.AdamW\",\n",
    "                # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "                # \"bitsandbytes.optim.AdamW\",\n",
    "                # params=PLACEHOLDER,\n",
    "                lr=lr,\n",
    "                fused=True,\n",
    "            ),\n",
    "            loss=ComponentConfig(\n",
    "                PPOLoss,\n",
    "                policy_coef=0.0,\n",
    "                clip_epsilon=0.2,\n",
    "                tanh_log_policy_coef=0.8,\n",
    "                advantage_prediction_coef=0.0,\n",
    "                predicted_advantage_weight=0.0,\n",
    "                entropy_coef=0.0,\n",
    "                entropy_target=0.6,\n",
    "                entropy_target_coef=0.05,\n",
    "                kl_coef=0.05,\n",
    "                self_kl_coef=(\n",
    "                    0.06 * torch.cuda.device_count()\n",
    "                    if torch.cuda.device_count() > 1\n",
    "                    else 0.0\n",
    "                ),\n",
    "                peer_kl_coef=(\n",
    "                    -0.08 / (1 - 1 / torch.cuda.device_count())\n",
    "                    if torch.cuda.device_count() > 1\n",
    "                    else 0.0\n",
    "                ),\n",
    "                normalize_values=False,\n",
    "                normalize_value_predictions=False,\n",
    "                normalize_advantages=False,\n",
    "            ),\n",
    "            compile=False,\n",
    "            optimizer_in_bwd=False,\n",
    "            gradient_accumulation_steps=1,\n",
    "            enable_activation_checkpointing=True,\n",
    "            enable_activation_offloading=False,\n",
    "            custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "            log_every_n_steps=1,\n",
    "            log_peak_memory_stats=True,\n",
    "        )\n",
    "        for lr in [3e-6]\n",
    "    ],\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_chunked_prefill=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384,\n",
    "            preemption_mode=\"swap\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=100,\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        min_time_between_requests=0.0,\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl90/0065 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n"
     ]
    }
   ],
   "source": [
    "await trainer.train(iterations=100, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl90/0065 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7655778b3b4740d68d44b67df3f21941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c2ca1e987140f5ad06a8023123c6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping temporal_clue evaluation due to expired patience (1 remaining episodes x 60.0 patience per episode = 60.0 seconds)\n",
      "Tuning model on 19 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'ProcessLookupError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl90/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8e8d3f10964f448fcb8f7f87355551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 66 model files to /home/ubuntu/atreides/experiments/models/rl90/0066\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl90/0066 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6470495f14e6445fb31b7a0b2c2a5bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2815dd78e14584a22e5df8862e5cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (32 remaining episodes x 60 patience per episode = 1920 seconds)\n",
      "Early stopping temporal_clue evaluation due to expired patience (39 remaining episodes x 60.0 patience per episode = 2340.0 seconds)\n",
      "Tuning model on 0 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl90/cuda:0/config.yaml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bac109fbbc4bdc9903303f6dd8c099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "No model checkpoint files found to save in output directory /home/ubuntu/atreides/experiments/models/rl90/cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtrain(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:316\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, iterations, verbosity)\u001b[0m\n\u001b[1;32m    314\u001b[0m     explore_result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(explore_result, ExploreResult)\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune(explore_result, verbosity\u001b[38;5;241m=\u001b[39mverbosity)\n\u001b[1;32m    317\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:643\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, result, verbosity)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuning model on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39msequences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_vllms()\n\u001b[0;32m--> 643\u001b[0m model_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tune_model(\n\u001b[1;32m    646\u001b[0m             result,\n\u001b[1;32m    647\u001b[0m             model,\n\u001b[1;32m    648\u001b[0m             \u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m    649\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune_recipe_configs[\u001b[38;5;28mid\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune_recipe_configs)],\n\u001b[1;32m    650\u001b[0m             verbosity,\n\u001b[1;32m    651\u001b[0m         )\n\u001b[1;32m    652\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_models)\n\u001b[1;32m    653\u001b[0m     ]\n\u001b[1;32m    654\u001b[0m )\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mextend(model_names)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:385\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:709\u001b[0m, in \u001b[0;36mTrainer._tune_model\u001b[0;34m(self, result, model, id, config, verbosity)\u001b[0m\n\u001b[1;32m    707\u001b[0m     cleanup_before_training()\n\u001b[1;32m    708\u001b[0m     recipe_main(config)\n\u001b[0;32m--> 709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_subdir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:845\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, base_checkpoint_dir, output_subdir)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Find the latest epoch number from model checkpoint files\u001b[39;00m\n\u001b[1;32m    830\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    831\u001b[0m     (\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;28mint\u001b[39m(result\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    841\u001b[0m     default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    842\u001b[0m )\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 845\u001b[0m     epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    846\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model checkpoint files found to save in output directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00moutput_subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m iteration, iteration_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_iteration_dir(base_checkpoint_dir)\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# Move model checkpoint files to the iteration directory\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: No model checkpoint files found to save in output directory /home/ubuntu/atreides/experiments/models/rl90/cuda:0"
     ]
    }
   ],
   "source": [
    "await trainer.train(iterations=100, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl90/0066 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d7802c489d4c7497923fbdc8802ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b70db4c076d497fb3013d8626349a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue/0:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=100, verbosity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
