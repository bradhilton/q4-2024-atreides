{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from /home/ubuntu/atreides/experiments/models/rl39/0010\n",
      "INFO 12-18 00:35:29 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20241218_003531-rl39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl39' target=\"_blank\">rl39</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl39' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "import json\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import ExploreOptions, Trainer, vLLMConfig\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import Any, AsyncIterable, Literal, Optional\n",
    "\n",
    "with open(\"./data/chain-of-thought-examples.json\") as f:\n",
    "    chain_of_thought_examples: list[dict[str, str]] = json.load(f)\n",
    "\n",
    "client = httpx.AsyncClient(\n",
    "    timeout=httpx.Timeout(5.0, read=30.0),\n",
    "    limits=httpx.Limits(max_connections=512, max_keepalive_connections=512),\n",
    ")\n",
    "\n",
    "\n",
    "async def sample_random_episode(\n",
    "    difficulty: Literal[\"easy\", \"variable\"] = \"variable\",\n",
    "    example_probability: float = 0.0,\n",
    "    max_prompt_characters: int = 8192,\n",
    "    reward_follow_up_completion: bool = True,\n",
    "    return_first_solver_as_winner: Optional[bool] = None,\n",
    "    length_penalty: float = 0.0,\n",
    ") -> Episode:\n",
    "    while True:\n",
    "        params: dict[str, Any] = {\n",
    "            \"difficulty\": difficulty,\n",
    "        }\n",
    "        if return_first_solver_as_winner is not None:\n",
    "            params[\"return_first_solver_as_winner\"] = return_first_solver_as_winner\n",
    "        try:\n",
    "            response = await client.get(\n",
    "                \"http://0.0.0.0:2218/new-episode-data\",\n",
    "                params=params,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "        except httpx.TimeoutException:\n",
    "            continue\n",
    "        result = response.json()\n",
    "        prompt = result[\"prompt\"]\n",
    "        follow_up = result[\"follow_up\"]\n",
    "        solution = result[\"solution\"]\n",
    "        if len(prompt) <= max_prompt_characters:\n",
    "            break\n",
    "\n",
    "    async def reward_completion(completion: EpisodeCompletion) -> EpisodeCompletion:\n",
    "        if len(completion.messages) == 2:\n",
    "            follow_up_completion = await completion.follow_up(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": follow_up},\n",
    "                ],\n",
    "                max_tokens=10\n",
    "                + len(\"\\n\".join(f\"{key}: {value}\" for key, value in solution.items()))\n",
    "                // 2,\n",
    "            )\n",
    "        else:\n",
    "            follow_up_completion = completion\n",
    "        answer = follow_up_completion.last_assistant_message.get(\"content\")\n",
    "        assert isinstance(answer, str)\n",
    "        if reward_follow_up_completion:\n",
    "            completion = follow_up_completion\n",
    "        completion.reward = sum(\n",
    "            [\n",
    "                bool(\n",
    "                    # Find first match of key followed by colon and capture following text\n",
    "                    (\n",
    "                        match := re.search(\n",
    "                            rf\"{key}: ([A-Za-z \\.:-]+)\",\n",
    "                            answer,\n",
    "                            re.IGNORECASE,\n",
    "                        )\n",
    "                    )\n",
    "                    # Check if captured group matches expected value\n",
    "                    and match.group(1).strip().lower() == value.strip().lower()\n",
    "                )\n",
    "                for key, value in solution.items()\n",
    "            ]\n",
    "        ) / len(solution)\n",
    "        completion.reward -= (\n",
    "            completion.all_absent_stop_tokens\n",
    "            / (3 if reward_follow_up_completion else 2)\n",
    "            / len(solution)\n",
    "        )\n",
    "        completion.reward -= (\n",
    "            completion.completion_tokens\n",
    "            / (len(prompt) + len(solution) * 10)\n",
    "            * length_penalty\n",
    "        )\n",
    "        return completion\n",
    "\n",
    "    async def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in await asyncio.gather(\n",
    "            *[reward_completion(completion) for completion in completions]\n",
    "        ):\n",
    "            completion.commit()\n",
    "\n",
    "    example = random.choice(chain_of_thought_examples)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        examples=lambda: (\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": example[\"chain_of_thought\"]\n",
    "                    + (example[\"answer\"] and f\"\\n\\n---\\n\\n{example['answer']}\"),\n",
    "                },\n",
    "            ]\n",
    "            if random.random() < example_probability\n",
    "            else []\n",
    "        ),\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "episodes_per_iteration = 64 * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "async def train_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    pending: set[asyncio.Task[Episode | BaseException]] = set()\n",
    "    while True:\n",
    "        pending.update(\n",
    "            asyncio.create_task(sample_random_episode())\n",
    "            for _ in range(episodes_per_iteration - len(pending))\n",
    "        )\n",
    "        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "        for task in done:\n",
    "            try:\n",
    "                yield task.result()\n",
    "            except BaseException as e:\n",
    "                yield e\n",
    "\n",
    "\n",
    "async def val_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    for fut in asyncio.as_completed(\n",
    "        sample_random_episode() for _ in range(64 * torch.cuda.device_count())\n",
    "    ):\n",
    "        try:\n",
    "            yield await fut\n",
    "        except BaseException as e:\n",
    "            yield e\n",
    "\n",
    "\n",
    "model_name = \"rl39\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=ExploreOptions(\n",
    "        iterations=7,\n",
    "        num_parents=6,\n",
    "        branch_factor=3,\n",
    "        patience=5,\n",
    "        sample_probability_power=None,\n",
    "        sampling_kwargs={\"max_tokens\": 2048},\n",
    "        # split_method=\"prob\",\n",
    "        # split_point_std_deviation=0.5,\n",
    "    ),\n",
    "    train_episodes=train_episodes(),\n",
    "    episodes_per_iteration=episodes_per_iteration,\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    val_episodes=val_episodes(),\n",
    "    val_patience=15,\n",
    "    val_samples_per_episode=3,\n",
    "    val_sampling_kwargs={\"max_tokens\": 2048},\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        # max_steps_per_epoch=32,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=4e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=0.0,\n",
    "            unclipped_policy_coef=0.0,\n",
    "            tanh_log_policy_coef=0.0,\n",
    "            reinforce_coef=0.2,\n",
    "            clip_epsilon=0.2,\n",
    "            exploitation_penalty=0.0,\n",
    "            value_coef=0.0,\n",
    "            entropy_coef=0.0,\n",
    "            entropy_target=0.6,\n",
    "            entropy_target_coef=0.05,\n",
    "            kl_coef=0.05,\n",
    "            weighted_entropy_coef=0.2,\n",
    "            weighted_kl_coef=0.0,\n",
    "            weighted_ce_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_value_predictions=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_chunked_prefill=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384,\n",
    "            preemption_mode=\"swap\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=100,\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        min_time_between_requests=0.0,\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0010 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bef55a575b49bcb93596b022e857ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val: 0episode [00:00, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3685cb64d1f4b8898c06345b33362cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|21|Loss: 0.0076: 100%|██████████| 21/21 [06:22<00:00, 17.74s/it, entropy=0.5601, entropy_target=0.0399, kl_div=0.1442, policy=0.0294, reinforce=-0.0053, tanh_log_policy=0.0007, unclipped_policy=0.0255, value=0.0000, weighted_ce=-0.0053, weighted_entropy=0.0029, weighted_kl_div=0.0013]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 11 model files to /home/ubuntu/atreides/experiments/models/rl39/0011\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0011 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2832059e314ec68ea59228c0c4638e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e3b6fbb5da4e57a01b360d18e2c2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|17|Loss: 0.0048: 100%|██████████| 17/17 [05:08<00:00, 17.67s/it, entropy=0.3323, entropy_target=0.2677, kl_div=0.2064, policy=-0.0401, reinforce=-0.0365, tanh_log_policy=-0.0375, unclipped_policy=-0.2169, value=0.0000, weighted_ce=-0.0365, weighted_entropy=0.0582, weighted_kl_div=0.0480]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 12 model files to /home/ubuntu/atreides/experiments/models/rl39/0012\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0012 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15f3e78754d4a30bd59b15aaccb4048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8074b659b97b47cd8050a6eb0a40926e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|25|Loss: -0.0292: 100%|██████████| 25/25 [07:31<00:00, 17.69s/it, entropy=0.3495, entropy_target=0.2505, kl_div=0.3108, policy=-0.1481, reinforce=-0.3550, tanh_log_policy=-0.0490, unclipped_policy=-0.3172, value=0.0000, weighted_ce=-0.3550, weighted_entropy=-0.0686, weighted_kl_div=0.2388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 13 model files to /home/ubuntu/atreides/experiments/models/rl39/0013\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0013 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff21092d6ab84df08ea50de29ebf63c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c842f022a92941159d97d7cc1d753376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|24|Loss: 0.0129: 100%|██████████| 24/24 [07:14<00:00, 17.68s/it, entropy=0.6277, entropy_target=0.0277, kl_div=0.1399, policy=0.0173, reinforce=0.0103, tanh_log_policy=0.0001, unclipped_policy=0.0103, value=0.0000, weighted_ce=0.0103, weighted_entropy=-0.0121, weighted_kl_div=-0.0030]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 14 model files to /home/ubuntu/atreides/experiments/models/rl39/0014\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0014 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3015b33c7f0e4117aa870558806bae42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05b35b187644d0b8a5874e99d94f50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (1 remaining episodes x 15 patience per episode = 15 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|23|Loss: 0.0093: 100%|██████████| 23/23 [06:56<00:00, 17.65s/it, entropy=0.5671, entropy_target=0.0329, kl_div=0.1683, policy=0.0087, reinforce=-0.0013, tanh_log_policy=-0.0002, unclipped_policy=0.0054, value=0.0000, weighted_ce=-0.0013, weighted_entropy=0.0024, weighted_kl_div=0.0012]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 15 model files to /home/ubuntu/atreides/experiments/models/rl39/0015\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0015 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f34a3624c1d4ce19d4183e10f4464c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984db4d6217b418faff46f21e6a5ed2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (2 remaining episodes x 15 patience per episode = 30 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|20|Loss: -0.0820: 100%|██████████| 20/20 [06:08<00:00, 17.58s/it, entropy=0.5933, entropy_target=0.0067, kl_div=0.3983, policy=-0.0897, reinforce=-0.3598, tanh_log_policy=-0.0983, unclipped_policy=-0.4029, value=0.0000, weighted_ce=-0.3598, weighted_entropy=0.1516, weighted_kl_div=0.2797]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 16 model files to /home/ubuntu/atreides/experiments/models/rl39/0016\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0016 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3fa19b61154455afe0a68b85d8a988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9886936e3ad84d9bbe20e8cd2ac8bede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (1 remaining episodes x 15 patience per episode = 15 seconds)\n",
      "Early stopping exploration due to expired patience (0 remaining episodes x 5 patience per episode = 0 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|21|Loss: 0.0112: 100%|██████████| 21/21 [06:25<00:00, 17.81s/it, entropy=0.4685, entropy_target=0.1315, kl_div=0.1317, policy=0.0038, reinforce=-0.0065, tanh_log_policy=-0.0001, unclipped_policy=-0.0036, value=0.0000, weighted_ce=-0.0065, weighted_entropy=0.0032, weighted_kl_div=0.0009]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 17 model files to /home/ubuntu/atreides/experiments/models/rl39/0017\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0017 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee057a47a9414eff907c09dc19faf16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddaad37e96348ea8f4db4a826120cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping val evaluation due to expired patience (2 remaining episodes x 15 patience per episode = 30 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|21|Loss: -0.0074: 100%|██████████| 21/21 [06:53<00:00, 19.67s/it, entropy=0.6074, entropy_target=0.0074, kl_div=0.1473, policy=0.0031, reinforce=-0.0588, tanh_log_policy=-0.0089, unclipped_policy=-0.0305, value=0.0000, weighted_ce=-0.0588, weighted_entropy=0.0168, weighted_kl_div=0.0133] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 18 model files to /home/ubuntu/atreides/experiments/models/rl39/0018\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0018 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca60903b388941dd9e693707e0dbd64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e52a250c25e48c1935377a063f9ffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping exploration due to expired patience (0 remaining episodes x 5 patience per episode = 0 seconds)\n",
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|19|Loss: 0.0000: 100%|██████████| 19/19 [06:16<00:00, 19.79s/it, entropy=0.4638, entropy_target=0.1362, kl_div=0.1044, policy=0.0075, reinforce=-0.0279, tanh_log_policy=-0.0031, unclipped_policy=-0.0194, value=0.0000, weighted_ce=-0.0279, weighted_entropy=0.0321, weighted_kl_div=-0.0013] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 19 model files to /home/ubuntu/atreides/experiments/models/rl39/0019\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0019 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102b3be9efaa40eab792a41535f793c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b08309ad9ed482abd9dfc6f3d04380a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ tune run lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl39/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|23|Loss: 0.0153: 100%|██████████| 23/23 [07:02<00:00, 17.69s/it, entropy=0.7640, entropy_target=0.1640, kl_div=0.2035, policy=0.0211, reinforce=-0.0069, tanh_log_policy=0.0007, unclipped_policy=0.0184, value=0.0000, weighted_ce=-0.0069, weighted_entropy=0.0085, weighted_kl_div=0.0016]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 20 model files to /home/ubuntu/atreides/experiments/models/rl39/0020\n",
      "Starting 1 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl39/0020 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab79cc05ad184b9bac9e324506baed7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=10, verbosity=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
