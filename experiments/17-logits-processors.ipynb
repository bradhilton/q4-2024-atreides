{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --block-size=32 --disable-log-requests --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=65536 --return-tokens-as-token-ids --swap-space=32 --port=8002 --api-key=default\n",
      "INFO 12-12 22:23:43 api_server.py:634] vLLM API server version 0.1.dev3774+g0b7ca0f\n",
      "INFO 12-12 22:23:43 api_server.py:635] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8002, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='xgrammar', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=32.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=65536, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, mm_cache_preprocessor=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x74fa75362c00>)\n",
      "INFO 12-12 22:23:43 api_server.py:198] Started engine process with PID 43163\n",
      "WARNING 12-12 22:23:44 config.py:2059] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 12-12 22:23:49 config.py:2059] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 12-12 22:23:50 config.py:446] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 12-12 22:23:50 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-12 22:23:50 config.py:572] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 12-12 22:23:54 config.py:446] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 12-12 22:23:54 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 12-12 22:23:54 config.py:572] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 12-12 22:23:54 llm_engine.py:250] Initializing an LLM engine (v0.1.dev3774+g0b7ca0f) with config: VllmConfig(model_config=<vllm.config.ModelConfig object at 0x7c028a8e0470>, cache_config=<vllm.config.CacheConfig object at 0x7c028a8e3290>, parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, worker_use_ray=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend=None, worker_cls='vllm.worker.worker.Worker', sd_worker_cls='auto', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=65536, max_num_seqs=512, max_model_len=16384, num_lookahead_slots=0, delay_factor=0.0, enable_chunked_prefill=False, is_multimodal_model=False, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=False), device_config=<vllm.config.DeviceConfig object at 0x7c03ab15fb90>, load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config=None, ignore_patterns=['original/**/*']), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), prompt_adapter_config=None, quant_config=None, compilation_config=CompilationConfig(level=0, debug_dump_path='', backend='', custom_ops=[], splitting_ops=['vllm.unified_attention', 'vllm.unified_attention_with_output'], use_inductor=True, candidate_compile_sizes=[], inductor_compile_config={}, inductor_passes={}, use_cudagraph=False, cudagraph_num_of_warmups=0, cudagraph_capture_sizes=None, cudagraph_copy_inputs=False, pass_config=PassConfig(dump_graph_stages=[], dump_graph_dir=PosixPath('.'), enable_fusion=True, enable_reshape=True), compile_sizes=[], capture_sizes=[], enabled_custom_ops=Counter(), disabled_custom_ops=Counter(), compilation_time=0.0, static_forward_context={}), kv_transfer_config=None, instance_id='d2198'),use_cached_outputs=True, \n",
      "INFO 12-12 22:23:56 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-12 22:23:56 model_runner.py:1091] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 12-12 22:23:57 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  4.09it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.53it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.21it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.11it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 22:24:00 model_runner.py:1096] Loading model weights took 14.9595 GB\n",
      "INFO 12-12 22:24:03 worker.py:237] Memory profiling results: duration=2.60 seconds, total_gpu_memory=79.10GiB, initial_memory_usage=15.50GiB, peak_torch_memory=21.74GiB, memory_usage_post_profile=15.60GiB, non_torch_memory=0.61GiB, kv_cache_size=48.84GiB, gpu_memory_utilization=0.90.\n",
      "INFO 12-12 22:24:03 gpu_executor.py:76] # GPU blocks: 12502, # CPU blocks: 8192\n",
      "INFO 12-12 22:24:03 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 24.42x\n",
      "INFO 12-12 22:24:15 llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 14.61 seconds\n",
      "INFO 12-12 22:24:16 api_server.py:569] Using supplied chat template:\n",
      "INFO 12-12 22:24:16 api_server.py:569] None\n",
      "INFO 12-12 22:24:16 launcher.py:19] Available routes are:\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 12-12 22:24:16 launcher.py:27] Route: /v1/score, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [43128]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8002 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 22:24:20 chat_utils.py:331] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO:     127.0.0.1:51798 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    }
   ],
   "source": [
    "from lib.vllm import start_vllm\n",
    "\n",
    "vllm = await start_vllm(\n",
    "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "    block_size=32,\n",
    "    disable_log_requests=True,\n",
    "    enable_prefix_caching=True,\n",
    "    enforce_eager=True,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=16384,\n",
    "    max_num_seqs=512,\n",
    "    max_num_batched_tokens=16384 * 4,\n",
    "    return_tokens_as_token_ids=True,\n",
    "    swap_space=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(\n",
      "    id=\"chatcmpl-bda17d762e3f46f18db6efa29ff655cc\",\n",
      "    choices=[\n",
      "        Choice(\n",
      "            finish_reason=\"length\",\n",
      "            index=0,\n",
      "            logprobs=None,\n",
      "            message=ChatCompletionMessage(\n",
      "                content=\"curacy\",\n",
      "                refusal=None,\n",
      "                role=\"assistant\",\n",
      "                audio=None,\n",
      "                function_call=None,\n",
      "                tool_calls=[],\n",
      "            ),\n",
      "            stop_reason=None,\n",
      "        )\n",
      "    ],\n",
      "    created=1734042625,\n",
      "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
      "    object=\"chat.completion\",\n",
      "    service_tier=None,\n",
      "    system_fingerprint=None,\n",
      "    usage=CompletionUsage(\n",
      "        completion_tokens=1,\n",
      "        prompt_tokens=10,\n",
      "        total_tokens=11,\n",
      "        completion_tokens_details=None,\n",
      "        prompt_tokens_details=None,\n",
      "    ),\n",
      "    prompt_logprobs=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from lib.utils import black_print\n",
    "\n",
    "black_print(\n",
    "    await vllm.client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "        model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "        max_tokens=1,\n",
    "        extra_body={\n",
    "            \"logits_processors\": [\n",
    "                {\n",
    "                    \"qualname\": \"vllm.entrypoints.openai.logits_processors.AllowedTokenIdsLogitsProcessor\",\n",
    "                    \"args\": [[22222]],\n",
    "                    # \"kwargs\": {\"allowed_ids\": [22222]},\n",
    "                },\n",
    "                \"experiments.lib.logits_processors.random_logits_processor\",\n",
    "            ],\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
