{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.recipes.full_finetune import ComponentConfig, FullFinetuneConfig, recipe_main\n",
    "import subprocess\n",
    "from torch.optim.adamw import AdamW\n",
    "from torchtune.datasets import alpaca_dataset\n",
    "from torchtune.models.llama3 import llama3_tokenizer\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from torchtune.modules.loss import CEWithChunkedOutputLoss\n",
    "from torchtune.training import FullModelHFCheckpointer\n",
    "from torchtune.training.metric_logging import DiskLogger\n",
    "from typing import Any\n",
    "\n",
    "PLACEHOLDER: Any = None\n",
    "\n",
    "checkpoint_dir = subprocess.run(\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "\n",
    "recipe_main(\n",
    "    FullFinetuneConfig(\n",
    "        # Tokenizer\n",
    "        tokenizer=ComponentConfig(\n",
    "            llama3_tokenizer,  # type: ignore\n",
    "            path=\"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/llama_models/llama3/api/tokenizer.model\",\n",
    "            max_seq_len=None,\n",
    "        ),\n",
    "        # Dataset\n",
    "        dataset=ComponentConfig(alpaca_dataset, tokenizer=PLACEHOLDER, packed=False),\n",
    "        seed=None,\n",
    "        shuffle=True,\n",
    "        # Model\n",
    "        model=ComponentConfig(llama3_1_8b),\n",
    "        # Checkpointer\n",
    "        checkpointer=ComponentConfig(\n",
    "            FullModelHFCheckpointer,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            checkpoint_files=[\n",
    "                \"model-00001-of-00004.safetensors\",\n",
    "                \"model-00002-of-00004.safetensors\",\n",
    "                \"model-00003-of-00004.safetensors\",\n",
    "                \"model-00004-of-00004.safetensors\",\n",
    "            ],\n",
    "            recipe_checkpoint=None,\n",
    "            output_dir=\"/tmp/Hermes-2-Theta-Llama-3-8B/\",\n",
    "            model_type=\"LLAMA3\",\n",
    "        ),\n",
    "        resume_from_checkpoint=False,\n",
    "        # Fine-tuning arguments\n",
    "        batch_size=12,\n",
    "        epochs=3,\n",
    "        optimizer=ComponentConfig(AdamW, params=PLACEHOLDER, lr=2e-5, fused=True),\n",
    "        loss=ComponentConfig(CEWithChunkedOutputLoss),\n",
    "        max_steps_per_epoch=None,\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        # Training env\n",
    "        device=\"cuda\",\n",
    "        # Memory management\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        # Reduced precision\n",
    "        dtype=\"bf16\",\n",
    "        # Logging\n",
    "        metric_logger=ComponentConfig(\n",
    "            DiskLogger, log_dir=\"/home/ubuntu/atreides/experiments/logs\"\n",
    "        ),\n",
    "        output_dir=\"/home/ubuntu/atreides/experiments/logs\",\n",
    "        log_every_n_steps=16,\n",
    "        log_peak_memory_stats=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: !!python/name:lib.rl.trajectory.Trajectories ''\n",
      "  dir: ./data/trajectories\n",
      "  rows: 64\n",
      "  seqlen: 8192\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 4\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: !!python/name:lib.rl.ppo.PPOLoss ''\n",
      "  entropy_coef: 0.0\n",
      "  kl_coef: 1.0\n",
      "  policy_coef: 0.0\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: !!python/name:torchtune.training.metric_logging.DiskLogger ''\n",
      "  log_dir: /home/ubuntu/atreides/experiments/logs\n",
      "model:\n",
      "  _component_: !!python/name:torchtune.models.llama3_1._model_builders.llama3_1_8b ''\n",
      "num_output_chunks: 8\n",
      "optimizer:\n",
      "  _component_: bitsandbytes.optim.PagedAdamW8bit\n",
      "  lr: 2.0e-05\n",
      "  params: null\n",
      "optimizer_in_bwd: false\n",
      "output_dir: /home/ubuntu/atreides/experiments/logs\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: false\n",
      "tokenizer:\n",
      "  _component_: !!python/name:torchtune.models.llama3._model_builders.llama3_tokenizer ''\n",
      "  max_seq_len: null\n",
      "  path: /home/ubuntu/atreides/.venv/lib/python3.12/site-packages/llama_models/llama3/api/tokenizer.model\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 1479785282. Local seed is seed + rank = 1479785282 + 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing logs to /home/ubuntu/atreides/experiments/logs/log_1731726234.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 2.87 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.02 GiB\n",
      "\tGPU peak memory reserved: 15.14 GiB\n",
      "\tGPU peak memory active: 15.02 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9796332716941833\n",
      "0.4069623649120331\n",
      "0.2428489774465561\n",
      "0.21742001175880432\n",
      "0.1409558802843094\n",
      "0.05768474563956261\n",
      "0.11080523580312729\n",
      "0.02910163626074791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "1|1|Loss: 0.5732055306434631:   6%|▋         | 1/16 [00:11<02:46, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7007660269737244\n",
      "0.2073778361082077\n",
      "0.1726682186126709\n",
      "0.18785500526428223\n",
      "0.17818306386470795\n",
      "0.09780827164649963\n",
      "0.18233007192611694\n",
      "0.08450165390968323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|2|Loss: 0.9761295318603516:  12%|█▎        | 2/16 [00:19<02:13,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7382146120071411\n",
      "0.2733241021633148\n",
      "0.16987130045890808\n",
      "0.11096843332052231\n",
      "0.1245405524969101\n",
      "0.1456393450498581\n",
      "0.08356813341379166\n",
      "0.053677719086408615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|3|Loss: 0.954547107219696:  19%|█▉        | 3/16 [00:28<01:57,  9.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852419376373291\n",
      "0.4596032500267029\n",
      "0.2477872520685196\n",
      "0.16790050268173218\n",
      "0.17584165930747986\n",
      "0.14888666570186615\n",
      "0.03408844769001007\n",
      "0.12233440577983856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|4|Loss: 0.5631217956542969:  25%|██▌       | 4/16 [00:36<01:45,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8264569044113159\n",
      "0.26612284779548645\n",
      "0.10914016515016556\n",
      "0.2141476571559906\n",
      "0.14685174822807312\n",
      "0.14411896467208862\n",
      "0.11655472218990326\n",
      "0.15910430252552032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|5|Loss: 0.490572065114975:  31%|███▏      | 5/16 [00:44<01:35,  8.70s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8184608221054077\n",
      "0.22658227384090424\n",
      "0.1504455953836441\n",
      "0.24943718314170837\n",
      "0.17534524202346802\n",
      "0.17088118195533752\n",
      "0.0848056823015213\n",
      "0.11278649419546127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|6|Loss: 0.4599432051181793:  38%|███▊      | 6/16 [00:53<01:26,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8514193296432495\n",
      "0.29765111207962036\n",
      "0.21367309987545013\n",
      "0.20665451884269714\n",
      "0.13137389719486237\n",
      "0.09455224126577377\n",
      "0.11017384380102158\n",
      "0.07322470098733902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|7|Loss: 0.424756795167923:  44%|████▍     | 7/16 [01:01<01:17,  8.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8836952447891235\n",
      "0.33418747782707214\n",
      "0.1484004706144333\n",
      "0.20883631706237793\n",
      "0.23512962460517883\n",
      "0.21830639243125916\n",
      "0.12820032238960266\n",
      "0.07203420251607895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|8|Loss: 0.41359126567840576:  50%|█████     | 8/16 [01:10<01:08,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8658627271652222\n",
      "0.33858659863471985\n",
      "0.1120242103934288\n",
      "0.17850235104560852\n",
      "0.2169209122657776\n",
      "0.15585964918136597\n",
      "0.12817494571208954\n",
      "0.1354888379573822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|9|Loss: 0.42370614409446716:  56%|█████▋    | 9/16 [01:19<00:59,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8762689828872681\n",
      "0.2747672200202942\n",
      "0.2659517526626587\n",
      "0.22416619956493378\n",
      "0.19021129608154297\n",
      "0.15314172208309174\n",
      "0.18659624457359314\n",
      "0.16077107191085815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|10|Loss: 0.38965141773223877:  62%|██████▎   | 10/16 [01:27<00:51,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9085795283317566\n",
      "0.2850969433784485\n",
      "0.13537360727787018\n",
      "0.15171530842781067\n",
      "0.1805664747953415\n",
      "0.2122650444507599\n",
      "0.14927813410758972\n",
      "0.10628407448530197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|11|Loss: 0.3994769752025604:  69%|██████▉   | 11/16 [01:36<00:42,  8.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.887200653553009\n",
      "0.2456841766834259\n",
      "0.13437752425670624\n",
      "0.2433578222990036\n",
      "0.32327958941459656\n",
      "0.08836952596902847\n",
      "0.18314141035079956\n",
      "0.1404028981924057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|12|Loss: 0.35594600439071655:  75%|███████▌  | 12/16 [01:44<00:34,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8650654554367065\n",
      "0.4375864267349243\n",
      "0.25822216272354126\n",
      "0.21933135390281677\n",
      "0.18941274285316467\n",
      "0.10738907009363174\n",
      "0.10803672671318054\n",
      "0.13299255073070526\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m checkpoint_output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/atreides/experiments/models/rl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(checkpoint_output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mrecipe_main\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mRLConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Tokenizer\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mComponentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mllama3_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/llama_models/llama3/api/tokenizer.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Dataset\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mComponentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mTrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/trajectories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Model\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mComponentConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama3_1_8b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_output_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Checkpointer\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mComponentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchtune.training.FullModelHFCheckpointer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-00001-of-00004.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-00002-of-00004.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-00003-of-00004.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-00004-of-00004.safetensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecipe_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/atreides/experiments/models/rl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLLAMA3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Fine-tuning arguments\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mComponentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# AdamW,\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes.optim.PagedAdamW8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPLACEHOLDER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# fused=True,\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mComponentConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPPOLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkl_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_in_bwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Training env\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Memory management\u001b[39;49;00m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_activation_checkpointing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_activation_offloading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_sharded_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtok_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Reduced precision\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbf16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Logging\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_logger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mComponentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mDiskLogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/atreides/experiments/logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/atreides/experiments/logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_peak_memory_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/experiments/lib/recipes/rl.py:1137\u001b[0m, in \u001b[0;36mrecipe_main\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m   1135\u001b[0m recipe \u001b[38;5;241m=\u001b[39m RLRecipe(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[1;32m   1136\u001b[0m recipe\u001b[38;5;241m.\u001b[39msetup(cfg\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m-> 1137\u001b[0m \u001b[43mrecipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m recipe\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m~/atreides/experiments/lib/recipes/rl.py:1019\u001b[0m, in \u001b[0;36mRLRecipe.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1017\u001b[0m     current_loss \u001b[38;5;241m=\u001b[39m current_result\u001b[38;5;241m.\u001b[39mtotal_loss\n\u001b[0;32m-> 1019\u001b[0m \u001b[43mcurrent_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# Step with optimizer\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lib.recipes.rl import ComponentConfig, RLConfig, recipe_main\n",
    "from lib.rl.trajectory import Trajectories\n",
    "from lib.rl.ppo import PPOLoss\n",
    "import os\n",
    "import subprocess\n",
    "from torch.optim.adamw import AdamW\n",
    "from torchtune.models.llama3 import llama3_tokenizer\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from torchtune.training import FullModelHFCheckpointer\n",
    "from torchtune.training.metric_logging import DiskLogger\n",
    "from typing import Any\n",
    "\n",
    "PLACEHOLDER: Any = None\n",
    "\n",
    "checkpoint_dir = subprocess.run(\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "\n",
    "checkpoint_output_dir = \"/home/ubuntu/atreides/experiments/models/rl\"\n",
    "os.makedirs(checkpoint_output_dir, exist_ok=True)\n",
    "\n",
    "recipe_main(\n",
    "    RLConfig(\n",
    "        # Tokenizer\n",
    "        tokenizer=ComponentConfig(\n",
    "            llama3_tokenizer,  # type: ignore\n",
    "            path=\"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/llama_models/llama3/api/tokenizer.model\",\n",
    "            max_seq_len=None,\n",
    "        ),\n",
    "        # Dataset\n",
    "        dataset=ComponentConfig(\n",
    "            Trajectories, dir=\"./data/trajectories\", rows=64, seqlen=8192\n",
    "        ),\n",
    "        seed=None,\n",
    "        shuffle=False,\n",
    "        # Model\n",
    "        model=ComponentConfig(llama3_1_8b),\n",
    "        num_output_chunks=8,\n",
    "        # Checkpointer\n",
    "        checkpointer=ComponentConfig(\n",
    "            \"torchtune.training.FullModelHFCheckpointer\",\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            checkpoint_files=[\n",
    "                \"model-00001-of-00004.safetensors\",\n",
    "                \"model-00002-of-00004.safetensors\",\n",
    "                \"model-00003-of-00004.safetensors\",\n",
    "                \"model-00004-of-00004.safetensors\",\n",
    "            ],\n",
    "            recipe_checkpoint=None,\n",
    "            output_dir=\"/home/ubuntu/atreides/experiments/models/rl\",\n",
    "            model_type=\"LLAMA3\",\n",
    "        ),\n",
    "        resume_from_checkpoint=False,\n",
    "        # Fine-tuning arguments\n",
    "        batch_size=4,\n",
    "        epochs=4,\n",
    "        optimizer=ComponentConfig(\n",
    "            # AdamW,\n",
    "            \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            params=PLACEHOLDER,\n",
    "            lr=2e-5,\n",
    "            # fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(PPOLoss, policy_coef=0.0, entropy_coef=0.0, kl_coef=1.0),\n",
    "        max_steps_per_epoch=None,\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        # Training env\n",
    "        device=\"cuda\",\n",
    "        # Memory management\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        # Reduced precision\n",
    "        dtype=\"bf16\",\n",
    "        # Logging\n",
    "        metric_logger=ComponentConfig(\n",
    "            DiskLogger, log_dir=\"/home/ubuntu/atreides/experiments/logs\"\n",
    "        ),\n",
    "        output_dir=\"/home/ubuntu/atreides/experiments/logs\",\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 218019,  218020,  218021,  ..., 2097149, 2097150, 2097151]),)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.rl.trajectory import Trajectories\n",
    "import torch\n",
    "\n",
    "tensors = Trajectories(dir=\"./data/trajectories\", rows=256, seqlen=8192).tensors\n",
    "advantages = tensors[\"advantages\"]\n",
    "torch.where(tensors[\"advantages\"].view(-1) == 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2080768"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8192 * 254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elix'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_tokenizer(\n",
    "    path=\"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/llama_models/llama3/api/tokenizer.model\",\n",
    "    max_seq_len=None,\n",
    ").decode([68818])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-31 00:23:16 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
     ]
    }
   ],
   "source": [
    "from lib.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "\n",
    "model = llama3_1_8b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "model_dir = subprocess.run(\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from torchtune.training.checkpointing import FullModelHFCheckpointer\n",
    "\n",
    "output_dir = \"./models/test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "    checkpoint_dir=model_dir,\n",
    "    checkpoint_files=glob.glob(f\"{model_dir}/*.safetensors\")\n",
    "    + glob.glob(f\"{model_dir}/*.pt\"),\n",
    "    output_dir=output_dir,\n",
    "    model_type=\"LLAMA3\",  # type: ignore\n",
    ")\n",
    "state_dict = checkpointer.load_checkpoint()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check if model is compiled\n",
    "is_compiled = hasattr(model, '_orig_mod')\n",
    "print(is_compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rl import Completion\n",
    "\n",
    "root = Completion.model_validate_json(open(\"./data/completions.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for leaf in root.leaves():\n",
    "    length = len(tokenizer.encode(leaf.all_message_params()))\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "        clear_output()\n",
    "        print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode([{\"role\": \"user\", \"content\": \"Hello, how are you?\"},]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torchtune.utils._logging:Model checkpoint of size 9.95 GB saved to models/test/hf_model_0001_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 10.00 GB saved to models/test/hf_model_0002_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 9.83 GB saved to models/test/hf_model_0003_1.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 2.34 GB saved to models/test/hf_model_0004_1.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "epoch = 1\n",
    "\n",
    "# Save the model\n",
    "checkpointer.save_checkpoint(dict(model=model.state_dict()), epoch)\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(f\"{output_dir}/{epoch:04d}\", exist_ok=True)\n",
    "\n",
    "# Copy all non-safetensors files from model_dir to target\n",
    "for file in os.listdir(model_dir):\n",
    "    if not file.endswith(\".safetensors\") and not file.endswith(\".pt\"):\n",
    "        src = os.path.join(model_dir, file)\n",
    "        dst = os.path.join(f\"{output_dir}/{epoch:04d}\", file)\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "# Move all .pt files from ./models/test to ./models/test/0000\n",
    "for file in os.listdir(output_dir):\n",
    "    if file.endswith(\".pt\"):\n",
    "        src = os.path.join(output_dir, file)\n",
    "        dst = os.path.join(f\"{output_dir}/{epoch:04d}\", file)\n",
    "        shutil.move(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128000,\n",
       " 128002,\n",
       " 882,\n",
       " 198,\n",
       " 1966,\n",
       " 264,\n",
       " 8369,\n",
       " 10683,\n",
       " 1938,\n",
       " 19367,\n",
       " 11,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 323,\n",
       " 58280,\n",
       " 7731,\n",
       " 1523,\n",
       " 311,\n",
       " 1514,\n",
       " 264,\n",
       " 16736,\n",
       " 23347,\n",
       " 1847,\n",
       " 382,\n",
       " 7009,\n",
       " 35105,\n",
       " 220,\n",
       " 18,\n",
       " 30881,\n",
       " 315,\n",
       " 7563,\n",
       " 11,\n",
       " 1855,\n",
       " 369,\n",
       " 264,\n",
       " 8821,\n",
       " 955,\n",
       " 315,\n",
       " 2038,\n",
       " 24306,\n",
       " 315,\n",
       " 279,\n",
       " 2768,\n",
       " 1473,\n",
       " 78524,\n",
       " 1002,\n",
       " 512,\n",
       " 12,\n",
       " 9083,\n",
       " 81818,\n",
       " 198,\n",
       " 12,\n",
       " 4491,\n",
       " 13,\n",
       " 7997,\n",
       " 198,\n",
       " 12,\n",
       " 18083,\n",
       " 13,\n",
       " 5929,\n",
       " 271,\n",
       " 29314,\n",
       " 512,\n",
       " 12,\n",
       " 73997,\n",
       " 30133,\n",
       " 198,\n",
       " 12,\n",
       " 62302,\n",
       " 198,\n",
       " 12,\n",
       " 30982,\n",
       " 28905,\n",
       " 271,\n",
       " 14330,\n",
       " 512,\n",
       " 12,\n",
       " 11166,\n",
       " 198,\n",
       " 12,\n",
       " 50767,\n",
       " 198,\n",
       " 12,\n",
       " 39190,\n",
       " 10637,\n",
       " 271,\n",
       " 6153,\n",
       " 27716,\n",
       " 320,\n",
       " 438,\n",
       " 89447,\n",
       " 8,\n",
       " 19301,\n",
       " 832,\n",
       " 3786,\n",
       " 505,\n",
       " 1855,\n",
       " 1912,\n",
       " 323,\n",
       " 25012,\n",
       " 1124,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 17011,\n",
       " 785,\n",
       " 11,\n",
       " 814,\n",
       " 75371,\n",
       " 279,\n",
       " 9861,\n",
       " 7563,\n",
       " 323,\n",
       " 27023,\n",
       " 704,\n",
       " 279,\n",
       " 2768,\n",
       " 311,\n",
       " 1855,\n",
       " 2851,\n",
       " 1473,\n",
       " 12,\n",
       " 19367,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 4417,\n",
       " 43,\n",
       " 26645,\n",
       " 518,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 1329,\n",
       " 12,\n",
       " 58280,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 271,\n",
       " 791,\n",
       " 1847,\n",
       " 45374,\n",
       " 439,\n",
       " 11263,\n",
       " 1473,\n",
       " 16,\n",
       " 13,\n",
       " 1952,\n",
       " 872,\n",
       " 2543,\n",
       " 11,\n",
       " 264,\n",
       " 2851,\n",
       " 4691,\n",
       " 922,\n",
       " 264,\n",
       " 743,\n",
       " 315,\n",
       " 7041,\n",
       " 220,\n",
       " 18,\n",
       " 7563,\n",
       " 11,\n",
       " 832,\n",
       " 505,\n",
       " 1855,\n",
       " 315,\n",
       " 279,\n",
       " 1847,\n",
       " 596,\n",
       " 11306,\n",
       " 13,\n",
       " 320,\n",
       " 9290,\n",
       " 25,\n",
       " 25640,\n",
       " 1436,\n",
       " 2610,\n",
       " 922,\n",
       " 904,\n",
       " 7563,\n",
       " 11,\n",
       " 2737,\n",
       " 1884,\n",
       " 304,\n",
       " 872,\n",
       " 1866,\n",
       " 1450,\n",
       " 29275,\n",
       " 17,\n",
       " 13,\n",
       " 578,\n",
       " 2851,\n",
       " 15910,\n",
       " 420,\n",
       " 3488,\n",
       " 311,\n",
       " 279,\n",
       " 1023,\n",
       " 4311,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 11,\n",
       " 6041,\n",
       " 449,\n",
       " 279,\n",
       " 2851,\n",
       " 311,\n",
       " 872,\n",
       " 2163,\n",
       " 627,\n",
       " 18,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 1047,\n",
       " 832,\n",
       " 477,\n",
       " 810,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1047,\n",
       " 311,\n",
       " 1501,\n",
       " 832,\n",
       " 315,\n",
       " 1884,\n",
       " 7563,\n",
       " 320,\n",
       " 1073,\n",
       " 872,\n",
       " 5873,\n",
       " 8,\n",
       " 311,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 38171,\n",
       " 13,\n",
       " 578,\n",
       " 2543,\n",
       " 1243,\n",
       " 9670,\n",
       " 11,\n",
       " 323,\n",
       " 1514,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 19,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1071,\n",
       " 779,\n",
       " 11,\n",
       " 323,\n",
       " 279,\n",
       " 3488,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 627,\n",
       " 20,\n",
       " 13,\n",
       " 1115,\n",
       " 8738,\n",
       " 3156,\n",
       " 3060,\n",
       " 512,\n",
       " 262,\n",
       " 264,\n",
       " 8,\n",
       " 362,\n",
       " 2851,\n",
       " 8710,\n",
       " 264,\n",
       " 3786,\n",
       " 311,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 11,\n",
       " 477,\n",
       " 198,\n",
       " 262,\n",
       " 293,\n",
       " 8,\n",
       " 2052,\n",
       " 279,\n",
       " 79002,\n",
       " 4311,\n",
       " 1047,\n",
       " 11224,\n",
       " 814,\n",
       " 3287,\n",
       " 956,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 627,\n",
       " 21,\n",
       " 13,\n",
       " 4740,\n",
       " 264,\n",
       " 2851,\n",
       " 596,\n",
       " 2543,\n",
       " 9670,\n",
       " 320,\n",
       " 50998,\n",
       " 555,\n",
       " 1694,\n",
       " 6982,\n",
       " 264,\n",
       " 3786,\n",
       " 477,\n",
       " 3515,\n",
       " 682,\n",
       " 79002,\n",
       " 4311,\n",
       " 1522,\n",
       " 705,\n",
       " 1514,\n",
       " 7882,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 382,\n",
       " 8586,\n",
       " 374,\n",
       " 1268,\n",
       " 279,\n",
       " 1847,\n",
       " 6476,\n",
       " 704,\n",
       " 1473,\n",
       " 51787,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 50329,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 5859,\n",
       " 10637,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 8710,\n",
       " 19367,\n",
       " 264,\n",
       " 3786,\n",
       " 271,\n",
       " 38,\n",
       " 285,\n",
       " 6853,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 50329,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 43,\n",
       " 26645,\n",
       " 3730,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 57987,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 3397,\n",
       " 30133,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 8710,\n",
       " 58280,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 3961,\n",
       " 51787,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 12555,\n",
       " 13,\n",
       " 7997,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 1688,\n",
       " 420,\n",
       " 1486,\n",
       " 11,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 574,\n",
       " 3025,\n",
       " 311,\n",
       " 12722,\n",
       " 24499,\n",
       " 279,\n",
       " 6425,\n",
       " 323,\n",
       " 3243,\n",
       " 279,\n",
       " 1847,\n",
       " 382,\n",
       " 3923,\n",
       " 1051,\n",
       " 279,\n",
       " 17011,\n",
       " 785,\n",
       " 7563,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 30,\n",
       " 128003,\n",
       " 198,\n",
       " 128002,\n",
       " 78191,\n",
       " 198]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens(messages: list[dict]) -> list[int]:\n",
    "    generate = llm.generate\n",
    "\n",
    "    def get_tokens(prompts: list[dict], *args: object, **kwargs: object) -> list[int]:\n",
    "        return llm.get_tokenizer().encode(prompts[0][\"prompt\"])\n",
    "\n",
    "    llm.generate = get_tokens  # type: ignore\n",
    "    tokens = llm.chat(messages)  # type: ignore\n",
    "    llm.generate = generate  # type: ignore\n",
    "    return tokens  # type: ignore\n",
    "\n",
    "\n",
    "get_tokens([dict(role=\"user\", content=prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "\n",
    "destroy_model_parallel()\n",
    "del llm.llm_engine.model_executor.driver_worker  # type: ignore\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtune.generation import generate\n",
    "\n",
    "result = generate(\n",
    "    model,\n",
    "    torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])], device=\"cuda\"),\n",
    "    max_generated_tokens=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|im_start|>user\n",
      "On a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\n",
      "\n",
      "They assembled 3 decks of cards, each for a separate type of information composed of the following:\n",
      "\n",
      "Suspect:\n",
      "- Miss Scarlet\n",
      "- Mr. Green\n",
      "- Mrs. White\n",
      "\n",
      "Weapon:\n",
      "- Candlestick\n",
      "- Knife\n",
      "- Lead Pipe\n",
      "\n",
      "Room:\n",
      "- Hall\n",
      "- Lounge\n",
      "- Dining Room\n",
      "\n",
      "After randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\n",
      "\n",
      "- Summer: 2 cards\n",
      "- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\n",
      "- Connor: 2 cards\n",
      "\n",
      "The game proceeded as follows:\n",
      "\n",
      "1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\n",
      "2. The player directed this question to the other players in clockwise order, starting with the player to their left.\n",
      "3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\n",
      "4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\n",
      "5. This continued until either:\n",
      "    a) A player showed a card to the asking player, or\n",
      "    b) All the queried players had stated they didn't have any of the asked-about cards.\n",
      "6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\n",
      "\n",
      "Here is how the game played out:\n",
      "\n",
      "Summer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor showed Summer a card\n",
      "\n",
      "Giselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\n",
      "- Connor did not have any of the cards\n",
      "- Summer did not have any of the cards\n",
      "\n",
      "Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\n",
      "- Summer did not have any of the cards\n",
      "- Giselle showed Connor 'Miss Scarlet'\n",
      "\n",
      "Summer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor did not have any of the cards\n",
      "\n",
      "At this point, Giselle was able to correctly infer the solution and win the game.\n",
      "\n",
      "What were the facedown cards in the middle of the table?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's analyze the game step by step:\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_tokenizer().decode(result[0].squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 585])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])] * 2, device=\"cuda\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 37.38 MiB is free. Including non-PyTorch memory, this process has 39.33 GiB memory in use. Of the allocated memory 38.62 GiB is allocated by PyTorch, and 166.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torchtune/modules/transformer.py:599\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tokens, mask, encoder_input, encoder_mask, input_pos)\u001b[0m\n\u001b[1;32m    597\u001b[0m         hidden\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;66;03m# shape: [b, s, d]\u001b[39;00m\n\u001b[0;32m--> 599\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# shape: [b, s, d]\u001b[39;00m\n\u001b[1;32m    608\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torchtune/modules/transformer.py:120\u001b[0m, in \u001b[0;36mTransformerSelfAttentionLayer.forward\u001b[0;34m(self, x, mask, input_pos, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_scale(attn_out) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Norm applied before the feedforward layer\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Residual connection; shape: [batch_size, seq_length, embed_dim]\u001b[39;00m\n\u001b[1;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_scale(mlp_out)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torchtune/modules/feed_forward.py:52\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw1(x))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw3 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw2(h)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 37.38 MiB is free. Including non-PyTorch memory, this process has 39.33 GiB memory in use. Of the allocated memory 38.62 GiB is allocated by PyTorch, and 166.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.forward(torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])] * 2, device=\"cuda\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
