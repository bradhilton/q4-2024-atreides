import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1390 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Jan  9 2025, 22:54:50) [GCC 13.2.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Thu Jan  9 23:17:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   25C    P0            121W /  700W |    7746MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   29C    P0            122W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   49C    P0            131W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    3216MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin', 'data/fineweb10B/fineweb_train_000010.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1390 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1390 train_time:248598ms step_avg:nanms
step:2/1390 train_time:248691ms step_avg:nanms
step:3/1390 train_time:250132ms step_avg:nanms
step:4/1390 train_time:250265ms step_avg:nanms
step:5/1390 train_time:250399ms step_avg:nanms
step:6/1390 train_time:250533ms step_avg:nanms
step:7/1390 train_time:250665ms step_avg:nanms
step:8/1390 train_time:250797ms step_avg:nanms
step:9/1390 train_time:250930ms step_avg:nanms
step:10/1390 train_time:251067ms step_avg:nanms
step:11/1390 train_time:137ms step_avg:nanms
step:12/1390 train_time:273ms step_avg:nanms
step:13/1390 train_time:407ms step_avg:135.59ms
step:14/1390 train_time:541ms step_avg:135.16ms
step:15/1390 train_time:674ms step_avg:134.86ms
step:16/1390 train_time:808ms step_avg:134.69ms
step:17/1390 train_time:942ms step_avg:134.59ms
step:18/1390 train_time:1079ms step_avg:134.82ms
step:19/1390 train_time:1213ms step_avg:134.79ms
step:20/1390 train_time:1349ms step_avg:134.93ms
step:21/1390 train_time:1485ms step_avg:134.97ms
step:22/1390 train_time:1619ms step_avg:134.90ms
step:23/1390 train_time:1752ms step_avg:134.78ms
step:24/1390 train_time:1888ms step_avg:134.83ms
step:25/1390 train_time:2021ms step_avg:134.72ms
step:26/1390 train_time:2156ms step_avg:134.76ms
step:27/1390 train_time:2291ms step_avg:134.74ms
step:28/1390 train_time:2426ms step_avg:134.80ms
step:29/1390 train_time:2560ms step_avg:134.75ms
step:30/1390 train_time:2694ms step_avg:134.72ms
step:31/1390 train_time:2829ms step_avg:134.70ms
step:32/1390 train_time:2964ms step_avg:134.72ms
step:33/1390 train_time:3097ms step_avg:134.66ms
step:34/1390 train_time:3233ms step_avg:134.70ms
step:35/1390 train_time:3371ms step_avg:134.83ms
step:36/1390 train_time:3505ms step_avg:134.82ms
step:37/1390 train_time:3641ms step_avg:134.84ms
step:38/1390 train_time:3775ms step_avg:134.84ms
step:39/1390 train_time:3909ms step_avg:134.81ms
step:40/1390 train_time:4044ms step_avg:134.80ms
step:41/1390 train_time:4178ms step_avg:134.79ms
step:42/1390 train_time:4313ms step_avg:134.78ms
step:43/1390 train_time:4448ms step_avg:134.80ms
step:44/1390 train_time:4584ms step_avg:134.82ms
step:45/1390 train_time:4718ms step_avg:134.79ms
step:46/1390 train_time:4852ms step_avg:134.79ms
step:47/1390 train_time:4988ms step_avg:134.81ms
step:48/1390 train_time:5123ms step_avg:134.80ms
step:49/1390 train_time:5257ms step_avg:134.79ms
step:50/1390 train_time:5391ms step_avg:134.78ms
step:51/1390 train_time:5527ms step_avg:134.82ms
step:52/1390 train_time:5662ms step_avg:134.81ms
step:53/1390 train_time:5796ms step_avg:134.79ms
step:54/1390 train_time:5931ms step_avg:134.79ms
step:55/1390 train_time:6067ms step_avg:134.82ms
step:56/1390 train_time:6201ms step_avg:134.80ms
step:57/1390 train_time:6335ms step_avg:134.79ms
step:58/1390 train_time:6470ms step_avg:134.80ms
step:59/1390 train_time:6605ms step_avg:134.79ms
step:60/1390 train_time:6739ms step_avg:134.78ms
step:61/1390 train_time:6875ms step_avg:134.81ms
step:62/1390 train_time:7011ms step_avg:134.82ms
step:63/1390 train_time:7146ms step_avg:134.83ms
step:64/1390 train_time:7280ms step_avg:134.82ms
step:65/1390 train_time:7415ms step_avg:134.81ms
step:66/1390 train_time:7550ms step_avg:134.82ms
step:67/1390 train_time:7685ms step_avg:134.82ms
step:68/1390 train_time:7818ms step_avg:134.80ms
step:69/1390 train_time:7955ms step_avg:134.83ms
step:70/1390 train_time:8090ms step_avg:134.83ms
step:71/1390 train_time:8223ms step_avg:134.81ms
step:72/1390 train_time:8357ms step_avg:134.79ms
step:73/1390 train_time:8492ms step_avg:134.79ms
step:74/1390 train_time:8628ms step_avg:134.81ms
step:75/1390 train_time:8761ms step_avg:134.78ms
step:76/1390 train_time:8895ms step_avg:134.77ms
step:77/1390 train_time:9030ms step_avg:134.78ms
step:78/1390 train_time:9167ms step_avg:134.81ms
step:79/1390 train_time:9300ms step_avg:134.78ms
step:80/1390 train_time:9434ms step_avg:134.77ms
step:81/1390 train_time:9570ms step_avg:134.79ms
step:82/1390 train_time:9704ms step_avg:134.77ms
step:83/1390 train_time:9838ms step_avg:134.77ms
step:84/1390 train_time:9973ms step_avg:134.77ms
step:85/1390 train_time:10108ms step_avg:134.78ms
step:86/1390 train_time:10243ms step_avg:134.77ms
step:87/1390 train_time:10378ms step_avg:134.78ms
step:88/1390 train_time:10512ms step_avg:134.77ms
step:89/1390 train_time:10648ms step_avg:134.78ms
step:90/1390 train_time:10782ms step_avg:134.77ms
step:91/1390 train_time:10916ms step_avg:134.77ms
step:92/1390 train_time:11050ms step_avg:134.75ms
step:93/1390 train_time:11186ms step_avg:134.77ms
step:94/1390 train_time:11319ms step_avg:134.75ms
step:95/1390 train_time:11454ms step_avg:134.75ms
step:96/1390 train_time:11590ms step_avg:134.76ms
step:97/1390 train_time:11723ms step_avg:134.75ms
step:98/1390 train_time:11858ms step_avg:134.75ms
step:99/1390 train_time:11993ms step_avg:134.75ms
step:100/1390 train_time:12128ms step_avg:134.76ms
step:101/1390 train_time:12262ms step_avg:134.74ms
step:102/1390 train_time:12396ms step_avg:134.74ms
step:103/1390 train_time:12531ms step_avg:134.74ms
step:104/1390 train_time:12669ms step_avg:134.77ms
step:105/1390 train_time:12807ms step_avg:134.81ms
step:106/1390 train_time:12944ms step_avg:134.84ms
step:107/1390 train_time:13082ms step_avg:134.87ms
step:108/1390 train_time:13219ms step_avg:134.88ms
step:109/1390 train_time:13356ms step_avg:134.91ms
step:110/1390 train_time:13494ms step_avg:134.94ms
step:111/1390 train_time:13632ms step_avg:134.97ms
step:112/1390 train_time:13771ms step_avg:135.01ms
step:113/1390 train_time:13909ms step_avg:135.04ms
step:114/1390 train_time:14046ms step_avg:135.06ms
step:115/1390 train_time:14187ms step_avg:135.11ms
step:116/1390 train_time:14322ms step_avg:135.12ms
step:117/1390 train_time:14461ms step_avg:135.15ms
step:118/1390 train_time:14598ms step_avg:135.17ms
step:119/1390 train_time:14736ms step_avg:135.19ms
step:120/1390 train_time:14875ms step_avg:135.23ms
step:121/1390 train_time:15013ms step_avg:135.25ms
step:122/1390 train_time:15151ms step_avg:135.27ms
step:123/1390 train_time:15290ms step_avg:135.31ms
step:124/1390 train_time:15428ms step_avg:135.33ms
step:125/1390 train_time:15567ms step_avg:135.36ms
step:125/1390 val_loss:4.3938 train_time:15635ms step_avg:135.96ms
step:126/1390 train_time:15708ms step_avg:135.41ms
step:127/1390 train_time:15851ms step_avg:135.48ms
step:128/1390 train_time:15989ms step_avg:135.50ms
step:129/1390 train_time:16127ms step_avg:135.52ms
step:130/1390 train_time:16264ms step_avg:135.53ms
step:131/1390 train_time:16401ms step_avg:135.54ms
step:132/1390 train_time:16538ms step_avg:135.56ms
step:133/1390 train_time:16677ms step_avg:135.59ms
step:134/1390 train_time:16819ms step_avg:135.64ms
step:135/1390 train_time:16958ms step_avg:135.66ms
step:136/1390 train_time:17095ms step_avg:135.68ms
step:137/1390 train_time:17234ms step_avg:135.70ms
step:138/1390 train_time:17371ms step_avg:135.71ms
step:139/1390 train_time:17509ms step_avg:135.73ms
step:140/1390 train_time:17646ms step_avg:135.74ms
step:141/1390 train_time:17785ms step_avg:135.76ms
step:142/1390 train_time:17924ms step_avg:135.79ms
step:143/1390 train_time:18063ms step_avg:135.81ms
step:144/1390 train_time:18200ms step_avg:135.82ms
step:145/1390 train_time:18340ms step_avg:135.85ms
step:146/1390 train_time:18479ms step_avg:135.87ms
step:147/1390 train_time:18617ms step_avg:135.89ms
step:148/1390 train_time:18755ms step_avg:135.91ms
step:149/1390 train_time:18892ms step_avg:135.91ms
step:150/1390 train_time:19030ms step_avg:135.93ms
step:151/1390 train_time:19168ms step_avg:135.95ms
step:152/1390 train_time:19307ms step_avg:135.96ms
step:153/1390 train_time:19445ms step_avg:135.98ms
step:154/1390 train_time:19583ms step_avg:135.99ms
step:155/1390 train_time:19721ms step_avg:136.01ms
step:156/1390 train_time:19861ms step_avg:136.03ms
step:157/1390 train_time:19999ms step_avg:136.05ms
step:158/1390 train_time:20139ms step_avg:136.07ms
step:159/1390 train_time:20277ms step_avg:136.09ms
step:160/1390 train_time:20416ms step_avg:136.11ms
step:161/1390 train_time:20554ms step_avg:136.12ms
step:162/1390 train_time:20693ms step_avg:136.14ms
step:163/1390 train_time:20832ms step_avg:136.16ms
step:164/1390 train_time:20970ms step_avg:136.17ms
step:165/1390 train_time:21108ms step_avg:136.18ms
step:166/1390 train_time:21247ms step_avg:136.20ms
step:167/1390 train_time:21385ms step_avg:136.21ms
step:168/1390 train_time:21523ms step_avg:136.22ms
step:169/1390 train_time:21661ms step_avg:136.23ms
step:170/1390 train_time:21801ms step_avg:136.26ms
step:171/1390 train_time:21940ms step_avg:136.28ms
step:172/1390 train_time:22080ms step_avg:136.29ms
step:173/1390 train_time:22218ms step_avg:136.31ms
step:174/1390 train_time:22357ms step_avg:136.32ms
step:175/1390 train_time:22495ms step_avg:136.33ms
step:176/1390 train_time:22634ms step_avg:136.35ms
step:177/1390 train_time:22772ms step_avg:136.36ms
step:178/1390 train_time:22912ms step_avg:136.38ms
step:179/1390 train_time:23049ms step_avg:136.38ms
step:180/1390 train_time:23189ms step_avg:136.40ms
step:181/1390 train_time:23328ms step_avg:136.42ms
step:182/1390 train_time:23467ms step_avg:136.44ms
step:183/1390 train_time:23606ms step_avg:136.45ms
step:184/1390 train_time:23744ms step_avg:136.46ms
step:185/1390 train_time:23883ms step_avg:136.48ms
step:186/1390 train_time:24022ms step_avg:136.49ms
step:187/1390 train_time:24161ms step_avg:136.50ms
step:188/1390 train_time:24300ms step_avg:136.52ms
step:189/1390 train_time:24441ms step_avg:136.54ms
step:190/1390 train_time:24580ms step_avg:136.55ms
step:191/1390 train_time:24769ms step_avg:136.85ms
step:192/1390 train_time:24906ms step_avg:136.85ms
step:193/1390 train_time:25044ms step_avg:136.85ms
step:194/1390 train_time:25182ms step_avg:136.86ms
step:195/1390 train_time:25319ms step_avg:136.86ms
step:196/1390 train_time:25457ms step_avg:136.86ms
step:197/1390 train_time:25595ms step_avg:136.87ms
step:198/1390 train_time:25739ms step_avg:136.91ms
step:199/1390 train_time:25880ms step_avg:136.93ms
step:200/1390 train_time:26019ms step_avg:136.94ms
step:201/1390 train_time:26156ms step_avg:136.94ms
step:202/1390 train_time:26295ms step_avg:136.95ms
step:203/1390 train_time:26432ms step_avg:136.95ms
step:204/1390 train_time:26572ms step_avg:136.97ms
step:205/1390 train_time:26711ms step_avg:136.98ms
step:206/1390 train_time:26850ms step_avg:136.99ms
step:207/1390 train_time:26991ms step_avg:137.01ms
step:208/1390 train_time:27133ms step_avg:137.04ms
step:209/1390 train_time:27273ms step_avg:137.05ms
step:210/1390 train_time:27414ms step_avg:137.07ms
step:211/1390 train_time:27554ms step_avg:137.08ms
step:212/1390 train_time:27695ms step_avg:137.10ms
step:213/1390 train_time:27838ms step_avg:137.13ms
step:214/1390 train_time:27979ms step_avg:137.15ms
step:215/1390 train_time:28121ms step_avg:137.18ms
step:216/1390 train_time:28262ms step_avg:137.20ms
step:217/1390 train_time:28402ms step_avg:137.21ms
step:218/1390 train_time:28543ms step_avg:137.23ms
step:219/1390 train_time:28684ms step_avg:137.24ms
step:220/1390 train_time:28824ms step_avg:137.26ms
step:221/1390 train_time:28966ms step_avg:137.28ms
step:222/1390 train_time:29108ms step_avg:137.30ms
step:223/1390 train_time:29249ms step_avg:137.32ms
step:224/1390 train_time:29389ms step_avg:137.33ms
step:225/1390 train_time:29531ms step_avg:137.36ms
step:226/1390 train_time:29671ms step_avg:137.37ms
step:227/1390 train_time:29813ms step_avg:137.39ms
step:228/1390 train_time:29953ms step_avg:137.40ms
step:229/1390 train_time:30095ms step_avg:137.42ms
step:230/1390 train_time:30237ms step_avg:137.44ms
step:231/1390 train_time:30378ms step_avg:137.46ms
step:232/1390 train_time:30520ms step_avg:137.48ms
step:233/1390 train_time:30660ms step_avg:137.49ms
step:234/1390 train_time:30801ms step_avg:137.50ms
step:235/1390 train_time:30943ms step_avg:137.52ms
step:236/1390 train_time:31084ms step_avg:137.54ms
step:237/1390 train_time:31224ms step_avg:137.55ms
step:238/1390 train_time:31365ms step_avg:137.57ms
step:239/1390 train_time:31506ms step_avg:137.58ms
step:240/1390 train_time:31648ms step_avg:137.60ms
step:241/1390 train_time:31791ms step_avg:137.62ms
step:242/1390 train_time:31933ms step_avg:137.64ms
step:243/1390 train_time:32073ms step_avg:137.65ms
step:244/1390 train_time:32214ms step_avg:137.67ms
step:245/1390 train_time:32355ms step_avg:137.68ms
step:246/1390 train_time:32497ms step_avg:137.70ms
step:247/1390 train_time:32640ms step_avg:137.72ms
step:248/1390 train_time:32781ms step_avg:137.74ms
step:249/1390 train_time:32923ms step_avg:137.75ms
step:250/1390 train_time:33064ms step_avg:137.77ms
step:250/1390 val_loss:3.9570 train_time:33131ms step_avg:138.05ms
step:251/1390 train_time:33205ms step_avg:137.78ms
step:252/1390 train_time:33351ms step_avg:137.81ms
step:253/1390 train_time:33493ms step_avg:137.83ms
step:254/1390 train_time:33634ms step_avg:137.84ms
step:255/1390 train_time:33773ms step_avg:137.85ms
step:256/1390 train_time:33914ms step_avg:137.86ms
step:257/1390 train_time:34054ms step_avg:137.87ms
step:258/1390 train_time:34195ms step_avg:137.88ms
step:259/1390 train_time:34338ms step_avg:137.90ms
step:260/1390 train_time:34479ms step_avg:137.92ms
step:261/1390 train_time:34619ms step_avg:137.92ms
step:262/1390 train_time:34761ms step_avg:137.94ms
step:263/1390 train_time:34904ms step_avg:137.96ms
step:264/1390 train_time:35044ms step_avg:137.97ms
step:265/1390 train_time:35184ms step_avg:137.98ms
step:266/1390 train_time:35326ms step_avg:137.99ms
step:267/1390 train_time:35467ms step_avg:138.00ms
step:268/1390 train_time:35608ms step_avg:138.01ms
step:269/1390 train_time:35750ms step_avg:138.03ms
step:270/1390 train_time:35893ms step_avg:138.05ms
step:271/1390 train_time:36034ms step_avg:138.06ms
step:272/1390 train_time:36175ms step_avg:138.07ms
step:273/1390 train_time:36315ms step_avg:138.08ms
step:274/1390 train_time:36457ms step_avg:138.09ms
step:275/1390 train_time:36600ms step_avg:138.11ms
step:276/1390 train_time:36741ms step_avg:138.12ms
step:277/1390 train_time:36884ms step_avg:138.14ms
step:278/1390 train_time:37025ms step_avg:138.15ms
step:279/1390 train_time:37167ms step_avg:138.17ms
step:280/1390 train_time:37308ms step_avg:138.18ms
step:281/1390 train_time:37451ms step_avg:138.19ms
step:282/1390 train_time:37592ms step_avg:138.21ms
step:283/1390 train_time:37733ms step_avg:138.22ms
step:284/1390 train_time:37873ms step_avg:138.22ms
step:285/1390 train_time:38015ms step_avg:138.24ms
step:286/1390 train_time:38158ms step_avg:138.25ms
step:287/1390 train_time:38300ms step_avg:138.27ms
step:288/1390 train_time:38442ms step_avg:138.28ms
step:289/1390 train_time:38583ms step_avg:138.29ms
step:290/1390 train_time:38724ms step_avg:138.30ms
step:291/1390 train_time:38865ms step_avg:138.31ms
step:292/1390 train_time:39005ms step_avg:138.32ms
step:293/1390 train_time:39147ms step_avg:138.33ms
step:294/1390 train_time:39289ms step_avg:138.34ms
step:295/1390 train_time:39430ms step_avg:138.35ms
step:296/1390 train_time:39572ms step_avg:138.36ms
step:297/1390 train_time:39714ms step_avg:138.37ms
step:298/1390 train_time:39855ms step_avg:138.39ms
step:299/1390 train_time:39997ms step_avg:138.40ms
step:300/1390 train_time:40140ms step_avg:138.41ms
step:301/1390 train_time:40281ms step_avg:138.42ms
step:302/1390 train_time:40422ms step_avg:138.43ms
step:303/1390 train_time:40564ms step_avg:138.44ms
step:304/1390 train_time:40705ms step_avg:138.45ms
step:305/1390 train_time:40848ms step_avg:138.47ms
step:306/1390 train_time:40990ms step_avg:138.48ms
step:307/1390 train_time:41132ms step_avg:138.49ms
step:308/1390 train_time:41274ms step_avg:138.50ms
step:309/1390 train_time:41414ms step_avg:138.51ms
step:310/1390 train_time:41557ms step_avg:138.52ms
step:311/1390 train_time:41701ms step_avg:138.54ms
step:312/1390 train_time:41844ms step_avg:138.56ms
step:313/1390 train_time:41986ms step_avg:138.57ms
step:314/1390 train_time:42130ms step_avg:138.59ms
step:315/1390 train_time:42273ms step_avg:138.60ms
step:316/1390 train_time:42417ms step_avg:138.62ms
step:317/1390 train_time:42562ms step_avg:138.64ms
step:318/1390 train_time:42706ms step_avg:138.65ms
step:319/1390 train_time:42849ms step_avg:138.67ms
step:320/1390 train_time:42993ms step_avg:138.69ms
step:321/1390 train_time:43136ms step_avg:138.70ms
step:322/1390 train_time:43280ms step_avg:138.72ms
step:323/1390 train_time:43422ms step_avg:138.73ms
step:324/1390 train_time:43566ms step_avg:138.75ms
step:325/1390 train_time:43709ms step_avg:138.76ms
step:326/1390 train_time:43855ms step_avg:138.78ms
step:327/1390 train_time:44000ms step_avg:138.80ms
step:328/1390 train_time:44144ms step_avg:138.82ms
step:329/1390 train_time:44288ms step_avg:138.83ms
step:330/1390 train_time:44431ms step_avg:138.85ms
step:331/1390 train_time:44576ms step_avg:138.87ms
step:332/1390 train_time:44720ms step_avg:138.88ms
step:333/1390 train_time:44864ms step_avg:138.90ms
step:334/1390 train_time:45008ms step_avg:138.91ms
step:335/1390 train_time:45150ms step_avg:138.92ms
step:336/1390 train_time:45295ms step_avg:138.94ms
step:337/1390 train_time:45439ms step_avg:138.96ms
step:338/1390 train_time:45582ms step_avg:138.97ms
step:339/1390 train_time:45726ms step_avg:138.98ms
step:340/1390 train_time:45870ms step_avg:139.00ms
step:341/1390 train_time:46013ms step_avg:139.01ms
step:342/1390 train_time:46157ms step_avg:139.03ms
step:343/1390 train_time:46302ms step_avg:139.04ms
step:344/1390 train_time:46446ms step_avg:139.06ms
step:345/1390 train_time:46589ms step_avg:139.07ms
step:346/1390 train_time:46733ms step_avg:139.09ms
step:347/1390 train_time:46876ms step_avg:139.10ms
step:348/1390 train_time:47020ms step_avg:139.11ms
step:349/1390 train_time:47163ms step_avg:139.13ms
step:350/1390 train_time:47307ms step_avg:139.14ms
step:351/1390 train_time:47451ms step_avg:139.15ms
step:352/1390 train_time:47594ms step_avg:139.16ms
step:353/1390 train_time:47738ms step_avg:139.18ms
step:354/1390 train_time:47882ms step_avg:139.19ms
step:355/1390 train_time:48025ms step_avg:139.20ms
step:356/1390 train_time:48168ms step_avg:139.22ms
step:357/1390 train_time:48313ms step_avg:139.23ms
step:358/1390 train_time:48459ms step_avg:139.25ms
step:359/1390 train_time:48602ms step_avg:139.26ms
step:360/1390 train_time:48747ms step_avg:139.28ms
step:361/1390 train_time:48890ms step_avg:139.29ms
step:362/1390 train_time:49035ms step_avg:139.30ms
step:363/1390 train_time:49178ms step_avg:139.31ms
step:364/1390 train_time:49322ms step_avg:139.33ms
step:365/1390 train_time:49466ms step_avg:139.34ms
step:366/1390 train_time:49609ms step_avg:139.35ms
step:367/1390 train_time:49754ms step_avg:139.37ms
step:368/1390 train_time:49897ms step_avg:139.38ms
step:369/1390 train_time:50041ms step_avg:139.39ms
step:370/1390 train_time:50183ms step_avg:139.40ms
step:371/1390 train_time:50328ms step_avg:139.41ms
step:372/1390 train_time:50471ms step_avg:139.42ms
step:373/1390 train_time:50617ms step_avg:139.44ms
step:374/1390 train_time:50762ms step_avg:139.46ms
step:375/1390 train_time:50905ms step_avg:139.46ms
step:375/1390 val_loss:3.7729 train_time:50975ms step_avg:139.66ms
step:376/1390 train_time:51050ms step_avg:139.48ms
step:377/1390 train_time:51196ms step_avg:139.50ms
step:378/1390 train_time:51342ms step_avg:139.52ms
step:379/1390 train_time:51484ms step_avg:139.52ms
step:380/1390 train_time:51627ms step_avg:139.53ms
step:381/1390 train_time:51827ms step_avg:139.69ms
step:382/1390 train_time:51968ms step_avg:139.70ms
step:383/1390 train_time:52110ms step_avg:139.70ms
step:384/1390 train_time:52253ms step_avg:139.71ms
step:385/1390 train_time:52396ms step_avg:139.72ms
step:386/1390 train_time:52538ms step_avg:139.73ms
step:387/1390 train_time:52682ms step_avg:139.74ms
step:388/1390 train_time:52830ms step_avg:139.76ms
step:389/1390 train_time:52974ms step_avg:139.77ms
step:390/1390 train_time:53118ms step_avg:139.78ms
step:391/1390 train_time:53262ms step_avg:139.79ms
step:392/1390 train_time:53406ms step_avg:139.81ms
step:393/1390 train_time:53549ms step_avg:139.82ms
step:394/1390 train_time:53693ms step_avg:139.82ms
step:395/1390 train_time:53836ms step_avg:139.83ms
step:396/1390 train_time:53982ms step_avg:139.85ms
step:397/1390 train_time:54125ms step_avg:139.86ms
step:398/1390 train_time:54268ms step_avg:139.87ms
step:399/1390 train_time:54411ms step_avg:139.87ms
step:400/1390 train_time:54554ms step_avg:139.88ms
step:401/1390 train_time:54698ms step_avg:139.89ms
step:402/1390 train_time:54842ms step_avg:139.90ms
step:403/1390 train_time:54986ms step_avg:139.91ms
step:404/1390 train_time:55130ms step_avg:139.92ms
step:405/1390 train_time:55274ms step_avg:139.93ms
step:406/1390 train_time:55417ms step_avg:139.94ms
step:407/1390 train_time:55561ms step_avg:139.95ms
step:408/1390 train_time:55705ms step_avg:139.96ms
step:409/1390 train_time:55849ms step_avg:139.97ms
step:410/1390 train_time:55992ms step_avg:139.98ms
step:411/1390 train_time:56135ms step_avg:139.99ms
step:412/1390 train_time:56278ms step_avg:140.00ms
step:413/1390 train_time:56422ms step_avg:140.01ms
step:414/1390 train_time:56570ms step_avg:140.02ms
step:415/1390 train_time:56714ms step_avg:140.04ms
step:416/1390 train_time:56861ms step_avg:140.05ms
step:417/1390 train_time:57006ms step_avg:140.07ms
step:418/1390 train_time:57152ms step_avg:140.08ms
step:419/1390 train_time:57297ms step_avg:140.09ms
step:420/1390 train_time:57444ms step_avg:140.11ms
step:421/1390 train_time:57588ms step_avg:140.12ms
step:422/1390 train_time:57732ms step_avg:140.13ms
step:423/1390 train_time:57878ms step_avg:140.14ms
step:424/1390 train_time:58024ms step_avg:140.15ms
step:425/1390 train_time:58169ms step_avg:140.17ms
step:426/1390 train_time:58315ms step_avg:140.18ms
step:427/1390 train_time:58462ms step_avg:140.20ms
step:428/1390 train_time:58608ms step_avg:140.21ms
step:429/1390 train_time:58754ms step_avg:140.23ms
step:430/1390 train_time:58900ms step_avg:140.24ms
step:431/1390 train_time:59046ms step_avg:140.25ms
step:432/1390 train_time:59191ms step_avg:140.26ms
step:433/1390 train_time:59337ms step_avg:140.28ms
step:434/1390 train_time:59482ms step_avg:140.29ms
step:435/1390 train_time:59627ms step_avg:140.30ms
step:436/1390 train_time:59773ms step_avg:140.31ms
step:437/1390 train_time:59919ms step_avg:140.33ms
step:438/1390 train_time:60065ms step_avg:140.34ms
step:439/1390 train_time:60211ms step_avg:140.35ms
step:440/1390 train_time:60356ms step_avg:140.36ms
step:441/1390 train_time:60502ms step_avg:140.38ms
step:442/1390 train_time:60648ms step_avg:140.39ms
step:443/1390 train_time:60793ms step_avg:140.40ms
step:444/1390 train_time:60939ms step_avg:140.41ms
step:445/1390 train_time:61085ms step_avg:140.43ms
step:446/1390 train_time:61230ms step_avg:140.44ms
step:447/1390 train_time:61377ms step_avg:140.45ms
step:448/1390 train_time:61522ms step_avg:140.46ms
step:449/1390 train_time:61669ms step_avg:140.48ms
step:450/1390 train_time:61815ms step_avg:140.49ms
step:451/1390 train_time:61962ms step_avg:140.50ms
step:452/1390 train_time:62107ms step_avg:140.51ms
step:453/1390 train_time:62253ms step_avg:140.53ms
step:454/1390 train_time:62398ms step_avg:140.54ms
step:455/1390 train_time:62545ms step_avg:140.55ms
step:456/1390 train_time:62690ms step_avg:140.56ms
step:457/1390 train_time:62836ms step_avg:140.57ms
step:458/1390 train_time:62982ms step_avg:140.59ms
step:459/1390 train_time:63127ms step_avg:140.60ms
step:460/1390 train_time:63273ms step_avg:140.61ms
step:461/1390 train_time:63419ms step_avg:140.62ms
step:462/1390 train_time:63565ms step_avg:140.63ms
step:463/1390 train_time:63710ms step_avg:140.64ms
step:464/1390 train_time:63855ms step_avg:140.65ms
step:465/1390 train_time:64001ms step_avg:140.66ms
step:466/1390 train_time:64148ms step_avg:140.67ms
step:467/1390 train_time:64292ms step_avg:140.68ms
step:468/1390 train_time:64437ms step_avg:140.69ms
step:469/1390 train_time:64583ms step_avg:140.70ms
step:470/1390 train_time:64728ms step_avg:140.71ms
step:471/1390 train_time:64873ms step_avg:140.72ms
step:472/1390 train_time:65019ms step_avg:140.73ms
step:473/1390 train_time:65166ms step_avg:140.75ms
step:474/1390 train_time:65313ms step_avg:140.76ms
step:475/1390 train_time:65457ms step_avg:140.77ms
step:476/1390 train_time:65603ms step_avg:140.78ms
step:477/1390 train_time:65748ms step_avg:140.79ms
step:478/1390 train_time:65892ms step_avg:140.80ms
step:479/1390 train_time:66038ms step_avg:140.81ms
step:480/1390 train_time:66184ms step_avg:140.82ms
step:481/1390 train_time:66330ms step_avg:140.83ms
step:482/1390 train_time:66475ms step_avg:140.84ms
step:483/1390 train_time:66619ms step_avg:140.84ms
step:484/1390 train_time:66766ms step_avg:140.86ms
step:485/1390 train_time:66911ms step_avg:140.87ms
step:486/1390 train_time:67058ms step_avg:140.88ms
step:487/1390 train_time:67204ms step_avg:140.89ms
step:488/1390 train_time:67349ms step_avg:140.90ms
step:489/1390 train_time:67493ms step_avg:140.90ms
step:490/1390 train_time:67639ms step_avg:140.91ms
step:491/1390 train_time:67784ms step_avg:140.92ms
step:492/1390 train_time:67930ms step_avg:140.93ms
step:493/1390 train_time:68074ms step_avg:140.94ms
step:494/1390 train_time:68221ms step_avg:140.95ms
step:495/1390 train_time:68368ms step_avg:140.97ms
step:496/1390 train_time:68513ms step_avg:140.97ms
step:497/1390 train_time:68658ms step_avg:140.98ms
step:498/1390 train_time:68804ms step_avg:140.99ms
step:499/1390 train_time:68949ms step_avg:141.00ms
step:500/1390 train_time:69093ms step_avg:141.01ms
step:500/1390 val_loss:3.6558 train_time:69165ms step_avg:141.15ms
step:501/1390 train_time:69242ms step_avg:141.02ms
step:502/1390 train_time:69390ms step_avg:141.04ms
step:503/1390 train_time:69537ms step_avg:141.05ms
step:504/1390 train_time:69680ms step_avg:141.05ms
step:505/1390 train_time:69825ms step_avg:141.06ms
step:506/1390 train_time:69970ms step_avg:141.07ms
step:507/1390 train_time:70114ms step_avg:141.08ms
step:508/1390 train_time:70262ms step_avg:141.09ms
step:509/1390 train_time:70408ms step_avg:141.10ms
step:510/1390 train_time:70555ms step_avg:141.11ms
step:511/1390 train_time:70701ms step_avg:141.12ms
step:512/1390 train_time:70847ms step_avg:141.13ms
step:513/1390 train_time:70992ms step_avg:141.14ms
step:514/1390 train_time:71137ms step_avg:141.15ms
step:515/1390 train_time:71283ms step_avg:141.15ms
step:516/1390 train_time:71431ms step_avg:141.17ms
step:517/1390 train_time:71578ms step_avg:141.18ms
step:518/1390 train_time:71725ms step_avg:141.19ms
step:519/1390 train_time:71872ms step_avg:141.20ms
step:520/1390 train_time:72019ms step_avg:141.21ms
step:521/1390 train_time:72165ms step_avg:141.22ms
step:522/1390 train_time:72313ms step_avg:141.24ms
step:523/1390 train_time:72462ms step_avg:141.25ms
step:524/1390 train_time:72610ms step_avg:141.27ms
step:525/1390 train_time:72757ms step_avg:141.28ms
step:526/1390 train_time:72904ms step_avg:141.29ms
step:527/1390 train_time:73054ms step_avg:141.30ms
step:528/1390 train_time:73200ms step_avg:141.31ms
step:529/1390 train_time:73348ms step_avg:141.33ms
step:530/1390 train_time:73494ms step_avg:141.33ms
step:531/1390 train_time:73641ms step_avg:141.35ms
step:532/1390 train_time:73788ms step_avg:141.36ms
step:533/1390 train_time:73936ms step_avg:141.37ms
step:534/1390 train_time:74082ms step_avg:141.38ms
step:535/1390 train_time:74232ms step_avg:141.39ms
step:536/1390 train_time:74378ms step_avg:141.40ms
step:537/1390 train_time:74526ms step_avg:141.42ms
step:538/1390 train_time:74673ms step_avg:141.43ms
step:539/1390 train_time:74822ms step_avg:141.44ms
step:540/1390 train_time:74970ms step_avg:141.45ms
step:541/1390 train_time:75115ms step_avg:141.46ms
step:542/1390 train_time:75263ms step_avg:141.47ms
step:543/1390 train_time:75410ms step_avg:141.48ms
step:544/1390 train_time:75557ms step_avg:141.49ms
step:545/1390 train_time:75705ms step_avg:141.50ms
step:546/1390 train_time:75853ms step_avg:141.52ms
step:547/1390 train_time:75999ms step_avg:141.53ms
step:548/1390 train_time:76148ms step_avg:141.54ms
step:549/1390 train_time:76295ms step_avg:141.55ms
step:550/1390 train_time:76442ms step_avg:141.56ms
step:551/1390 train_time:76590ms step_avg:141.57ms
step:552/1390 train_time:76738ms step_avg:141.58ms
step:553/1390 train_time:76885ms step_avg:141.59ms
step:554/1390 train_time:77033ms step_avg:141.60ms
step:555/1390 train_time:77179ms step_avg:141.61ms
step:556/1390 train_time:77326ms step_avg:141.62ms
step:557/1390 train_time:77474ms step_avg:141.63ms
step:558/1390 train_time:77620ms step_avg:141.64ms
step:559/1390 train_time:77767ms step_avg:141.65ms
step:560/1390 train_time:77914ms step_avg:141.66ms
step:561/1390 train_time:78062ms step_avg:141.67ms
step:562/1390 train_time:78208ms step_avg:141.68ms
step:563/1390 train_time:78356ms step_avg:141.69ms
step:564/1390 train_time:78504ms step_avg:141.70ms
step:565/1390 train_time:78652ms step_avg:141.72ms
step:566/1390 train_time:78799ms step_avg:141.73ms
step:567/1390 train_time:78946ms step_avg:141.73ms
step:568/1390 train_time:79094ms step_avg:141.75ms
step:569/1390 train_time:79240ms step_avg:141.75ms
step:570/1390 train_time:79388ms step_avg:141.76ms
step:571/1390 train_time:79593ms step_avg:141.88ms
step:572/1390 train_time:79738ms step_avg:141.88ms
step:573/1390 train_time:79885ms step_avg:141.89ms
step:574/1390 train_time:80034ms step_avg:141.90ms
step:575/1390 train_time:80180ms step_avg:141.91ms
step:576/1390 train_time:80327ms step_avg:141.92ms
step:577/1390 train_time:80475ms step_avg:141.93ms
step:578/1390 train_time:80624ms step_avg:141.94ms
step:579/1390 train_time:80771ms step_avg:141.95ms
step:580/1390 train_time:80919ms step_avg:141.96ms
step:581/1390 train_time:81067ms step_avg:141.97ms
step:582/1390 train_time:81214ms step_avg:141.98ms
step:583/1390 train_time:81362ms step_avg:141.99ms
step:584/1390 train_time:81509ms step_avg:142.00ms
step:585/1390 train_time:81656ms step_avg:142.01ms
step:586/1390 train_time:81804ms step_avg:142.02ms
step:587/1390 train_time:81952ms step_avg:142.03ms
step:588/1390 train_time:82101ms step_avg:142.04ms
step:589/1390 train_time:82249ms step_avg:142.05ms
step:590/1390 train_time:82395ms step_avg:142.06ms
step:591/1390 train_time:82541ms step_avg:142.07ms
step:592/1390 train_time:82690ms step_avg:142.08ms
step:593/1390 train_time:82837ms step_avg:142.09ms
step:594/1390 train_time:82985ms step_avg:142.10ms
step:595/1390 train_time:83133ms step_avg:142.11ms
step:596/1390 train_time:83280ms step_avg:142.12ms
step:597/1390 train_time:83428ms step_avg:142.13ms
step:598/1390 train_time:83574ms step_avg:142.13ms
step:599/1390 train_time:83722ms step_avg:142.14ms
step:600/1390 train_time:83871ms step_avg:142.15ms
step:601/1390 train_time:84018ms step_avg:142.16ms
step:602/1390 train_time:84165ms step_avg:142.17ms
step:603/1390 train_time:84313ms step_avg:142.18ms
step:604/1390 train_time:84461ms step_avg:142.19ms
step:605/1390 train_time:84609ms step_avg:142.20ms
step:606/1390 train_time:84756ms step_avg:142.21ms
step:607/1390 train_time:84903ms step_avg:142.22ms
step:608/1390 train_time:85052ms step_avg:142.23ms
step:609/1390 train_time:85198ms step_avg:142.23ms
step:610/1390 train_time:85347ms step_avg:142.24ms
step:611/1390 train_time:85493ms step_avg:142.25ms
step:612/1390 train_time:85640ms step_avg:142.26ms
step:613/1390 train_time:85789ms step_avg:142.27ms
step:614/1390 train_time:85937ms step_avg:142.28ms
step:615/1390 train_time:86083ms step_avg:142.29ms
step:616/1390 train_time:86230ms step_avg:142.29ms
step:617/1390 train_time:86377ms step_avg:142.30ms
step:618/1390 train_time:86523ms step_avg:142.31ms
step:619/1390 train_time:86672ms step_avg:142.32ms
step:620/1390 train_time:86821ms step_avg:142.33ms
step:621/1390 train_time:86971ms step_avg:142.34ms
step:622/1390 train_time:87118ms step_avg:142.35ms
step:623/1390 train_time:87268ms step_avg:142.36ms
step:624/1390 train_time:87416ms step_avg:142.37ms
step:625/1390 train_time:87565ms step_avg:142.38ms
step:625/1390 val_loss:3.5730 train_time:87641ms step_avg:142.51ms
step:626/1390 train_time:87717ms step_avg:142.40ms
step:627/1390 train_time:87867ms step_avg:142.41ms
step:628/1390 train_time:88017ms step_avg:142.42ms
step:629/1390 train_time:88165ms step_avg:142.43ms
step:630/1390 train_time:88313ms step_avg:142.44ms
step:631/1390 train_time:88460ms step_avg:142.45ms
step:632/1390 train_time:88607ms step_avg:142.46ms
step:633/1390 train_time:88756ms step_avg:142.47ms
step:634/1390 train_time:88906ms step_avg:142.48ms
step:635/1390 train_time:89056ms step_avg:142.49ms
step:636/1390 train_time:89206ms step_avg:142.50ms
step:637/1390 train_time:89356ms step_avg:142.51ms
step:638/1390 train_time:89503ms step_avg:142.52ms
step:639/1390 train_time:89650ms step_avg:142.53ms
step:640/1390 train_time:89799ms step_avg:142.54ms
step:641/1390 train_time:89947ms step_avg:142.55ms
step:642/1390 train_time:90096ms step_avg:142.56ms
step:643/1390 train_time:90245ms step_avg:142.57ms
step:644/1390 train_time:90395ms step_avg:142.58ms
step:645/1390 train_time:90543ms step_avg:142.59ms
step:646/1390 train_time:90691ms step_avg:142.60ms
step:647/1390 train_time:90840ms step_avg:142.61ms
step:648/1390 train_time:90991ms step_avg:142.62ms
step:649/1390 train_time:91140ms step_avg:142.63ms
step:650/1390 train_time:91288ms step_avg:142.64ms
step:651/1390 train_time:91437ms step_avg:142.65ms
step:652/1390 train_time:91585ms step_avg:142.66ms
step:653/1390 train_time:91735ms step_avg:142.67ms
step:654/1390 train_time:91883ms step_avg:142.68ms
step:655/1390 train_time:92032ms step_avg:142.68ms
step:656/1390 train_time:92181ms step_avg:142.69ms
step:657/1390 train_time:92329ms step_avg:142.70ms
step:658/1390 train_time:92479ms step_avg:142.71ms
step:659/1390 train_time:92627ms step_avg:142.72ms
step:660/1390 train_time:92777ms step_avg:142.73ms
step:661/1390 train_time:92926ms step_avg:142.74ms
step:662/1390 train_time:93075ms step_avg:142.75ms
step:663/1390 train_time:93223ms step_avg:142.76ms
step:664/1390 train_time:93373ms step_avg:142.77ms
step:665/1390 train_time:93523ms step_avg:142.78ms
step:666/1390 train_time:93671ms step_avg:142.79ms
step:667/1390 train_time:93822ms step_avg:142.80ms
step:668/1390 train_time:93971ms step_avg:142.81ms
step:669/1390 train_time:94121ms step_avg:142.82ms
step:670/1390 train_time:94267ms step_avg:142.83ms
step:671/1390 train_time:94417ms step_avg:142.84ms
step:672/1390 train_time:94565ms step_avg:142.85ms
step:673/1390 train_time:94716ms step_avg:142.86ms
step:674/1390 train_time:94866ms step_avg:142.87ms
step:675/1390 train_time:95017ms step_avg:142.88ms
step:676/1390 train_time:95166ms step_avg:142.89ms
step:677/1390 train_time:95314ms step_avg:142.90ms
step:678/1390 train_time:95462ms step_avg:142.91ms
step:679/1390 train_time:95610ms step_avg:142.91ms
step:680/1390 train_time:95759ms step_avg:142.92ms
step:681/1390 train_time:95910ms step_avg:142.94ms
step:682/1390 train_time:96061ms step_avg:142.95ms
step:683/1390 train_time:96208ms step_avg:142.95ms
step:684/1390 train_time:96358ms step_avg:142.96ms
step:685/1390 train_time:96508ms step_avg:142.98ms
step:686/1390 train_time:96657ms step_avg:142.98ms
step:687/1390 train_time:96805ms step_avg:142.99ms
step:688/1390 train_time:96954ms step_avg:143.00ms
step:689/1390 train_time:97104ms step_avg:143.01ms
step:690/1390 train_time:97254ms step_avg:143.02ms
step:691/1390 train_time:97403ms step_avg:143.03ms
step:692/1390 train_time:97551ms step_avg:143.04ms
step:693/1390 train_time:97700ms step_avg:143.05ms
step:694/1390 train_time:97848ms step_avg:143.05ms
step:695/1390 train_time:97997ms step_avg:143.06ms
step:696/1390 train_time:98144ms step_avg:143.07ms
step:697/1390 train_time:98294ms step_avg:143.08ms
step:698/1390 train_time:98443ms step_avg:143.09ms
step:699/1390 train_time:98592ms step_avg:143.09ms
step:700/1390 train_time:98741ms step_avg:143.10ms
step:701/1390 train_time:98888ms step_avg:143.11ms
step:702/1390 train_time:99038ms step_avg:143.12ms
step:703/1390 train_time:99186ms step_avg:143.12ms
step:704/1390 train_time:99335ms step_avg:143.13ms
step:705/1390 train_time:99484ms step_avg:143.14ms
step:706/1390 train_time:99637ms step_avg:143.16ms
step:707/1390 train_time:99784ms step_avg:143.16ms
step:708/1390 train_time:99935ms step_avg:143.17ms
step:709/1390 train_time:100084ms step_avg:143.18ms
step:710/1390 train_time:100235ms step_avg:143.19ms
step:711/1390 train_time:100384ms step_avg:143.20ms
step:712/1390 train_time:100533ms step_avg:143.21ms
step:713/1390 train_time:100683ms step_avg:143.22ms
step:714/1390 train_time:100830ms step_avg:143.22ms
step:715/1390 train_time:100979ms step_avg:143.23ms
step:716/1390 train_time:101130ms step_avg:143.24ms
step:717/1390 train_time:101279ms step_avg:143.25ms
step:718/1390 train_time:101428ms step_avg:143.26ms
step:719/1390 train_time:101576ms step_avg:143.27ms
step:720/1390 train_time:101725ms step_avg:143.28ms
step:721/1390 train_time:101874ms step_avg:143.28ms
step:722/1390 train_time:102025ms step_avg:143.29ms
step:723/1390 train_time:102175ms step_avg:143.30ms
step:724/1390 train_time:102325ms step_avg:143.31ms
step:725/1390 train_time:102477ms step_avg:143.32ms
step:726/1390 train_time:102627ms step_avg:143.33ms
step:727/1390 train_time:102778ms step_avg:143.35ms
step:728/1390 train_time:102927ms step_avg:143.35ms
step:729/1390 train_time:103076ms step_avg:143.36ms
step:730/1390 train_time:103227ms step_avg:143.37ms
step:731/1390 train_time:103377ms step_avg:143.38ms
step:732/1390 train_time:103526ms step_avg:143.39ms
step:733/1390 train_time:103676ms step_avg:143.40ms
step:734/1390 train_time:103825ms step_avg:143.41ms
step:735/1390 train_time:103976ms step_avg:143.41ms
step:736/1390 train_time:104126ms step_avg:143.42ms
step:737/1390 train_time:104276ms step_avg:143.43ms
step:738/1390 train_time:104426ms step_avg:143.44ms
step:739/1390 train_time:104576ms step_avg:143.45ms
step:740/1390 train_time:104727ms step_avg:143.46ms
step:741/1390 train_time:104877ms step_avg:143.47ms
step:742/1390 train_time:105027ms step_avg:143.48ms
step:743/1390 train_time:105177ms step_avg:143.49ms
step:744/1390 train_time:105328ms step_avg:143.50ms
step:745/1390 train_time:105482ms step_avg:143.51ms
step:746/1390 train_time:105633ms step_avg:143.52ms
step:747/1390 train_time:105782ms step_avg:143.53ms
step:748/1390 train_time:105933ms step_avg:143.54ms
step:749/1390 train_time:106085ms step_avg:143.55ms
step:750/1390 train_time:106238ms step_avg:143.57ms
step:750/1390 val_loss:3.5208 train_time:106315ms step_avg:143.67ms
step:751/1390 train_time:106394ms step_avg:143.58ms
step:752/1390 train_time:106545ms step_avg:143.59ms
step:753/1390 train_time:106695ms step_avg:143.60ms
step:754/1390 train_time:106845ms step_avg:143.61ms
step:755/1390 train_time:106993ms step_avg:143.61ms
step:756/1390 train_time:107141ms step_avg:143.62ms
step:757/1390 train_time:107293ms step_avg:143.63ms
step:758/1390 train_time:107445ms step_avg:143.64ms
step:759/1390 train_time:107597ms step_avg:143.65ms
step:760/1390 train_time:107746ms step_avg:143.66ms
step:761/1390 train_time:107951ms step_avg:143.74ms
step:762/1390 train_time:108100ms step_avg:143.75ms
step:763/1390 train_time:108248ms step_avg:143.76ms
step:764/1390 train_time:108400ms step_avg:143.77ms
step:765/1390 train_time:108548ms step_avg:143.77ms
step:766/1390 train_time:108699ms step_avg:143.78ms
step:767/1390 train_time:108850ms step_avg:143.79ms
step:768/1390 train_time:109004ms step_avg:143.80ms
step:769/1390 train_time:109156ms step_avg:143.82ms
step:770/1390 train_time:109306ms step_avg:143.82ms
step:771/1390 train_time:109457ms step_avg:143.83ms
step:772/1390 train_time:109607ms step_avg:143.84ms
step:773/1390 train_time:109756ms step_avg:143.85ms
step:774/1390 train_time:109906ms step_avg:143.86ms
step:775/1390 train_time:110057ms step_avg:143.87ms
step:776/1390 train_time:110208ms step_avg:143.87ms
step:777/1390 train_time:110360ms step_avg:143.89ms
step:778/1390 train_time:110509ms step_avg:143.89ms
step:779/1390 train_time:110659ms step_avg:143.90ms
step:780/1390 train_time:110810ms step_avg:143.91ms
step:781/1390 train_time:110960ms step_avg:143.92ms
step:782/1390 train_time:111111ms step_avg:143.93ms
step:783/1390 train_time:111262ms step_avg:143.93ms
step:784/1390 train_time:111412ms step_avg:143.94ms
step:785/1390 train_time:111562ms step_avg:143.95ms
step:786/1390 train_time:111713ms step_avg:143.96ms
step:787/1390 train_time:111863ms step_avg:143.97ms
step:788/1390 train_time:112013ms step_avg:143.98ms
step:789/1390 train_time:112162ms step_avg:143.98ms
step:790/1390 train_time:112313ms step_avg:143.99ms
step:791/1390 train_time:112464ms step_avg:144.00ms
step:792/1390 train_time:112615ms step_avg:144.01ms
step:793/1390 train_time:112767ms step_avg:144.02ms
step:794/1390 train_time:112916ms step_avg:144.03ms
step:795/1390 train_time:113069ms step_avg:144.04ms
step:796/1390 train_time:113220ms step_avg:144.05ms
step:797/1390 train_time:113370ms step_avg:144.05ms
step:798/1390 train_time:113520ms step_avg:144.06ms
step:799/1390 train_time:113672ms step_avg:144.07ms
step:800/1390 train_time:113821ms step_avg:144.08ms
step:801/1390 train_time:113971ms step_avg:144.08ms
step:802/1390 train_time:114123ms step_avg:144.09ms
step:803/1390 train_time:114271ms step_avg:144.10ms
step:804/1390 train_time:114420ms step_avg:144.11ms
step:805/1390 train_time:114572ms step_avg:144.12ms
step:806/1390 train_time:114722ms step_avg:144.12ms
step:807/1390 train_time:114871ms step_avg:144.13ms
step:808/1390 train_time:115022ms step_avg:144.14ms
step:809/1390 train_time:115172ms step_avg:144.15ms
step:810/1390 train_time:115321ms step_avg:144.15ms
step:811/1390 train_time:115472ms step_avg:144.16ms
step:812/1390 train_time:115623ms step_avg:144.17ms
step:813/1390 train_time:115772ms step_avg:144.17ms
step:814/1390 train_time:115921ms step_avg:144.18ms
step:815/1390 train_time:116071ms step_avg:144.19ms
step:816/1390 train_time:116222ms step_avg:144.20ms
step:817/1390 train_time:116372ms step_avg:144.20ms
step:818/1390 train_time:116521ms step_avg:144.21ms
step:819/1390 train_time:116674ms step_avg:144.22ms
step:820/1390 train_time:116824ms step_avg:144.23ms
step:821/1390 train_time:116972ms step_avg:144.23ms
step:822/1390 train_time:117121ms step_avg:144.24ms
step:823/1390 train_time:117272ms step_avg:144.25ms
step:824/1390 train_time:117420ms step_avg:144.25ms
step:825/1390 train_time:117572ms step_avg:144.26ms
step:826/1390 train_time:117727ms step_avg:144.27ms
step:827/1390 train_time:117876ms step_avg:144.28ms
step:828/1390 train_time:118028ms step_avg:144.29ms
step:829/1390 train_time:118179ms step_avg:144.30ms
step:830/1390 train_time:118330ms step_avg:144.31ms
step:831/1390 train_time:118482ms step_avg:144.31ms
step:832/1390 train_time:118633ms step_avg:144.32ms
step:833/1390 train_time:118786ms step_avg:144.33ms
step:834/1390 train_time:118936ms step_avg:144.34ms
step:835/1390 train_time:119089ms step_avg:144.35ms
step:836/1390 train_time:119243ms step_avg:144.36ms
step:837/1390 train_time:119396ms step_avg:144.37ms
step:838/1390 train_time:119548ms step_avg:144.38ms
step:839/1390 train_time:119700ms step_avg:144.39ms
step:840/1390 train_time:119850ms step_avg:144.40ms
step:841/1390 train_time:120001ms step_avg:144.41ms
step:842/1390 train_time:120153ms step_avg:144.41ms
step:843/1390 train_time:120306ms step_avg:144.42ms
step:844/1390 train_time:120458ms step_avg:144.43ms
step:845/1390 train_time:120609ms step_avg:144.44ms
step:846/1390 train_time:120762ms step_avg:144.45ms
step:847/1390 train_time:120914ms step_avg:144.46ms
step:848/1390 train_time:121064ms step_avg:144.47ms
step:849/1390 train_time:121215ms step_avg:144.48ms
step:850/1390 train_time:121368ms step_avg:144.49ms
step:851/1390 train_time:121521ms step_avg:144.50ms
step:852/1390 train_time:121672ms step_avg:144.50ms
step:853/1390 train_time:121823ms step_avg:144.51ms
step:854/1390 train_time:121974ms step_avg:144.52ms
step:855/1390 train_time:122128ms step_avg:144.53ms
step:856/1390 train_time:122277ms step_avg:144.54ms
step:857/1390 train_time:122430ms step_avg:144.55ms
step:858/1390 train_time:122583ms step_avg:144.56ms
step:859/1390 train_time:122734ms step_avg:144.56ms
step:860/1390 train_time:122885ms step_avg:144.57ms
step:861/1390 train_time:123036ms step_avg:144.58ms
step:862/1390 train_time:123189ms step_avg:144.59ms
step:863/1390 train_time:123343ms step_avg:144.60ms
step:864/1390 train_time:123494ms step_avg:144.61ms
step:865/1390 train_time:123644ms step_avg:144.61ms
step:866/1390 train_time:123802ms step_avg:144.63ms
step:867/1390 train_time:123952ms step_avg:144.63ms
step:868/1390 train_time:124102ms step_avg:144.64ms
step:869/1390 train_time:124252ms step_avg:144.65ms
step:870/1390 train_time:124407ms step_avg:144.66ms
step:871/1390 train_time:124559ms step_avg:144.67ms
step:872/1390 train_time:124710ms step_avg:144.68ms
step:873/1390 train_time:124861ms step_avg:144.68ms
step:874/1390 train_time:125013ms step_avg:144.69ms
step:875/1390 train_time:125164ms step_avg:144.70ms
step:875/1390 val_loss:3.4700 train_time:125239ms step_avg:144.79ms
step:876/1390 train_time:125316ms step_avg:144.71ms
step:877/1390 train_time:125469ms step_avg:144.72ms
step:878/1390 train_time:125622ms step_avg:144.73ms
step:879/1390 train_time:125773ms step_avg:144.73ms
step:880/1390 train_time:125924ms step_avg:144.74ms
step:881/1390 train_time:126073ms step_avg:144.75ms
step:882/1390 train_time:126226ms step_avg:144.75ms
step:883/1390 train_time:126378ms step_avg:144.76ms
step:884/1390 train_time:126530ms step_avg:144.77ms
step:885/1390 train_time:126682ms step_avg:144.78ms
step:886/1390 train_time:126833ms step_avg:144.79ms
step:887/1390 train_time:126986ms step_avg:144.80ms
step:888/1390 train_time:127141ms step_avg:144.81ms
step:889/1390 train_time:127292ms step_avg:144.81ms
step:890/1390 train_time:127443ms step_avg:144.82ms
step:891/1390 train_time:127593ms step_avg:144.83ms
step:892/1390 train_time:127746ms step_avg:144.84ms
step:893/1390 train_time:127897ms step_avg:144.84ms
step:894/1390 train_time:128047ms step_avg:144.85ms
step:895/1390 train_time:128200ms step_avg:144.86ms
step:896/1390 train_time:128351ms step_avg:144.87ms
step:897/1390 train_time:128502ms step_avg:144.87ms
step:898/1390 train_time:128658ms step_avg:144.88ms
step:899/1390 train_time:128810ms step_avg:144.89ms
step:900/1390 train_time:128963ms step_avg:144.90ms
step:901/1390 train_time:129113ms step_avg:144.91ms
step:902/1390 train_time:129264ms step_avg:144.91ms
step:903/1390 train_time:129418ms step_avg:144.92ms
step:904/1390 train_time:129569ms step_avg:144.93ms
step:905/1390 train_time:129721ms step_avg:144.94ms
step:906/1390 train_time:129872ms step_avg:144.95ms
step:907/1390 train_time:130026ms step_avg:144.96ms
step:908/1390 train_time:130176ms step_avg:144.96ms
step:909/1390 train_time:130326ms step_avg:144.97ms
step:910/1390 train_time:130483ms step_avg:144.98ms
step:911/1390 train_time:130634ms step_avg:144.99ms
step:912/1390 train_time:130784ms step_avg:144.99ms
step:913/1390 train_time:130935ms step_avg:145.00ms
step:914/1390 train_time:131087ms step_avg:145.01ms
step:915/1390 train_time:131240ms step_avg:145.02ms
step:916/1390 train_time:131392ms step_avg:145.02ms
step:917/1390 train_time:131545ms step_avg:145.03ms
step:918/1390 train_time:131696ms step_avg:145.04ms
step:919/1390 train_time:131853ms step_avg:145.05ms
step:920/1390 train_time:132004ms step_avg:145.06ms
step:921/1390 train_time:132154ms step_avg:145.07ms
step:922/1390 train_time:132310ms step_avg:145.08ms
step:923/1390 train_time:132460ms step_avg:145.08ms
step:924/1390 train_time:132611ms step_avg:145.09ms
step:925/1390 train_time:132763ms step_avg:145.10ms
step:926/1390 train_time:132914ms step_avg:145.10ms
step:927/1390 train_time:133066ms step_avg:145.11ms
step:928/1390 train_time:133220ms step_avg:145.12ms
step:929/1390 train_time:133372ms step_avg:145.13ms
step:930/1390 train_time:133525ms step_avg:145.14ms
step:931/1390 train_time:133676ms step_avg:145.14ms
step:932/1390 train_time:133830ms step_avg:145.15ms
step:933/1390 train_time:133984ms step_avg:145.16ms
step:934/1390 train_time:134137ms step_avg:145.17ms
step:935/1390 train_time:134290ms step_avg:145.18ms
step:936/1390 train_time:134445ms step_avg:145.19ms
step:937/1390 train_time:134601ms step_avg:145.20ms
step:938/1390 train_time:134758ms step_avg:145.21ms
step:939/1390 train_time:134911ms step_avg:145.22ms
step:940/1390 train_time:135065ms step_avg:145.23ms
step:941/1390 train_time:135218ms step_avg:145.24ms
step:942/1390 train_time:135370ms step_avg:145.25ms
step:943/1390 train_time:135525ms step_avg:145.26ms
step:944/1390 train_time:135682ms step_avg:145.27ms
step:945/1390 train_time:135835ms step_avg:145.28ms
step:946/1390 train_time:135989ms step_avg:145.29ms
step:947/1390 train_time:136145ms step_avg:145.30ms
step:948/1390 train_time:136296ms step_avg:145.30ms
step:949/1390 train_time:136448ms step_avg:145.31ms
step:950/1390 train_time:136599ms step_avg:145.32ms
step:951/1390 train_time:136807ms step_avg:145.39ms
step:952/1390 train_time:136958ms step_avg:145.39ms
step:953/1390 train_time:137112ms step_avg:145.40ms
step:954/1390 train_time:137263ms step_avg:145.41ms
step:955/1390 train_time:137414ms step_avg:145.41ms
step:956/1390 train_time:137568ms step_avg:145.42ms
step:957/1390 train_time:137721ms step_avg:145.43ms
step:958/1390 train_time:137877ms step_avg:145.44ms
step:959/1390 train_time:138034ms step_avg:145.45ms
step:960/1390 train_time:138188ms step_avg:145.46ms
step:961/1390 train_time:138339ms step_avg:145.47ms
step:962/1390 train_time:138490ms step_avg:145.47ms
step:963/1390 train_time:138650ms step_avg:145.49ms
step:964/1390 train_time:138802ms step_avg:145.50ms
step:965/1390 train_time:138952ms step_avg:145.50ms
step:966/1390 train_time:139105ms step_avg:145.51ms
step:967/1390 train_time:139262ms step_avg:145.52ms
step:968/1390 train_time:139413ms step_avg:145.53ms
step:969/1390 train_time:139569ms step_avg:145.54ms
step:970/1390 train_time:139722ms step_avg:145.54ms
step:971/1390 train_time:139873ms step_avg:145.55ms
step:972/1390 train_time:140027ms step_avg:145.56ms
step:973/1390 train_time:140180ms step_avg:145.57ms
step:974/1390 train_time:140336ms step_avg:145.58ms
step:975/1390 train_time:140489ms step_avg:145.58ms
step:976/1390 train_time:140641ms step_avg:145.59ms
step:977/1390 train_time:140792ms step_avg:145.60ms
step:978/1390 train_time:140945ms step_avg:145.60ms
step:979/1390 train_time:141097ms step_avg:145.61ms
step:980/1390 train_time:141247ms step_avg:145.62ms
step:981/1390 train_time:141397ms step_avg:145.62ms
step:982/1390 train_time:141549ms step_avg:145.63ms
step:983/1390 train_time:141701ms step_avg:145.63ms
step:984/1390 train_time:141853ms step_avg:145.64ms
step:985/1390 train_time:142006ms step_avg:145.65ms
step:986/1390 train_time:142166ms step_avg:145.66ms
step:987/1390 train_time:142315ms step_avg:145.67ms
step:988/1390 train_time:142468ms step_avg:145.67ms
step:989/1390 train_time:142623ms step_avg:145.68ms
step:990/1390 train_time:142776ms step_avg:145.69ms
step:991/1390 train_time:142928ms step_avg:145.70ms
step:992/1390 train_time:143085ms step_avg:145.71ms
step:993/1390 train_time:143245ms step_avg:145.72ms
step:994/1390 train_time:143396ms step_avg:145.73ms
step:995/1390 train_time:143547ms step_avg:145.73ms
step:996/1390 train_time:143699ms step_avg:145.74ms
step:997/1390 train_time:143850ms step_avg:145.74ms
step:998/1390 train_time:144000ms step_avg:145.75ms
step:999/1390 train_time:144152ms step_avg:145.76ms
step:1000/1390 train_time:144305ms step_avg:145.76ms
step:1000/1390 val_loss:3.4052 train_time:144381ms step_avg:145.84ms
step:1001/1390 train_time:144458ms step_avg:145.77ms
step:1002/1390 train_time:144614ms step_avg:145.78ms
step:1003/1390 train_time:144767ms step_avg:145.79ms
step:1004/1390 train_time:144919ms step_avg:145.79ms
step:1005/1390 train_time:145074ms step_avg:145.80ms
step:1006/1390 train_time:145224ms step_avg:145.81ms
step:1007/1390 train_time:145377ms step_avg:145.81ms
step:1008/1390 train_time:145531ms step_avg:145.82ms
step:1009/1390 train_time:145689ms step_avg:145.83ms
step:1010/1390 train_time:145840ms step_avg:145.84ms
step:1011/1390 train_time:145993ms step_avg:145.85ms
step:1012/1390 train_time:146147ms step_avg:145.86ms
step:1013/1390 train_time:146300ms step_avg:145.86ms
step:1014/1390 train_time:146452ms step_avg:145.87ms
step:1015/1390 train_time:146605ms step_avg:145.88ms
step:1016/1390 train_time:146758ms step_avg:145.88ms
step:1017/1390 train_time:146915ms step_avg:145.89ms
step:1018/1390 train_time:147067ms step_avg:145.90ms
step:1019/1390 train_time:147220ms step_avg:145.91ms
step:1020/1390 train_time:147375ms step_avg:145.92ms
step:1021/1390 train_time:147525ms step_avg:145.92ms
step:1022/1390 train_time:147677ms step_avg:145.93ms
step:1023/1390 train_time:147830ms step_avg:145.93ms
step:1024/1390 train_time:147987ms step_avg:145.94ms
step:1025/1390 train_time:148142ms step_avg:145.95ms
step:1026/1390 train_time:148296ms step_avg:145.96ms
step:1027/1390 train_time:148448ms step_avg:145.97ms
step:1028/1390 train_time:148603ms step_avg:145.98ms
step:1029/1390 train_time:148760ms step_avg:145.99ms
step:1030/1390 train_time:148914ms step_avg:145.99ms
step:1031/1390 train_time:149064ms step_avg:146.00ms
step:1032/1390 train_time:149217ms step_avg:146.00ms
step:1033/1390 train_time:149371ms step_avg:146.01ms
step:1034/1390 train_time:149527ms step_avg:146.02ms
step:1035/1390 train_time:149682ms step_avg:146.03ms
step:1036/1390 train_time:149837ms step_avg:146.04ms
step:1037/1390 train_time:149995ms step_avg:146.05ms
step:1038/1390 train_time:150149ms step_avg:146.06ms
step:1039/1390 train_time:150301ms step_avg:146.07ms
step:1040/1390 train_time:150457ms step_avg:146.07ms
step:1041/1390 train_time:150612ms step_avg:146.08ms
step:1042/1390 train_time:150766ms step_avg:146.09ms
step:1043/1390 train_time:150921ms step_avg:146.10ms
step:1044/1390 train_time:151080ms step_avg:146.11ms
step:1045/1390 train_time:151236ms step_avg:146.12ms
step:1046/1390 train_time:151392ms step_avg:146.13ms
step:1047/1390 train_time:151543ms step_avg:146.14ms
step:1048/1390 train_time:151699ms step_avg:146.15ms
step:1049/1390 train_time:151851ms step_avg:146.15ms
step:1050/1390 train_time:152006ms step_avg:146.16ms
step:1051/1390 train_time:152164ms step_avg:146.17ms
step:1052/1390 train_time:152318ms step_avg:146.18ms
step:1053/1390 train_time:152471ms step_avg:146.18ms
step:1054/1390 train_time:152623ms step_avg:146.19ms
step:1055/1390 train_time:152775ms step_avg:146.20ms
step:1056/1390 train_time:152928ms step_avg:146.20ms
step:1057/1390 train_time:153081ms step_avg:146.21ms
step:1058/1390 train_time:153237ms step_avg:146.22ms
step:1059/1390 train_time:153394ms step_avg:146.23ms
step:1060/1390 train_time:153547ms step_avg:146.24ms
step:1061/1390 train_time:153698ms step_avg:146.24ms
step:1062/1390 train_time:153853ms step_avg:146.25ms
step:1063/1390 train_time:154007ms step_avg:146.26ms
step:1064/1390 train_time:154159ms step_avg:146.26ms
step:1065/1390 train_time:154315ms step_avg:146.27ms
step:1066/1390 train_time:154472ms step_avg:146.28ms
step:1067/1390 train_time:154627ms step_avg:146.29ms
step:1068/1390 train_time:154777ms step_avg:146.29ms
step:1069/1390 train_time:154936ms step_avg:146.30ms
step:1070/1390 train_time:155088ms step_avg:146.31ms
step:1071/1390 train_time:155245ms step_avg:146.32ms
step:1072/1390 train_time:155398ms step_avg:146.33ms
step:1073/1390 train_time:155549ms step_avg:146.33ms
step:1074/1390 train_time:155701ms step_avg:146.34ms
step:1075/1390 train_time:155859ms step_avg:146.35ms
step:1076/1390 train_time:156014ms step_avg:146.35ms
step:1077/1390 train_time:156167ms step_avg:146.36ms
step:1078/1390 train_time:156323ms step_avg:146.37ms
step:1079/1390 train_time:156479ms step_avg:146.38ms
step:1080/1390 train_time:156634ms step_avg:146.39ms
step:1081/1390 train_time:156787ms step_avg:146.39ms
step:1082/1390 train_time:156937ms step_avg:146.40ms
step:1083/1390 train_time:157092ms step_avg:146.40ms
step:1084/1390 train_time:157249ms step_avg:146.41ms
step:1085/1390 train_time:157403ms step_avg:146.42ms
step:1086/1390 train_time:157558ms step_avg:146.43ms
step:1087/1390 train_time:157712ms step_avg:146.44ms
step:1088/1390 train_time:157869ms step_avg:146.45ms
step:1089/1390 train_time:158026ms step_avg:146.46ms
step:1090/1390 train_time:158183ms step_avg:146.47ms
step:1091/1390 train_time:158337ms step_avg:146.47ms
step:1092/1390 train_time:158490ms step_avg:146.48ms
step:1093/1390 train_time:158644ms step_avg:146.49ms
step:1094/1390 train_time:158798ms step_avg:146.49ms
step:1095/1390 train_time:158950ms step_avg:146.50ms
step:1096/1390 train_time:159105ms step_avg:146.51ms
step:1097/1390 train_time:159261ms step_avg:146.51ms
step:1098/1390 train_time:159415ms step_avg:146.52ms
step:1099/1390 train_time:159573ms step_avg:146.53ms
step:1100/1390 train_time:159724ms step_avg:146.54ms
step:1101/1390 train_time:159878ms step_avg:146.54ms
step:1102/1390 train_time:160033ms step_avg:146.55ms
step:1103/1390 train_time:160187ms step_avg:146.56ms
step:1104/1390 train_time:160339ms step_avg:146.56ms
step:1105/1390 train_time:160497ms step_avg:146.57ms
step:1106/1390 train_time:160651ms step_avg:146.58ms
step:1107/1390 train_time:160803ms step_avg:146.58ms
step:1108/1390 train_time:160964ms step_avg:146.60ms
step:1109/1390 train_time:161116ms step_avg:146.60ms
step:1110/1390 train_time:161272ms step_avg:146.61ms
step:1111/1390 train_time:161423ms step_avg:146.62ms
step:1112/1390 train_time:161578ms step_avg:146.62ms
step:1113/1390 train_time:161730ms step_avg:146.63ms
step:1114/1390 train_time:161887ms step_avg:146.64ms
step:1115/1390 train_time:162044ms step_avg:146.65ms
step:1116/1390 train_time:162197ms step_avg:146.65ms
step:1117/1390 train_time:162351ms step_avg:146.66ms
step:1118/1390 train_time:162511ms step_avg:146.67ms
step:1119/1390 train_time:162662ms step_avg:146.67ms
step:1120/1390 train_time:162816ms step_avg:146.68ms
step:1121/1390 train_time:162969ms step_avg:146.69ms
step:1122/1390 train_time:163121ms step_avg:146.69ms
step:1123/1390 train_time:163275ms step_avg:146.70ms
step:1124/1390 train_time:163428ms step_avg:146.70ms
step:1125/1390 train_time:163580ms step_avg:146.71ms
step:1125/1390 val_loss:3.3529 train_time:163658ms step_avg:146.78ms
step:1126/1390 train_time:163736ms step_avg:146.72ms
step:1127/1390 train_time:163890ms step_avg:146.72ms
step:1128/1390 train_time:164046ms step_avg:146.73ms
step:1129/1390 train_time:164205ms step_avg:146.74ms
step:1130/1390 train_time:164359ms step_avg:146.75ms
step:1131/1390 train_time:164515ms step_avg:146.76ms
step:1132/1390 train_time:164669ms step_avg:146.76ms
step:1133/1390 train_time:164823ms step_avg:146.77ms
step:1134/1390 train_time:164978ms step_avg:146.78ms
step:1135/1390 train_time:165132ms step_avg:146.78ms
step:1136/1390 train_time:165296ms step_avg:146.80ms
step:1137/1390 train_time:165450ms step_avg:146.81ms
step:1138/1390 train_time:165607ms step_avg:146.81ms
step:1139/1390 train_time:165761ms step_avg:146.82ms
step:1140/1390 train_time:165917ms step_avg:146.83ms
step:1141/1390 train_time:166127ms step_avg:146.89ms
step:1142/1390 train_time:166282ms step_avg:146.89ms
step:1143/1390 train_time:166439ms step_avg:146.90ms
step:1144/1390 train_time:166593ms step_avg:146.91ms
step:1145/1390 train_time:166747ms step_avg:146.91ms
step:1146/1390 train_time:166902ms step_avg:146.92ms
step:1147/1390 train_time:167058ms step_avg:146.93ms
step:1148/1390 train_time:167214ms step_avg:146.94ms
step:1149/1390 train_time:167371ms step_avg:146.95ms
step:1150/1390 train_time:167527ms step_avg:146.95ms
step:1151/1390 train_time:167683ms step_avg:146.96ms
step:1152/1390 train_time:167839ms step_avg:146.97ms
step:1153/1390 train_time:167998ms step_avg:146.98ms
step:1154/1390 train_time:168151ms step_avg:146.99ms
step:1155/1390 train_time:168307ms step_avg:146.99ms
step:1156/1390 train_time:168470ms step_avg:147.01ms
step:1157/1390 train_time:168626ms step_avg:147.01ms
step:1158/1390 train_time:168779ms step_avg:147.02ms
step:1159/1390 train_time:168933ms step_avg:147.03ms
step:1160/1390 train_time:169085ms step_avg:147.03ms
step:1161/1390 train_time:169242ms step_avg:147.04ms
step:1162/1390 train_time:169396ms step_avg:147.04ms
step:1163/1390 train_time:169551ms step_avg:147.05ms
step:1164/1390 train_time:169706ms step_avg:147.06ms
step:1165/1390 train_time:169859ms step_avg:147.06ms
step:1166/1390 train_time:170013ms step_avg:147.07ms
step:1167/1390 train_time:170167ms step_avg:147.08ms
step:1168/1390 train_time:170324ms step_avg:147.08ms
step:1169/1390 train_time:170479ms step_avg:147.09ms
step:1170/1390 train_time:170634ms step_avg:147.10ms
step:1171/1390 train_time:170788ms step_avg:147.10ms
step:1172/1390 train_time:170943ms step_avg:147.11ms
step:1173/1390 train_time:171097ms step_avg:147.12ms
step:1174/1390 train_time:171261ms step_avg:147.13ms
step:1175/1390 train_time:171418ms step_avg:147.14ms
step:1176/1390 train_time:171574ms step_avg:147.15ms
step:1177/1390 train_time:171736ms step_avg:147.16ms
step:1178/1390 train_time:171891ms step_avg:147.17ms
step:1179/1390 train_time:172043ms step_avg:147.17ms
step:1180/1390 train_time:172205ms step_avg:147.18ms
step:1181/1390 train_time:172359ms step_avg:147.19ms
step:1182/1390 train_time:172512ms step_avg:147.19ms
step:1183/1390 train_time:172668ms step_avg:147.20ms
step:1184/1390 train_time:172822ms step_avg:147.21ms
step:1185/1390 train_time:172979ms step_avg:147.22ms
step:1186/1390 train_time:173133ms step_avg:147.22ms
step:1187/1390 train_time:173300ms step_avg:147.24ms
step:1188/1390 train_time:173453ms step_avg:147.24ms
step:1189/1390 train_time:173610ms step_avg:147.25ms
step:1190/1390 train_time:173764ms step_avg:147.26ms
step:1191/1390 train_time:173919ms step_avg:147.26ms
step:1192/1390 train_time:174071ms step_avg:147.27ms
step:1193/1390 train_time:174225ms step_avg:147.27ms
step:1194/1390 train_time:174379ms step_avg:147.28ms
step:1195/1390 train_time:174535ms step_avg:147.29ms
step:1196/1390 train_time:174689ms step_avg:147.29ms
step:1197/1390 train_time:174845ms step_avg:147.30ms
step:1198/1390 train_time:175011ms step_avg:147.32ms
step:1199/1390 train_time:175164ms step_avg:147.32ms
step:1200/1390 train_time:175319ms step_avg:147.33ms
step:1201/1390 train_time:175472ms step_avg:147.33ms
step:1202/1390 train_time:175641ms step_avg:147.35ms
step:1203/1390 train_time:175798ms step_avg:147.36ms
step:1204/1390 train_time:175957ms step_avg:147.37ms
step:1205/1390 train_time:176115ms step_avg:147.38ms
step:1206/1390 train_time:176271ms step_avg:147.38ms
step:1207/1390 train_time:176426ms step_avg:147.39ms
step:1208/1390 train_time:176582ms step_avg:147.40ms
step:1209/1390 train_time:176738ms step_avg:147.40ms
step:1210/1390 train_time:176894ms step_avg:147.41ms
step:1211/1390 train_time:177053ms step_avg:147.42ms
step:1212/1390 train_time:177209ms step_avg:147.43ms
step:1213/1390 train_time:177363ms step_avg:147.43ms
step:1214/1390 train_time:177519ms step_avg:147.44ms
step:1215/1390 train_time:177677ms step_avg:147.45ms
step:1216/1390 train_time:177831ms step_avg:147.46ms
step:1217/1390 train_time:177988ms step_avg:147.46ms
step:1218/1390 train_time:178142ms step_avg:147.47ms
step:1219/1390 train_time:178295ms step_avg:147.47ms
step:1220/1390 train_time:178449ms step_avg:147.48ms
step:1221/1390 train_time:178604ms step_avg:147.48ms
step:1222/1390 train_time:178758ms step_avg:147.49ms
step:1223/1390 train_time:178914ms step_avg:147.50ms
step:1224/1390 train_time:179071ms step_avg:147.50ms
step:1225/1390 train_time:179228ms step_avg:147.51ms
step:1226/1390 train_time:179382ms step_avg:147.52ms
step:1227/1390 train_time:179538ms step_avg:147.52ms
step:1228/1390 train_time:179692ms step_avg:147.53ms
step:1229/1390 train_time:179846ms step_avg:147.54ms
step:1230/1390 train_time:180004ms step_avg:147.54ms
step:1231/1390 train_time:180160ms step_avg:147.55ms
step:1232/1390 train_time:180318ms step_avg:147.56ms
step:1233/1390 train_time:180472ms step_avg:147.57ms
step:1234/1390 train_time:180625ms step_avg:147.57ms
step:1235/1390 train_time:180783ms step_avg:147.58ms
step:1236/1390 train_time:180940ms step_avg:147.59ms
step:1237/1390 train_time:181094ms step_avg:147.59ms
step:1238/1390 train_time:181260ms step_avg:147.61ms
step:1239/1390 train_time:181416ms step_avg:147.61ms
step:1240/1390 train_time:181571ms step_avg:147.62ms
step:1241/1390 train_time:181735ms step_avg:147.63ms
step:1242/1390 train_time:181889ms step_avg:147.64ms
step:1243/1390 train_time:182051ms step_avg:147.65ms
step:1244/1390 train_time:182206ms step_avg:147.65ms
step:1245/1390 train_time:182363ms step_avg:147.66ms
step:1246/1390 train_time:182518ms step_avg:147.67ms
step:1247/1390 train_time:182672ms step_avg:147.67ms
step:1248/1390 train_time:182828ms step_avg:147.68ms
step:1249/1390 train_time:182982ms step_avg:147.69ms
step:1250/1390 train_time:183139ms step_avg:147.69ms
step:1250/1390 val_loss:3.3066 train_time:183219ms step_avg:147.76ms
step:1251/1390 train_time:183299ms step_avg:147.70ms
step:1252/1390 train_time:183454ms step_avg:147.71ms
step:1253/1390 train_time:183608ms step_avg:147.71ms
step:1254/1390 train_time:183763ms step_avg:147.72ms
step:1255/1390 train_time:183930ms step_avg:147.73ms
step:1256/1390 train_time:184086ms step_avg:147.74ms
step:1257/1390 train_time:184243ms step_avg:147.75ms
step:1258/1390 train_time:184403ms step_avg:147.76ms
step:1259/1390 train_time:184561ms step_avg:147.77ms
step:1260/1390 train_time:184713ms step_avg:147.77ms
step:1261/1390 train_time:184872ms step_avg:147.78ms
step:1262/1390 train_time:185030ms step_avg:147.79ms
step:1263/1390 train_time:185188ms step_avg:147.80ms
step:1264/1390 train_time:185341ms step_avg:147.80ms
step:1265/1390 train_time:185494ms step_avg:147.80ms
step:1266/1390 train_time:185651ms step_avg:147.81ms
step:1267/1390 train_time:185808ms step_avg:147.82ms
step:1268/1390 train_time:185967ms step_avg:147.83ms
step:1269/1390 train_time:186126ms step_avg:147.84ms
step:1270/1390 train_time:186281ms step_avg:147.84ms
step:1271/1390 train_time:186436ms step_avg:147.85ms
step:1272/1390 train_time:186591ms step_avg:147.85ms
step:1273/1390 train_time:186745ms step_avg:147.86ms
step:1274/1390 train_time:186902ms step_avg:147.87ms
step:1275/1390 train_time:187061ms step_avg:147.87ms
step:1276/1390 train_time:187213ms step_avg:147.88ms
step:1277/1390 train_time:187371ms step_avg:147.89ms
step:1278/1390 train_time:187527ms step_avg:147.89ms
step:1279/1390 train_time:187683ms step_avg:147.90ms
step:1280/1390 train_time:187846ms step_avg:147.91ms
step:1281/1390 train_time:188002ms step_avg:147.92ms
step:1282/1390 train_time:188155ms step_avg:147.92ms
step:1283/1390 train_time:188310ms step_avg:147.93ms
step:1284/1390 train_time:188467ms step_avg:147.93ms
step:1285/1390 train_time:188622ms step_avg:147.94ms
step:1286/1390 train_time:188776ms step_avg:147.94ms
step:1287/1390 train_time:188934ms step_avg:147.95ms
step:1288/1390 train_time:189091ms step_avg:147.96ms
step:1289/1390 train_time:189256ms step_avg:147.97ms
step:1290/1390 train_time:189417ms step_avg:147.98ms
step:1291/1390 train_time:189575ms step_avg:147.99ms
step:1292/1390 train_time:189734ms step_avg:148.00ms
step:1293/1390 train_time:189897ms step_avg:148.01ms
step:1294/1390 train_time:190051ms step_avg:148.01ms
step:1295/1390 train_time:190205ms step_avg:148.02ms
step:1296/1390 train_time:190362ms step_avg:148.03ms
step:1297/1390 train_time:190521ms step_avg:148.04ms
step:1298/1390 train_time:190676ms step_avg:148.04ms
step:1299/1390 train_time:190834ms step_avg:148.05ms
step:1300/1390 train_time:190988ms step_avg:148.05ms
step:1301/1390 train_time:191143ms step_avg:148.06ms
step:1302/1390 train_time:191300ms step_avg:148.06ms
step:1303/1390 train_time:191460ms step_avg:148.07ms
step:1304/1390 train_time:191617ms step_avg:148.08ms
step:1305/1390 train_time:191771ms step_avg:148.09ms
step:1306/1390 train_time:191928ms step_avg:148.09ms
step:1307/1390 train_time:192083ms step_avg:148.10ms
step:1308/1390 train_time:192243ms step_avg:148.11ms
step:1309/1390 train_time:192402ms step_avg:148.12ms
step:1310/1390 train_time:192557ms step_avg:148.12ms
step:1311/1390 train_time:192710ms step_avg:148.12ms
step:1312/1390 train_time:192865ms step_avg:148.13ms
step:1313/1390 train_time:193020ms step_avg:148.14ms
step:1314/1390 train_time:193178ms step_avg:148.14ms
step:1315/1390 train_time:193333ms step_avg:148.15ms
step:1316/1390 train_time:193487ms step_avg:148.15ms
step:1317/1390 train_time:193645ms step_avg:148.16ms
step:1318/1390 train_time:193807ms step_avg:148.17ms
step:1319/1390 train_time:193962ms step_avg:148.18ms
step:1320/1390 train_time:194119ms step_avg:148.18ms
step:1321/1390 train_time:194272ms step_avg:148.19ms
step:1322/1390 train_time:194433ms step_avg:148.20ms
step:1323/1390 train_time:194589ms step_avg:148.20ms
step:1324/1390 train_time:194745ms step_avg:148.21ms
step:1325/1390 train_time:194905ms step_avg:148.22ms
step:1326/1390 train_time:195065ms step_avg:148.23ms
step:1327/1390 train_time:195219ms step_avg:148.23ms
step:1328/1390 train_time:195372ms step_avg:148.23ms
step:1329/1390 train_time:195540ms step_avg:148.25ms
step:1330/1390 train_time:195698ms step_avg:148.26ms
step:1331/1390 train_time:195916ms step_avg:148.31ms
step:1332/1390 train_time:196082ms step_avg:148.32ms
step:1333/1390 train_time:196239ms step_avg:148.33ms
step:1334/1390 train_time:196395ms step_avg:148.33ms
step:1335/1390 train_time:196549ms step_avg:148.34ms
step:1336/1390 train_time:196711ms step_avg:148.35ms
step:1337/1390 train_time:196870ms step_avg:148.36ms
step:1338/1390 train_time:197027ms step_avg:148.36ms
step:1339/1390 train_time:197184ms step_avg:148.37ms
step:1340/1390 train_time:197347ms step_avg:148.38ms
step:1341/1390 train_time:197503ms step_avg:148.39ms
step:1342/1390 train_time:197662ms step_avg:148.40ms
step:1343/1390 train_time:197820ms step_avg:148.40ms
step:1344/1390 train_time:197975ms step_avg:148.41ms
step:1345/1390 train_time:198130ms step_avg:148.41ms
step:1346/1390 train_time:198287ms step_avg:148.42ms
step:1347/1390 train_time:198445ms step_avg:148.43ms
step:1348/1390 train_time:198600ms step_avg:148.43ms
step:1349/1390 train_time:198755ms step_avg:148.44ms
step:1350/1390 train_time:198910ms step_avg:148.44ms
step:1351/1390 train_time:199066ms step_avg:148.45ms
step:1352/1390 train_time:199228ms step_avg:148.46ms
step:1353/1390 train_time:199390ms step_avg:148.47ms
step:1354/1390 train_time:199547ms step_avg:148.47ms
step:1355/1390 train_time:199703ms step_avg:148.48ms
step:1356/1390 train_time:199858ms step_avg:148.48ms
step:1357/1390 train_time:200015ms step_avg:148.49ms
step:1358/1390 train_time:200172ms step_avg:148.50ms
step:1359/1390 train_time:200327ms step_avg:148.50ms
step:1360/1390 train_time:200485ms step_avg:148.51ms
step:1361/1390 train_time:200644ms step_avg:148.52ms
step:1362/1390 train_time:200802ms step_avg:148.52ms
step:1363/1390 train_time:200966ms step_avg:148.53ms
step:1364/1390 train_time:201124ms step_avg:148.54ms
step:1365/1390 train_time:201276ms step_avg:148.54ms
step:1366/1390 train_time:201435ms step_avg:148.55ms
step:1367/1390 train_time:201592ms step_avg:148.56ms
step:1368/1390 train_time:201751ms step_avg:148.56ms
step:1369/1390 train_time:201917ms step_avg:148.58ms
step:1370/1390 train_time:202082ms step_avg:148.59ms
step:1371/1390 train_time:202240ms step_avg:148.60ms
step:1372/1390 train_time:202399ms step_avg:148.60ms
step:1373/1390 train_time:202554ms step_avg:148.61ms
step:1374/1390 train_time:202713ms step_avg:148.62ms
step:1375/1390 train_time:202870ms step_avg:148.62ms
step:1375/1390 val_loss:3.2782 train_time:202946ms step_avg:148.68ms
step:1376/1390 train_time:203026ms step_avg:148.63ms
step:1377/1390 train_time:203182ms step_avg:148.63ms
step:1378/1390 train_time:203338ms step_avg:148.64ms
step:1379/1390 train_time:203495ms step_avg:148.65ms
step:1380/1390 train_time:203651ms step_avg:148.65ms
step:1381/1390 train_time:203813ms step_avg:148.66ms
step:1382/1390 train_time:203972ms step_avg:148.67ms
step:1383/1390 train_time:204126ms step_avg:148.67ms
step:1384/1390 train_time:204288ms step_avg:148.68ms
step:1385/1390 train_time:204441ms step_avg:148.68ms
step:1386/1390 train_time:204597ms step_avg:148.69ms
step:1387/1390 train_time:204754ms step_avg:148.70ms
step:1388/1390 train_time:204909ms step_avg:148.70ms
step:1389/1390 train_time:205066ms step_avg:148.71ms
step:1390/1390 train_time:205222ms step_avg:148.71ms
step:1390/1390 val_loss:3.2774 train_time:205300ms step_avg:148.77ms
peak memory consumption: 31563 MiB
