import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
from torch.utils.checkpoint import checkpoint

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

def forward_block(block, x, ve, x0, block_mask):
    # small helper for checkpoint
    return block(x, ve, x0, block_mask)

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16())
        x = x0
        ve = self.value_embeds(inputs)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        skip_connections = []
        # Encoder pass (apply checkpoint)
        for i in range(self.num_encoder_layers):
            x = checkpoint(forward_block, self.blocks[i], x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass (apply checkpoint)
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = checkpoint(forward_block, self.blocks[self.num_encoder_layers + i],
                           x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1390 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Jan  9 2025, 22:54:50) [GCC 13.2.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Fri Jan 10 01:25:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   26C    P0            138W /  700W |    7746MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   29C    P0            124W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   27C    P0            124W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   49C    P0            139W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   27C    P0            123W /  700W |    3216MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin', 'data/fineweb10B/fineweb_train_000010.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1390 val_loss:10.8750 train_time:0ms step_avg:nanms
step:1/1390 train_time:255522ms step_avg:nanms
step:2/1390 train_time:255784ms step_avg:nanms
step:3/1390 train_time:255936ms step_avg:nanms
step:4/1390 train_time:256091ms step_avg:nanms
step:5/1390 train_time:256247ms step_avg:nanms
step:6/1390 train_time:256402ms step_avg:nanms
step:7/1390 train_time:256557ms step_avg:nanms
step:8/1390 train_time:256718ms step_avg:nanms
step:9/1390 train_time:256877ms step_avg:nanms
step:10/1390 train_time:257035ms step_avg:nanms
step:11/1390 train_time:156ms step_avg:nanms
step:12/1390 train_time:311ms step_avg:nanms
step:13/1390 train_time:467ms step_avg:155.78ms
step:14/1390 train_time:624ms step_avg:156.07ms
step:15/1390 train_time:785ms step_avg:156.91ms
step:16/1390 train_time:943ms step_avg:157.16ms
step:17/1390 train_time:1100ms step_avg:157.21ms
step:18/1390 train_time:1257ms step_avg:157.13ms
step:19/1390 train_time:1415ms step_avg:157.19ms
step:20/1390 train_time:1572ms step_avg:157.18ms
step:21/1390 train_time:1731ms step_avg:157.33ms
step:22/1390 train_time:1890ms step_avg:157.47ms
step:23/1390 train_time:2049ms step_avg:157.59ms
step:24/1390 train_time:2207ms step_avg:157.61ms
step:25/1390 train_time:2362ms step_avg:157.48ms
step:26/1390 train_time:2520ms step_avg:157.52ms
step:27/1390 train_time:2677ms step_avg:157.50ms
step:28/1390 train_time:2836ms step_avg:157.53ms
step:29/1390 train_time:2993ms step_avg:157.54ms
step:30/1390 train_time:3152ms step_avg:157.58ms
step:31/1390 train_time:3309ms step_avg:157.55ms
step:32/1390 train_time:3466ms step_avg:157.55ms
step:33/1390 train_time:3625ms step_avg:157.61ms
step:34/1390 train_time:3781ms step_avg:157.56ms
step:35/1390 train_time:3940ms step_avg:157.60ms
step:36/1390 train_time:4100ms step_avg:157.69ms
step:37/1390 train_time:4256ms step_avg:157.65ms
step:38/1390 train_time:4414ms step_avg:157.63ms
step:39/1390 train_time:4573ms step_avg:157.68ms
step:40/1390 train_time:4729ms step_avg:157.65ms
step:41/1390 train_time:4887ms step_avg:157.64ms
step:42/1390 train_time:5044ms step_avg:157.64ms
step:43/1390 train_time:5204ms step_avg:157.68ms
step:44/1390 train_time:5362ms step_avg:157.70ms
step:45/1390 train_time:5521ms step_avg:157.75ms
step:46/1390 train_time:5679ms step_avg:157.76ms
step:47/1390 train_time:5837ms step_avg:157.75ms
step:48/1390 train_time:5994ms step_avg:157.73ms
step:49/1390 train_time:6152ms step_avg:157.74ms
step:50/1390 train_time:6310ms step_avg:157.74ms
step:51/1390 train_time:6468ms step_avg:157.75ms
step:52/1390 train_time:6626ms step_avg:157.76ms
step:53/1390 train_time:6782ms step_avg:157.72ms
step:54/1390 train_time:6939ms step_avg:157.71ms
step:55/1390 train_time:7097ms step_avg:157.71ms
step:56/1390 train_time:7254ms step_avg:157.69ms
step:57/1390 train_time:7413ms step_avg:157.73ms
step:58/1390 train_time:7570ms step_avg:157.72ms
step:59/1390 train_time:7728ms step_avg:157.72ms
step:60/1390 train_time:7885ms step_avg:157.70ms
step:61/1390 train_time:8043ms step_avg:157.70ms
step:62/1390 train_time:8201ms step_avg:157.71ms
step:63/1390 train_time:8360ms step_avg:157.73ms
step:64/1390 train_time:8518ms step_avg:157.74ms
step:65/1390 train_time:8675ms step_avg:157.73ms
step:66/1390 train_time:8833ms step_avg:157.74ms
step:67/1390 train_time:8991ms step_avg:157.73ms
step:68/1390 train_time:9149ms step_avg:157.74ms
step:69/1390 train_time:9307ms step_avg:157.74ms
step:70/1390 train_time:9465ms step_avg:157.75ms
step:71/1390 train_time:9623ms step_avg:157.75ms
step:72/1390 train_time:9780ms step_avg:157.75ms
step:73/1390 train_time:9936ms step_avg:157.72ms
step:74/1390 train_time:10097ms step_avg:157.76ms
step:75/1390 train_time:10254ms step_avg:157.75ms
step:76/1390 train_time:10412ms step_avg:157.75ms
step:77/1390 train_time:10570ms step_avg:157.76ms
step:78/1390 train_time:10730ms step_avg:157.79ms
step:79/1390 train_time:10887ms step_avg:157.79ms
step:80/1390 train_time:11046ms step_avg:157.81ms
step:81/1390 train_time:11205ms step_avg:157.82ms
step:82/1390 train_time:11362ms step_avg:157.81ms
step:83/1390 train_time:11520ms step_avg:157.81ms
step:84/1390 train_time:11680ms step_avg:157.84ms
step:85/1390 train_time:11838ms step_avg:157.84ms
step:86/1390 train_time:11995ms step_avg:157.83ms
step:87/1390 train_time:12153ms step_avg:157.83ms
step:88/1390 train_time:12310ms step_avg:157.82ms
step:89/1390 train_time:12467ms step_avg:157.82ms
step:90/1390 train_time:12626ms step_avg:157.82ms
step:91/1390 train_time:12784ms step_avg:157.83ms
step:92/1390 train_time:12940ms step_avg:157.81ms
step:93/1390 train_time:13100ms step_avg:157.83ms
step:94/1390 train_time:13256ms step_avg:157.80ms
step:95/1390 train_time:13414ms step_avg:157.81ms
step:96/1390 train_time:13572ms step_avg:157.82ms
step:97/1390 train_time:13730ms step_avg:157.82ms
step:98/1390 train_time:13888ms step_avg:157.82ms
step:99/1390 train_time:14048ms step_avg:157.84ms
step:100/1390 train_time:14207ms step_avg:157.85ms
step:101/1390 train_time:14363ms step_avg:157.83ms
step:102/1390 train_time:14521ms step_avg:157.84ms
step:103/1390 train_time:14679ms step_avg:157.84ms
step:104/1390 train_time:14838ms step_avg:157.85ms
step:105/1390 train_time:14999ms step_avg:157.88ms
step:106/1390 train_time:15161ms step_avg:157.93ms
step:107/1390 train_time:15323ms step_avg:157.97ms
step:108/1390 train_time:15485ms step_avg:158.01ms
step:109/1390 train_time:15647ms step_avg:158.05ms
step:110/1390 train_time:15808ms step_avg:158.08ms
step:111/1390 train_time:15969ms step_avg:158.11ms
step:112/1390 train_time:16132ms step_avg:158.15ms
step:113/1390 train_time:16293ms step_avg:158.18ms
step:114/1390 train_time:16456ms step_avg:158.23ms
step:115/1390 train_time:16617ms step_avg:158.26ms
step:116/1390 train_time:16778ms step_avg:158.28ms
step:117/1390 train_time:16939ms step_avg:158.31ms
step:118/1390 train_time:17100ms step_avg:158.34ms
step:119/1390 train_time:17262ms step_avg:158.36ms
step:120/1390 train_time:17424ms step_avg:158.40ms
step:121/1390 train_time:17586ms step_avg:158.44ms
step:122/1390 train_time:17748ms step_avg:158.46ms
step:123/1390 train_time:17909ms step_avg:158.49ms
step:124/1390 train_time:18070ms step_avg:158.51ms
step:125/1390 train_time:18232ms step_avg:158.54ms
step:125/1390 val_loss:4.3750 train_time:18297ms step_avg:159.11ms
step:126/1390 train_time:18397ms step_avg:158.60ms
step:127/1390 train_time:18561ms step_avg:158.64ms
step:128/1390 train_time:18723ms step_avg:158.67ms
step:129/1390 train_time:18882ms step_avg:158.68ms
step:130/1390 train_time:19042ms step_avg:158.68ms
step:131/1390 train_time:19202ms step_avg:158.70ms
step:132/1390 train_time:19367ms step_avg:158.74ms
step:133/1390 train_time:19531ms step_avg:158.78ms
step:134/1390 train_time:19693ms step_avg:158.82ms
step:135/1390 train_time:19856ms step_avg:158.85ms
step:136/1390 train_time:20017ms step_avg:158.87ms
step:137/1390 train_time:20178ms step_avg:158.88ms
step:138/1390 train_time:20340ms step_avg:158.91ms
step:139/1390 train_time:20503ms step_avg:158.94ms
step:140/1390 train_time:20665ms step_avg:158.96ms
step:141/1390 train_time:20827ms step_avg:158.99ms
step:142/1390 train_time:20988ms step_avg:159.00ms
step:143/1390 train_time:21150ms step_avg:159.02ms
step:144/1390 train_time:21310ms step_avg:159.03ms
step:145/1390 train_time:21472ms step_avg:159.05ms
step:146/1390 train_time:21634ms step_avg:159.08ms
step:147/1390 train_time:21796ms step_avg:159.09ms
step:148/1390 train_time:21958ms step_avg:159.12ms
step:149/1390 train_time:22121ms step_avg:159.14ms
step:150/1390 train_time:22281ms step_avg:159.15ms
step:151/1390 train_time:22444ms step_avg:159.18ms
step:152/1390 train_time:22605ms step_avg:159.19ms
step:153/1390 train_time:22768ms step_avg:159.22ms
step:154/1390 train_time:22932ms step_avg:159.25ms
step:155/1390 train_time:23094ms step_avg:159.27ms
step:156/1390 train_time:23256ms step_avg:159.29ms
step:157/1390 train_time:23419ms step_avg:159.31ms
step:158/1390 train_time:23582ms step_avg:159.34ms
step:159/1390 train_time:23744ms step_avg:159.36ms
step:160/1390 train_time:23905ms step_avg:159.37ms
step:161/1390 train_time:24068ms step_avg:159.39ms
step:162/1390 train_time:24230ms step_avg:159.41ms
step:163/1390 train_time:24393ms step_avg:159.43ms
step:164/1390 train_time:24554ms step_avg:159.44ms
step:165/1390 train_time:24718ms step_avg:159.47ms
step:166/1390 train_time:24880ms step_avg:159.49ms
step:167/1390 train_time:25042ms step_avg:159.50ms
step:168/1390 train_time:25203ms step_avg:159.51ms
step:169/1390 train_time:25365ms step_avg:159.53ms
step:170/1390 train_time:25527ms step_avg:159.54ms
step:171/1390 train_time:25689ms step_avg:159.56ms
step:172/1390 train_time:25852ms step_avg:159.58ms
step:173/1390 train_time:26012ms step_avg:159.59ms
step:174/1390 train_time:26176ms step_avg:159.61ms
step:175/1390 train_time:26337ms step_avg:159.62ms
step:176/1390 train_time:26499ms step_avg:159.63ms
step:177/1390 train_time:26663ms step_avg:159.66ms
step:178/1390 train_time:26824ms step_avg:159.66ms
step:179/1390 train_time:26984ms step_avg:159.67ms
step:180/1390 train_time:27147ms step_avg:159.69ms
step:181/1390 train_time:27311ms step_avg:159.71ms
step:182/1390 train_time:27475ms step_avg:159.74ms
step:183/1390 train_time:27637ms step_avg:159.75ms
step:184/1390 train_time:27799ms step_avg:159.76ms
step:185/1390 train_time:27961ms step_avg:159.78ms
step:186/1390 train_time:28122ms step_avg:159.79ms
step:187/1390 train_time:28285ms step_avg:159.80ms
step:188/1390 train_time:28447ms step_avg:159.81ms
step:189/1390 train_time:28610ms step_avg:159.83ms
step:190/1390 train_time:28772ms step_avg:159.85ms
step:191/1390 train_time:28982ms step_avg:160.12ms
step:192/1390 train_time:29142ms step_avg:160.12ms
step:193/1390 train_time:29301ms step_avg:160.12ms
step:194/1390 train_time:29463ms step_avg:160.13ms
step:195/1390 train_time:29622ms step_avg:160.12ms
step:196/1390 train_time:29785ms step_avg:160.13ms
step:197/1390 train_time:29952ms step_avg:160.17ms
step:198/1390 train_time:30117ms step_avg:160.20ms
step:199/1390 train_time:30280ms step_avg:160.21ms
step:200/1390 train_time:30440ms step_avg:160.21ms
step:201/1390 train_time:30600ms step_avg:160.21ms
step:202/1390 train_time:30762ms step_avg:160.22ms
step:203/1390 train_time:30925ms step_avg:160.24ms
step:204/1390 train_time:31088ms step_avg:160.25ms
step:205/1390 train_time:31251ms step_avg:160.26ms
step:206/1390 train_time:31414ms step_avg:160.27ms
step:207/1390 train_time:31576ms step_avg:160.29ms
step:208/1390 train_time:31742ms step_avg:160.31ms
step:209/1390 train_time:31908ms step_avg:160.34ms
step:210/1390 train_time:32073ms step_avg:160.37ms
step:211/1390 train_time:32242ms step_avg:160.41ms
step:212/1390 train_time:32407ms step_avg:160.43ms
step:213/1390 train_time:32572ms step_avg:160.45ms
step:214/1390 train_time:32738ms step_avg:160.48ms
step:215/1390 train_time:32903ms step_avg:160.50ms
step:216/1390 train_time:33068ms step_avg:160.52ms
step:217/1390 train_time:33232ms step_avg:160.54ms
step:218/1390 train_time:33397ms step_avg:160.56ms
step:219/1390 train_time:33564ms step_avg:160.59ms
step:220/1390 train_time:33729ms step_avg:160.61ms
step:221/1390 train_time:33892ms step_avg:160.63ms
step:222/1390 train_time:34059ms step_avg:160.66ms
step:223/1390 train_time:34224ms step_avg:160.68ms
step:224/1390 train_time:34390ms step_avg:160.70ms
step:225/1390 train_time:34556ms step_avg:160.73ms
step:226/1390 train_time:34722ms step_avg:160.75ms
step:227/1390 train_time:34887ms step_avg:160.77ms
step:228/1390 train_time:35051ms step_avg:160.79ms
step:229/1390 train_time:35219ms step_avg:160.82ms
step:230/1390 train_time:35385ms step_avg:160.84ms
step:231/1390 train_time:35549ms step_avg:160.86ms
step:232/1390 train_time:35713ms step_avg:160.87ms
step:233/1390 train_time:35879ms step_avg:160.89ms
step:234/1390 train_time:36045ms step_avg:160.92ms
step:235/1390 train_time:36210ms step_avg:160.93ms
step:236/1390 train_time:36374ms step_avg:160.95ms
step:237/1390 train_time:36539ms step_avg:160.96ms
step:238/1390 train_time:36706ms step_avg:160.99ms
step:239/1390 train_time:36872ms step_avg:161.01ms
step:240/1390 train_time:37038ms step_avg:161.03ms
step:241/1390 train_time:37204ms step_avg:161.06ms
step:242/1390 train_time:37369ms step_avg:161.07ms
step:243/1390 train_time:37533ms step_avg:161.09ms
step:244/1390 train_time:37697ms step_avg:161.10ms
step:245/1390 train_time:37864ms step_avg:161.12ms
step:246/1390 train_time:38028ms step_avg:161.14ms
step:247/1390 train_time:38193ms step_avg:161.15ms
step:248/1390 train_time:38360ms step_avg:161.18ms
step:249/1390 train_time:38525ms step_avg:161.19ms
step:250/1390 train_time:38690ms step_avg:161.21ms
step:250/1390 val_loss:4.0000 train_time:38757ms step_avg:161.49ms
step:251/1390 train_time:38860ms step_avg:161.25ms
step:252/1390 train_time:39026ms step_avg:161.26ms
step:253/1390 train_time:39191ms step_avg:161.28ms
step:254/1390 train_time:39355ms step_avg:161.29ms
step:255/1390 train_time:39519ms step_avg:161.30ms
step:256/1390 train_time:39684ms step_avg:161.32ms
step:257/1390 train_time:39853ms step_avg:161.35ms
step:258/1390 train_time:40020ms step_avg:161.37ms
step:259/1390 train_time:40186ms step_avg:161.39ms
step:260/1390 train_time:40351ms step_avg:161.41ms
step:261/1390 train_time:40516ms step_avg:161.42ms
step:262/1390 train_time:40681ms step_avg:161.43ms
step:263/1390 train_time:40847ms step_avg:161.45ms
step:264/1390 train_time:41012ms step_avg:161.47ms
step:265/1390 train_time:41180ms step_avg:161.49ms
step:266/1390 train_time:41343ms step_avg:161.50ms
step:267/1390 train_time:41507ms step_avg:161.51ms
step:268/1390 train_time:41674ms step_avg:161.53ms
step:269/1390 train_time:41840ms step_avg:161.55ms
step:270/1390 train_time:42005ms step_avg:161.56ms
step:271/1390 train_time:42172ms step_avg:161.58ms
step:272/1390 train_time:42339ms step_avg:161.60ms
step:273/1390 train_time:42503ms step_avg:161.61ms
step:274/1390 train_time:42670ms step_avg:161.63ms
step:275/1390 train_time:42836ms step_avg:161.64ms
step:276/1390 train_time:43001ms step_avg:161.66ms
step:277/1390 train_time:43167ms step_avg:161.67ms
step:278/1390 train_time:43333ms step_avg:161.69ms
step:279/1390 train_time:43499ms step_avg:161.71ms
step:280/1390 train_time:43664ms step_avg:161.72ms
step:281/1390 train_time:43830ms step_avg:161.73ms
step:282/1390 train_time:43996ms step_avg:161.75ms
step:283/1390 train_time:44161ms step_avg:161.76ms
step:284/1390 train_time:44326ms step_avg:161.77ms
step:285/1390 train_time:44492ms step_avg:161.79ms
step:286/1390 train_time:44658ms step_avg:161.80ms
step:287/1390 train_time:44824ms step_avg:161.82ms
step:288/1390 train_time:44989ms step_avg:161.83ms
step:289/1390 train_time:45156ms step_avg:161.85ms
step:290/1390 train_time:45320ms step_avg:161.86ms
step:291/1390 train_time:45486ms step_avg:161.87ms
step:292/1390 train_time:45652ms step_avg:161.89ms
step:293/1390 train_time:45817ms step_avg:161.90ms
step:294/1390 train_time:45983ms step_avg:161.91ms
step:295/1390 train_time:46150ms step_avg:161.93ms
step:296/1390 train_time:46314ms step_avg:161.94ms
step:297/1390 train_time:46481ms step_avg:161.96ms
step:298/1390 train_time:46647ms step_avg:161.97ms
step:299/1390 train_time:46811ms step_avg:161.98ms
step:300/1390 train_time:46978ms step_avg:161.99ms
step:301/1390 train_time:47142ms step_avg:162.00ms
step:302/1390 train_time:47307ms step_avg:162.01ms
step:303/1390 train_time:47474ms step_avg:162.03ms
step:304/1390 train_time:47641ms step_avg:162.04ms
step:305/1390 train_time:47806ms step_avg:162.05ms
step:306/1390 train_time:47971ms step_avg:162.06ms
step:307/1390 train_time:48137ms step_avg:162.08ms
step:308/1390 train_time:48302ms step_avg:162.09ms
step:309/1390 train_time:48468ms step_avg:162.10ms
step:310/1390 train_time:48636ms step_avg:162.12ms
step:311/1390 train_time:48803ms step_avg:162.14ms
step:312/1390 train_time:48971ms step_avg:162.16ms
step:313/1390 train_time:49140ms step_avg:162.18ms
step:314/1390 train_time:49307ms step_avg:162.19ms
step:315/1390 train_time:49477ms step_avg:162.22ms
step:316/1390 train_time:49644ms step_avg:162.24ms
step:317/1390 train_time:49812ms step_avg:162.25ms
step:318/1390 train_time:49982ms step_avg:162.28ms
step:319/1390 train_time:50150ms step_avg:162.30ms
step:320/1390 train_time:50320ms step_avg:162.32ms
step:321/1390 train_time:50488ms step_avg:162.34ms
step:322/1390 train_time:50656ms step_avg:162.36ms
step:323/1390 train_time:50824ms step_avg:162.38ms
step:324/1390 train_time:50993ms step_avg:162.40ms
step:325/1390 train_time:51162ms step_avg:162.42ms
step:326/1390 train_time:51329ms step_avg:162.43ms
step:327/1390 train_time:51499ms step_avg:162.46ms
step:328/1390 train_time:51666ms step_avg:162.47ms
step:329/1390 train_time:51834ms step_avg:162.49ms
step:330/1390 train_time:52004ms step_avg:162.51ms
step:331/1390 train_time:52172ms step_avg:162.53ms
step:332/1390 train_time:52340ms step_avg:162.55ms
step:333/1390 train_time:52508ms step_avg:162.56ms
step:334/1390 train_time:52678ms step_avg:162.59ms
step:335/1390 train_time:52844ms step_avg:162.60ms
step:336/1390 train_time:53013ms step_avg:162.62ms
step:337/1390 train_time:53182ms step_avg:162.64ms
step:338/1390 train_time:53348ms step_avg:162.65ms
step:339/1390 train_time:53519ms step_avg:162.67ms
step:340/1390 train_time:53687ms step_avg:162.69ms
step:341/1390 train_time:53856ms step_avg:162.71ms
step:342/1390 train_time:54024ms step_avg:162.72ms
step:343/1390 train_time:54192ms step_avg:162.74ms
step:344/1390 train_time:54361ms step_avg:162.76ms
step:345/1390 train_time:54528ms step_avg:162.77ms
step:346/1390 train_time:54699ms step_avg:162.79ms
step:347/1390 train_time:54866ms step_avg:162.81ms
step:348/1390 train_time:55034ms step_avg:162.82ms
step:349/1390 train_time:55203ms step_avg:162.84ms
step:350/1390 train_time:55371ms step_avg:162.86ms
step:351/1390 train_time:55539ms step_avg:162.87ms
step:352/1390 train_time:55707ms step_avg:162.89ms
step:353/1390 train_time:55876ms step_avg:162.91ms
step:354/1390 train_time:56043ms step_avg:162.92ms
step:355/1390 train_time:56210ms step_avg:162.93ms
step:356/1390 train_time:56381ms step_avg:162.95ms
step:357/1390 train_time:56548ms step_avg:162.96ms
step:358/1390 train_time:56718ms step_avg:162.98ms
step:359/1390 train_time:56886ms step_avg:163.00ms
step:360/1390 train_time:57055ms step_avg:163.02ms
step:361/1390 train_time:57223ms step_avg:163.03ms
step:362/1390 train_time:57392ms step_avg:163.04ms
step:363/1390 train_time:57561ms step_avg:163.06ms
step:364/1390 train_time:57728ms step_avg:163.07ms
step:365/1390 train_time:57897ms step_avg:163.09ms
step:366/1390 train_time:58064ms step_avg:163.10ms
step:367/1390 train_time:58232ms step_avg:163.11ms
step:368/1390 train_time:58401ms step_avg:163.13ms
step:369/1390 train_time:58568ms step_avg:163.14ms
step:370/1390 train_time:58738ms step_avg:163.16ms
step:371/1390 train_time:58905ms step_avg:163.17ms
step:372/1390 train_time:59075ms step_avg:163.19ms
step:373/1390 train_time:59243ms step_avg:163.20ms
step:374/1390 train_time:59411ms step_avg:163.22ms
step:375/1390 train_time:59580ms step_avg:163.23ms
step:375/1390 val_loss:3.7500 train_time:59648ms step_avg:163.42ms
step:376/1390 train_time:59749ms step_avg:163.25ms
step:377/1390 train_time:59923ms step_avg:163.28ms
step:378/1390 train_time:60091ms step_avg:163.29ms
step:379/1390 train_time:60259ms step_avg:163.30ms
step:380/1390 train_time:60425ms step_avg:163.31ms
step:381/1390 train_time:60639ms step_avg:163.45ms
step:382/1390 train_time:60805ms step_avg:163.45ms
step:383/1390 train_time:60973ms step_avg:163.47ms
step:384/1390 train_time:61138ms step_avg:163.47ms
step:385/1390 train_time:61304ms step_avg:163.48ms
step:386/1390 train_time:61473ms step_avg:163.49ms
step:387/1390 train_time:61647ms step_avg:163.52ms
step:388/1390 train_time:61816ms step_avg:163.53ms
step:389/1390 train_time:61984ms step_avg:163.55ms
step:390/1390 train_time:62150ms step_avg:163.55ms
step:391/1390 train_time:62316ms step_avg:163.56ms
step:392/1390 train_time:62486ms step_avg:163.57ms
step:393/1390 train_time:62655ms step_avg:163.59ms
step:394/1390 train_time:62824ms step_avg:163.61ms
step:395/1390 train_time:62993ms step_avg:163.62ms
step:396/1390 train_time:63162ms step_avg:163.63ms
step:397/1390 train_time:63328ms step_avg:163.64ms
step:398/1390 train_time:63499ms step_avg:163.66ms
step:399/1390 train_time:63667ms step_avg:163.67ms
step:400/1390 train_time:63836ms step_avg:163.68ms
step:401/1390 train_time:64004ms step_avg:163.69ms
step:402/1390 train_time:64171ms step_avg:163.70ms
step:403/1390 train_time:64340ms step_avg:163.71ms
step:404/1390 train_time:64507ms step_avg:163.72ms
step:405/1390 train_time:64677ms step_avg:163.74ms
step:406/1390 train_time:64846ms step_avg:163.75ms
step:407/1390 train_time:65015ms step_avg:163.77ms
step:408/1390 train_time:65184ms step_avg:163.78ms
step:409/1390 train_time:65350ms step_avg:163.78ms
step:410/1390 train_time:65519ms step_avg:163.80ms
step:411/1390 train_time:65688ms step_avg:163.81ms
step:412/1390 train_time:65859ms step_avg:163.83ms
step:413/1390 train_time:66028ms step_avg:163.84ms
step:414/1390 train_time:66199ms step_avg:163.86ms
step:415/1390 train_time:66367ms step_avg:163.87ms
step:416/1390 train_time:66538ms step_avg:163.89ms
step:417/1390 train_time:66708ms step_avg:163.90ms
step:418/1390 train_time:66881ms step_avg:163.92ms
step:419/1390 train_time:67050ms step_avg:163.94ms
step:420/1390 train_time:67223ms step_avg:163.96ms
step:421/1390 train_time:67392ms step_avg:163.97ms
step:422/1390 train_time:67563ms step_avg:163.99ms
step:423/1390 train_time:67733ms step_avg:164.00ms
step:424/1390 train_time:67905ms step_avg:164.02ms
step:425/1390 train_time:68075ms step_avg:164.03ms
step:426/1390 train_time:68245ms step_avg:164.05ms
step:427/1390 train_time:68416ms step_avg:164.07ms
step:428/1390 train_time:68587ms step_avg:164.08ms
step:429/1390 train_time:68758ms step_avg:164.10ms
step:430/1390 train_time:68929ms step_avg:164.12ms
step:431/1390 train_time:69102ms step_avg:164.14ms
step:432/1390 train_time:69271ms step_avg:164.15ms
step:433/1390 train_time:69443ms step_avg:164.17ms
step:434/1390 train_time:69613ms step_avg:164.18ms
step:435/1390 train_time:69785ms step_avg:164.20ms
step:436/1390 train_time:69954ms step_avg:164.21ms
step:437/1390 train_time:70125ms step_avg:164.23ms
step:438/1390 train_time:70296ms step_avg:164.24ms
step:439/1390 train_time:70467ms step_avg:164.26ms
step:440/1390 train_time:70637ms step_avg:164.27ms
step:441/1390 train_time:70807ms step_avg:164.29ms
step:442/1390 train_time:70980ms step_avg:164.31ms
step:443/1390 train_time:71150ms step_avg:164.32ms
step:444/1390 train_time:71321ms step_avg:164.33ms
step:445/1390 train_time:71490ms step_avg:164.34ms
step:446/1390 train_time:71662ms step_avg:164.36ms
step:447/1390 train_time:71831ms step_avg:164.37ms
step:448/1390 train_time:72003ms step_avg:164.39ms
step:449/1390 train_time:72173ms step_avg:164.40ms
step:450/1390 train_time:72344ms step_avg:164.42ms
step:451/1390 train_time:72515ms step_avg:164.43ms
step:452/1390 train_time:72687ms step_avg:164.45ms
step:453/1390 train_time:72858ms step_avg:164.47ms
step:454/1390 train_time:73029ms step_avg:164.48ms
step:455/1390 train_time:73203ms step_avg:164.50ms
step:456/1390 train_time:73373ms step_avg:164.51ms
step:457/1390 train_time:73544ms step_avg:164.53ms
step:458/1390 train_time:73714ms step_avg:164.54ms
step:459/1390 train_time:73887ms step_avg:164.56ms
step:460/1390 train_time:74057ms step_avg:164.57ms
step:461/1390 train_time:74228ms step_avg:164.59ms
step:462/1390 train_time:74399ms step_avg:164.60ms
step:463/1390 train_time:74567ms step_avg:164.61ms
step:464/1390 train_time:74737ms step_avg:164.62ms
step:465/1390 train_time:74907ms step_avg:164.63ms
step:466/1390 train_time:75078ms step_avg:164.64ms
step:467/1390 train_time:75248ms step_avg:164.66ms
step:468/1390 train_time:75420ms step_avg:164.67ms
step:469/1390 train_time:75589ms step_avg:164.68ms
step:470/1390 train_time:75762ms step_avg:164.70ms
step:471/1390 train_time:75931ms step_avg:164.71ms
step:472/1390 train_time:76103ms step_avg:164.72ms
step:473/1390 train_time:76273ms step_avg:164.74ms
step:474/1390 train_time:76444ms step_avg:164.75ms
step:475/1390 train_time:76614ms step_avg:164.76ms
step:476/1390 train_time:76786ms step_avg:164.78ms
step:477/1390 train_time:76957ms step_avg:164.79ms
step:478/1390 train_time:77127ms step_avg:164.80ms
step:479/1390 train_time:77297ms step_avg:164.81ms
step:480/1390 train_time:77467ms step_avg:164.82ms
step:481/1390 train_time:77638ms step_avg:164.84ms
step:482/1390 train_time:77807ms step_avg:164.85ms
step:483/1390 train_time:77979ms step_avg:164.86ms
step:484/1390 train_time:78149ms step_avg:164.87ms
step:485/1390 train_time:78320ms step_avg:164.88ms
step:486/1390 train_time:78491ms step_avg:164.90ms
step:487/1390 train_time:78661ms step_avg:164.91ms
step:488/1390 train_time:78830ms step_avg:164.92ms
step:489/1390 train_time:79003ms step_avg:164.93ms
step:490/1390 train_time:79172ms step_avg:164.94ms
step:491/1390 train_time:79344ms step_avg:164.96ms
step:492/1390 train_time:79514ms step_avg:164.97ms
step:493/1390 train_time:79686ms step_avg:164.98ms
step:494/1390 train_time:79856ms step_avg:164.99ms
step:495/1390 train_time:80028ms step_avg:165.01ms
step:496/1390 train_time:80199ms step_avg:165.02ms
step:497/1390 train_time:80369ms step_avg:165.03ms
step:498/1390 train_time:80540ms step_avg:165.04ms
step:499/1390 train_time:80709ms step_avg:165.05ms
step:500/1390 train_time:80879ms step_avg:165.06ms
step:500/1390 val_loss:3.6250 train_time:80950ms step_avg:165.20ms
step:501/1390 train_time:81053ms step_avg:165.08ms
step:502/1390 train_time:81226ms step_avg:165.09ms
step:503/1390 train_time:81396ms step_avg:165.10ms
step:504/1390 train_time:81566ms step_avg:165.11ms
step:505/1390 train_time:81734ms step_avg:165.12ms
step:506/1390 train_time:81906ms step_avg:165.13ms
step:507/1390 train_time:82079ms step_avg:165.15ms
step:508/1390 train_time:82252ms step_avg:165.16ms
step:509/1390 train_time:82422ms step_avg:165.17ms
step:510/1390 train_time:82590ms step_avg:165.18ms
step:511/1390 train_time:82759ms step_avg:165.19ms
step:512/1390 train_time:82929ms step_avg:165.20ms
step:513/1390 train_time:83102ms step_avg:165.21ms
step:514/1390 train_time:83274ms step_avg:165.23ms
step:515/1390 train_time:83446ms step_avg:165.24ms
step:516/1390 train_time:83616ms step_avg:165.25ms
step:517/1390 train_time:83788ms step_avg:165.26ms
step:518/1390 train_time:83961ms step_avg:165.28ms
step:519/1390 train_time:84134ms step_avg:165.29ms
step:520/1390 train_time:84309ms step_avg:165.31ms
step:521/1390 train_time:84481ms step_avg:165.33ms
step:522/1390 train_time:84653ms step_avg:165.34ms
step:523/1390 train_time:84825ms step_avg:165.35ms
step:524/1390 train_time:84996ms step_avg:165.36ms
step:525/1390 train_time:85170ms step_avg:165.38ms
step:526/1390 train_time:85346ms step_avg:165.40ms
step:527/1390 train_time:85518ms step_avg:165.41ms
step:528/1390 train_time:85689ms step_avg:165.42ms
step:529/1390 train_time:85863ms step_avg:165.44ms
step:530/1390 train_time:86034ms step_avg:165.45ms
step:531/1390 train_time:86207ms step_avg:165.47ms
step:532/1390 train_time:86381ms step_avg:165.48ms
step:533/1390 train_time:86556ms step_avg:165.50ms
step:534/1390 train_time:86729ms step_avg:165.51ms
step:535/1390 train_time:86901ms step_avg:165.53ms
step:536/1390 train_time:87074ms step_avg:165.54ms
step:537/1390 train_time:87247ms step_avg:165.55ms
step:538/1390 train_time:87420ms step_avg:165.57ms
step:539/1390 train_time:87592ms step_avg:165.58ms
step:540/1390 train_time:87766ms step_avg:165.60ms
step:541/1390 train_time:87937ms step_avg:165.61ms
step:542/1390 train_time:88111ms step_avg:165.62ms
step:543/1390 train_time:88285ms step_avg:165.64ms
step:544/1390 train_time:88456ms step_avg:165.65ms
step:545/1390 train_time:88630ms step_avg:165.66ms
step:546/1390 train_time:88803ms step_avg:165.68ms
step:547/1390 train_time:88977ms step_avg:165.69ms
step:548/1390 train_time:89153ms step_avg:165.71ms
step:549/1390 train_time:89325ms step_avg:165.72ms
step:550/1390 train_time:89495ms step_avg:165.73ms
step:551/1390 train_time:89670ms step_avg:165.75ms
step:552/1390 train_time:89845ms step_avg:165.77ms
step:553/1390 train_time:90017ms step_avg:165.78ms
step:554/1390 train_time:90190ms step_avg:165.79ms
step:555/1390 train_time:90365ms step_avg:165.81ms
step:556/1390 train_time:90534ms step_avg:165.81ms
step:557/1390 train_time:90710ms step_avg:165.83ms
step:558/1390 train_time:90881ms step_avg:165.84ms
step:559/1390 train_time:91055ms step_avg:165.86ms
step:560/1390 train_time:91226ms step_avg:165.87ms
step:561/1390 train_time:91397ms step_avg:165.87ms
step:562/1390 train_time:91570ms step_avg:165.89ms
step:563/1390 train_time:91741ms step_avg:165.90ms
step:564/1390 train_time:91914ms step_avg:165.91ms
step:565/1390 train_time:92089ms step_avg:165.93ms
step:566/1390 train_time:92262ms step_avg:165.94ms
step:567/1390 train_time:92432ms step_avg:165.95ms
step:568/1390 train_time:92605ms step_avg:165.96ms
step:569/1390 train_time:92779ms step_avg:165.97ms
step:570/1390 train_time:92954ms step_avg:165.99ms
step:571/1390 train_time:93173ms step_avg:166.08ms
step:572/1390 train_time:93345ms step_avg:166.09ms
step:573/1390 train_time:93515ms step_avg:166.10ms
step:574/1390 train_time:93689ms step_avg:166.11ms
step:575/1390 train_time:93860ms step_avg:166.12ms
step:576/1390 train_time:94033ms step_avg:166.14ms
step:577/1390 train_time:94208ms step_avg:166.15ms
step:578/1390 train_time:94382ms step_avg:166.17ms
step:579/1390 train_time:94554ms step_avg:166.18ms
step:580/1390 train_time:94725ms step_avg:166.18ms
step:581/1390 train_time:94897ms step_avg:166.20ms
step:582/1390 train_time:95073ms step_avg:166.21ms
step:583/1390 train_time:95250ms step_avg:166.23ms
step:584/1390 train_time:95424ms step_avg:166.24ms
step:585/1390 train_time:95595ms step_avg:166.25ms
step:586/1390 train_time:95767ms step_avg:166.26ms
step:587/1390 train_time:95939ms step_avg:166.27ms
step:588/1390 train_time:96112ms step_avg:166.28ms
step:589/1390 train_time:96287ms step_avg:166.30ms
step:590/1390 train_time:96460ms step_avg:166.31ms
step:591/1390 train_time:96631ms step_avg:166.32ms
step:592/1390 train_time:96804ms step_avg:166.33ms
step:593/1390 train_time:96977ms step_avg:166.34ms
step:594/1390 train_time:97150ms step_avg:166.35ms
step:595/1390 train_time:97324ms step_avg:166.37ms
step:596/1390 train_time:97496ms step_avg:166.38ms
step:597/1390 train_time:97669ms step_avg:166.39ms
step:598/1390 train_time:97840ms step_avg:166.40ms
step:599/1390 train_time:98014ms step_avg:166.41ms
step:600/1390 train_time:98189ms step_avg:166.42ms
step:601/1390 train_time:98362ms step_avg:166.43ms
step:602/1390 train_time:98533ms step_avg:166.44ms
step:603/1390 train_time:98706ms step_avg:166.45ms
step:604/1390 train_time:98878ms step_avg:166.46ms
step:605/1390 train_time:99054ms step_avg:166.48ms
step:606/1390 train_time:99228ms step_avg:166.49ms
step:607/1390 train_time:99403ms step_avg:166.50ms
step:608/1390 train_time:99575ms step_avg:166.51ms
step:609/1390 train_time:99748ms step_avg:166.52ms
step:610/1390 train_time:99918ms step_avg:166.53ms
step:611/1390 train_time:100094ms step_avg:166.55ms
step:612/1390 train_time:100268ms step_avg:166.56ms
step:613/1390 train_time:100440ms step_avg:166.57ms
step:614/1390 train_time:100613ms step_avg:166.58ms
step:615/1390 train_time:100785ms step_avg:166.59ms
step:616/1390 train_time:100955ms step_avg:166.59ms
step:617/1390 train_time:101128ms step_avg:166.60ms
step:618/1390 train_time:101300ms step_avg:166.61ms
step:619/1390 train_time:101476ms step_avg:166.63ms
step:620/1390 train_time:101652ms step_avg:166.64ms
step:621/1390 train_time:101826ms step_avg:166.66ms
step:622/1390 train_time:102001ms step_avg:166.67ms
step:623/1390 train_time:102177ms step_avg:166.68ms
step:624/1390 train_time:102352ms step_avg:166.70ms
step:625/1390 train_time:102527ms step_avg:166.71ms
step:625/1390 val_loss:3.5469 train_time:102602ms step_avg:166.83ms
step:626/1390 train_time:102705ms step_avg:166.73ms
step:627/1390 train_time:102884ms step_avg:166.75ms
step:628/1390 train_time:103057ms step_avg:166.76ms
step:629/1390 train_time:103230ms step_avg:166.77ms
step:630/1390 train_time:103401ms step_avg:166.78ms
step:631/1390 train_time:103575ms step_avg:166.79ms
step:632/1390 train_time:103753ms step_avg:166.81ms
step:633/1390 train_time:103926ms step_avg:166.82ms
step:634/1390 train_time:104101ms step_avg:166.83ms
step:635/1390 train_time:104276ms step_avg:166.84ms
step:636/1390 train_time:104451ms step_avg:166.85ms
step:637/1390 train_time:104626ms step_avg:166.87ms
step:638/1390 train_time:104802ms step_avg:166.88ms
step:639/1390 train_time:104974ms step_avg:166.89ms
step:640/1390 train_time:105148ms step_avg:166.90ms
step:641/1390 train_time:105323ms step_avg:166.91ms
step:642/1390 train_time:105498ms step_avg:166.93ms
step:643/1390 train_time:105672ms step_avg:166.94ms
step:644/1390 train_time:105847ms step_avg:166.95ms
step:645/1390 train_time:106022ms step_avg:166.96ms
step:646/1390 train_time:106198ms step_avg:166.98ms
step:647/1390 train_time:106368ms step_avg:166.98ms
step:648/1390 train_time:106547ms step_avg:167.00ms
step:649/1390 train_time:106722ms step_avg:167.01ms
step:650/1390 train_time:106896ms step_avg:167.03ms
step:651/1390 train_time:107071ms step_avg:167.04ms
step:652/1390 train_time:107246ms step_avg:167.05ms
step:653/1390 train_time:107421ms step_avg:167.06ms
step:654/1390 train_time:107597ms step_avg:167.08ms
step:655/1390 train_time:107771ms step_avg:167.09ms
step:656/1390 train_time:107947ms step_avg:167.10ms
step:657/1390 train_time:108122ms step_avg:167.11ms
step:658/1390 train_time:108295ms step_avg:167.12ms
step:659/1390 train_time:108471ms step_avg:167.14ms
step:660/1390 train_time:108646ms step_avg:167.15ms
step:661/1390 train_time:108823ms step_avg:167.16ms
step:662/1390 train_time:108996ms step_avg:167.17ms
step:663/1390 train_time:109169ms step_avg:167.18ms
step:664/1390 train_time:109345ms step_avg:167.19ms
step:665/1390 train_time:109521ms step_avg:167.21ms
step:666/1390 train_time:109694ms step_avg:167.22ms
step:667/1390 train_time:109868ms step_avg:167.23ms
step:668/1390 train_time:110043ms step_avg:167.24ms
step:669/1390 train_time:110219ms step_avg:167.25ms
step:670/1390 train_time:110392ms step_avg:167.26ms
step:671/1390 train_time:110566ms step_avg:167.27ms
step:672/1390 train_time:110742ms step_avg:167.28ms
step:673/1390 train_time:110918ms step_avg:167.30ms
step:674/1390 train_time:111093ms step_avg:167.31ms
step:675/1390 train_time:111268ms step_avg:167.32ms
step:676/1390 train_time:111443ms step_avg:167.33ms
step:677/1390 train_time:111618ms step_avg:167.34ms
step:678/1390 train_time:111791ms step_avg:167.35ms
step:679/1390 train_time:111965ms step_avg:167.36ms
step:680/1390 train_time:112142ms step_avg:167.38ms
step:681/1390 train_time:112319ms step_avg:167.39ms
step:682/1390 train_time:112494ms step_avg:167.40ms
step:683/1390 train_time:112668ms step_avg:167.41ms
step:684/1390 train_time:112845ms step_avg:167.43ms
step:685/1390 train_time:113021ms step_avg:167.44ms
step:686/1390 train_time:113194ms step_avg:167.45ms
step:687/1390 train_time:113367ms step_avg:167.45ms
step:688/1390 train_time:113545ms step_avg:167.47ms
step:689/1390 train_time:113720ms step_avg:167.48ms
step:690/1390 train_time:113898ms step_avg:167.50ms
step:691/1390 train_time:114070ms step_avg:167.50ms
step:692/1390 train_time:114244ms step_avg:167.51ms
step:693/1390 train_time:114420ms step_avg:167.53ms
step:694/1390 train_time:114593ms step_avg:167.53ms
step:695/1390 train_time:114767ms step_avg:167.54ms
step:696/1390 train_time:114943ms step_avg:167.55ms
step:697/1390 train_time:115118ms step_avg:167.57ms
step:698/1390 train_time:115291ms step_avg:167.57ms
step:699/1390 train_time:115466ms step_avg:167.58ms
step:700/1390 train_time:115643ms step_avg:167.60ms
step:701/1390 train_time:115817ms step_avg:167.61ms
step:702/1390 train_time:115991ms step_avg:167.62ms
step:703/1390 train_time:116165ms step_avg:167.63ms
step:704/1390 train_time:116340ms step_avg:167.64ms
step:705/1390 train_time:116515ms step_avg:167.65ms
step:706/1390 train_time:116692ms step_avg:167.66ms
step:707/1390 train_time:116865ms step_avg:167.67ms
step:708/1390 train_time:117043ms step_avg:167.68ms
step:709/1390 train_time:117221ms step_avg:167.70ms
step:710/1390 train_time:117397ms step_avg:167.71ms
step:711/1390 train_time:117572ms step_avg:167.72ms
step:712/1390 train_time:117750ms step_avg:167.74ms
step:713/1390 train_time:117927ms step_avg:167.75ms
step:714/1390 train_time:118100ms step_avg:167.76ms
step:715/1390 train_time:118275ms step_avg:167.77ms
step:716/1390 train_time:118453ms step_avg:167.78ms
step:717/1390 train_time:118627ms step_avg:167.79ms
step:718/1390 train_time:118803ms step_avg:167.80ms
step:719/1390 train_time:118976ms step_avg:167.81ms
step:720/1390 train_time:119153ms step_avg:167.82ms
step:721/1390 train_time:119327ms step_avg:167.83ms
step:722/1390 train_time:119503ms step_avg:167.84ms
step:723/1390 train_time:119678ms step_avg:167.85ms
step:724/1390 train_time:119858ms step_avg:167.87ms
step:725/1390 train_time:120034ms step_avg:167.88ms
step:726/1390 train_time:120212ms step_avg:167.89ms
step:727/1390 train_time:120391ms step_avg:167.91ms
step:728/1390 train_time:120565ms step_avg:167.92ms
step:729/1390 train_time:120742ms step_avg:167.93ms
step:730/1390 train_time:120921ms step_avg:167.95ms
step:731/1390 train_time:121096ms step_avg:167.96ms
step:732/1390 train_time:121270ms step_avg:167.96ms
step:733/1390 train_time:121446ms step_avg:167.98ms
step:734/1390 train_time:121622ms step_avg:167.99ms
step:735/1390 train_time:121799ms step_avg:168.00ms
step:736/1390 train_time:121974ms step_avg:168.01ms
step:737/1390 train_time:122151ms step_avg:168.02ms
step:738/1390 train_time:122328ms step_avg:168.03ms
step:739/1390 train_time:122504ms step_avg:168.04ms
step:740/1390 train_time:122684ms step_avg:168.06ms
step:741/1390 train_time:122860ms step_avg:168.07ms
step:742/1390 train_time:123037ms step_avg:168.08ms
step:743/1390 train_time:123215ms step_avg:168.10ms
step:744/1390 train_time:123396ms step_avg:168.11ms
step:745/1390 train_time:123575ms step_avg:168.13ms
step:746/1390 train_time:123751ms step_avg:168.14ms
step:747/1390 train_time:123926ms step_avg:168.15ms
step:748/1390 train_time:124102ms step_avg:168.16ms
step:749/1390 train_time:124280ms step_avg:168.17ms
step:750/1390 train_time:124458ms step_avg:168.19ms
step:750/1390 val_loss:3.5000 train_time:124532ms step_avg:168.29ms
step:751/1390 train_time:124638ms step_avg:168.20ms
step:752/1390 train_time:124816ms step_avg:168.22ms
step:753/1390 train_time:124991ms step_avg:168.22ms
step:754/1390 train_time:125164ms step_avg:168.23ms
step:755/1390 train_time:125340ms step_avg:168.24ms
step:756/1390 train_time:125517ms step_avg:168.25ms
step:757/1390 train_time:125700ms step_avg:168.27ms
step:758/1390 train_time:125878ms step_avg:168.29ms
step:759/1390 train_time:126055ms step_avg:168.30ms
step:760/1390 train_time:126229ms step_avg:168.31ms
step:761/1390 train_time:126454ms step_avg:168.38ms
step:762/1390 train_time:126627ms step_avg:168.39ms
step:763/1390 train_time:126803ms step_avg:168.40ms
step:764/1390 train_time:126978ms step_avg:168.41ms
step:765/1390 train_time:127152ms step_avg:168.41ms
step:766/1390 train_time:127330ms step_avg:168.43ms
step:767/1390 train_time:127509ms step_avg:168.44ms
step:768/1390 train_time:127688ms step_avg:168.45ms
step:769/1390 train_time:127863ms step_avg:168.46ms
step:770/1390 train_time:128038ms step_avg:168.47ms
step:771/1390 train_time:128216ms step_avg:168.48ms
step:772/1390 train_time:128394ms step_avg:168.50ms
step:773/1390 train_time:128571ms step_avg:168.51ms
step:774/1390 train_time:128748ms step_avg:168.52ms
step:775/1390 train_time:128925ms step_avg:168.53ms
step:776/1390 train_time:129103ms step_avg:168.54ms
step:777/1390 train_time:129280ms step_avg:168.55ms
step:778/1390 train_time:129455ms step_avg:168.56ms
step:779/1390 train_time:129632ms step_avg:168.57ms
step:780/1390 train_time:129809ms step_avg:168.58ms
step:781/1390 train_time:129986ms step_avg:168.59ms
step:782/1390 train_time:130162ms step_avg:168.60ms
step:783/1390 train_time:130337ms step_avg:168.61ms
step:784/1390 train_time:130515ms step_avg:168.62ms
step:785/1390 train_time:130693ms step_avg:168.64ms
step:786/1390 train_time:130870ms step_avg:168.65ms
step:787/1390 train_time:131046ms step_avg:168.66ms
step:788/1390 train_time:131222ms step_avg:168.67ms
step:789/1390 train_time:131397ms step_avg:168.67ms
step:790/1390 train_time:131573ms step_avg:168.68ms
step:791/1390 train_time:131752ms step_avg:168.70ms
step:792/1390 train_time:131928ms step_avg:168.71ms
step:793/1390 train_time:132105ms step_avg:168.72ms
step:794/1390 train_time:132283ms step_avg:168.73ms
step:795/1390 train_time:132463ms step_avg:168.74ms
step:796/1390 train_time:132642ms step_avg:168.76ms
step:797/1390 train_time:132820ms step_avg:168.77ms
step:798/1390 train_time:132996ms step_avg:168.78ms
step:799/1390 train_time:133174ms step_avg:168.79ms
step:800/1390 train_time:133350ms step_avg:168.80ms
step:801/1390 train_time:133527ms step_avg:168.81ms
step:802/1390 train_time:133707ms step_avg:168.82ms
step:803/1390 train_time:133882ms step_avg:168.83ms
step:804/1390 train_time:134058ms step_avg:168.84ms
step:805/1390 train_time:134235ms step_avg:168.85ms
step:806/1390 train_time:134412ms step_avg:168.86ms
step:807/1390 train_time:134587ms step_avg:168.87ms
step:808/1390 train_time:134763ms step_avg:168.88ms
step:809/1390 train_time:134940ms step_avg:168.89ms
step:810/1390 train_time:135116ms step_avg:168.89ms
step:811/1390 train_time:135293ms step_avg:168.91ms
step:812/1390 train_time:135472ms step_avg:168.92ms
step:813/1390 train_time:135648ms step_avg:168.93ms
step:814/1390 train_time:135825ms step_avg:168.94ms
step:815/1390 train_time:135998ms step_avg:168.94ms
step:816/1390 train_time:136177ms step_avg:168.95ms
step:817/1390 train_time:136353ms step_avg:168.96ms
step:818/1390 train_time:136529ms step_avg:168.97ms
step:819/1390 train_time:136707ms step_avg:168.98ms
step:820/1390 train_time:136884ms step_avg:168.99ms
step:821/1390 train_time:137058ms step_avg:169.00ms
step:822/1390 train_time:137234ms step_avg:169.01ms
step:823/1390 train_time:137413ms step_avg:169.02ms
step:824/1390 train_time:137588ms step_avg:169.03ms
step:825/1390 train_time:137766ms step_avg:169.04ms
step:826/1390 train_time:137947ms step_avg:169.05ms
step:827/1390 train_time:138125ms step_avg:169.06ms
step:828/1390 train_time:138304ms step_avg:169.08ms
step:829/1390 train_time:138482ms step_avg:169.09ms
step:830/1390 train_time:138657ms step_avg:169.09ms
step:831/1390 train_time:138838ms step_avg:169.11ms
step:832/1390 train_time:139018ms step_avg:169.12ms
step:833/1390 train_time:139197ms step_avg:169.13ms
step:834/1390 train_time:139377ms step_avg:169.15ms
step:835/1390 train_time:139554ms step_avg:169.16ms
step:836/1390 train_time:139732ms step_avg:169.17ms
step:837/1390 train_time:139912ms step_avg:169.18ms
step:838/1390 train_time:140091ms step_avg:169.19ms
step:839/1390 train_time:140268ms step_avg:169.20ms
step:840/1390 train_time:140445ms step_avg:169.21ms
step:841/1390 train_time:140624ms step_avg:169.22ms
step:842/1390 train_time:140803ms step_avg:169.23ms
step:843/1390 train_time:140981ms step_avg:169.24ms
step:844/1390 train_time:141159ms step_avg:169.25ms
step:845/1390 train_time:141338ms step_avg:169.27ms
step:846/1390 train_time:141519ms step_avg:169.28ms
step:847/1390 train_time:141698ms step_avg:169.29ms
step:848/1390 train_time:141873ms step_avg:169.30ms
step:849/1390 train_time:142052ms step_avg:169.31ms
step:850/1390 train_time:142230ms step_avg:169.32ms
step:851/1390 train_time:142412ms step_avg:169.34ms
step:852/1390 train_time:142590ms step_avg:169.35ms
step:853/1390 train_time:142767ms step_avg:169.36ms
step:854/1390 train_time:142944ms step_avg:169.36ms
step:855/1390 train_time:143123ms step_avg:169.38ms
step:856/1390 train_time:143299ms step_avg:169.38ms
step:857/1390 train_time:143479ms step_avg:169.40ms
step:858/1390 train_time:143658ms step_avg:169.41ms
step:859/1390 train_time:143837ms step_avg:169.42ms
step:860/1390 train_time:144014ms step_avg:169.43ms
step:861/1390 train_time:144192ms step_avg:169.44ms
step:862/1390 train_time:144371ms step_avg:169.45ms
step:863/1390 train_time:144553ms step_avg:169.46ms
step:864/1390 train_time:144730ms step_avg:169.47ms
step:865/1390 train_time:144908ms step_avg:169.48ms
step:866/1390 train_time:145094ms step_avg:169.50ms
step:867/1390 train_time:145271ms step_avg:169.51ms
step:868/1390 train_time:145446ms step_avg:169.52ms
step:869/1390 train_time:145625ms step_avg:169.53ms
step:870/1390 train_time:145805ms step_avg:169.54ms
step:871/1390 train_time:145983ms step_avg:169.55ms
step:872/1390 train_time:146161ms step_avg:169.56ms
step:873/1390 train_time:146337ms step_avg:169.57ms
step:874/1390 train_time:146518ms step_avg:169.58ms
step:875/1390 train_time:146696ms step_avg:169.59ms
step:875/1390 val_loss:3.5000 train_time:146774ms step_avg:169.68ms
step:876/1390 train_time:146881ms step_avg:169.61ms
step:877/1390 train_time:147060ms step_avg:169.62ms
step:878/1390 train_time:147238ms step_avg:169.63ms
step:879/1390 train_time:147415ms step_avg:169.64ms
step:880/1390 train_time:147592ms step_avg:169.65ms
step:881/1390 train_time:147769ms step_avg:169.65ms
step:882/1390 train_time:147952ms step_avg:169.67ms
step:883/1390 train_time:148131ms step_avg:169.68ms
step:884/1390 train_time:148308ms step_avg:169.69ms
step:885/1390 train_time:148483ms step_avg:169.70ms
step:886/1390 train_time:148662ms step_avg:169.71ms
step:887/1390 train_time:148842ms step_avg:169.72ms
step:888/1390 train_time:149025ms step_avg:169.73ms
step:889/1390 train_time:149203ms step_avg:169.74ms
step:890/1390 train_time:149380ms step_avg:169.75ms
step:891/1390 train_time:149558ms step_avg:169.76ms
step:892/1390 train_time:149735ms step_avg:169.77ms
step:893/1390 train_time:149910ms step_avg:169.77ms
step:894/1390 train_time:150090ms step_avg:169.79ms
step:895/1390 train_time:150269ms step_avg:169.80ms
step:896/1390 train_time:150447ms step_avg:169.81ms
step:897/1390 train_time:150626ms step_avg:169.81ms
step:898/1390 train_time:150806ms step_avg:169.83ms
step:899/1390 train_time:150985ms step_avg:169.84ms
step:900/1390 train_time:151165ms step_avg:169.85ms
step:901/1390 train_time:151343ms step_avg:169.86ms
step:902/1390 train_time:151520ms step_avg:169.86ms
step:903/1390 train_time:151701ms step_avg:169.88ms
step:904/1390 train_time:151880ms step_avg:169.89ms
step:905/1390 train_time:152058ms step_avg:169.90ms
step:906/1390 train_time:152238ms step_avg:169.91ms
step:907/1390 train_time:152418ms step_avg:169.92ms
step:908/1390 train_time:152594ms step_avg:169.93ms
step:909/1390 train_time:152775ms step_avg:169.94ms
step:910/1390 train_time:152958ms step_avg:169.95ms
step:911/1390 train_time:153135ms step_avg:169.96ms
step:912/1390 train_time:153312ms step_avg:169.97ms
step:913/1390 train_time:153492ms step_avg:169.98ms
step:914/1390 train_time:153670ms step_avg:169.99ms
step:915/1390 train_time:153851ms step_avg:170.00ms
step:916/1390 train_time:154032ms step_avg:170.01ms
step:917/1390 train_time:154212ms step_avg:170.02ms
step:918/1390 train_time:154391ms step_avg:170.03ms
step:919/1390 train_time:154578ms step_avg:170.05ms
step:920/1390 train_time:154755ms step_avg:170.06ms
step:921/1390 train_time:154932ms step_avg:170.07ms
step:922/1390 train_time:155112ms step_avg:170.08ms
step:923/1390 train_time:155286ms step_avg:170.08ms
step:924/1390 train_time:155464ms step_avg:170.09ms
step:925/1390 train_time:155643ms step_avg:170.10ms
step:926/1390 train_time:155823ms step_avg:170.11ms
step:927/1390 train_time:156003ms step_avg:170.12ms
step:928/1390 train_time:156184ms step_avg:170.13ms
step:929/1390 train_time:156364ms step_avg:170.15ms
step:930/1390 train_time:156544ms step_avg:170.16ms
step:931/1390 train_time:156721ms step_avg:170.16ms
step:932/1390 train_time:156902ms step_avg:170.18ms
step:933/1390 train_time:157082ms step_avg:170.19ms
step:934/1390 train_time:157261ms step_avg:170.20ms
step:935/1390 train_time:157446ms step_avg:170.21ms
step:936/1390 train_time:157628ms step_avg:170.22ms
step:937/1390 train_time:157812ms step_avg:170.24ms
step:938/1390 train_time:157994ms step_avg:170.25ms
step:939/1390 train_time:158180ms step_avg:170.27ms
step:940/1390 train_time:158359ms step_avg:170.28ms
step:941/1390 train_time:158538ms step_avg:170.29ms
step:942/1390 train_time:158717ms step_avg:170.30ms
step:943/1390 train_time:158902ms step_avg:170.31ms
step:944/1390 train_time:159088ms step_avg:170.33ms
step:945/1390 train_time:159268ms step_avg:170.34ms
step:946/1390 train_time:159453ms step_avg:170.36ms
step:947/1390 train_time:159633ms step_avg:170.37ms
step:948/1390 train_time:159812ms step_avg:170.38ms
step:949/1390 train_time:159996ms step_avg:170.39ms
step:950/1390 train_time:160174ms step_avg:170.40ms
step:951/1390 train_time:160401ms step_avg:170.46ms
step:952/1390 train_time:160577ms step_avg:170.46ms
step:953/1390 train_time:160758ms step_avg:170.47ms
step:954/1390 train_time:160936ms step_avg:170.48ms
step:955/1390 train_time:161113ms step_avg:170.49ms
step:956/1390 train_time:161299ms step_avg:170.51ms
step:957/1390 train_time:161479ms step_avg:170.52ms
step:958/1390 train_time:161662ms step_avg:170.53ms
step:959/1390 train_time:161846ms step_avg:170.54ms
step:960/1390 train_time:162025ms step_avg:170.55ms
step:961/1390 train_time:162203ms step_avg:170.56ms
step:962/1390 train_time:162382ms step_avg:170.57ms
step:963/1390 train_time:162569ms step_avg:170.59ms
step:964/1390 train_time:162747ms step_avg:170.59ms
step:965/1390 train_time:162924ms step_avg:170.60ms
step:966/1390 train_time:163104ms step_avg:170.61ms
step:967/1390 train_time:163285ms step_avg:170.62ms
step:968/1390 train_time:163466ms step_avg:170.63ms
step:969/1390 train_time:163646ms step_avg:170.64ms
step:970/1390 train_time:163825ms step_avg:170.65ms
step:971/1390 train_time:164004ms step_avg:170.66ms
step:972/1390 train_time:164185ms step_avg:170.67ms
step:973/1390 train_time:164364ms step_avg:170.68ms
step:974/1390 train_time:164547ms step_avg:170.69ms
step:975/1390 train_time:164727ms step_avg:170.70ms
step:976/1390 train_time:164905ms step_avg:170.71ms
step:977/1390 train_time:165084ms step_avg:170.72ms
step:978/1390 train_time:165264ms step_avg:170.73ms
step:979/1390 train_time:165443ms step_avg:170.74ms
step:980/1390 train_time:165620ms step_avg:170.74ms
step:981/1390 train_time:165796ms step_avg:170.75ms
step:982/1390 train_time:165976ms step_avg:170.76ms
step:983/1390 train_time:166155ms step_avg:170.77ms
step:984/1390 train_time:166331ms step_avg:170.77ms
step:985/1390 train_time:166513ms step_avg:170.78ms
step:986/1390 train_time:166700ms step_avg:170.80ms
step:987/1390 train_time:166875ms step_avg:170.80ms
step:988/1390 train_time:167059ms step_avg:170.82ms
step:989/1390 train_time:167239ms step_avg:170.83ms
step:990/1390 train_time:167421ms step_avg:170.84ms
step:991/1390 train_time:167598ms step_avg:170.84ms
step:992/1390 train_time:167784ms step_avg:170.86ms
step:993/1390 train_time:167971ms step_avg:170.88ms
step:994/1390 train_time:168150ms step_avg:170.88ms
step:995/1390 train_time:168328ms step_avg:170.89ms
step:996/1390 train_time:168506ms step_avg:170.90ms
step:997/1390 train_time:168684ms step_avg:170.91ms
step:998/1390 train_time:168864ms step_avg:170.92ms
step:999/1390 train_time:169042ms step_avg:170.92ms
step:1000/1390 train_time:169222ms step_avg:170.93ms
step:1000/1390 val_loss:3.4062 train_time:169298ms step_avg:171.01ms
step:1001/1390 train_time:169406ms step_avg:170.94ms
step:1002/1390 train_time:169585ms step_avg:170.95ms
step:1003/1390 train_time:169766ms step_avg:170.96ms
step:1004/1390 train_time:169949ms step_avg:170.97ms
step:1005/1390 train_time:170129ms step_avg:170.98ms
step:1006/1390 train_time:170309ms step_avg:170.99ms
step:1007/1390 train_time:170490ms step_avg:171.00ms
step:1008/1390 train_time:170670ms step_avg:171.01ms
step:1009/1390 train_time:170855ms step_avg:171.03ms
step:1010/1390 train_time:171032ms step_avg:171.03ms
step:1011/1390 train_time:171215ms step_avg:171.04ms
step:1012/1390 train_time:171398ms step_avg:171.06ms
step:1013/1390 train_time:171579ms step_avg:171.07ms
step:1014/1390 train_time:171757ms step_avg:171.07ms
step:1015/1390 train_time:171936ms step_avg:171.08ms
step:1016/1390 train_time:172118ms step_avg:171.09ms
step:1017/1390 train_time:172301ms step_avg:171.10ms
step:1018/1390 train_time:172480ms step_avg:171.11ms
step:1019/1390 train_time:172662ms step_avg:171.12ms
step:1020/1390 train_time:172844ms step_avg:171.13ms
step:1021/1390 train_time:173021ms step_avg:171.14ms
step:1022/1390 train_time:173204ms step_avg:171.15ms
step:1023/1390 train_time:173384ms step_avg:171.16ms
step:1024/1390 train_time:173568ms step_avg:171.17ms
step:1025/1390 train_time:173748ms step_avg:171.18ms
step:1026/1390 train_time:173928ms step_avg:171.19ms
step:1027/1390 train_time:174107ms step_avg:171.20ms
step:1028/1390 train_time:174289ms step_avg:171.21ms
step:1029/1390 train_time:174474ms step_avg:171.22ms
step:1030/1390 train_time:174655ms step_avg:171.23ms
step:1031/1390 train_time:174832ms step_avg:171.24ms
step:1032/1390 train_time:175011ms step_avg:171.24ms
step:1033/1390 train_time:175190ms step_avg:171.25ms
step:1034/1390 train_time:175373ms step_avg:171.26ms
step:1035/1390 train_time:175556ms step_avg:171.27ms
step:1036/1390 train_time:175737ms step_avg:171.28ms
step:1037/1390 train_time:175923ms step_avg:171.30ms
step:1038/1390 train_time:176104ms step_avg:171.31ms
step:1039/1390 train_time:176285ms step_avg:171.32ms
step:1040/1390 train_time:176465ms step_avg:171.33ms
step:1041/1390 train_time:176649ms step_avg:171.34ms
step:1042/1390 train_time:176830ms step_avg:171.35ms
step:1043/1390 train_time:177011ms step_avg:171.36ms
step:1044/1390 train_time:177197ms step_avg:171.37ms
step:1045/1390 train_time:177380ms step_avg:171.38ms
step:1046/1390 train_time:177560ms step_avg:171.39ms
step:1047/1390 train_time:177741ms step_avg:171.40ms
step:1048/1390 train_time:177924ms step_avg:171.41ms
step:1049/1390 train_time:178105ms step_avg:171.42ms
step:1050/1390 train_time:178287ms step_avg:171.43ms
step:1051/1390 train_time:178470ms step_avg:171.44ms
step:1052/1390 train_time:178652ms step_avg:171.45ms
step:1053/1390 train_time:178830ms step_avg:171.46ms
step:1054/1390 train_time:179012ms step_avg:171.47ms
step:1055/1390 train_time:179190ms step_avg:171.47ms
step:1056/1390 train_time:179369ms step_avg:171.48ms
step:1057/1390 train_time:179551ms step_avg:171.49ms
step:1058/1390 train_time:179736ms step_avg:171.50ms
step:1059/1390 train_time:179922ms step_avg:171.52ms
step:1060/1390 train_time:180103ms step_avg:171.53ms
step:1061/1390 train_time:180281ms step_avg:171.53ms
step:1062/1390 train_time:180465ms step_avg:171.55ms
step:1063/1390 train_time:180645ms step_avg:171.55ms
step:1064/1390 train_time:180825ms step_avg:171.56ms
step:1065/1390 train_time:181008ms step_avg:171.57ms
step:1066/1390 train_time:181194ms step_avg:171.59ms
step:1067/1390 train_time:181376ms step_avg:171.59ms
step:1068/1390 train_time:181554ms step_avg:171.60ms
step:1069/1390 train_time:181738ms step_avg:171.61ms
step:1070/1390 train_time:181916ms step_avg:171.62ms
step:1071/1390 train_time:182103ms step_avg:171.63ms
step:1072/1390 train_time:182285ms step_avg:171.64ms
step:1073/1390 train_time:182460ms step_avg:171.65ms
step:1074/1390 train_time:182642ms step_avg:171.66ms
step:1075/1390 train_time:182827ms step_avg:171.67ms
step:1076/1390 train_time:183008ms step_avg:171.68ms
step:1077/1390 train_time:183188ms step_avg:171.68ms
step:1078/1390 train_time:183376ms step_avg:171.70ms
step:1079/1390 train_time:183562ms step_avg:171.71ms
step:1080/1390 train_time:183743ms step_avg:171.72ms
step:1081/1390 train_time:183925ms step_avg:171.73ms
step:1082/1390 train_time:184104ms step_avg:171.74ms
step:1083/1390 train_time:184286ms step_avg:171.75ms
step:1084/1390 train_time:184470ms step_avg:171.76ms
step:1085/1390 train_time:184651ms step_avg:171.77ms
step:1086/1390 train_time:184833ms step_avg:171.78ms
step:1087/1390 train_time:185015ms step_avg:171.79ms
step:1088/1390 train_time:185198ms step_avg:171.80ms
step:1089/1390 train_time:185382ms step_avg:171.81ms
step:1090/1390 train_time:185568ms step_avg:171.82ms
step:1091/1390 train_time:185749ms step_avg:171.83ms
step:1092/1390 train_time:185928ms step_avg:171.84ms
step:1093/1390 train_time:186111ms step_avg:171.85ms
step:1094/1390 train_time:186291ms step_avg:171.85ms
step:1095/1390 train_time:186470ms step_avg:171.86ms
step:1096/1390 train_time:186654ms step_avg:171.87ms
step:1097/1390 train_time:186836ms step_avg:171.88ms
step:1098/1390 train_time:187021ms step_avg:171.89ms
step:1099/1390 train_time:187204ms step_avg:171.90ms
step:1100/1390 train_time:187384ms step_avg:171.91ms
step:1101/1390 train_time:187566ms step_avg:171.92ms
step:1102/1390 train_time:187750ms step_avg:171.93ms
step:1103/1390 train_time:187932ms step_avg:171.94ms
step:1104/1390 train_time:188112ms step_avg:171.95ms
step:1105/1390 train_time:188298ms step_avg:171.96ms
step:1106/1390 train_time:188479ms step_avg:171.97ms
step:1107/1390 train_time:188661ms step_avg:171.98ms
step:1108/1390 train_time:188848ms step_avg:171.99ms
step:1109/1390 train_time:189029ms step_avg:172.00ms
step:1110/1390 train_time:189211ms step_avg:172.01ms
step:1111/1390 train_time:189393ms step_avg:172.02ms
step:1112/1390 train_time:189573ms step_avg:172.03ms
step:1113/1390 train_time:189752ms step_avg:172.03ms
step:1114/1390 train_time:189937ms step_avg:172.04ms
step:1115/1390 train_time:190121ms step_avg:172.05ms
step:1116/1390 train_time:190302ms step_avg:172.06ms
step:1117/1390 train_time:190486ms step_avg:172.07ms
step:1118/1390 train_time:190671ms step_avg:172.09ms
step:1119/1390 train_time:190850ms step_avg:172.09ms
step:1120/1390 train_time:191032ms step_avg:172.10ms
step:1121/1390 train_time:191213ms step_avg:172.11ms
step:1122/1390 train_time:191392ms step_avg:172.12ms
step:1123/1390 train_time:191571ms step_avg:172.12ms
step:1124/1390 train_time:191754ms step_avg:172.13ms
step:1125/1390 train_time:191934ms step_avg:172.14ms
step:1125/1390 val_loss:3.3438 train_time:192012ms step_avg:172.21ms
step:1126/1390 train_time:192119ms step_avg:172.15ms
step:1127/1390 train_time:192302ms step_avg:172.16ms
step:1128/1390 train_time:192485ms step_avg:172.17ms
step:1129/1390 train_time:192668ms step_avg:172.18ms
step:1130/1390 train_time:192849ms step_avg:172.19ms
step:1131/1390 train_time:193035ms step_avg:172.20ms
step:1132/1390 train_time:193214ms step_avg:172.20ms
step:1133/1390 train_time:193394ms step_avg:172.21ms
step:1134/1390 train_time:193574ms step_avg:172.22ms
step:1135/1390 train_time:193762ms step_avg:172.23ms
step:1136/1390 train_time:193952ms step_avg:172.25ms
step:1137/1390 train_time:194137ms step_avg:172.26ms
step:1138/1390 train_time:194322ms step_avg:172.27ms
step:1139/1390 train_time:194504ms step_avg:172.28ms
step:1140/1390 train_time:194686ms step_avg:172.29ms
step:1141/1390 train_time:194918ms step_avg:172.34ms
step:1142/1390 train_time:195101ms step_avg:172.35ms
step:1143/1390 train_time:195286ms step_avg:172.36ms
step:1144/1390 train_time:195469ms step_avg:172.37ms
step:1145/1390 train_time:195649ms step_avg:172.38ms
step:1146/1390 train_time:195835ms step_avg:172.39ms
step:1147/1390 train_time:196018ms step_avg:172.40ms
step:1148/1390 train_time:196199ms step_avg:172.41ms
step:1149/1390 train_time:196385ms step_avg:172.42ms
step:1150/1390 train_time:196567ms step_avg:172.43ms
step:1151/1390 train_time:196752ms step_avg:172.44ms
step:1152/1390 train_time:196939ms step_avg:172.45ms
step:1153/1390 train_time:197126ms step_avg:172.46ms
step:1154/1390 train_time:197305ms step_avg:172.47ms
step:1155/1390 train_time:197489ms step_avg:172.48ms
step:1156/1390 train_time:197680ms step_avg:172.50ms
step:1157/1390 train_time:197865ms step_avg:172.51ms
step:1158/1390 train_time:198045ms step_avg:172.51ms
step:1159/1390 train_time:198226ms step_avg:172.52ms
step:1160/1390 train_time:198407ms step_avg:172.53ms
step:1161/1390 train_time:198594ms step_avg:172.54ms
step:1162/1390 train_time:198774ms step_avg:172.55ms
step:1163/1390 train_time:198957ms step_avg:172.56ms
step:1164/1390 train_time:199139ms step_avg:172.56ms
step:1165/1390 train_time:199317ms step_avg:172.57ms
step:1166/1390 train_time:199499ms step_avg:172.58ms
step:1167/1390 train_time:199682ms step_avg:172.59ms
step:1168/1390 train_time:199867ms step_avg:172.60ms
step:1169/1390 train_time:200048ms step_avg:172.60ms
step:1170/1390 train_time:200230ms step_avg:172.61ms
step:1171/1390 train_time:200411ms step_avg:172.62ms
step:1172/1390 train_time:200595ms step_avg:172.63ms
step:1173/1390 train_time:200780ms step_avg:172.64ms
step:1174/1390 train_time:200976ms step_avg:172.66ms
step:1175/1390 train_time:201161ms step_avg:172.67ms
step:1176/1390 train_time:201347ms step_avg:172.68ms
step:1177/1390 train_time:201536ms step_avg:172.70ms
step:1178/1390 train_time:201716ms step_avg:172.70ms
step:1179/1390 train_time:201896ms step_avg:172.71ms
step:1180/1390 train_time:202089ms step_avg:172.73ms
step:1181/1390 train_time:202270ms step_avg:172.73ms
step:1182/1390 train_time:202452ms step_avg:172.74ms
step:1183/1390 train_time:202636ms step_avg:172.75ms
step:1184/1390 train_time:202817ms step_avg:172.76ms
step:1185/1390 train_time:203002ms step_avg:172.77ms
step:1186/1390 train_time:203188ms step_avg:172.78ms
step:1187/1390 train_time:203383ms step_avg:172.80ms
step:1188/1390 train_time:203564ms step_avg:172.80ms
step:1189/1390 train_time:203749ms step_avg:172.81ms
step:1190/1390 train_time:203930ms step_avg:172.82ms
step:1191/1390 train_time:204114ms step_avg:172.83ms
step:1192/1390 train_time:204293ms step_avg:172.84ms
step:1193/1390 train_time:204474ms step_avg:172.84ms
step:1194/1390 train_time:204657ms step_avg:172.85ms
step:1195/1390 train_time:204842ms step_avg:172.86ms
step:1196/1390 train_time:205026ms step_avg:172.87ms
step:1197/1390 train_time:205209ms step_avg:172.88ms
step:1198/1390 train_time:205402ms step_avg:172.90ms
step:1199/1390 train_time:205583ms step_avg:172.90ms
step:1200/1390 train_time:205765ms step_avg:172.91ms
step:1201/1390 train_time:205948ms step_avg:172.92ms
step:1202/1390 train_time:206148ms step_avg:172.94ms
step:1203/1390 train_time:206334ms step_avg:172.95ms
step:1204/1390 train_time:206520ms step_avg:172.97ms
step:1205/1390 train_time:206706ms step_avg:172.98ms
step:1206/1390 train_time:206889ms step_avg:172.98ms
step:1207/1390 train_time:207072ms step_avg:172.99ms
step:1208/1390 train_time:207256ms step_avg:173.00ms
step:1209/1390 train_time:207444ms step_avg:173.01ms
step:1210/1390 train_time:207629ms step_avg:173.02ms
step:1211/1390 train_time:207812ms step_avg:173.03ms
step:1212/1390 train_time:207996ms step_avg:173.04ms
step:1213/1390 train_time:208178ms step_avg:173.05ms
step:1214/1390 train_time:208366ms step_avg:173.06ms
step:1215/1390 train_time:208550ms step_avg:173.07ms
step:1216/1390 train_time:208733ms step_avg:173.08ms
step:1217/1390 train_time:208920ms step_avg:173.09ms
step:1218/1390 train_time:209099ms step_avg:173.10ms
step:1219/1390 train_time:209280ms step_avg:173.10ms
step:1220/1390 train_time:209465ms step_avg:173.11ms
step:1221/1390 train_time:209646ms step_avg:173.12ms
step:1222/1390 train_time:209826ms step_avg:173.12ms
step:1223/1390 train_time:210008ms step_avg:173.13ms
step:1224/1390 train_time:210196ms step_avg:173.14ms
step:1225/1390 train_time:210380ms step_avg:173.15ms
step:1226/1390 train_time:210565ms step_avg:173.16ms
step:1227/1390 train_time:210749ms step_avg:173.17ms
step:1228/1390 train_time:210929ms step_avg:173.18ms
step:1229/1390 train_time:211110ms step_avg:173.18ms
step:1230/1390 train_time:211299ms step_avg:173.20ms
step:1231/1390 train_time:211483ms step_avg:173.20ms
step:1232/1390 train_time:211668ms step_avg:173.21ms
step:1233/1390 train_time:211851ms step_avg:173.22ms
step:1234/1390 train_time:212032ms step_avg:173.23ms
step:1235/1390 train_time:212217ms step_avg:173.24ms
step:1236/1390 train_time:212404ms step_avg:173.25ms
step:1237/1390 train_time:212589ms step_avg:173.26ms
step:1238/1390 train_time:212785ms step_avg:173.28ms
step:1239/1390 train_time:212969ms step_avg:173.29ms
step:1240/1390 train_time:213156ms step_avg:173.30ms
step:1241/1390 train_time:213346ms step_avg:173.31ms
step:1242/1390 train_time:213528ms step_avg:173.32ms
step:1243/1390 train_time:213714ms step_avg:173.33ms
step:1244/1390 train_time:213896ms step_avg:173.34ms
step:1245/1390 train_time:214081ms step_avg:173.35ms
step:1246/1390 train_time:214267ms step_avg:173.36ms
step:1247/1390 train_time:214449ms step_avg:173.36ms
step:1248/1390 train_time:214632ms step_avg:173.37ms
step:1249/1390 train_time:214813ms step_avg:173.38ms
step:1250/1390 train_time:214997ms step_avg:173.38ms
step:1250/1390 val_loss:3.2969 train_time:215076ms step_avg:173.45ms
step:1251/1390 train_time:215187ms step_avg:173.40ms
step:1252/1390 train_time:215370ms step_avg:173.41ms
step:1253/1390 train_time:215551ms step_avg:173.41ms
step:1254/1390 train_time:215735ms step_avg:173.42ms
step:1255/1390 train_time:215930ms step_avg:173.44ms
step:1256/1390 train_time:216117ms step_avg:173.45ms
step:1257/1390 train_time:216303ms step_avg:173.46ms
step:1258/1390 train_time:216492ms step_avg:173.47ms
step:1259/1390 train_time:216676ms step_avg:173.48ms
step:1260/1390 train_time:216856ms step_avg:173.48ms
step:1261/1390 train_time:217041ms step_avg:173.49ms
step:1262/1390 train_time:217229ms step_avg:173.51ms
step:1263/1390 train_time:217415ms step_avg:173.52ms
step:1264/1390 train_time:217596ms step_avg:173.52ms
step:1265/1390 train_time:217777ms step_avg:173.53ms
step:1266/1390 train_time:217961ms step_avg:173.54ms
step:1267/1390 train_time:218145ms step_avg:173.54ms
step:1268/1390 train_time:218331ms step_avg:173.55ms
step:1269/1390 train_time:218523ms step_avg:173.57ms
step:1270/1390 train_time:218703ms step_avg:173.57ms
step:1271/1390 train_time:218888ms step_avg:173.58ms
step:1272/1390 train_time:219069ms step_avg:173.59ms
step:1273/1390 train_time:219254ms step_avg:173.60ms
step:1274/1390 train_time:219439ms step_avg:173.61ms
step:1275/1390 train_time:219625ms step_avg:173.62ms
step:1276/1390 train_time:219804ms step_avg:173.62ms
step:1277/1390 train_time:219991ms step_avg:173.63ms
step:1278/1390 train_time:220173ms step_avg:173.64ms
step:1279/1390 train_time:220358ms step_avg:173.65ms
step:1280/1390 train_time:220550ms step_avg:173.66ms
step:1281/1390 train_time:220731ms step_avg:173.67ms
step:1282/1390 train_time:220911ms step_avg:173.67ms
step:1283/1390 train_time:221098ms step_avg:173.68ms
step:1284/1390 train_time:221281ms step_avg:173.69ms
step:1285/1390 train_time:221466ms step_avg:173.70ms
step:1286/1390 train_time:221649ms step_avg:173.71ms
step:1287/1390 train_time:221833ms step_avg:173.71ms
step:1288/1390 train_time:222018ms step_avg:173.72ms
step:1289/1390 train_time:222211ms step_avg:173.74ms
step:1290/1390 train_time:222404ms step_avg:173.75ms
step:1291/1390 train_time:222591ms step_avg:173.76ms
step:1292/1390 train_time:222777ms step_avg:173.77ms
step:1293/1390 train_time:222965ms step_avg:173.78ms
step:1294/1390 train_time:223151ms step_avg:173.79ms
step:1295/1390 train_time:223334ms step_avg:173.80ms
step:1296/1390 train_time:223522ms step_avg:173.81ms
step:1297/1390 train_time:223708ms step_avg:173.82ms
step:1298/1390 train_time:223892ms step_avg:173.83ms
step:1299/1390 train_time:224078ms step_avg:173.84ms
step:1300/1390 train_time:224258ms step_avg:173.84ms
step:1301/1390 train_time:224439ms step_avg:173.85ms
step:1302/1390 train_time:224623ms step_avg:173.86ms
step:1303/1390 train_time:224814ms step_avg:173.87ms
step:1304/1390 train_time:225001ms step_avg:173.88ms
step:1305/1390 train_time:225181ms step_avg:173.89ms
step:1306/1390 train_time:225368ms step_avg:173.89ms
step:1307/1390 train_time:225552ms step_avg:173.90ms
step:1308/1390 train_time:225739ms step_avg:173.91ms
step:1309/1390 train_time:225924ms step_avg:173.92ms
step:1310/1390 train_time:226107ms step_avg:173.93ms
step:1311/1390 train_time:226288ms step_avg:173.93ms
step:1312/1390 train_time:226471ms step_avg:173.94ms
step:1313/1390 train_time:226656ms step_avg:173.95ms
step:1314/1390 train_time:226841ms step_avg:173.96ms
step:1315/1390 train_time:227024ms step_avg:173.96ms
step:1316/1390 train_time:227203ms step_avg:173.97ms
step:1317/1390 train_time:227388ms step_avg:173.98ms
step:1318/1390 train_time:227579ms step_avg:173.99ms
step:1319/1390 train_time:227761ms step_avg:174.00ms
step:1320/1390 train_time:227945ms step_avg:174.00ms
step:1321/1390 train_time:228130ms step_avg:174.01ms
step:1322/1390 train_time:228321ms step_avg:174.02ms
step:1323/1390 train_time:228502ms step_avg:174.03ms
step:1324/1390 train_time:228687ms step_avg:174.04ms
step:1325/1390 train_time:228873ms step_avg:174.05ms
step:1326/1390 train_time:229061ms step_avg:174.06ms
step:1327/1390 train_time:229243ms step_avg:174.06ms
step:1328/1390 train_time:229427ms step_avg:174.07ms
step:1329/1390 train_time:229628ms step_avg:174.09ms
step:1330/1390 train_time:229815ms step_avg:174.10ms
step:1331/1390 train_time:230051ms step_avg:174.15ms
step:1332/1390 train_time:230246ms step_avg:174.16ms
step:1333/1390 train_time:230431ms step_avg:174.17ms
step:1334/1390 train_time:230612ms step_avg:174.18ms
step:1335/1390 train_time:230796ms step_avg:174.19ms
step:1336/1390 train_time:230988ms step_avg:174.20ms
step:1337/1390 train_time:231174ms step_avg:174.21ms
step:1338/1390 train_time:231360ms step_avg:174.22ms
step:1339/1390 train_time:231544ms step_avg:174.22ms
step:1340/1390 train_time:231731ms step_avg:174.23ms
step:1341/1390 train_time:231919ms step_avg:174.24ms
step:1342/1390 train_time:232107ms step_avg:174.25ms
step:1343/1390 train_time:232290ms step_avg:174.26ms
step:1344/1390 train_time:232474ms step_avg:174.27ms
step:1345/1390 train_time:232657ms step_avg:174.28ms
step:1346/1390 train_time:232842ms step_avg:174.28ms
step:1347/1390 train_time:233030ms step_avg:174.29ms
step:1348/1390 train_time:233212ms step_avg:174.30ms
step:1349/1390 train_time:233397ms step_avg:174.31ms
step:1350/1390 train_time:233580ms step_avg:174.31ms
step:1351/1390 train_time:233765ms step_avg:174.32ms
step:1352/1390 train_time:233958ms step_avg:174.34ms
step:1353/1390 train_time:234144ms step_avg:174.34ms
step:1354/1390 train_time:234330ms step_avg:174.35ms
step:1355/1390 train_time:234510ms step_avg:174.36ms
step:1356/1390 train_time:234693ms step_avg:174.36ms
step:1357/1390 train_time:234880ms step_avg:174.37ms
step:1358/1390 train_time:235066ms step_avg:174.38ms
step:1359/1390 train_time:235250ms step_avg:174.39ms
step:1360/1390 train_time:235438ms step_avg:174.40ms
step:1361/1390 train_time:235628ms step_avg:174.41ms
step:1362/1390 train_time:235813ms step_avg:174.42ms
step:1363/1390 train_time:236005ms step_avg:174.43ms
step:1364/1390 train_time:236191ms step_avg:174.44ms
step:1365/1390 train_time:236370ms step_avg:174.44ms
step:1366/1390 train_time:236556ms step_avg:174.45ms
step:1367/1390 train_time:236740ms step_avg:174.46ms
step:1368/1390 train_time:236928ms step_avg:174.47ms
step:1369/1390 train_time:237126ms step_avg:174.49ms
step:1370/1390 train_time:237317ms step_avg:174.50ms
step:1371/1390 train_time:237504ms step_avg:174.51ms
step:1372/1390 train_time:237695ms step_avg:174.52ms
step:1373/1390 train_time:237879ms step_avg:174.53ms
step:1374/1390 train_time:238068ms step_avg:174.54ms
step:1375/1390 train_time:238250ms step_avg:174.54ms
step:1375/1390 val_loss:3.2812 train_time:238327ms step_avg:174.60ms
step:1376/1390 train_time:238436ms step_avg:174.55ms
step:1377/1390 train_time:238624ms step_avg:174.56ms
step:1378/1390 train_time:238806ms step_avg:174.57ms
step:1379/1390 train_time:238992ms step_avg:174.57ms
step:1380/1390 train_time:239175ms step_avg:174.58ms
step:1381/1390 train_time:239367ms step_avg:174.59ms
step:1382/1390 train_time:239553ms step_avg:174.60ms
step:1383/1390 train_time:239736ms step_avg:174.61ms
step:1384/1390 train_time:239927ms step_avg:174.62ms
step:1385/1390 train_time:240107ms step_avg:174.62ms
step:1386/1390 train_time:240292ms step_avg:174.63ms
step:1387/1390 train_time:240478ms step_avg:174.64ms
step:1388/1390 train_time:240658ms step_avg:174.64ms
step:1389/1390 train_time:240846ms step_avg:174.65ms
step:1390/1390 train_time:241029ms step_avg:174.66ms
step:1390/1390 val_loss:3.2812 train_time:241106ms step_avg:174.71ms
peak memory consumption: 10886 MiB
