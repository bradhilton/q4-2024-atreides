import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch

# Enable TF32 on matmul and convolution. 
# (Has effect on Linear layers as well, since internally they use GEMM ops.)
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# (Optional) If you also want to let cuDNN auto-tune for your model shapes:
torch.backends.cudnn.benchmark = True

torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1390 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    ddp_model(inputs_train, targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Jan  9 2025, 22:54:50) [GCC 13.2.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Fri Jan 10 02:22:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   26C    P0            144W /  700W |    7746MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   29C    P0            123W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   28C    P0            128W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   49C    P0            138W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   27C    P0            124W /  700W |    3216MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin', 'data/fineweb10B/fineweb_train_000010.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1390 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1390 train_time:242045ms step_avg:nanms
step:2/1390 train_time:242242ms step_avg:nanms
step:3/1390 train_time:243677ms step_avg:nanms
step:4/1390 train_time:243810ms step_avg:nanms
step:5/1390 train_time:243944ms step_avg:nanms
step:6/1390 train_time:244077ms step_avg:nanms
step:7/1390 train_time:244209ms step_avg:nanms
step:8/1390 train_time:244342ms step_avg:nanms
step:9/1390 train_time:244476ms step_avg:nanms
step:10/1390 train_time:244614ms step_avg:nanms
step:11/1390 train_time:133ms step_avg:nanms
step:12/1390 train_time:269ms step_avg:nanms
step:13/1390 train_time:402ms step_avg:134.02ms
step:14/1390 train_time:537ms step_avg:134.13ms
step:15/1390 train_time:670ms step_avg:134.02ms
step:16/1390 train_time:804ms step_avg:134.06ms
step:17/1390 train_time:938ms step_avg:133.94ms
step:18/1390 train_time:1074ms step_avg:134.23ms
step:19/1390 train_time:1209ms step_avg:134.28ms
step:20/1390 train_time:1346ms step_avg:134.55ms
step:21/1390 train_time:1480ms step_avg:134.54ms
step:22/1390 train_time:1615ms step_avg:134.56ms
step:23/1390 train_time:1748ms step_avg:134.45ms
step:24/1390 train_time:1882ms step_avg:134.44ms
step:25/1390 train_time:2017ms step_avg:134.45ms
step:26/1390 train_time:2151ms step_avg:134.44ms
step:27/1390 train_time:2285ms step_avg:134.43ms
step:28/1390 train_time:2420ms step_avg:134.46ms
step:29/1390 train_time:2554ms step_avg:134.44ms
step:30/1390 train_time:2690ms step_avg:134.50ms
step:31/1390 train_time:2825ms step_avg:134.51ms
step:32/1390 train_time:2960ms step_avg:134.53ms
step:33/1390 train_time:3093ms step_avg:134.46ms
step:34/1390 train_time:3228ms step_avg:134.48ms
step:35/1390 train_time:3363ms step_avg:134.52ms
step:36/1390 train_time:3499ms step_avg:134.56ms
step:37/1390 train_time:3632ms step_avg:134.53ms
step:38/1390 train_time:3767ms step_avg:134.54ms
step:39/1390 train_time:3902ms step_avg:134.55ms
step:40/1390 train_time:4037ms step_avg:134.55ms
step:41/1390 train_time:4170ms step_avg:134.51ms
step:42/1390 train_time:4305ms step_avg:134.53ms
step:43/1390 train_time:4439ms step_avg:134.52ms
step:44/1390 train_time:4575ms step_avg:134.55ms
step:45/1390 train_time:4710ms step_avg:134.56ms
step:46/1390 train_time:4845ms step_avg:134.57ms
step:47/1390 train_time:4979ms step_avg:134.57ms
step:48/1390 train_time:5113ms step_avg:134.56ms
step:49/1390 train_time:5249ms step_avg:134.58ms
step:50/1390 train_time:5383ms step_avg:134.56ms
step:51/1390 train_time:5519ms step_avg:134.60ms
step:52/1390 train_time:5653ms step_avg:134.58ms
step:53/1390 train_time:5787ms step_avg:134.58ms
step:54/1390 train_time:5922ms step_avg:134.60ms
step:55/1390 train_time:6057ms step_avg:134.60ms
step:56/1390 train_time:6191ms step_avg:134.58ms
step:57/1390 train_time:6326ms step_avg:134.61ms
step:58/1390 train_time:6461ms step_avg:134.60ms
step:59/1390 train_time:6595ms step_avg:134.60ms
step:60/1390 train_time:6729ms step_avg:134.58ms
step:61/1390 train_time:6864ms step_avg:134.59ms
step:62/1390 train_time:6998ms step_avg:134.58ms
step:63/1390 train_time:7133ms step_avg:134.58ms
step:64/1390 train_time:7268ms step_avg:134.59ms
step:65/1390 train_time:7402ms step_avg:134.59ms
step:66/1390 train_time:7536ms step_avg:134.57ms
step:67/1390 train_time:7672ms step_avg:134.60ms
step:68/1390 train_time:7806ms step_avg:134.59ms
step:69/1390 train_time:7940ms step_avg:134.57ms
step:70/1390 train_time:8074ms step_avg:134.57ms
step:71/1390 train_time:8209ms step_avg:134.57ms
step:72/1390 train_time:8343ms step_avg:134.57ms
step:73/1390 train_time:8479ms step_avg:134.59ms
step:74/1390 train_time:8613ms step_avg:134.58ms
step:75/1390 train_time:8748ms step_avg:134.59ms
step:76/1390 train_time:8881ms step_avg:134.56ms
step:77/1390 train_time:9016ms step_avg:134.56ms
step:78/1390 train_time:9150ms step_avg:134.55ms
step:79/1390 train_time:9286ms step_avg:134.57ms
step:80/1390 train_time:9420ms step_avg:134.57ms
step:81/1390 train_time:9555ms step_avg:134.57ms
step:82/1390 train_time:9690ms step_avg:134.58ms
step:83/1390 train_time:9825ms step_avg:134.58ms
step:84/1390 train_time:9959ms step_avg:134.58ms
step:85/1390 train_time:10093ms step_avg:134.58ms
step:86/1390 train_time:10228ms step_avg:134.58ms
step:87/1390 train_time:10362ms step_avg:134.57ms
step:88/1390 train_time:10498ms step_avg:134.59ms
step:89/1390 train_time:10632ms step_avg:134.58ms
step:90/1390 train_time:10768ms step_avg:134.60ms
step:91/1390 train_time:10903ms step_avg:134.61ms
step:92/1390 train_time:11038ms step_avg:134.61ms
step:93/1390 train_time:11173ms step_avg:134.61ms
step:94/1390 train_time:11307ms step_avg:134.61ms
step:95/1390 train_time:11442ms step_avg:134.61ms
step:96/1390 train_time:11576ms step_avg:134.61ms
step:97/1390 train_time:11711ms step_avg:134.61ms
step:98/1390 train_time:11847ms step_avg:134.63ms
step:99/1390 train_time:11983ms step_avg:134.63ms
step:100/1390 train_time:12118ms step_avg:134.64ms
step:101/1390 train_time:12254ms step_avg:134.66ms
step:102/1390 train_time:12388ms step_avg:134.66ms
step:103/1390 train_time:12522ms step_avg:134.65ms
step:104/1390 train_time:12659ms step_avg:134.67ms
step:105/1390 train_time:12796ms step_avg:134.70ms
step:106/1390 train_time:12933ms step_avg:134.72ms
step:107/1390 train_time:13071ms step_avg:134.76ms
step:108/1390 train_time:13210ms step_avg:134.80ms
step:109/1390 train_time:13348ms step_avg:134.83ms
step:110/1390 train_time:13486ms step_avg:134.86ms
step:111/1390 train_time:13625ms step_avg:134.90ms
step:112/1390 train_time:13763ms step_avg:134.93ms
step:113/1390 train_time:13902ms step_avg:134.97ms
step:114/1390 train_time:14040ms step_avg:135.00ms
step:115/1390 train_time:14178ms step_avg:135.02ms
step:116/1390 train_time:14315ms step_avg:135.05ms
step:117/1390 train_time:14454ms step_avg:135.08ms
step:118/1390 train_time:14592ms step_avg:135.11ms
step:119/1390 train_time:14730ms step_avg:135.14ms
step:120/1390 train_time:14869ms step_avg:135.17ms
step:121/1390 train_time:15007ms step_avg:135.19ms
step:122/1390 train_time:15144ms step_avg:135.22ms
step:123/1390 train_time:15281ms step_avg:135.23ms
step:124/1390 train_time:15419ms step_avg:135.25ms
step:125/1390 train_time:15557ms step_avg:135.28ms
step:125/1390 val_loss:4.3829 train_time:15626ms step_avg:135.87ms
step:126/1390 train_time:15702ms step_avg:135.36ms
step:127/1390 train_time:15844ms step_avg:135.42ms
step:128/1390 train_time:15983ms step_avg:135.45ms
step:129/1390 train_time:16120ms step_avg:135.46ms
step:130/1390 train_time:16257ms step_avg:135.47ms
step:131/1390 train_time:16394ms step_avg:135.48ms
step:132/1390 train_time:16530ms step_avg:135.49ms
step:133/1390 train_time:16670ms step_avg:135.53ms
step:134/1390 train_time:16811ms step_avg:135.57ms
step:135/1390 train_time:16949ms step_avg:135.59ms
step:136/1390 train_time:17088ms step_avg:135.62ms
step:137/1390 train_time:17226ms step_avg:135.63ms
step:138/1390 train_time:17363ms step_avg:135.65ms
step:139/1390 train_time:17500ms step_avg:135.66ms
step:140/1390 train_time:17638ms step_avg:135.67ms
step:141/1390 train_time:17775ms step_avg:135.69ms
step:142/1390 train_time:17914ms step_avg:135.71ms
step:143/1390 train_time:18054ms step_avg:135.74ms
step:144/1390 train_time:18192ms step_avg:135.76ms
step:145/1390 train_time:18331ms step_avg:135.78ms
step:146/1390 train_time:18469ms step_avg:135.80ms
step:147/1390 train_time:18607ms step_avg:135.82ms
step:148/1390 train_time:18746ms step_avg:135.84ms
step:149/1390 train_time:18883ms step_avg:135.85ms
step:150/1390 train_time:19021ms step_avg:135.87ms
step:151/1390 train_time:19161ms step_avg:135.89ms
step:152/1390 train_time:19299ms step_avg:135.91ms
step:153/1390 train_time:19438ms step_avg:135.93ms
step:154/1390 train_time:19575ms step_avg:135.94ms
step:155/1390 train_time:19714ms step_avg:135.96ms
step:156/1390 train_time:19851ms step_avg:135.97ms
step:157/1390 train_time:19990ms step_avg:135.98ms
step:158/1390 train_time:20128ms step_avg:136.00ms
step:159/1390 train_time:20267ms step_avg:136.02ms
step:160/1390 train_time:20405ms step_avg:136.03ms
step:161/1390 train_time:20543ms step_avg:136.05ms
step:162/1390 train_time:20681ms step_avg:136.06ms
step:163/1390 train_time:20820ms step_avg:136.08ms
step:164/1390 train_time:20959ms step_avg:136.10ms
step:165/1390 train_time:21097ms step_avg:136.11ms
step:166/1390 train_time:21237ms step_avg:136.13ms
step:167/1390 train_time:21374ms step_avg:136.14ms
step:168/1390 train_time:21513ms step_avg:136.16ms
step:169/1390 train_time:21651ms step_avg:136.17ms
step:170/1390 train_time:21790ms step_avg:136.18ms
step:171/1390 train_time:21928ms step_avg:136.20ms
step:172/1390 train_time:22069ms step_avg:136.23ms
step:173/1390 train_time:22206ms step_avg:136.23ms
step:174/1390 train_time:22345ms step_avg:136.25ms
step:175/1390 train_time:22483ms step_avg:136.26ms
step:176/1390 train_time:22621ms step_avg:136.27ms
step:177/1390 train_time:22759ms step_avg:136.28ms
step:178/1390 train_time:22899ms step_avg:136.30ms
step:179/1390 train_time:23038ms step_avg:136.32ms
step:180/1390 train_time:23176ms step_avg:136.33ms
step:181/1390 train_time:23315ms step_avg:136.35ms
step:182/1390 train_time:23454ms step_avg:136.36ms
step:183/1390 train_time:23592ms step_avg:136.37ms
step:184/1390 train_time:23731ms step_avg:136.38ms
step:185/1390 train_time:23870ms step_avg:136.40ms
step:186/1390 train_time:24009ms step_avg:136.42ms
step:187/1390 train_time:24147ms step_avg:136.43ms
step:188/1390 train_time:24287ms step_avg:136.44ms
step:189/1390 train_time:24426ms step_avg:136.46ms
step:190/1390 train_time:24564ms step_avg:136.47ms
step:191/1390 train_time:24752ms step_avg:136.75ms
step:192/1390 train_time:24888ms step_avg:136.75ms
step:193/1390 train_time:25026ms step_avg:136.75ms
step:194/1390 train_time:25164ms step_avg:136.76ms
step:195/1390 train_time:25301ms step_avg:136.76ms
step:196/1390 train_time:25438ms step_avg:136.76ms
step:197/1390 train_time:25576ms step_avg:136.77ms
step:198/1390 train_time:25718ms step_avg:136.80ms
step:199/1390 train_time:25858ms step_avg:136.82ms
step:200/1390 train_time:25996ms step_avg:136.82ms
step:201/1390 train_time:26135ms step_avg:136.83ms
step:202/1390 train_time:26273ms step_avg:136.84ms
step:203/1390 train_time:26410ms step_avg:136.84ms
step:204/1390 train_time:26548ms step_avg:136.85ms
step:205/1390 train_time:26688ms step_avg:136.86ms
step:206/1390 train_time:26826ms step_avg:136.87ms
step:207/1390 train_time:26967ms step_avg:136.89ms
step:208/1390 train_time:27108ms step_avg:136.91ms
step:209/1390 train_time:27248ms step_avg:136.93ms
step:210/1390 train_time:27390ms step_avg:136.95ms
step:211/1390 train_time:27532ms step_avg:136.97ms
step:212/1390 train_time:27673ms step_avg:136.99ms
step:213/1390 train_time:27815ms step_avg:137.02ms
step:214/1390 train_time:27957ms step_avg:137.04ms
step:215/1390 train_time:28097ms step_avg:137.06ms
step:216/1390 train_time:28239ms step_avg:137.08ms
step:217/1390 train_time:28379ms step_avg:137.10ms
step:218/1390 train_time:28522ms step_avg:137.13ms
step:219/1390 train_time:28664ms step_avg:137.15ms
step:220/1390 train_time:28805ms step_avg:137.17ms
step:221/1390 train_time:28946ms step_avg:137.18ms
step:222/1390 train_time:29086ms step_avg:137.20ms
step:223/1390 train_time:29226ms step_avg:137.21ms
step:224/1390 train_time:29368ms step_avg:137.23ms
step:225/1390 train_time:29508ms step_avg:137.25ms
step:226/1390 train_time:29648ms step_avg:137.26ms
step:227/1390 train_time:29789ms step_avg:137.28ms
step:228/1390 train_time:29931ms step_avg:137.30ms
step:229/1390 train_time:30072ms step_avg:137.32ms
step:230/1390 train_time:30214ms step_avg:137.34ms
step:231/1390 train_time:30356ms step_avg:137.36ms
step:232/1390 train_time:30497ms step_avg:137.37ms
step:233/1390 train_time:30638ms step_avg:137.39ms
step:234/1390 train_time:30780ms step_avg:137.41ms
step:235/1390 train_time:30921ms step_avg:137.43ms
step:236/1390 train_time:31063ms step_avg:137.45ms
step:237/1390 train_time:31204ms step_avg:137.46ms
step:238/1390 train_time:31346ms step_avg:137.48ms
step:239/1390 train_time:31487ms step_avg:137.50ms
step:240/1390 train_time:31628ms step_avg:137.51ms
step:241/1390 train_time:31770ms step_avg:137.53ms
step:242/1390 train_time:31911ms step_avg:137.55ms
step:243/1390 train_time:32052ms step_avg:137.56ms
step:244/1390 train_time:32192ms step_avg:137.57ms
step:245/1390 train_time:32334ms step_avg:137.59ms
step:246/1390 train_time:32475ms step_avg:137.60ms
step:247/1390 train_time:32615ms step_avg:137.62ms
step:248/1390 train_time:32757ms step_avg:137.63ms
step:249/1390 train_time:32899ms step_avg:137.65ms
step:250/1390 train_time:33041ms step_avg:137.67ms
step:250/1390 val_loss:3.9524 train_time:33110ms step_avg:137.96ms
step:251/1390 train_time:33186ms step_avg:137.70ms
step:252/1390 train_time:33330ms step_avg:137.73ms
step:253/1390 train_time:33471ms step_avg:137.74ms
step:254/1390 train_time:33612ms step_avg:137.75ms
step:255/1390 train_time:33753ms step_avg:137.77ms
step:256/1390 train_time:33893ms step_avg:137.78ms
step:257/1390 train_time:34033ms step_avg:137.79ms
step:258/1390 train_time:34176ms step_avg:137.80ms
step:259/1390 train_time:34320ms step_avg:137.83ms
step:260/1390 train_time:34462ms step_avg:137.85ms
step:261/1390 train_time:34603ms step_avg:137.86ms
step:262/1390 train_time:34743ms step_avg:137.87ms
step:263/1390 train_time:34885ms step_avg:137.88ms
step:264/1390 train_time:35025ms step_avg:137.90ms
step:265/1390 train_time:35166ms step_avg:137.90ms
step:266/1390 train_time:35307ms step_avg:137.92ms
step:267/1390 train_time:35449ms step_avg:137.93ms
step:268/1390 train_time:35590ms step_avg:137.94ms
step:269/1390 train_time:35732ms step_avg:137.96ms
step:270/1390 train_time:35873ms step_avg:137.97ms
step:271/1390 train_time:36014ms step_avg:137.98ms
step:272/1390 train_time:36157ms step_avg:138.01ms
step:273/1390 train_time:36298ms step_avg:138.02ms
step:274/1390 train_time:36440ms step_avg:138.03ms
step:275/1390 train_time:36582ms step_avg:138.04ms
step:276/1390 train_time:36723ms step_avg:138.06ms
step:277/1390 train_time:36865ms step_avg:138.07ms
step:278/1390 train_time:37006ms step_avg:138.08ms
step:279/1390 train_time:37146ms step_avg:138.09ms
step:280/1390 train_time:37286ms step_avg:138.10ms
step:281/1390 train_time:37429ms step_avg:138.11ms
step:282/1390 train_time:37571ms step_avg:138.13ms
step:283/1390 train_time:37712ms step_avg:138.14ms
step:284/1390 train_time:37853ms step_avg:138.15ms
step:285/1390 train_time:37994ms step_avg:138.16ms
step:286/1390 train_time:38135ms step_avg:138.17ms
step:287/1390 train_time:38276ms step_avg:138.18ms
step:288/1390 train_time:38418ms step_avg:138.19ms
step:289/1390 train_time:38559ms step_avg:138.20ms
step:290/1390 train_time:38700ms step_avg:138.21ms
step:291/1390 train_time:38840ms step_avg:138.22ms
step:292/1390 train_time:38982ms step_avg:138.24ms
step:293/1390 train_time:39124ms step_avg:138.25ms
step:294/1390 train_time:39265ms step_avg:138.26ms
step:295/1390 train_time:39406ms step_avg:138.27ms
step:296/1390 train_time:39547ms step_avg:138.28ms
step:297/1390 train_time:39688ms step_avg:138.28ms
step:298/1390 train_time:39829ms step_avg:138.30ms
step:299/1390 train_time:39971ms step_avg:138.31ms
step:300/1390 train_time:40112ms step_avg:138.32ms
step:301/1390 train_time:40255ms step_avg:138.33ms
step:302/1390 train_time:40395ms step_avg:138.34ms
step:303/1390 train_time:40536ms step_avg:138.35ms
step:304/1390 train_time:40679ms step_avg:138.36ms
step:305/1390 train_time:40819ms step_avg:138.37ms
step:306/1390 train_time:40960ms step_avg:138.38ms
step:307/1390 train_time:41100ms step_avg:138.38ms
step:308/1390 train_time:41242ms step_avg:138.40ms
step:309/1390 train_time:41386ms step_avg:138.41ms
step:310/1390 train_time:41528ms step_avg:138.43ms
step:311/1390 train_time:41672ms step_avg:138.44ms
step:312/1390 train_time:41815ms step_avg:138.46ms
step:313/1390 train_time:41959ms step_avg:138.48ms
step:314/1390 train_time:42102ms step_avg:138.49ms
step:315/1390 train_time:42245ms step_avg:138.51ms
step:316/1390 train_time:42389ms step_avg:138.53ms
step:317/1390 train_time:42533ms step_avg:138.54ms
step:318/1390 train_time:42677ms step_avg:138.56ms
step:319/1390 train_time:42820ms step_avg:138.58ms
step:320/1390 train_time:42962ms step_avg:138.59ms
step:321/1390 train_time:43104ms step_avg:138.60ms
step:322/1390 train_time:43248ms step_avg:138.61ms
step:323/1390 train_time:43391ms step_avg:138.63ms
step:324/1390 train_time:43536ms step_avg:138.65ms
step:325/1390 train_time:43679ms step_avg:138.66ms
step:326/1390 train_time:43823ms step_avg:138.68ms
step:327/1390 train_time:43965ms step_avg:138.69ms
step:328/1390 train_time:44109ms step_avg:138.71ms
step:329/1390 train_time:44254ms step_avg:138.73ms
step:330/1390 train_time:44396ms step_avg:138.74ms
step:331/1390 train_time:44539ms step_avg:138.75ms
step:332/1390 train_time:44682ms step_avg:138.76ms
step:333/1390 train_time:44825ms step_avg:138.78ms
step:334/1390 train_time:44968ms step_avg:138.79ms
step:335/1390 train_time:45112ms step_avg:138.80ms
step:336/1390 train_time:45254ms step_avg:138.82ms
step:337/1390 train_time:45400ms step_avg:138.84ms
step:338/1390 train_time:45544ms step_avg:138.85ms
step:339/1390 train_time:45688ms step_avg:138.87ms
step:340/1390 train_time:45831ms step_avg:138.88ms
step:341/1390 train_time:45976ms step_avg:138.90ms
step:342/1390 train_time:46120ms step_avg:138.92ms
step:343/1390 train_time:46262ms step_avg:138.93ms
step:344/1390 train_time:46405ms step_avg:138.94ms
step:345/1390 train_time:46549ms step_avg:138.95ms
step:346/1390 train_time:46693ms step_avg:138.97ms
step:347/1390 train_time:46836ms step_avg:138.98ms
step:348/1390 train_time:46980ms step_avg:138.99ms
step:349/1390 train_time:47123ms step_avg:139.00ms
step:350/1390 train_time:47265ms step_avg:139.02ms
step:351/1390 train_time:47409ms step_avg:139.03ms
step:352/1390 train_time:47553ms step_avg:139.04ms
step:353/1390 train_time:47697ms step_avg:139.06ms
step:354/1390 train_time:47840ms step_avg:139.07ms
step:355/1390 train_time:47984ms step_avg:139.08ms
step:356/1390 train_time:48128ms step_avg:139.10ms
step:357/1390 train_time:48270ms step_avg:139.11ms
step:358/1390 train_time:48415ms step_avg:139.12ms
step:359/1390 train_time:48560ms step_avg:139.14ms
step:360/1390 train_time:48703ms step_avg:139.15ms
step:361/1390 train_time:48847ms step_avg:139.17ms
step:362/1390 train_time:48990ms step_avg:139.18ms
step:363/1390 train_time:49135ms step_avg:139.19ms
step:364/1390 train_time:49279ms step_avg:139.21ms
step:365/1390 train_time:49422ms step_avg:139.22ms
step:366/1390 train_time:49565ms step_avg:139.23ms
step:367/1390 train_time:49709ms step_avg:139.24ms
step:368/1390 train_time:49853ms step_avg:139.25ms
step:369/1390 train_time:49998ms step_avg:139.27ms
step:370/1390 train_time:50143ms step_avg:139.28ms
step:371/1390 train_time:50286ms step_avg:139.30ms
step:372/1390 train_time:50429ms step_avg:139.31ms
step:373/1390 train_time:50573ms step_avg:139.32ms
step:374/1390 train_time:50717ms step_avg:139.33ms
step:375/1390 train_time:50861ms step_avg:139.35ms
step:375/1390 val_loss:3.7695 train_time:50932ms step_avg:139.54ms
step:376/1390 train_time:51010ms step_avg:139.37ms
step:377/1390 train_time:51155ms step_avg:139.39ms
step:378/1390 train_time:51298ms step_avg:139.40ms
step:379/1390 train_time:51439ms step_avg:139.40ms
step:380/1390 train_time:51581ms step_avg:139.41ms
step:381/1390 train_time:51768ms step_avg:139.54ms
step:382/1390 train_time:51911ms step_avg:139.55ms
step:383/1390 train_time:52055ms step_avg:139.56ms
step:384/1390 train_time:52197ms step_avg:139.56ms
step:385/1390 train_time:52339ms step_avg:139.57ms
step:386/1390 train_time:52483ms step_avg:139.58ms
step:387/1390 train_time:52626ms step_avg:139.59ms
step:388/1390 train_time:52773ms step_avg:139.61ms
step:389/1390 train_time:52917ms step_avg:139.62ms
step:390/1390 train_time:53060ms step_avg:139.63ms
step:391/1390 train_time:53203ms step_avg:139.64ms
step:392/1390 train_time:53346ms step_avg:139.65ms
step:393/1390 train_time:53489ms step_avg:139.66ms
step:394/1390 train_time:53632ms step_avg:139.67ms
step:395/1390 train_time:53777ms step_avg:139.68ms
step:396/1390 train_time:53921ms step_avg:139.69ms
step:397/1390 train_time:54063ms step_avg:139.70ms
step:398/1390 train_time:54207ms step_avg:139.71ms
step:399/1390 train_time:54351ms step_avg:139.72ms
step:400/1390 train_time:54494ms step_avg:139.73ms
step:401/1390 train_time:54638ms step_avg:139.74ms
step:402/1390 train_time:54783ms step_avg:139.75ms
step:403/1390 train_time:54928ms step_avg:139.77ms
step:404/1390 train_time:55072ms step_avg:139.78ms
step:405/1390 train_time:55216ms step_avg:139.79ms
step:406/1390 train_time:55359ms step_avg:139.79ms
step:407/1390 train_time:55502ms step_avg:139.80ms
step:408/1390 train_time:55645ms step_avg:139.81ms
step:409/1390 train_time:55789ms step_avg:139.82ms
step:410/1390 train_time:55934ms step_avg:139.83ms
step:411/1390 train_time:56078ms step_avg:139.84ms
step:412/1390 train_time:56221ms step_avg:139.85ms
step:413/1390 train_time:56365ms step_avg:139.86ms
step:414/1390 train_time:56510ms step_avg:139.88ms
step:415/1390 train_time:56656ms step_avg:139.89ms
step:416/1390 train_time:56801ms step_avg:139.90ms
step:417/1390 train_time:56947ms step_avg:139.92ms
step:418/1390 train_time:57092ms step_avg:139.93ms
step:419/1390 train_time:57238ms step_avg:139.95ms
step:420/1390 train_time:57384ms step_avg:139.96ms
step:421/1390 train_time:57529ms step_avg:139.97ms
step:422/1390 train_time:57675ms step_avg:139.99ms
step:423/1390 train_time:57821ms step_avg:140.00ms
step:424/1390 train_time:57966ms step_avg:140.01ms
step:425/1390 train_time:58111ms step_avg:140.03ms
step:426/1390 train_time:58259ms step_avg:140.05ms
step:427/1390 train_time:58403ms step_avg:140.06ms
step:428/1390 train_time:58550ms step_avg:140.07ms
step:429/1390 train_time:58694ms step_avg:140.08ms
step:430/1390 train_time:58842ms step_avg:140.10ms
step:431/1390 train_time:58988ms step_avg:140.11ms
step:432/1390 train_time:59134ms step_avg:140.13ms
step:433/1390 train_time:59279ms step_avg:140.14ms
step:434/1390 train_time:59423ms step_avg:140.15ms
step:435/1390 train_time:59569ms step_avg:140.16ms
step:436/1390 train_time:59715ms step_avg:140.18ms
step:437/1390 train_time:59861ms step_avg:140.19ms
step:438/1390 train_time:60006ms step_avg:140.20ms
step:439/1390 train_time:60151ms step_avg:140.21ms
step:440/1390 train_time:60297ms step_avg:140.22ms
step:441/1390 train_time:60441ms step_avg:140.23ms
step:442/1390 train_time:60587ms step_avg:140.25ms
step:443/1390 train_time:60733ms step_avg:140.26ms
step:444/1390 train_time:60880ms step_avg:140.28ms
step:445/1390 train_time:61024ms step_avg:140.29ms
step:446/1390 train_time:61170ms step_avg:140.30ms
step:447/1390 train_time:61314ms step_avg:140.31ms
step:448/1390 train_time:61461ms step_avg:140.32ms
step:449/1390 train_time:61608ms step_avg:140.34ms
step:450/1390 train_time:61753ms step_avg:140.35ms
step:451/1390 train_time:61899ms step_avg:140.36ms
step:452/1390 train_time:62044ms step_avg:140.37ms
step:453/1390 train_time:62189ms step_avg:140.38ms
step:454/1390 train_time:62335ms step_avg:140.39ms
step:455/1390 train_time:62482ms step_avg:140.41ms
step:456/1390 train_time:62627ms step_avg:140.42ms
step:457/1390 train_time:62772ms step_avg:140.43ms
step:458/1390 train_time:62918ms step_avg:140.44ms
step:459/1390 train_time:63063ms step_avg:140.45ms
step:460/1390 train_time:63210ms step_avg:140.47ms
step:461/1390 train_time:63356ms step_avg:140.48ms
step:462/1390 train_time:63501ms step_avg:140.49ms
step:463/1390 train_time:63646ms step_avg:140.50ms
step:464/1390 train_time:63791ms step_avg:140.51ms
step:465/1390 train_time:63936ms step_avg:140.52ms
step:466/1390 train_time:64082ms step_avg:140.53ms
step:467/1390 train_time:64227ms step_avg:140.54ms
step:468/1390 train_time:64373ms step_avg:140.55ms
step:469/1390 train_time:64521ms step_avg:140.57ms
step:470/1390 train_time:64666ms step_avg:140.58ms
step:471/1390 train_time:64812ms step_avg:140.59ms
step:472/1390 train_time:64958ms step_avg:140.60ms
step:473/1390 train_time:65102ms step_avg:140.61ms
step:474/1390 train_time:65248ms step_avg:140.62ms
step:475/1390 train_time:65394ms step_avg:140.63ms
step:476/1390 train_time:65541ms step_avg:140.65ms
step:477/1390 train_time:65688ms step_avg:140.66ms
step:478/1390 train_time:65833ms step_avg:140.67ms
step:479/1390 train_time:65977ms step_avg:140.68ms
step:480/1390 train_time:66123ms step_avg:140.69ms
step:481/1390 train_time:66267ms step_avg:140.69ms
step:482/1390 train_time:66413ms step_avg:140.70ms
step:483/1390 train_time:66559ms step_avg:140.72ms
step:484/1390 train_time:66705ms step_avg:140.73ms
step:485/1390 train_time:66850ms step_avg:140.74ms
step:486/1390 train_time:66996ms step_avg:140.75ms
step:487/1390 train_time:67140ms step_avg:140.76ms
step:488/1390 train_time:67287ms step_avg:140.77ms
step:489/1390 train_time:67432ms step_avg:140.78ms
step:490/1390 train_time:67578ms step_avg:140.79ms
step:491/1390 train_time:67726ms step_avg:140.80ms
step:492/1390 train_time:67871ms step_avg:140.81ms
step:493/1390 train_time:68017ms step_avg:140.82ms
step:494/1390 train_time:68163ms step_avg:140.83ms
step:495/1390 train_time:68309ms step_avg:140.84ms
step:496/1390 train_time:68454ms step_avg:140.85ms
step:497/1390 train_time:68599ms step_avg:140.86ms
step:498/1390 train_time:68746ms step_avg:140.87ms
step:499/1390 train_time:68892ms step_avg:140.88ms
step:500/1390 train_time:69038ms step_avg:140.89ms
step:500/1390 val_loss:3.6541 train_time:69108ms step_avg:141.04ms
step:501/1390 train_time:69185ms step_avg:140.91ms
step:502/1390 train_time:69334ms step_avg:140.92ms
step:503/1390 train_time:69478ms step_avg:140.93ms
step:504/1390 train_time:69623ms step_avg:140.94ms
step:505/1390 train_time:69769ms step_avg:140.95ms
step:506/1390 train_time:69914ms step_avg:140.95ms
step:507/1390 train_time:70059ms step_avg:140.96ms
step:508/1390 train_time:70207ms step_avg:140.98ms
step:509/1390 train_time:70352ms step_avg:140.99ms
step:510/1390 train_time:70498ms step_avg:141.00ms
step:511/1390 train_time:70644ms step_avg:141.01ms
step:512/1390 train_time:70789ms step_avg:141.01ms
step:513/1390 train_time:70935ms step_avg:141.02ms
step:514/1390 train_time:71080ms step_avg:141.03ms
step:515/1390 train_time:71225ms step_avg:141.04ms
step:516/1390 train_time:71372ms step_avg:141.05ms
step:517/1390 train_time:71519ms step_avg:141.06ms
step:518/1390 train_time:71666ms step_avg:141.08ms
step:519/1390 train_time:71812ms step_avg:141.09ms
step:520/1390 train_time:71961ms step_avg:141.10ms
step:521/1390 train_time:72107ms step_avg:141.11ms
step:522/1390 train_time:72254ms step_avg:141.12ms
step:523/1390 train_time:72402ms step_avg:141.13ms
step:524/1390 train_time:72549ms step_avg:141.15ms
step:525/1390 train_time:72696ms step_avg:141.16ms
step:526/1390 train_time:72845ms step_avg:141.17ms
step:527/1390 train_time:72991ms step_avg:141.18ms
step:528/1390 train_time:73137ms step_avg:141.19ms
step:529/1390 train_time:73284ms step_avg:141.20ms
step:530/1390 train_time:73430ms step_avg:141.21ms
step:531/1390 train_time:73577ms step_avg:141.22ms
step:532/1390 train_time:73724ms step_avg:141.23ms
step:533/1390 train_time:73873ms step_avg:141.25ms
step:534/1390 train_time:74020ms step_avg:141.26ms
step:535/1390 train_time:74167ms step_avg:141.27ms
step:536/1390 train_time:74315ms step_avg:141.28ms
step:537/1390 train_time:74463ms step_avg:141.30ms
step:538/1390 train_time:74609ms step_avg:141.31ms
step:539/1390 train_time:74758ms step_avg:141.32ms
step:540/1390 train_time:74905ms step_avg:141.33ms
step:541/1390 train_time:75051ms step_avg:141.34ms
step:542/1390 train_time:75198ms step_avg:141.35ms
step:543/1390 train_time:75346ms step_avg:141.36ms
step:544/1390 train_time:75493ms step_avg:141.37ms
step:545/1390 train_time:75642ms step_avg:141.39ms
step:546/1390 train_time:75789ms step_avg:141.40ms
step:547/1390 train_time:75935ms step_avg:141.41ms
step:548/1390 train_time:76083ms step_avg:141.42ms
step:549/1390 train_time:76230ms step_avg:141.43ms
step:550/1390 train_time:76379ms step_avg:141.44ms
step:551/1390 train_time:76526ms step_avg:141.45ms
step:552/1390 train_time:76675ms step_avg:141.47ms
step:553/1390 train_time:76822ms step_avg:141.48ms
step:554/1390 train_time:76968ms step_avg:141.49ms
step:555/1390 train_time:77115ms step_avg:141.50ms
step:556/1390 train_time:77264ms step_avg:141.51ms
step:557/1390 train_time:77410ms step_avg:141.52ms
step:558/1390 train_time:77557ms step_avg:141.53ms
step:559/1390 train_time:77704ms step_avg:141.54ms
step:560/1390 train_time:77850ms step_avg:141.55ms
step:561/1390 train_time:77997ms step_avg:141.56ms
step:562/1390 train_time:78144ms step_avg:141.57ms
step:563/1390 train_time:78290ms step_avg:141.57ms
step:564/1390 train_time:78438ms step_avg:141.58ms
step:565/1390 train_time:78585ms step_avg:141.59ms
step:566/1390 train_time:78733ms step_avg:141.61ms
step:567/1390 train_time:78879ms step_avg:141.61ms
step:568/1390 train_time:79026ms step_avg:141.62ms
step:569/1390 train_time:79173ms step_avg:141.63ms
step:570/1390 train_time:79320ms step_avg:141.64ms
step:571/1390 train_time:79519ms step_avg:141.75ms
step:572/1390 train_time:79666ms step_avg:141.75ms
step:573/1390 train_time:79813ms step_avg:141.76ms
step:574/1390 train_time:79962ms step_avg:141.78ms
step:575/1390 train_time:80108ms step_avg:141.78ms
step:576/1390 train_time:80253ms step_avg:141.79ms
step:577/1390 train_time:80401ms step_avg:141.80ms
step:578/1390 train_time:80548ms step_avg:141.81ms
step:579/1390 train_time:80697ms step_avg:141.82ms
step:580/1390 train_time:80845ms step_avg:141.83ms
step:581/1390 train_time:80993ms step_avg:141.84ms
step:582/1390 train_time:81141ms step_avg:141.85ms
step:583/1390 train_time:81287ms step_avg:141.86ms
step:584/1390 train_time:81436ms step_avg:141.87ms
step:585/1390 train_time:81583ms step_avg:141.88ms
step:586/1390 train_time:81730ms step_avg:141.89ms
step:587/1390 train_time:81879ms step_avg:141.90ms
step:588/1390 train_time:82026ms step_avg:141.91ms
step:589/1390 train_time:82172ms step_avg:141.92ms
step:590/1390 train_time:82320ms step_avg:141.93ms
step:591/1390 train_time:82468ms step_avg:141.94ms
step:592/1390 train_time:82616ms step_avg:141.95ms
step:593/1390 train_time:82765ms step_avg:141.96ms
step:594/1390 train_time:82914ms step_avg:141.98ms
step:595/1390 train_time:83064ms step_avg:141.99ms
step:596/1390 train_time:83210ms step_avg:142.00ms
step:597/1390 train_time:83357ms step_avg:142.00ms
step:598/1390 train_time:83505ms step_avg:142.01ms
step:599/1390 train_time:83650ms step_avg:142.02ms
step:600/1390 train_time:83798ms step_avg:142.03ms
step:601/1390 train_time:83946ms step_avg:142.04ms
step:602/1390 train_time:84094ms step_avg:142.05ms
step:603/1390 train_time:84243ms step_avg:142.06ms
step:604/1390 train_time:84389ms step_avg:142.07ms
step:605/1390 train_time:84538ms step_avg:142.08ms
step:606/1390 train_time:84685ms step_avg:142.09ms
step:607/1390 train_time:84832ms step_avg:142.10ms
step:608/1390 train_time:84980ms step_avg:142.11ms
step:609/1390 train_time:85127ms step_avg:142.12ms
step:610/1390 train_time:85274ms step_avg:142.12ms
step:611/1390 train_time:85422ms step_avg:142.13ms
step:612/1390 train_time:85569ms step_avg:142.14ms
step:613/1390 train_time:85718ms step_avg:142.15ms
step:614/1390 train_time:85866ms step_avg:142.16ms
step:615/1390 train_time:86013ms step_avg:142.17ms
step:616/1390 train_time:86160ms step_avg:142.18ms
step:617/1390 train_time:86307ms step_avg:142.19ms
step:618/1390 train_time:86455ms step_avg:142.20ms
step:619/1390 train_time:86603ms step_avg:142.21ms
step:620/1390 train_time:86751ms step_avg:142.22ms
step:621/1390 train_time:86900ms step_avg:142.23ms
step:622/1390 train_time:87049ms step_avg:142.24ms
step:623/1390 train_time:87198ms step_avg:142.25ms
step:624/1390 train_time:87348ms step_avg:142.26ms
step:625/1390 train_time:87496ms step_avg:142.27ms
step:625/1390 val_loss:3.5729 train_time:87570ms step_avg:142.39ms
step:626/1390 train_time:87648ms step_avg:142.29ms
step:627/1390 train_time:87797ms step_avg:142.30ms
step:628/1390 train_time:87944ms step_avg:142.30ms
step:629/1390 train_time:88093ms step_avg:142.31ms
step:630/1390 train_time:88240ms step_avg:142.32ms
step:631/1390 train_time:88388ms step_avg:142.33ms
step:632/1390 train_time:88535ms step_avg:142.34ms
step:633/1390 train_time:88686ms step_avg:142.35ms
step:634/1390 train_time:88835ms step_avg:142.36ms
step:635/1390 train_time:88986ms step_avg:142.38ms
step:636/1390 train_time:89135ms step_avg:142.39ms
step:637/1390 train_time:89284ms step_avg:142.40ms
step:638/1390 train_time:89432ms step_avg:142.41ms
step:639/1390 train_time:89578ms step_avg:142.41ms
step:640/1390 train_time:89727ms step_avg:142.42ms
step:641/1390 train_time:89875ms step_avg:142.43ms
step:642/1390 train_time:90025ms step_avg:142.44ms
step:643/1390 train_time:90174ms step_avg:142.46ms
step:644/1390 train_time:90323ms step_avg:142.47ms
step:645/1390 train_time:90473ms step_avg:142.48ms
step:646/1390 train_time:90620ms step_avg:142.49ms
step:647/1390 train_time:90769ms step_avg:142.49ms
step:648/1390 train_time:90919ms step_avg:142.51ms
step:649/1390 train_time:91069ms step_avg:142.52ms
step:650/1390 train_time:91218ms step_avg:142.53ms
step:651/1390 train_time:91368ms step_avg:142.54ms
step:652/1390 train_time:91516ms step_avg:142.55ms
step:653/1390 train_time:91667ms step_avg:142.56ms
step:654/1390 train_time:91816ms step_avg:142.57ms
step:655/1390 train_time:91966ms step_avg:142.58ms
step:656/1390 train_time:92114ms step_avg:142.59ms
step:657/1390 train_time:92262ms step_avg:142.60ms
step:658/1390 train_time:92411ms step_avg:142.61ms
step:659/1390 train_time:92559ms step_avg:142.62ms
step:660/1390 train_time:92709ms step_avg:142.63ms
step:661/1390 train_time:92857ms step_avg:142.64ms
step:662/1390 train_time:93006ms step_avg:142.65ms
step:663/1390 train_time:93154ms step_avg:142.66ms
step:664/1390 train_time:93305ms step_avg:142.67ms
step:665/1390 train_time:93453ms step_avg:142.68ms
step:666/1390 train_time:93601ms step_avg:142.68ms
step:667/1390 train_time:93749ms step_avg:142.69ms
step:668/1390 train_time:93897ms step_avg:142.70ms
step:669/1390 train_time:94046ms step_avg:142.71ms
step:670/1390 train_time:94194ms step_avg:142.72ms
step:671/1390 train_time:94344ms step_avg:142.73ms
step:672/1390 train_time:94493ms step_avg:142.74ms
step:673/1390 train_time:94642ms step_avg:142.75ms
step:674/1390 train_time:94791ms step_avg:142.76ms
step:675/1390 train_time:94941ms step_avg:142.77ms
step:676/1390 train_time:95091ms step_avg:142.78ms
step:677/1390 train_time:95238ms step_avg:142.79ms
step:678/1390 train_time:95388ms step_avg:142.80ms
step:679/1390 train_time:95537ms step_avg:142.81ms
step:680/1390 train_time:95688ms step_avg:142.82ms
step:681/1390 train_time:95838ms step_avg:142.83ms
step:682/1390 train_time:95987ms step_avg:142.84ms
step:683/1390 train_time:96135ms step_avg:142.84ms
step:684/1390 train_time:96285ms step_avg:142.86ms
step:685/1390 train_time:96433ms step_avg:142.86ms
step:686/1390 train_time:96582ms step_avg:142.87ms
step:687/1390 train_time:96731ms step_avg:142.88ms
step:688/1390 train_time:96880ms step_avg:142.89ms
step:689/1390 train_time:97029ms step_avg:142.90ms
step:690/1390 train_time:97179ms step_avg:142.91ms
step:691/1390 train_time:97327ms step_avg:142.92ms
step:692/1390 train_time:97475ms step_avg:142.92ms
step:693/1390 train_time:97623ms step_avg:142.93ms
step:694/1390 train_time:97772ms step_avg:142.94ms
step:695/1390 train_time:97919ms step_avg:142.95ms
step:696/1390 train_time:98068ms step_avg:142.96ms
step:697/1390 train_time:98217ms step_avg:142.96ms
step:698/1390 train_time:98366ms step_avg:142.97ms
step:699/1390 train_time:98515ms step_avg:142.98ms
step:700/1390 train_time:98665ms step_avg:142.99ms
step:701/1390 train_time:98813ms step_avg:143.00ms
step:702/1390 train_time:98962ms step_avg:143.01ms
step:703/1390 train_time:99112ms step_avg:143.02ms
step:704/1390 train_time:99260ms step_avg:143.03ms
step:705/1390 train_time:99411ms step_avg:143.04ms
step:706/1390 train_time:99560ms step_avg:143.05ms
step:707/1390 train_time:99711ms step_avg:143.06ms
step:708/1390 train_time:99861ms step_avg:143.07ms
step:709/1390 train_time:100012ms step_avg:143.08ms
step:710/1390 train_time:100162ms step_avg:143.09ms
step:711/1390 train_time:100312ms step_avg:143.10ms
step:712/1390 train_time:100462ms step_avg:143.11ms
step:713/1390 train_time:100613ms step_avg:143.12ms
step:714/1390 train_time:100760ms step_avg:143.13ms
step:715/1390 train_time:100910ms step_avg:143.13ms
step:716/1390 train_time:101060ms step_avg:143.14ms
step:717/1390 train_time:101210ms step_avg:143.15ms
step:718/1390 train_time:101357ms step_avg:143.16ms
step:719/1390 train_time:101506ms step_avg:143.17ms
step:720/1390 train_time:101656ms step_avg:143.18ms
step:721/1390 train_time:101805ms step_avg:143.19ms
step:722/1390 train_time:101953ms step_avg:143.19ms
step:723/1390 train_time:102103ms step_avg:143.20ms
step:724/1390 train_time:102252ms step_avg:143.21ms
step:725/1390 train_time:102402ms step_avg:143.22ms
step:726/1390 train_time:102554ms step_avg:143.23ms
step:727/1390 train_time:102706ms step_avg:143.24ms
step:728/1390 train_time:102855ms step_avg:143.25ms
step:729/1390 train_time:103007ms step_avg:143.26ms
step:730/1390 train_time:103157ms step_avg:143.27ms
step:731/1390 train_time:103309ms step_avg:143.29ms
step:732/1390 train_time:103459ms step_avg:143.30ms
step:733/1390 train_time:103610ms step_avg:143.31ms
step:734/1390 train_time:103757ms step_avg:143.31ms
step:735/1390 train_time:103909ms step_avg:143.32ms
step:736/1390 train_time:104058ms step_avg:143.33ms
step:737/1390 train_time:104210ms step_avg:143.34ms
step:738/1390 train_time:104358ms step_avg:143.35ms
step:739/1390 train_time:104509ms step_avg:143.36ms
step:740/1390 train_time:104658ms step_avg:143.37ms
step:741/1390 train_time:104810ms step_avg:143.38ms
step:742/1390 train_time:104959ms step_avg:143.39ms
step:743/1390 train_time:105110ms step_avg:143.40ms
step:744/1390 train_time:105259ms step_avg:143.41ms
step:745/1390 train_time:105413ms step_avg:143.42ms
step:746/1390 train_time:105563ms step_avg:143.43ms
step:747/1390 train_time:105713ms step_avg:143.44ms
step:748/1390 train_time:105864ms step_avg:143.45ms
step:749/1390 train_time:106018ms step_avg:143.46ms
step:750/1390 train_time:106170ms step_avg:143.47ms
step:750/1390 val_loss:3.5198 train_time:106246ms step_avg:143.58ms
step:751/1390 train_time:106325ms step_avg:143.49ms
step:752/1390 train_time:106474ms step_avg:143.50ms
step:753/1390 train_time:106625ms step_avg:143.51ms
step:754/1390 train_time:106773ms step_avg:143.51ms
step:755/1390 train_time:106922ms step_avg:143.52ms
step:756/1390 train_time:107072ms step_avg:143.53ms
step:757/1390 train_time:107223ms step_avg:143.54ms
step:758/1390 train_time:107376ms step_avg:143.55ms
step:759/1390 train_time:107528ms step_avg:143.56ms
step:760/1390 train_time:107675ms step_avg:143.57ms
step:761/1390 train_time:107869ms step_avg:143.63ms
step:762/1390 train_time:108017ms step_avg:143.64ms
step:763/1390 train_time:108168ms step_avg:143.65ms
step:764/1390 train_time:108317ms step_avg:143.66ms
step:765/1390 train_time:108467ms step_avg:143.66ms
step:766/1390 train_time:108617ms step_avg:143.67ms
step:767/1390 train_time:108770ms step_avg:143.69ms
step:768/1390 train_time:108920ms step_avg:143.69ms
step:769/1390 train_time:109070ms step_avg:143.70ms
step:770/1390 train_time:109219ms step_avg:143.71ms
step:771/1390 train_time:109370ms step_avg:143.72ms
step:772/1390 train_time:109518ms step_avg:143.72ms
step:773/1390 train_time:109669ms step_avg:143.73ms
step:774/1390 train_time:109819ms step_avg:143.74ms
step:775/1390 train_time:109970ms step_avg:143.75ms
step:776/1390 train_time:110120ms step_avg:143.76ms
step:777/1390 train_time:110271ms step_avg:143.77ms
step:778/1390 train_time:110419ms step_avg:143.78ms
step:779/1390 train_time:110569ms step_avg:143.78ms
step:780/1390 train_time:110720ms step_avg:143.79ms
step:781/1390 train_time:110871ms step_avg:143.80ms
step:782/1390 train_time:111022ms step_avg:143.81ms
step:783/1390 train_time:111172ms step_avg:143.82ms
step:784/1390 train_time:111321ms step_avg:143.83ms
step:785/1390 train_time:111471ms step_avg:143.83ms
step:786/1390 train_time:111622ms step_avg:143.84ms
step:787/1390 train_time:111773ms step_avg:143.85ms
step:788/1390 train_time:111921ms step_avg:143.86ms
step:789/1390 train_time:112072ms step_avg:143.87ms
step:790/1390 train_time:112222ms step_avg:143.87ms
step:791/1390 train_time:112373ms step_avg:143.88ms
step:792/1390 train_time:112524ms step_avg:143.89ms
step:793/1390 train_time:112672ms step_avg:143.90ms
step:794/1390 train_time:112826ms step_avg:143.91ms
step:795/1390 train_time:112978ms step_avg:143.92ms
step:796/1390 train_time:113130ms step_avg:143.93ms
step:797/1390 train_time:113280ms step_avg:143.94ms
step:798/1390 train_time:113432ms step_avg:143.95ms
step:799/1390 train_time:113583ms step_avg:143.96ms
step:800/1390 train_time:113732ms step_avg:143.97ms
step:801/1390 train_time:113883ms step_avg:143.97ms
step:802/1390 train_time:114034ms step_avg:143.98ms
step:803/1390 train_time:114185ms step_avg:143.99ms
step:804/1390 train_time:114334ms step_avg:144.00ms
step:805/1390 train_time:114487ms step_avg:144.01ms
step:806/1390 train_time:114636ms step_avg:144.02ms
step:807/1390 train_time:114786ms step_avg:144.02ms
step:808/1390 train_time:114935ms step_avg:144.03ms
step:809/1390 train_time:115087ms step_avg:144.04ms
step:810/1390 train_time:115236ms step_avg:144.04ms
step:811/1390 train_time:115387ms step_avg:144.05ms
step:812/1390 train_time:115536ms step_avg:144.06ms
step:813/1390 train_time:115686ms step_avg:144.07ms
step:814/1390 train_time:115834ms step_avg:144.07ms
step:815/1390 train_time:115985ms step_avg:144.08ms
step:816/1390 train_time:116136ms step_avg:144.09ms
step:817/1390 train_time:116287ms step_avg:144.10ms
step:818/1390 train_time:116436ms step_avg:144.10ms
step:819/1390 train_time:116586ms step_avg:144.11ms
step:820/1390 train_time:116736ms step_avg:144.12ms
step:821/1390 train_time:116886ms step_avg:144.13ms
step:822/1390 train_time:117034ms step_avg:144.13ms
step:823/1390 train_time:117187ms step_avg:144.14ms
step:824/1390 train_time:117334ms step_avg:144.14ms
step:825/1390 train_time:117485ms step_avg:144.15ms
step:826/1390 train_time:117638ms step_avg:144.16ms
step:827/1390 train_time:117791ms step_avg:144.17ms
step:828/1390 train_time:117941ms step_avg:144.18ms
step:829/1390 train_time:118094ms step_avg:144.19ms
step:830/1390 train_time:118245ms step_avg:144.20ms
step:831/1390 train_time:118395ms step_avg:144.21ms
step:832/1390 train_time:118547ms step_avg:144.22ms
step:833/1390 train_time:118698ms step_avg:144.23ms
step:834/1390 train_time:118852ms step_avg:144.24ms
step:835/1390 train_time:119003ms step_avg:144.25ms
step:836/1390 train_time:119156ms step_avg:144.26ms
step:837/1390 train_time:119309ms step_avg:144.27ms
step:838/1390 train_time:119458ms step_avg:144.27ms
step:839/1390 train_time:119609ms step_avg:144.28ms
step:840/1390 train_time:119761ms step_avg:144.29ms
step:841/1390 train_time:119910ms step_avg:144.30ms
step:842/1390 train_time:120063ms step_avg:144.31ms
step:843/1390 train_time:120215ms step_avg:144.32ms
step:844/1390 train_time:120366ms step_avg:144.32ms
step:845/1390 train_time:120517ms step_avg:144.33ms
step:846/1390 train_time:120669ms step_avg:144.34ms
step:847/1390 train_time:120822ms step_avg:144.35ms
step:848/1390 train_time:120972ms step_avg:144.36ms
step:849/1390 train_time:121124ms step_avg:144.37ms
step:850/1390 train_time:121276ms step_avg:144.38ms
step:851/1390 train_time:121429ms step_avg:144.39ms
step:852/1390 train_time:121580ms step_avg:144.39ms
step:853/1390 train_time:121730ms step_avg:144.40ms
step:854/1390 train_time:121880ms step_avg:144.41ms
step:855/1390 train_time:122031ms step_avg:144.42ms
step:856/1390 train_time:122182ms step_avg:144.42ms
step:857/1390 train_time:122333ms step_avg:144.43ms
step:858/1390 train_time:122488ms step_avg:144.44ms
step:859/1390 train_time:122638ms step_avg:144.45ms
step:860/1390 train_time:122789ms step_avg:144.46ms
step:861/1390 train_time:122941ms step_avg:144.47ms
step:862/1390 train_time:123095ms step_avg:144.48ms
step:863/1390 train_time:123250ms step_avg:144.49ms
step:864/1390 train_time:123401ms step_avg:144.50ms
step:865/1390 train_time:123552ms step_avg:144.51ms
step:866/1390 train_time:123710ms step_avg:144.52ms
step:867/1390 train_time:123860ms step_avg:144.53ms
step:868/1390 train_time:124009ms step_avg:144.53ms
step:869/1390 train_time:124162ms step_avg:144.54ms
step:870/1390 train_time:124315ms step_avg:144.55ms
step:871/1390 train_time:124468ms step_avg:144.56ms
step:872/1390 train_time:124618ms step_avg:144.57ms
step:873/1390 train_time:124770ms step_avg:144.58ms
step:874/1390 train_time:124921ms step_avg:144.58ms
step:875/1390 train_time:125072ms step_avg:144.59ms
step:875/1390 val_loss:3.4708 train_time:125149ms step_avg:144.68ms
step:876/1390 train_time:125227ms step_avg:144.60ms
step:877/1390 train_time:125379ms step_avg:144.61ms
step:878/1390 train_time:125530ms step_avg:144.62ms
step:879/1390 train_time:125681ms step_avg:144.63ms
step:880/1390 train_time:125831ms step_avg:144.63ms
step:881/1390 train_time:125981ms step_avg:144.64ms
step:882/1390 train_time:126135ms step_avg:144.65ms
step:883/1390 train_time:126288ms step_avg:144.66ms
step:884/1390 train_time:126440ms step_avg:144.67ms
step:885/1390 train_time:126590ms step_avg:144.67ms
step:886/1390 train_time:126743ms step_avg:144.68ms
step:887/1390 train_time:126895ms step_avg:144.69ms
step:888/1390 train_time:127047ms step_avg:144.70ms
step:889/1390 train_time:127200ms step_avg:144.71ms
step:890/1390 train_time:127353ms step_avg:144.72ms
step:891/1390 train_time:127503ms step_avg:144.73ms
step:892/1390 train_time:127655ms step_avg:144.73ms
step:893/1390 train_time:127804ms step_avg:144.74ms
step:894/1390 train_time:127956ms step_avg:144.75ms
step:895/1390 train_time:128108ms step_avg:144.76ms
step:896/1390 train_time:128259ms step_avg:144.76ms
step:897/1390 train_time:128410ms step_avg:144.77ms
step:898/1390 train_time:128562ms step_avg:144.78ms
step:899/1390 train_time:128718ms step_avg:144.79ms
step:900/1390 train_time:128871ms step_avg:144.80ms
step:901/1390 train_time:129023ms step_avg:144.81ms
step:902/1390 train_time:129174ms step_avg:144.81ms
step:903/1390 train_time:129328ms step_avg:144.82ms
step:904/1390 train_time:129479ms step_avg:144.83ms
step:905/1390 train_time:129629ms step_avg:144.84ms
step:906/1390 train_time:129781ms step_avg:144.85ms
step:907/1390 train_time:129936ms step_avg:144.86ms
step:908/1390 train_time:130084ms step_avg:144.86ms
step:909/1390 train_time:130237ms step_avg:144.87ms
step:910/1390 train_time:130392ms step_avg:144.88ms
step:911/1390 train_time:130542ms step_avg:144.89ms
step:912/1390 train_time:130693ms step_avg:144.89ms
step:913/1390 train_time:130847ms step_avg:144.90ms
step:914/1390 train_time:130999ms step_avg:144.91ms
step:915/1390 train_time:131152ms step_avg:144.92ms
step:916/1390 train_time:131304ms step_avg:144.93ms
step:917/1390 train_time:131457ms step_avg:144.94ms
step:918/1390 train_time:131610ms step_avg:144.94ms
step:919/1390 train_time:131766ms step_avg:144.96ms
step:920/1390 train_time:131917ms step_avg:144.96ms
step:921/1390 train_time:132067ms step_avg:144.97ms
step:922/1390 train_time:132221ms step_avg:144.98ms
step:923/1390 train_time:132371ms step_avg:144.98ms
step:924/1390 train_time:132522ms step_avg:144.99ms
step:925/1390 train_time:132675ms step_avg:145.00ms
step:926/1390 train_time:132825ms step_avg:145.01ms
step:927/1390 train_time:132977ms step_avg:145.01ms
step:928/1390 train_time:133129ms step_avg:145.02ms
step:929/1390 train_time:133281ms step_avg:145.03ms
step:930/1390 train_time:133434ms step_avg:145.04ms
step:931/1390 train_time:133585ms step_avg:145.04ms
step:932/1390 train_time:133739ms step_avg:145.05ms
step:933/1390 train_time:133892ms step_avg:145.06ms
step:934/1390 train_time:134044ms step_avg:145.07ms
step:935/1390 train_time:134201ms step_avg:145.08ms
step:936/1390 train_time:134355ms step_avg:145.09ms
step:937/1390 train_time:134514ms step_avg:145.11ms
step:938/1390 train_time:134667ms step_avg:145.12ms
step:939/1390 train_time:134821ms step_avg:145.13ms
step:940/1390 train_time:134975ms step_avg:145.13ms
step:941/1390 train_time:135125ms step_avg:145.14ms
step:942/1390 train_time:135278ms step_avg:145.15ms
step:943/1390 train_time:135433ms step_avg:145.16ms
step:944/1390 train_time:135589ms step_avg:145.17ms
step:945/1390 train_time:135741ms step_avg:145.18ms
step:946/1390 train_time:135897ms step_avg:145.19ms
step:947/1390 train_time:136048ms step_avg:145.20ms
step:948/1390 train_time:136201ms step_avg:145.20ms
step:949/1390 train_time:136355ms step_avg:145.21ms
step:950/1390 train_time:136506ms step_avg:145.22ms
step:951/1390 train_time:136712ms step_avg:145.28ms
step:952/1390 train_time:136861ms step_avg:145.29ms
step:953/1390 train_time:137014ms step_avg:145.30ms
step:954/1390 train_time:137164ms step_avg:145.30ms
step:955/1390 train_time:137316ms step_avg:145.31ms
step:956/1390 train_time:137472ms step_avg:145.32ms
step:957/1390 train_time:137625ms step_avg:145.33ms
step:958/1390 train_time:137781ms step_avg:145.34ms
step:959/1390 train_time:137937ms step_avg:145.35ms
step:960/1390 train_time:138092ms step_avg:145.36ms
step:961/1390 train_time:138242ms step_avg:145.37ms
step:962/1390 train_time:138395ms step_avg:145.37ms
step:963/1390 train_time:138553ms step_avg:145.39ms
step:964/1390 train_time:138704ms step_avg:145.39ms
step:965/1390 train_time:138858ms step_avg:145.40ms
step:966/1390 train_time:139010ms step_avg:145.41ms
step:967/1390 train_time:139165ms step_avg:145.42ms
step:968/1390 train_time:139316ms step_avg:145.42ms
step:969/1390 train_time:139469ms step_avg:145.43ms
step:970/1390 train_time:139621ms step_avg:145.44ms
step:971/1390 train_time:139776ms step_avg:145.45ms
step:972/1390 train_time:139928ms step_avg:145.45ms
step:973/1390 train_time:140082ms step_avg:145.46ms
step:974/1390 train_time:140236ms step_avg:145.47ms
step:975/1390 train_time:140388ms step_avg:145.48ms
step:976/1390 train_time:140540ms step_avg:145.49ms
step:977/1390 train_time:140691ms step_avg:145.49ms
step:978/1390 train_time:140844ms step_avg:145.50ms
step:979/1390 train_time:140997ms step_avg:145.51ms
step:980/1390 train_time:141147ms step_avg:145.51ms
step:981/1390 train_time:141299ms step_avg:145.52ms
step:982/1390 train_time:141453ms step_avg:145.53ms
step:983/1390 train_time:141604ms step_avg:145.53ms
step:984/1390 train_time:141756ms step_avg:145.54ms
step:985/1390 train_time:141909ms step_avg:145.55ms
step:986/1390 train_time:142068ms step_avg:145.56ms
step:987/1390 train_time:142220ms step_avg:145.57ms
step:988/1390 train_time:142374ms step_avg:145.58ms
step:989/1390 train_time:142527ms step_avg:145.58ms
step:990/1390 train_time:142682ms step_avg:145.59ms
step:991/1390 train_time:142833ms step_avg:145.60ms
step:992/1390 train_time:142989ms step_avg:145.61ms
step:993/1390 train_time:143151ms step_avg:145.63ms
step:994/1390 train_time:143301ms step_avg:145.63ms
step:995/1390 train_time:143454ms step_avg:145.64ms
step:996/1390 train_time:143605ms step_avg:145.64ms
step:997/1390 train_time:143755ms step_avg:145.65ms
step:998/1390 train_time:143905ms step_avg:145.65ms
step:999/1390 train_time:144058ms step_avg:145.66ms
step:1000/1390 train_time:144212ms step_avg:145.67ms
step:1000/1390 val_loss:3.4057 train_time:144287ms step_avg:145.74ms
step:1001/1390 train_time:144367ms step_avg:145.68ms
step:1002/1390 train_time:144519ms step_avg:145.68ms
step:1003/1390 train_time:144673ms step_avg:145.69ms
step:1004/1390 train_time:144828ms step_avg:145.70ms
step:1005/1390 train_time:144983ms step_avg:145.71ms
step:1006/1390 train_time:145133ms step_avg:145.72ms
step:1007/1390 train_time:145287ms step_avg:145.72ms
step:1008/1390 train_time:145443ms step_avg:145.73ms
step:1009/1390 train_time:145602ms step_avg:145.75ms
step:1010/1390 train_time:145753ms step_avg:145.75ms
step:1011/1390 train_time:145907ms step_avg:145.76ms
step:1012/1390 train_time:146060ms step_avg:145.77ms
step:1013/1390 train_time:146214ms step_avg:145.78ms
step:1014/1390 train_time:146368ms step_avg:145.78ms
step:1015/1390 train_time:146520ms step_avg:145.79ms
step:1016/1390 train_time:146672ms step_avg:145.80ms
step:1017/1390 train_time:146828ms step_avg:145.81ms
step:1018/1390 train_time:146981ms step_avg:145.81ms
step:1019/1390 train_time:147137ms step_avg:145.82ms
step:1020/1390 train_time:147292ms step_avg:145.83ms
step:1021/1390 train_time:147444ms step_avg:145.84ms
step:1022/1390 train_time:147596ms step_avg:145.85ms
step:1023/1390 train_time:147751ms step_avg:145.85ms
step:1024/1390 train_time:147905ms step_avg:145.86ms
step:1025/1390 train_time:148061ms step_avg:145.87ms
step:1026/1390 train_time:148213ms step_avg:145.88ms
step:1027/1390 train_time:148365ms step_avg:145.88ms
step:1028/1390 train_time:148519ms step_avg:145.89ms
step:1029/1390 train_time:148675ms step_avg:145.90ms
step:1030/1390 train_time:148828ms step_avg:145.91ms
step:1031/1390 train_time:148979ms step_avg:145.91ms
step:1032/1390 train_time:149131ms step_avg:145.92ms
step:1033/1390 train_time:149285ms step_avg:145.93ms
step:1034/1390 train_time:149441ms step_avg:145.94ms
step:1035/1390 train_time:149598ms step_avg:145.95ms
step:1036/1390 train_time:149753ms step_avg:145.96ms
step:1037/1390 train_time:149909ms step_avg:145.97ms
step:1038/1390 train_time:150065ms step_avg:145.98ms
step:1039/1390 train_time:150217ms step_avg:145.98ms
step:1040/1390 train_time:150370ms step_avg:145.99ms
step:1041/1390 train_time:150526ms step_avg:146.00ms
step:1042/1390 train_time:150681ms step_avg:146.01ms
step:1043/1390 train_time:150836ms step_avg:146.02ms
step:1044/1390 train_time:150994ms step_avg:146.03ms
step:1045/1390 train_time:151149ms step_avg:146.04ms
step:1046/1390 train_time:151305ms step_avg:146.05ms
step:1047/1390 train_time:151458ms step_avg:146.05ms
step:1048/1390 train_time:151611ms step_avg:146.06ms
step:1049/1390 train_time:151764ms step_avg:146.07ms
step:1050/1390 train_time:151918ms step_avg:146.07ms
step:1051/1390 train_time:152074ms step_avg:146.08ms
step:1052/1390 train_time:152227ms step_avg:146.09ms
step:1053/1390 train_time:152379ms step_avg:146.10ms
step:1054/1390 train_time:152533ms step_avg:146.10ms
step:1055/1390 train_time:152685ms step_avg:146.11ms
step:1056/1390 train_time:152836ms step_avg:146.11ms
step:1057/1390 train_time:152988ms step_avg:146.12ms
step:1058/1390 train_time:153146ms step_avg:146.13ms
step:1059/1390 train_time:153303ms step_avg:146.14ms
step:1060/1390 train_time:153457ms step_avg:146.15ms
step:1061/1390 train_time:153609ms step_avg:146.16ms
step:1062/1390 train_time:153765ms step_avg:146.16ms
step:1063/1390 train_time:153918ms step_avg:146.17ms
step:1064/1390 train_time:154070ms step_avg:146.18ms
step:1065/1390 train_time:154227ms step_avg:146.19ms
step:1066/1390 train_time:154382ms step_avg:146.20ms
step:1067/1390 train_time:154538ms step_avg:146.20ms
step:1068/1390 train_time:154690ms step_avg:146.21ms
step:1069/1390 train_time:154846ms step_avg:146.22ms
step:1070/1390 train_time:154998ms step_avg:146.22ms
step:1071/1390 train_time:155156ms step_avg:146.24ms
step:1072/1390 train_time:155309ms step_avg:146.24ms
step:1073/1390 train_time:155462ms step_avg:146.25ms
step:1074/1390 train_time:155613ms step_avg:146.25ms
step:1075/1390 train_time:155770ms step_avg:146.26ms
step:1076/1390 train_time:155925ms step_avg:146.27ms
step:1077/1390 train_time:156079ms step_avg:146.28ms
step:1078/1390 train_time:156235ms step_avg:146.29ms
step:1079/1390 train_time:156392ms step_avg:146.30ms
step:1080/1390 train_time:156547ms step_avg:146.31ms
step:1081/1390 train_time:156701ms step_avg:146.31ms
step:1082/1390 train_time:156853ms step_avg:146.32ms
step:1083/1390 train_time:157007ms step_avg:146.33ms
step:1084/1390 train_time:157165ms step_avg:146.34ms
step:1085/1390 train_time:157318ms step_avg:146.34ms
step:1086/1390 train_time:157473ms step_avg:146.35ms
step:1087/1390 train_time:157628ms step_avg:146.36ms
step:1088/1390 train_time:157783ms step_avg:146.37ms
step:1089/1390 train_time:157939ms step_avg:146.38ms
step:1090/1390 train_time:158097ms step_avg:146.39ms
step:1091/1390 train_time:158251ms step_avg:146.39ms
step:1092/1390 train_time:158405ms step_avg:146.40ms
step:1093/1390 train_time:158558ms step_avg:146.41ms
step:1094/1390 train_time:158712ms step_avg:146.41ms
step:1095/1390 train_time:158865ms step_avg:146.42ms
step:1096/1390 train_time:159020ms step_avg:146.43ms
step:1097/1390 train_time:159176ms step_avg:146.44ms
step:1098/1390 train_time:159331ms step_avg:146.44ms
step:1099/1390 train_time:159486ms step_avg:146.45ms
step:1100/1390 train_time:159638ms step_avg:146.46ms
step:1101/1390 train_time:159791ms step_avg:146.46ms
step:1102/1390 train_time:159946ms step_avg:146.47ms
step:1103/1390 train_time:160102ms step_avg:146.48ms
step:1104/1390 train_time:160255ms step_avg:146.49ms
step:1105/1390 train_time:160412ms step_avg:146.49ms
step:1106/1390 train_time:160567ms step_avg:146.50ms
step:1107/1390 train_time:160720ms step_avg:146.51ms
step:1108/1390 train_time:160877ms step_avg:146.52ms
step:1109/1390 train_time:161031ms step_avg:146.53ms
step:1110/1390 train_time:161187ms step_avg:146.53ms
step:1111/1390 train_time:161341ms step_avg:146.54ms
step:1112/1390 train_time:161493ms step_avg:146.55ms
step:1113/1390 train_time:161647ms step_avg:146.55ms
step:1114/1390 train_time:161805ms step_avg:146.56ms
step:1115/1390 train_time:161963ms step_avg:146.57ms
step:1116/1390 train_time:162116ms step_avg:146.58ms
step:1117/1390 train_time:162271ms step_avg:146.59ms
step:1118/1390 train_time:162430ms step_avg:146.60ms
step:1119/1390 train_time:162584ms step_avg:146.60ms
step:1120/1390 train_time:162738ms step_avg:146.61ms
step:1121/1390 train_time:162890ms step_avg:146.62ms
step:1122/1390 train_time:163043ms step_avg:146.62ms
step:1123/1390 train_time:163195ms step_avg:146.63ms
step:1124/1390 train_time:163351ms step_avg:146.63ms
step:1125/1390 train_time:163503ms step_avg:146.64ms
step:1125/1390 val_loss:3.3532 train_time:163581ms step_avg:146.71ms
step:1126/1390 train_time:163661ms step_avg:146.65ms
step:1127/1390 train_time:163814ms step_avg:146.65ms
step:1128/1390 train_time:163968ms step_avg:146.66ms
step:1129/1390 train_time:164125ms step_avg:146.67ms
step:1130/1390 train_time:164277ms step_avg:146.68ms
step:1131/1390 train_time:164431ms step_avg:146.68ms
step:1132/1390 train_time:164583ms step_avg:146.69ms
step:1133/1390 train_time:164738ms step_avg:146.69ms
step:1134/1390 train_time:164892ms step_avg:146.70ms
step:1135/1390 train_time:165048ms step_avg:146.71ms
step:1136/1390 train_time:165213ms step_avg:146.73ms
step:1137/1390 train_time:165368ms step_avg:146.73ms
step:1138/1390 train_time:165524ms step_avg:146.74ms
step:1139/1390 train_time:165679ms step_avg:146.75ms
step:1140/1390 train_time:165833ms step_avg:146.76ms
step:1141/1390 train_time:166038ms step_avg:146.81ms
step:1142/1390 train_time:166190ms step_avg:146.81ms
step:1143/1390 train_time:166346ms step_avg:146.82ms
step:1144/1390 train_time:166501ms step_avg:146.83ms
step:1145/1390 train_time:166653ms step_avg:146.83ms
step:1146/1390 train_time:166809ms step_avg:146.84ms
step:1147/1390 train_time:166965ms step_avg:146.85ms
step:1148/1390 train_time:167121ms step_avg:146.86ms
step:1149/1390 train_time:167279ms step_avg:146.86ms
step:1150/1390 train_time:167433ms step_avg:146.87ms
step:1151/1390 train_time:167590ms step_avg:146.88ms
step:1152/1390 train_time:167746ms step_avg:146.89ms
step:1153/1390 train_time:167903ms step_avg:146.90ms
step:1154/1390 train_time:168054ms step_avg:146.90ms
step:1155/1390 train_time:168210ms step_avg:146.91ms
step:1156/1390 train_time:168374ms step_avg:146.92ms
step:1157/1390 train_time:168531ms step_avg:146.93ms
step:1158/1390 train_time:168685ms step_avg:146.94ms
step:1159/1390 train_time:168838ms step_avg:146.94ms
step:1160/1390 train_time:168991ms step_avg:146.95ms
step:1161/1390 train_time:169147ms step_avg:146.96ms
step:1162/1390 train_time:169301ms step_avg:146.96ms
step:1163/1390 train_time:169457ms step_avg:146.97ms
step:1164/1390 train_time:169610ms step_avg:146.98ms
step:1165/1390 train_time:169764ms step_avg:146.98ms
step:1166/1390 train_time:169919ms step_avg:146.99ms
step:1167/1390 train_time:170071ms step_avg:146.99ms
step:1168/1390 train_time:170227ms step_avg:147.00ms
step:1169/1390 train_time:170383ms step_avg:147.01ms
step:1170/1390 train_time:170538ms step_avg:147.02ms
step:1171/1390 train_time:170693ms step_avg:147.02ms
step:1172/1390 train_time:170849ms step_avg:147.03ms
step:1173/1390 train_time:171003ms step_avg:147.04ms
step:1174/1390 train_time:171165ms step_avg:147.05ms
step:1175/1390 train_time:171322ms step_avg:147.06ms
step:1176/1390 train_time:171477ms step_avg:147.06ms
step:1177/1390 train_time:171637ms step_avg:147.08ms
step:1178/1390 train_time:171792ms step_avg:147.08ms
step:1179/1390 train_time:171944ms step_avg:147.09ms
step:1180/1390 train_time:172106ms step_avg:147.10ms
step:1181/1390 train_time:172262ms step_avg:147.11ms
step:1182/1390 train_time:172415ms step_avg:147.11ms
step:1183/1390 train_time:172574ms step_avg:147.12ms
step:1184/1390 train_time:172728ms step_avg:147.13ms
step:1185/1390 train_time:172885ms step_avg:147.14ms
step:1186/1390 train_time:173039ms step_avg:147.14ms
step:1187/1390 train_time:173206ms step_avg:147.16ms
step:1188/1390 train_time:173359ms step_avg:147.16ms
step:1189/1390 train_time:173518ms step_avg:147.17ms
step:1190/1390 train_time:173672ms step_avg:147.18ms
step:1191/1390 train_time:173828ms step_avg:147.19ms
step:1192/1390 train_time:173982ms step_avg:147.19ms
step:1193/1390 train_time:174139ms step_avg:147.20ms
step:1194/1390 train_time:174293ms step_avg:147.21ms
step:1195/1390 train_time:174450ms step_avg:147.22ms
step:1196/1390 train_time:174606ms step_avg:147.22ms
step:1197/1390 train_time:174760ms step_avg:147.23ms
step:1198/1390 train_time:174924ms step_avg:147.24ms
step:1199/1390 train_time:175078ms step_avg:147.25ms
step:1200/1390 train_time:175232ms step_avg:147.25ms
step:1201/1390 train_time:175388ms step_avg:147.26ms
step:1202/1390 train_time:175556ms step_avg:147.28ms
step:1203/1390 train_time:175716ms step_avg:147.29ms
step:1204/1390 train_time:175873ms step_avg:147.30ms
step:1205/1390 train_time:176028ms step_avg:147.30ms
step:1206/1390 train_time:176187ms step_avg:147.31ms
step:1207/1390 train_time:176344ms step_avg:147.32ms
step:1208/1390 train_time:176500ms step_avg:147.33ms
step:1209/1390 train_time:176656ms step_avg:147.34ms
step:1210/1390 train_time:176815ms step_avg:147.35ms
step:1211/1390 train_time:176971ms step_avg:147.35ms
step:1212/1390 train_time:177127ms step_avg:147.36ms
step:1213/1390 train_time:177283ms step_avg:147.37ms
step:1214/1390 train_time:177440ms step_avg:147.38ms
step:1215/1390 train_time:177598ms step_avg:147.38ms
step:1216/1390 train_time:177749ms step_avg:147.39ms
step:1217/1390 train_time:177906ms step_avg:147.40ms
step:1218/1390 train_time:178061ms step_avg:147.40ms
step:1219/1390 train_time:178213ms step_avg:147.41ms
step:1220/1390 train_time:178366ms step_avg:147.41ms
step:1221/1390 train_time:178522ms step_avg:147.42ms
step:1222/1390 train_time:178677ms step_avg:147.42ms
step:1223/1390 train_time:178833ms step_avg:147.43ms
step:1224/1390 train_time:178991ms step_avg:147.44ms
step:1225/1390 train_time:179147ms step_avg:147.45ms
step:1226/1390 train_time:179303ms step_avg:147.45ms
step:1227/1390 train_time:179458ms step_avg:147.46ms
step:1228/1390 train_time:179610ms step_avg:147.46ms
step:1229/1390 train_time:179767ms step_avg:147.47ms
step:1230/1390 train_time:179926ms step_avg:147.48ms
step:1231/1390 train_time:180082ms step_avg:147.49ms
step:1232/1390 train_time:180240ms step_avg:147.50ms
step:1233/1390 train_time:180395ms step_avg:147.50ms
step:1234/1390 train_time:180551ms step_avg:147.51ms
step:1235/1390 train_time:180706ms step_avg:147.52ms
step:1236/1390 train_time:180864ms step_avg:147.52ms
step:1237/1390 train_time:181018ms step_avg:147.53ms
step:1238/1390 train_time:181184ms step_avg:147.54ms
step:1239/1390 train_time:181341ms step_avg:147.55ms
step:1240/1390 train_time:181500ms step_avg:147.56ms
step:1241/1390 train_time:181662ms step_avg:147.57ms
step:1242/1390 train_time:181817ms step_avg:147.58ms
step:1243/1390 train_time:181976ms step_avg:147.59ms
step:1244/1390 train_time:182129ms step_avg:147.59ms
step:1245/1390 train_time:182284ms step_avg:147.60ms
step:1246/1390 train_time:182441ms step_avg:147.61ms
step:1247/1390 train_time:182598ms step_avg:147.61ms
step:1248/1390 train_time:182751ms step_avg:147.62ms
step:1249/1390 train_time:182904ms step_avg:147.62ms
step:1250/1390 train_time:183058ms step_avg:147.63ms
step:1250/1390 val_loss:3.3068 train_time:183138ms step_avg:147.69ms
step:1251/1390 train_time:183219ms step_avg:147.64ms
step:1252/1390 train_time:183376ms step_avg:147.65ms
step:1253/1390 train_time:183531ms step_avg:147.65ms
step:1254/1390 train_time:183682ms step_avg:147.65ms
step:1255/1390 train_time:183850ms step_avg:147.67ms
step:1256/1390 train_time:184007ms step_avg:147.68ms
step:1257/1390 train_time:184163ms step_avg:147.68ms
step:1258/1390 train_time:184320ms step_avg:147.69ms
step:1259/1390 train_time:184478ms step_avg:147.70ms
step:1260/1390 train_time:184632ms step_avg:147.71ms
step:1261/1390 train_time:184789ms step_avg:147.71ms
step:1262/1390 train_time:184948ms step_avg:147.72ms
step:1263/1390 train_time:185106ms step_avg:147.73ms
step:1264/1390 train_time:185260ms step_avg:147.74ms
step:1265/1390 train_time:185415ms step_avg:147.74ms
step:1266/1390 train_time:185573ms step_avg:147.75ms
step:1267/1390 train_time:185731ms step_avg:147.76ms
step:1268/1390 train_time:185887ms step_avg:147.76ms
step:1269/1390 train_time:186050ms step_avg:147.78ms
step:1270/1390 train_time:186204ms step_avg:147.78ms
step:1271/1390 train_time:186360ms step_avg:147.79ms
step:1272/1390 train_time:186514ms step_avg:147.79ms
step:1273/1390 train_time:186669ms step_avg:147.80ms
step:1274/1390 train_time:186823ms step_avg:147.80ms
step:1275/1390 train_time:186982ms step_avg:147.81ms
step:1276/1390 train_time:187136ms step_avg:147.82ms
step:1277/1390 train_time:187295ms step_avg:147.83ms
step:1278/1390 train_time:187450ms step_avg:147.83ms
step:1279/1390 train_time:187606ms step_avg:147.84ms
step:1280/1390 train_time:187769ms step_avg:147.85ms
step:1281/1390 train_time:187925ms step_avg:147.86ms
step:1282/1390 train_time:188078ms step_avg:147.86ms
step:1283/1390 train_time:188235ms step_avg:147.87ms
step:1284/1390 train_time:188392ms step_avg:147.87ms
step:1285/1390 train_time:188545ms step_avg:147.88ms
step:1286/1390 train_time:188701ms step_avg:147.88ms
step:1287/1390 train_time:188859ms step_avg:147.89ms
step:1288/1390 train_time:189015ms step_avg:147.90ms
step:1289/1390 train_time:189179ms step_avg:147.91ms
step:1290/1390 train_time:189339ms step_avg:147.92ms
step:1291/1390 train_time:189496ms step_avg:147.93ms
step:1292/1390 train_time:189653ms step_avg:147.94ms
step:1293/1390 train_time:189812ms step_avg:147.94ms
step:1294/1390 train_time:189970ms step_avg:147.95ms
step:1295/1390 train_time:190125ms step_avg:147.96ms
step:1296/1390 train_time:190282ms step_avg:147.96ms
step:1297/1390 train_time:190439ms step_avg:147.97ms
step:1298/1390 train_time:190596ms step_avg:147.98ms
step:1299/1390 train_time:190750ms step_avg:147.98ms
step:1300/1390 train_time:190902ms step_avg:147.99ms
step:1301/1390 train_time:191058ms step_avg:147.99ms
step:1302/1390 train_time:191214ms step_avg:148.00ms
step:1303/1390 train_time:191376ms step_avg:148.01ms
step:1304/1390 train_time:191533ms step_avg:148.02ms
step:1305/1390 train_time:191686ms step_avg:148.02ms
step:1306/1390 train_time:191845ms step_avg:148.03ms
step:1307/1390 train_time:192000ms step_avg:148.03ms
step:1308/1390 train_time:192158ms step_avg:148.04ms
step:1309/1390 train_time:192317ms step_avg:148.05ms
step:1310/1390 train_time:192475ms step_avg:148.06ms
step:1311/1390 train_time:192630ms step_avg:148.06ms
step:1312/1390 train_time:192786ms step_avg:148.07ms
step:1313/1390 train_time:192942ms step_avg:148.08ms
step:1314/1390 train_time:193099ms step_avg:148.08ms
step:1315/1390 train_time:193255ms step_avg:148.09ms
step:1316/1390 train_time:193408ms step_avg:148.09ms
step:1317/1390 train_time:193564ms step_avg:148.10ms
step:1318/1390 train_time:193725ms step_avg:148.11ms
step:1319/1390 train_time:193882ms step_avg:148.11ms
step:1320/1390 train_time:194038ms step_avg:148.12ms
step:1321/1390 train_time:194193ms step_avg:148.13ms
step:1322/1390 train_time:194354ms step_avg:148.14ms
step:1323/1390 train_time:194510ms step_avg:148.14ms
step:1324/1390 train_time:194666ms step_avg:148.15ms
step:1325/1390 train_time:194825ms step_avg:148.16ms
step:1326/1390 train_time:194984ms step_avg:148.16ms
step:1327/1390 train_time:195139ms step_avg:148.17ms
step:1328/1390 train_time:195294ms step_avg:148.17ms
step:1329/1390 train_time:195462ms step_avg:148.19ms
step:1330/1390 train_time:195620ms step_avg:148.20ms
step:1331/1390 train_time:195832ms step_avg:148.25ms
step:1332/1390 train_time:195997ms step_avg:148.26ms
step:1333/1390 train_time:196154ms step_avg:148.26ms
step:1334/1390 train_time:196310ms step_avg:148.27ms
step:1335/1390 train_time:196462ms step_avg:148.27ms
step:1336/1390 train_time:196626ms step_avg:148.29ms
step:1337/1390 train_time:196787ms step_avg:148.29ms
step:1338/1390 train_time:196945ms step_avg:148.30ms
step:1339/1390 train_time:197100ms step_avg:148.31ms
step:1340/1390 train_time:197261ms step_avg:148.32ms
step:1341/1390 train_time:197416ms step_avg:148.32ms
step:1342/1390 train_time:197574ms step_avg:148.33ms
step:1343/1390 train_time:197730ms step_avg:148.33ms
step:1344/1390 train_time:197885ms step_avg:148.34ms
step:1345/1390 train_time:198042ms step_avg:148.35ms
step:1346/1390 train_time:198199ms step_avg:148.35ms
step:1347/1390 train_time:198358ms step_avg:148.36ms
step:1348/1390 train_time:198514ms step_avg:148.37ms
step:1349/1390 train_time:198670ms step_avg:148.37ms
step:1350/1390 train_time:198824ms step_avg:148.38ms
step:1351/1390 train_time:198980ms step_avg:148.38ms
step:1352/1390 train_time:199141ms step_avg:148.39ms
step:1353/1390 train_time:199301ms step_avg:148.40ms
step:1354/1390 train_time:199461ms step_avg:148.41ms
step:1355/1390 train_time:199617ms step_avg:148.41ms
step:1356/1390 train_time:199772ms step_avg:148.42ms
step:1357/1390 train_time:199930ms step_avg:148.43ms
step:1358/1390 train_time:200089ms step_avg:148.43ms
step:1359/1390 train_time:200244ms step_avg:148.44ms
step:1360/1390 train_time:200405ms step_avg:148.45ms
step:1361/1390 train_time:200563ms step_avg:148.46ms
step:1362/1390 train_time:200721ms step_avg:148.46ms
step:1363/1390 train_time:200884ms step_avg:148.47ms
step:1364/1390 train_time:201041ms step_avg:148.48ms
step:1365/1390 train_time:201195ms step_avg:148.48ms
step:1366/1390 train_time:201352ms step_avg:148.49ms
step:1367/1390 train_time:201510ms step_avg:148.50ms
step:1368/1390 train_time:201669ms step_avg:148.50ms
step:1369/1390 train_time:201835ms step_avg:148.52ms
step:1370/1390 train_time:201996ms step_avg:148.53ms
step:1371/1390 train_time:202154ms step_avg:148.53ms
step:1372/1390 train_time:202314ms step_avg:148.54ms
step:1373/1390 train_time:202471ms step_avg:148.55ms
step:1374/1390 train_time:202632ms step_avg:148.56ms
step:1375/1390 train_time:202788ms step_avg:148.56ms
step:1375/1390 val_loss:3.2782 train_time:202864ms step_avg:148.62ms
step:1376/1390 train_time:202943ms step_avg:148.57ms
step:1377/1390 train_time:203100ms step_avg:148.57ms
step:1378/1390 train_time:203255ms step_avg:148.58ms
step:1379/1390 train_time:203411ms step_avg:148.58ms
step:1380/1390 train_time:203567ms step_avg:148.59ms
step:1381/1390 train_time:203727ms step_avg:148.60ms
step:1382/1390 train_time:203885ms step_avg:148.60ms
step:1383/1390 train_time:204041ms step_avg:148.61ms
step:1384/1390 train_time:204204ms step_avg:148.62ms
step:1385/1390 train_time:204358ms step_avg:148.62ms
step:1386/1390 train_time:204516ms step_avg:148.63ms
step:1387/1390 train_time:204674ms step_avg:148.64ms
step:1388/1390 train_time:204829ms step_avg:148.64ms
step:1389/1390 train_time:204988ms step_avg:148.65ms
step:1390/1390 train_time:205145ms step_avg:148.66ms
step:1390/1390 val_loss:3.2774 train_time:205222ms step_avg:148.71ms
peak memory consumption: 31563 MiB
