import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 512  # or 1024, etc., if memory allows
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1390 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.7 (main, Jan  9 2025, 22:54:50) [GCC 13.2.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Fri Jan 10 02:42:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   26C    P0            145W /  700W |    7746MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   29C    P0            123W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   28C    P0            129W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   49C    P0            139W /  700W |    3456MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   27C    P0            124W /  700W |    3216MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin', 'data/fineweb10B/fineweb_train_000010.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1390 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1390 train_time:995242ms step_avg:nanms
step:2/1390 train_time:995558ms step_avg:nanms
step:3/1390 train_time:997070ms step_avg:nanms
step:4/1390 train_time:997215ms step_avg:nanms
step:5/1390 train_time:997359ms step_avg:nanms
step:6/1390 train_time:997503ms step_avg:nanms
step:7/1390 train_time:997648ms step_avg:nanms
step:8/1390 train_time:997794ms step_avg:nanms
step:9/1390 train_time:997940ms step_avg:nanms
step:10/1390 train_time:998086ms step_avg:nanms
step:11/1390 train_time:145ms step_avg:nanms
step:12/1390 train_time:290ms step_avg:nanms
step:13/1390 train_time:435ms step_avg:144.86ms
step:14/1390 train_time:581ms step_avg:145.27ms
step:15/1390 train_time:727ms step_avg:145.48ms
step:16/1390 train_time:873ms step_avg:145.52ms
step:17/1390 train_time:1020ms step_avg:145.68ms
step:18/1390 train_time:1166ms step_avg:145.74ms
step:19/1390 train_time:1313ms step_avg:145.89ms
step:20/1390 train_time:1457ms step_avg:145.72ms
step:21/1390 train_time:1602ms step_avg:145.64ms
step:22/1390 train_time:1748ms step_avg:145.70ms
step:23/1390 train_time:1894ms step_avg:145.68ms
step:24/1390 train_time:2039ms step_avg:145.62ms
step:25/1390 train_time:2184ms step_avg:145.61ms
step:26/1390 train_time:2330ms step_avg:145.65ms
step:27/1390 train_time:2477ms step_avg:145.68ms
step:28/1390 train_time:2622ms step_avg:145.65ms
step:29/1390 train_time:2768ms step_avg:145.69ms
step:30/1390 train_time:2915ms step_avg:145.73ms
step:31/1390 train_time:3060ms step_avg:145.71ms
step:32/1390 train_time:3206ms step_avg:145.72ms
step:33/1390 train_time:3351ms step_avg:145.68ms
step:34/1390 train_time:3496ms step_avg:145.66ms
step:35/1390 train_time:3641ms step_avg:145.63ms
step:36/1390 train_time:3786ms step_avg:145.61ms
step:37/1390 train_time:3932ms step_avg:145.61ms
step:38/1390 train_time:4077ms step_avg:145.60ms
step:39/1390 train_time:4222ms step_avg:145.60ms
step:40/1390 train_time:4369ms step_avg:145.64ms
step:41/1390 train_time:4515ms step_avg:145.63ms
step:42/1390 train_time:4659ms step_avg:145.61ms
step:43/1390 train_time:4806ms step_avg:145.64ms
step:44/1390 train_time:4951ms step_avg:145.61ms
step:45/1390 train_time:5097ms step_avg:145.62ms
step:46/1390 train_time:5241ms step_avg:145.60ms
step:47/1390 train_time:5387ms step_avg:145.60ms
step:48/1390 train_time:5532ms step_avg:145.59ms
step:49/1390 train_time:5678ms step_avg:145.59ms
step:50/1390 train_time:5824ms step_avg:145.61ms
step:51/1390 train_time:5970ms step_avg:145.60ms
step:52/1390 train_time:6115ms step_avg:145.60ms
step:53/1390 train_time:6260ms step_avg:145.59ms
step:54/1390 train_time:6407ms step_avg:145.61ms
step:55/1390 train_time:6552ms step_avg:145.61ms
step:56/1390 train_time:6697ms step_avg:145.60ms
step:57/1390 train_time:6843ms step_avg:145.60ms
step:58/1390 train_time:6988ms step_avg:145.59ms
step:59/1390 train_time:7134ms step_avg:145.60ms
step:60/1390 train_time:7279ms step_avg:145.59ms
step:61/1390 train_time:7426ms step_avg:145.61ms
step:62/1390 train_time:7571ms step_avg:145.60ms
step:63/1390 train_time:7715ms step_avg:145.57ms
step:64/1390 train_time:7860ms step_avg:145.56ms
step:65/1390 train_time:8007ms step_avg:145.57ms
step:66/1390 train_time:8152ms step_avg:145.58ms
step:67/1390 train_time:8298ms step_avg:145.58ms
step:68/1390 train_time:8445ms step_avg:145.60ms
step:69/1390 train_time:8590ms step_avg:145.59ms
step:70/1390 train_time:8734ms step_avg:145.57ms
step:71/1390 train_time:8880ms step_avg:145.57ms
step:72/1390 train_time:9026ms step_avg:145.58ms
step:73/1390 train_time:9172ms step_avg:145.59ms
step:74/1390 train_time:9317ms step_avg:145.58ms
step:75/1390 train_time:9463ms step_avg:145.59ms
step:76/1390 train_time:9609ms step_avg:145.59ms
step:77/1390 train_time:9754ms step_avg:145.58ms
step:78/1390 train_time:9899ms step_avg:145.58ms
step:79/1390 train_time:10046ms step_avg:145.60ms
step:80/1390 train_time:10192ms step_avg:145.60ms
step:81/1390 train_time:10337ms step_avg:145.59ms
step:82/1390 train_time:10483ms step_avg:145.60ms
step:83/1390 train_time:10629ms step_avg:145.61ms
step:84/1390 train_time:10775ms step_avg:145.61ms
step:85/1390 train_time:10921ms step_avg:145.62ms
step:86/1390 train_time:11069ms step_avg:145.64ms
step:87/1390 train_time:11215ms step_avg:145.65ms
step:88/1390 train_time:11361ms step_avg:145.65ms
step:89/1390 train_time:11508ms step_avg:145.67ms
step:90/1390 train_time:11654ms step_avg:145.68ms
step:91/1390 train_time:11799ms step_avg:145.67ms
step:92/1390 train_time:11945ms step_avg:145.67ms
step:93/1390 train_time:12091ms step_avg:145.67ms
step:94/1390 train_time:12237ms step_avg:145.68ms
step:95/1390 train_time:12382ms step_avg:145.67ms
step:96/1390 train_time:12529ms step_avg:145.69ms
step:97/1390 train_time:12674ms step_avg:145.68ms
step:98/1390 train_time:12819ms step_avg:145.67ms
step:99/1390 train_time:12966ms step_avg:145.68ms
step:100/1390 train_time:13113ms step_avg:145.70ms
step:101/1390 train_time:13258ms step_avg:145.69ms
step:102/1390 train_time:13404ms step_avg:145.70ms
step:103/1390 train_time:13551ms step_avg:145.70ms
step:104/1390 train_time:13701ms step_avg:145.75ms
step:105/1390 train_time:13861ms step_avg:145.90ms
step:106/1390 train_time:14020ms step_avg:146.04ms
step:107/1390 train_time:14180ms step_avg:146.19ms
step:108/1390 train_time:14339ms step_avg:146.32ms
step:109/1390 train_time:14500ms step_avg:146.46ms
step:110/1390 train_time:14660ms step_avg:146.60ms
step:111/1390 train_time:14819ms step_avg:146.73ms
step:112/1390 train_time:14978ms step_avg:146.84ms
step:113/1390 train_time:15139ms step_avg:146.98ms
step:114/1390 train_time:15298ms step_avg:147.10ms
step:115/1390 train_time:15457ms step_avg:147.21ms
step:116/1390 train_time:15616ms step_avg:147.32ms
step:117/1390 train_time:15777ms step_avg:147.45ms
step:118/1390 train_time:15936ms step_avg:147.55ms
step:119/1390 train_time:16095ms step_avg:147.66ms
step:120/1390 train_time:16255ms step_avg:147.78ms
step:121/1390 train_time:16415ms step_avg:147.89ms
step:122/1390 train_time:16575ms step_avg:147.99ms
step:123/1390 train_time:16735ms step_avg:148.10ms
step:124/1390 train_time:16896ms step_avg:148.21ms
step:125/1390 train_time:17056ms step_avg:148.31ms
step:125/1390 val_loss:4.2923 train_time:17138ms step_avg:149.03ms
step:126/1390 train_time:17222ms step_avg:148.47ms
step:127/1390 train_time:17382ms step_avg:148.56ms
step:128/1390 train_time:17543ms step_avg:148.67ms
step:129/1390 train_time:17703ms step_avg:148.76ms
step:130/1390 train_time:17863ms step_avg:148.86ms
step:131/1390 train_time:18022ms step_avg:148.94ms
step:132/1390 train_time:18183ms step_avg:149.04ms
step:133/1390 train_time:18344ms step_avg:149.14ms
step:134/1390 train_time:18505ms step_avg:149.23ms
step:135/1390 train_time:18665ms step_avg:149.32ms
step:136/1390 train_time:18826ms step_avg:149.41ms
step:137/1390 train_time:18986ms step_avg:149.50ms
step:138/1390 train_time:19147ms step_avg:149.59ms
step:139/1390 train_time:19307ms step_avg:149.67ms
step:140/1390 train_time:19467ms step_avg:149.75ms
step:141/1390 train_time:19628ms step_avg:149.84ms
step:142/1390 train_time:19790ms step_avg:149.93ms
step:143/1390 train_time:19951ms step_avg:150.01ms
step:144/1390 train_time:20111ms step_avg:150.08ms
step:145/1390 train_time:20272ms step_avg:150.16ms
step:146/1390 train_time:20432ms step_avg:150.23ms
step:147/1390 train_time:20592ms step_avg:150.31ms
step:148/1390 train_time:20752ms step_avg:150.38ms
step:149/1390 train_time:20912ms step_avg:150.44ms
step:150/1390 train_time:21071ms step_avg:150.51ms
step:151/1390 train_time:21232ms step_avg:150.58ms
step:152/1390 train_time:21391ms step_avg:150.64ms
step:153/1390 train_time:21551ms step_avg:150.70ms
step:154/1390 train_time:21711ms step_avg:150.77ms
step:155/1390 train_time:21871ms step_avg:150.84ms
step:156/1390 train_time:22032ms step_avg:150.90ms
step:157/1390 train_time:22192ms step_avg:150.97ms
step:158/1390 train_time:22353ms step_avg:151.04ms
step:159/1390 train_time:22514ms step_avg:151.10ms
step:160/1390 train_time:22672ms step_avg:151.15ms
step:161/1390 train_time:22832ms step_avg:151.21ms
step:162/1390 train_time:22992ms step_avg:151.27ms
step:163/1390 train_time:23154ms step_avg:151.33ms
step:164/1390 train_time:23315ms step_avg:151.40ms
step:165/1390 train_time:23476ms step_avg:151.46ms
step:166/1390 train_time:23636ms step_avg:151.51ms
step:167/1390 train_time:23796ms step_avg:151.57ms
step:168/1390 train_time:23957ms step_avg:151.63ms
step:169/1390 train_time:24119ms step_avg:151.69ms
step:170/1390 train_time:24278ms step_avg:151.74ms
step:171/1390 train_time:24439ms step_avg:151.80ms
step:172/1390 train_time:24599ms step_avg:151.85ms
step:173/1390 train_time:24760ms step_avg:151.90ms
step:174/1390 train_time:24921ms step_avg:151.96ms
step:175/1390 train_time:25082ms step_avg:152.01ms
step:176/1390 train_time:25244ms step_avg:152.07ms
step:177/1390 train_time:25404ms step_avg:152.12ms
step:178/1390 train_time:25564ms step_avg:152.17ms
step:179/1390 train_time:25725ms step_avg:152.22ms
step:180/1390 train_time:25887ms step_avg:152.28ms
step:181/1390 train_time:26048ms step_avg:152.33ms
step:182/1390 train_time:26208ms step_avg:152.37ms
step:183/1390 train_time:26369ms step_avg:152.42ms
step:184/1390 train_time:26529ms step_avg:152.46ms
step:185/1390 train_time:26688ms step_avg:152.50ms
step:186/1390 train_time:26849ms step_avg:152.55ms
step:187/1390 train_time:27010ms step_avg:152.60ms
step:188/1390 train_time:27171ms step_avg:152.65ms
step:189/1390 train_time:27331ms step_avg:152.69ms
step:190/1390 train_time:27491ms step_avg:152.73ms
step:191/1390 train_time:27698ms step_avg:153.03ms
step:192/1390 train_time:27858ms step_avg:153.07ms
step:193/1390 train_time:28019ms step_avg:153.11ms
step:194/1390 train_time:28179ms step_avg:153.15ms
step:195/1390 train_time:28340ms step_avg:153.19ms
step:196/1390 train_time:28500ms step_avg:153.22ms
step:197/1390 train_time:28661ms step_avg:153.27ms
step:198/1390 train_time:28823ms step_avg:153.31ms
step:199/1390 train_time:28983ms step_avg:153.35ms
step:200/1390 train_time:29144ms step_avg:153.39ms
step:201/1390 train_time:29304ms step_avg:153.43ms
step:202/1390 train_time:29465ms step_avg:153.46ms
step:203/1390 train_time:29627ms step_avg:153.51ms
step:204/1390 train_time:29788ms step_avg:153.54ms
step:205/1390 train_time:29950ms step_avg:153.59ms
step:206/1390 train_time:30110ms step_avg:153.62ms
step:207/1390 train_time:30271ms step_avg:153.66ms
step:208/1390 train_time:30441ms step_avg:153.74ms
step:209/1390 train_time:30607ms step_avg:153.80ms
step:210/1390 train_time:30775ms step_avg:153.87ms
step:211/1390 train_time:30942ms step_avg:153.94ms
step:212/1390 train_time:31110ms step_avg:154.01ms
step:213/1390 train_time:31276ms step_avg:154.07ms
step:214/1390 train_time:31443ms step_avg:154.13ms
step:215/1390 train_time:31612ms step_avg:154.20ms
step:216/1390 train_time:31779ms step_avg:154.26ms
step:217/1390 train_time:31947ms step_avg:154.33ms
step:218/1390 train_time:32112ms step_avg:154.39ms
step:219/1390 train_time:32278ms step_avg:154.44ms
step:220/1390 train_time:32446ms step_avg:154.51ms
step:221/1390 train_time:32612ms step_avg:154.56ms
step:222/1390 train_time:32781ms step_avg:154.63ms
step:223/1390 train_time:32950ms step_avg:154.69ms
step:224/1390 train_time:33116ms step_avg:154.75ms
step:225/1390 train_time:33283ms step_avg:154.80ms
step:226/1390 train_time:33449ms step_avg:154.86ms
step:227/1390 train_time:33615ms step_avg:154.91ms
step:228/1390 train_time:33781ms step_avg:154.96ms
step:229/1390 train_time:33949ms step_avg:155.02ms
step:230/1390 train_time:34116ms step_avg:155.07ms
step:231/1390 train_time:34285ms step_avg:155.13ms
step:232/1390 train_time:34451ms step_avg:155.18ms
step:233/1390 train_time:34617ms step_avg:155.23ms
step:234/1390 train_time:34785ms step_avg:155.29ms
step:235/1390 train_time:34952ms step_avg:155.34ms
step:236/1390 train_time:35120ms step_avg:155.40ms
step:237/1390 train_time:35288ms step_avg:155.45ms
step:238/1390 train_time:35454ms step_avg:155.50ms
step:239/1390 train_time:35621ms step_avg:155.55ms
step:240/1390 train_time:35790ms step_avg:155.61ms
step:241/1390 train_time:35956ms step_avg:155.65ms
step:242/1390 train_time:36125ms step_avg:155.71ms
step:243/1390 train_time:36293ms step_avg:155.76ms
step:244/1390 train_time:36461ms step_avg:155.82ms
step:245/1390 train_time:36629ms step_avg:155.87ms
step:246/1390 train_time:36796ms step_avg:155.91ms
step:247/1390 train_time:36963ms step_avg:155.96ms
step:248/1390 train_time:37132ms step_avg:156.02ms
step:249/1390 train_time:37298ms step_avg:156.06ms
step:250/1390 train_time:37465ms step_avg:156.10ms
step:250/1390 val_loss:3.9045 train_time:37550ms step_avg:156.46ms
step:251/1390 train_time:37635ms step_avg:156.16ms
step:252/1390 train_time:37800ms step_avg:156.20ms
step:253/1390 train_time:37966ms step_avg:156.24ms
step:254/1390 train_time:38134ms step_avg:156.29ms
step:255/1390 train_time:38300ms step_avg:156.33ms
step:256/1390 train_time:38467ms step_avg:156.37ms
step:257/1390 train_time:38636ms step_avg:156.42ms
step:258/1390 train_time:38802ms step_avg:156.46ms
step:259/1390 train_time:38968ms step_avg:156.50ms
step:260/1390 train_time:39135ms step_avg:156.54ms
step:261/1390 train_time:39302ms step_avg:156.58ms
step:262/1390 train_time:39470ms step_avg:156.63ms
step:263/1390 train_time:39638ms step_avg:156.67ms
step:264/1390 train_time:39807ms step_avg:156.72ms
step:265/1390 train_time:39973ms step_avg:156.76ms
step:266/1390 train_time:40140ms step_avg:156.80ms
step:267/1390 train_time:40307ms step_avg:156.84ms
step:268/1390 train_time:40473ms step_avg:156.87ms
step:269/1390 train_time:40640ms step_avg:156.91ms
step:270/1390 train_time:40808ms step_avg:156.95ms
step:271/1390 train_time:40975ms step_avg:156.99ms
step:272/1390 train_time:41142ms step_avg:157.03ms
step:273/1390 train_time:41310ms step_avg:157.07ms
step:274/1390 train_time:41478ms step_avg:157.11ms
step:275/1390 train_time:41645ms step_avg:157.15ms
step:276/1390 train_time:41813ms step_avg:157.19ms
step:277/1390 train_time:41979ms step_avg:157.23ms
step:278/1390 train_time:42146ms step_avg:157.26ms
step:279/1390 train_time:42314ms step_avg:157.30ms
step:280/1390 train_time:42480ms step_avg:157.33ms
step:281/1390 train_time:42646ms step_avg:157.36ms
step:282/1390 train_time:42814ms step_avg:157.40ms
step:283/1390 train_time:42980ms step_avg:157.44ms
step:284/1390 train_time:43147ms step_avg:157.47ms
step:285/1390 train_time:43315ms step_avg:157.51ms
step:286/1390 train_time:43481ms step_avg:157.54ms
step:287/1390 train_time:43647ms step_avg:157.57ms
step:288/1390 train_time:43815ms step_avg:157.61ms
step:289/1390 train_time:43981ms step_avg:157.64ms
step:290/1390 train_time:44149ms step_avg:157.67ms
step:291/1390 train_time:44317ms step_avg:157.71ms
step:292/1390 train_time:44483ms step_avg:157.74ms
step:293/1390 train_time:44648ms step_avg:157.77ms
step:294/1390 train_time:44815ms step_avg:157.80ms
step:295/1390 train_time:44981ms step_avg:157.83ms
step:296/1390 train_time:45149ms step_avg:157.86ms
step:297/1390 train_time:45317ms step_avg:157.90ms
step:298/1390 train_time:45483ms step_avg:157.93ms
step:299/1390 train_time:45650ms step_avg:157.96ms
step:300/1390 train_time:45819ms step_avg:158.00ms
step:301/1390 train_time:45986ms step_avg:158.03ms
step:302/1390 train_time:46153ms step_avg:158.06ms
step:303/1390 train_time:46320ms step_avg:158.09ms
step:304/1390 train_time:46487ms step_avg:158.12ms
step:305/1390 train_time:46655ms step_avg:158.15ms
step:306/1390 train_time:46821ms step_avg:158.18ms
step:307/1390 train_time:46989ms step_avg:158.21ms
step:308/1390 train_time:47156ms step_avg:158.24ms
step:309/1390 train_time:47323ms step_avg:158.27ms
step:310/1390 train_time:47491ms step_avg:158.30ms
step:311/1390 train_time:47662ms step_avg:158.34ms
step:312/1390 train_time:47832ms step_avg:158.38ms
step:313/1390 train_time:48004ms step_avg:158.43ms
step:314/1390 train_time:48176ms step_avg:158.47ms
step:315/1390 train_time:48348ms step_avg:158.52ms
step:316/1390 train_time:48519ms step_avg:158.56ms
step:317/1390 train_time:48693ms step_avg:158.61ms
step:318/1390 train_time:48864ms step_avg:158.65ms
step:319/1390 train_time:49039ms step_avg:158.70ms
step:320/1390 train_time:49213ms step_avg:158.75ms
step:321/1390 train_time:49382ms step_avg:158.78ms
step:322/1390 train_time:49555ms step_avg:158.83ms
step:323/1390 train_time:49724ms step_avg:158.86ms
step:324/1390 train_time:49896ms step_avg:158.91ms
step:325/1390 train_time:50067ms step_avg:158.94ms
step:326/1390 train_time:50240ms step_avg:158.99ms
step:327/1390 train_time:50413ms step_avg:159.03ms
step:328/1390 train_time:50587ms step_avg:159.08ms
step:329/1390 train_time:50763ms step_avg:159.13ms
step:330/1390 train_time:50936ms step_avg:159.18ms
step:331/1390 train_time:51106ms step_avg:159.21ms
step:332/1390 train_time:51276ms step_avg:159.24ms
step:333/1390 train_time:51449ms step_avg:159.28ms
step:334/1390 train_time:51620ms step_avg:159.32ms
step:335/1390 train_time:51790ms step_avg:159.35ms
step:336/1390 train_time:51962ms step_avg:159.39ms
step:337/1390 train_time:52135ms step_avg:159.44ms
step:338/1390 train_time:52307ms step_avg:159.47ms
step:339/1390 train_time:52479ms step_avg:159.51ms
step:340/1390 train_time:52652ms step_avg:159.55ms
step:341/1390 train_time:52826ms step_avg:159.59ms
step:342/1390 train_time:52997ms step_avg:159.63ms
step:343/1390 train_time:53169ms step_avg:159.67ms
step:344/1390 train_time:53341ms step_avg:159.70ms
step:345/1390 train_time:53516ms step_avg:159.75ms
step:346/1390 train_time:53688ms step_avg:159.78ms
step:347/1390 train_time:53860ms step_avg:159.82ms
step:348/1390 train_time:54032ms step_avg:159.86ms
step:349/1390 train_time:54204ms step_avg:159.89ms
step:350/1390 train_time:54375ms step_avg:159.93ms
step:351/1390 train_time:54547ms step_avg:159.96ms
step:352/1390 train_time:54717ms step_avg:159.99ms
step:353/1390 train_time:54889ms step_avg:160.03ms
step:354/1390 train_time:55059ms step_avg:160.06ms
step:355/1390 train_time:55233ms step_avg:160.10ms
step:356/1390 train_time:55404ms step_avg:160.13ms
step:357/1390 train_time:55577ms step_avg:160.16ms
step:358/1390 train_time:55746ms step_avg:160.19ms
step:359/1390 train_time:55920ms step_avg:160.23ms
step:360/1390 train_time:56095ms step_avg:160.27ms
step:361/1390 train_time:56267ms step_avg:160.31ms
step:362/1390 train_time:56439ms step_avg:160.34ms
step:363/1390 train_time:56613ms step_avg:160.38ms
step:364/1390 train_time:56783ms step_avg:160.40ms
step:365/1390 train_time:56957ms step_avg:160.44ms
step:366/1390 train_time:57131ms step_avg:160.48ms
step:367/1390 train_time:57300ms step_avg:160.50ms
step:368/1390 train_time:57472ms step_avg:160.54ms
step:369/1390 train_time:57643ms step_avg:160.56ms
step:370/1390 train_time:57815ms step_avg:160.60ms
step:371/1390 train_time:57985ms step_avg:160.62ms
step:372/1390 train_time:58157ms step_avg:160.65ms
step:373/1390 train_time:58329ms step_avg:160.69ms
step:374/1390 train_time:58499ms step_avg:160.71ms
step:375/1390 train_time:58670ms step_avg:160.74ms
step:375/1390 val_loss:3.7389 train_time:58758ms step_avg:160.98ms
step:376/1390 train_time:58843ms step_avg:160.77ms
step:377/1390 train_time:59017ms step_avg:160.81ms
step:378/1390 train_time:59194ms step_avg:160.85ms
step:379/1390 train_time:59366ms step_avg:160.88ms
step:380/1390 train_time:59537ms step_avg:160.91ms
step:381/1390 train_time:59756ms step_avg:161.07ms
step:382/1390 train_time:59933ms step_avg:161.11ms
step:383/1390 train_time:60104ms step_avg:161.14ms
step:384/1390 train_time:60277ms step_avg:161.17ms
step:385/1390 train_time:60450ms step_avg:161.20ms
step:386/1390 train_time:60620ms step_avg:161.22ms
step:387/1390 train_time:60793ms step_avg:161.25ms
step:388/1390 train_time:60965ms step_avg:161.28ms
step:389/1390 train_time:61136ms step_avg:161.31ms
step:390/1390 train_time:61307ms step_avg:161.33ms
step:391/1390 train_time:61480ms step_avg:161.36ms
step:392/1390 train_time:61652ms step_avg:161.39ms
step:393/1390 train_time:61822ms step_avg:161.42ms
step:394/1390 train_time:61992ms step_avg:161.44ms
step:395/1390 train_time:62162ms step_avg:161.46ms
step:396/1390 train_time:62333ms step_avg:161.48ms
step:397/1390 train_time:62506ms step_avg:161.52ms
step:398/1390 train_time:62679ms step_avg:161.54ms
step:399/1390 train_time:62847ms step_avg:161.56ms
step:400/1390 train_time:63021ms step_avg:161.59ms
step:401/1390 train_time:63192ms step_avg:161.62ms
step:402/1390 train_time:63363ms step_avg:161.64ms
step:403/1390 train_time:63535ms step_avg:161.67ms
step:404/1390 train_time:63708ms step_avg:161.69ms
step:405/1390 train_time:63881ms step_avg:161.73ms
step:406/1390 train_time:64055ms step_avg:161.75ms
step:407/1390 train_time:64225ms step_avg:161.78ms
step:408/1390 train_time:64398ms step_avg:161.80ms
step:409/1390 train_time:64572ms step_avg:161.84ms
step:410/1390 train_time:64744ms step_avg:161.86ms
step:411/1390 train_time:64916ms step_avg:161.89ms
step:412/1390 train_time:65086ms step_avg:161.91ms
step:413/1390 train_time:65260ms step_avg:161.94ms
step:414/1390 train_time:65436ms step_avg:161.97ms
step:415/1390 train_time:65612ms step_avg:162.01ms
step:416/1390 train_time:65784ms step_avg:162.03ms
step:417/1390 train_time:65965ms step_avg:162.08ms
step:418/1390 train_time:66139ms step_avg:162.10ms
step:419/1390 train_time:66316ms step_avg:162.14ms
step:420/1390 train_time:66494ms step_avg:162.18ms
step:421/1390 train_time:66668ms step_avg:162.21ms
step:422/1390 train_time:66843ms step_avg:162.24ms
step:423/1390 train_time:67019ms step_avg:162.27ms
step:424/1390 train_time:67191ms step_avg:162.30ms
step:425/1390 train_time:67368ms step_avg:162.33ms
step:426/1390 train_time:67545ms step_avg:162.37ms
step:427/1390 train_time:67720ms step_avg:162.40ms
step:428/1390 train_time:67898ms step_avg:162.44ms
step:429/1390 train_time:68075ms step_avg:162.47ms
step:430/1390 train_time:68254ms step_avg:162.51ms
step:431/1390 train_time:68430ms step_avg:162.54ms
step:432/1390 train_time:68608ms step_avg:162.58ms
step:433/1390 train_time:68782ms step_avg:162.61ms
step:434/1390 train_time:68959ms step_avg:162.64ms
step:435/1390 train_time:69135ms step_avg:162.67ms
step:436/1390 train_time:69311ms step_avg:162.70ms
step:437/1390 train_time:69484ms step_avg:162.73ms
step:438/1390 train_time:69656ms step_avg:162.75ms
step:439/1390 train_time:69831ms step_avg:162.78ms
step:440/1390 train_time:70005ms step_avg:162.80ms
step:441/1390 train_time:70181ms step_avg:162.83ms
step:442/1390 train_time:70361ms step_avg:162.87ms
step:443/1390 train_time:70541ms step_avg:162.91ms
step:444/1390 train_time:70714ms step_avg:162.93ms
step:445/1390 train_time:70893ms step_avg:162.97ms
step:446/1390 train_time:71069ms step_avg:163.00ms
step:447/1390 train_time:71245ms step_avg:163.03ms
step:448/1390 train_time:71418ms step_avg:163.06ms
step:449/1390 train_time:71598ms step_avg:163.09ms
step:450/1390 train_time:71776ms step_avg:163.13ms
step:451/1390 train_time:71954ms step_avg:163.16ms
step:452/1390 train_time:72128ms step_avg:163.18ms
step:453/1390 train_time:72303ms step_avg:163.21ms
step:454/1390 train_time:72482ms step_avg:163.25ms
step:455/1390 train_time:72658ms step_avg:163.28ms
step:456/1390 train_time:72835ms step_avg:163.31ms
step:457/1390 train_time:73010ms step_avg:163.33ms
step:458/1390 train_time:73184ms step_avg:163.36ms
step:459/1390 train_time:73361ms step_avg:163.39ms
step:460/1390 train_time:73534ms step_avg:163.41ms
step:461/1390 train_time:73717ms step_avg:163.45ms
step:462/1390 train_time:73895ms step_avg:163.48ms
step:463/1390 train_time:74070ms step_avg:163.51ms
step:464/1390 train_time:74245ms step_avg:163.54ms
step:465/1390 train_time:74418ms step_avg:163.56ms
step:466/1390 train_time:74596ms step_avg:163.59ms
step:467/1390 train_time:74773ms step_avg:163.62ms
step:468/1390 train_time:74949ms step_avg:163.64ms
step:469/1390 train_time:75124ms step_avg:163.67ms
step:470/1390 train_time:75298ms step_avg:163.69ms
step:471/1390 train_time:75473ms step_avg:163.72ms
step:472/1390 train_time:75657ms step_avg:163.76ms
step:473/1390 train_time:75830ms step_avg:163.78ms
step:474/1390 train_time:76004ms step_avg:163.80ms
step:475/1390 train_time:76181ms step_avg:163.83ms
step:476/1390 train_time:76357ms step_avg:163.86ms
step:477/1390 train_time:76533ms step_avg:163.88ms
step:478/1390 train_time:76707ms step_avg:163.90ms
step:479/1390 train_time:76883ms step_avg:163.93ms
step:480/1390 train_time:77061ms step_avg:163.96ms
step:481/1390 train_time:77236ms step_avg:163.98ms
step:482/1390 train_time:77411ms step_avg:164.01ms
step:483/1390 train_time:77586ms step_avg:164.03ms
step:484/1390 train_time:77762ms step_avg:164.05ms
step:485/1390 train_time:77938ms step_avg:164.08ms
step:486/1390 train_time:78116ms step_avg:164.11ms
step:487/1390 train_time:78290ms step_avg:164.13ms
step:488/1390 train_time:78466ms step_avg:164.15ms
step:489/1390 train_time:78639ms step_avg:164.17ms
step:490/1390 train_time:78812ms step_avg:164.19ms
step:491/1390 train_time:78992ms step_avg:164.23ms
step:492/1390 train_time:79167ms step_avg:164.25ms
step:493/1390 train_time:79342ms step_avg:164.27ms
step:494/1390 train_time:79518ms step_avg:164.29ms
step:495/1390 train_time:79700ms step_avg:164.33ms
step:496/1390 train_time:79878ms step_avg:164.36ms
step:497/1390 train_time:80050ms step_avg:164.37ms
step:498/1390 train_time:80233ms step_avg:164.41ms
step:499/1390 train_time:80409ms step_avg:164.43ms
step:500/1390 train_time:80584ms step_avg:164.46ms
step:500/1390 val_loss:3.6286 train_time:80677ms step_avg:164.65ms
step:501/1390 train_time:80764ms step_avg:164.49ms
step:502/1390 train_time:80942ms step_avg:164.52ms
step:503/1390 train_time:81119ms step_avg:164.54ms
step:504/1390 train_time:81294ms step_avg:164.56ms
step:505/1390 train_time:81473ms step_avg:164.59ms
step:506/1390 train_time:81648ms step_avg:164.61ms
step:507/1390 train_time:81820ms step_avg:164.63ms
step:508/1390 train_time:82004ms step_avg:164.67ms
step:509/1390 train_time:82178ms step_avg:164.69ms
step:510/1390 train_time:82354ms step_avg:164.71ms
step:511/1390 train_time:82529ms step_avg:164.73ms
step:512/1390 train_time:82714ms step_avg:164.77ms
step:513/1390 train_time:82891ms step_avg:164.79ms
step:514/1390 train_time:83079ms step_avg:164.84ms
step:515/1390 train_time:83256ms step_avg:164.86ms
step:516/1390 train_time:83436ms step_avg:164.89ms
step:517/1390 train_time:83619ms step_avg:164.93ms
step:518/1390 train_time:83796ms step_avg:164.95ms
step:519/1390 train_time:83977ms step_avg:164.98ms
step:520/1390 train_time:84159ms step_avg:165.02ms
step:521/1390 train_time:84335ms step_avg:165.04ms
step:522/1390 train_time:84516ms step_avg:165.07ms
step:523/1390 train_time:84696ms step_avg:165.10ms
step:524/1390 train_time:84874ms step_avg:165.13ms
step:525/1390 train_time:85048ms step_avg:165.14ms
step:526/1390 train_time:85233ms step_avg:165.18ms
step:527/1390 train_time:85411ms step_avg:165.21ms
step:528/1390 train_time:85588ms step_avg:165.23ms
step:529/1390 train_time:85768ms step_avg:165.26ms
step:530/1390 train_time:85946ms step_avg:165.28ms
step:531/1390 train_time:86126ms step_avg:165.31ms
step:532/1390 train_time:86304ms step_avg:165.33ms
step:533/1390 train_time:86502ms step_avg:165.40ms
step:534/1390 train_time:86680ms step_avg:165.42ms
step:535/1390 train_time:86857ms step_avg:165.44ms
step:536/1390 train_time:87053ms step_avg:165.50ms
step:537/1390 train_time:87230ms step_avg:165.52ms
step:538/1390 train_time:87411ms step_avg:165.55ms
step:539/1390 train_time:87597ms step_avg:165.59ms
step:540/1390 train_time:87782ms step_avg:165.63ms
step:541/1390 train_time:87970ms step_avg:165.67ms
step:542/1390 train_time:88149ms step_avg:165.69ms
step:543/1390 train_time:88331ms step_avg:165.72ms
step:544/1390 train_time:88508ms step_avg:165.75ms
step:545/1390 train_time:88686ms step_avg:165.77ms
step:546/1390 train_time:88868ms step_avg:165.80ms
step:547/1390 train_time:89045ms step_avg:165.82ms
step:548/1390 train_time:89232ms step_avg:165.86ms
step:549/1390 train_time:89413ms step_avg:165.89ms
step:550/1390 train_time:89607ms step_avg:165.94ms
step:551/1390 train_time:89782ms step_avg:165.96ms
step:552/1390 train_time:89964ms step_avg:165.99ms
step:553/1390 train_time:90146ms step_avg:166.02ms
step:554/1390 train_time:90320ms step_avg:166.03ms
step:555/1390 train_time:90502ms step_avg:166.06ms
step:556/1390 train_time:90676ms step_avg:166.07ms
step:557/1390 train_time:90859ms step_avg:166.11ms
step:558/1390 train_time:91036ms step_avg:166.12ms
step:559/1390 train_time:91213ms step_avg:166.14ms
step:560/1390 train_time:91392ms step_avg:166.17ms
step:561/1390 train_time:91570ms step_avg:166.19ms
step:562/1390 train_time:91749ms step_avg:166.21ms
step:563/1390 train_time:91925ms step_avg:166.23ms
step:564/1390 train_time:92102ms step_avg:166.25ms
step:565/1390 train_time:92279ms step_avg:166.27ms
step:566/1390 train_time:92468ms step_avg:166.31ms
step:567/1390 train_time:92643ms step_avg:166.32ms
step:568/1390 train_time:92821ms step_avg:166.35ms
step:569/1390 train_time:93000ms step_avg:166.37ms
step:570/1390 train_time:93179ms step_avg:166.39ms
step:571/1390 train_time:93409ms step_avg:166.50ms
step:572/1390 train_time:93593ms step_avg:166.53ms
step:573/1390 train_time:93776ms step_avg:166.57ms
step:574/1390 train_time:93976ms step_avg:166.62ms
step:575/1390 train_time:94156ms step_avg:166.65ms
step:576/1390 train_time:94332ms step_avg:166.66ms
step:577/1390 train_time:94510ms step_avg:166.68ms
step:578/1390 train_time:94685ms step_avg:166.70ms
step:579/1390 train_time:94861ms step_avg:166.71ms
step:580/1390 train_time:95042ms step_avg:166.74ms
step:581/1390 train_time:95226ms step_avg:166.77ms
step:582/1390 train_time:95405ms step_avg:166.79ms
step:583/1390 train_time:95586ms step_avg:166.82ms
step:584/1390 train_time:95772ms step_avg:166.85ms
step:585/1390 train_time:95946ms step_avg:166.86ms
step:586/1390 train_time:96129ms step_avg:166.89ms
step:587/1390 train_time:96307ms step_avg:166.91ms
step:588/1390 train_time:96485ms step_avg:166.93ms
step:589/1390 train_time:96664ms step_avg:166.95ms
step:590/1390 train_time:96846ms step_avg:166.98ms
step:591/1390 train_time:97020ms step_avg:166.99ms
step:592/1390 train_time:97207ms step_avg:167.02ms
step:593/1390 train_time:97387ms step_avg:167.04ms
step:594/1390 train_time:97567ms step_avg:167.07ms
step:595/1390 train_time:97756ms step_avg:167.10ms
step:596/1390 train_time:97944ms step_avg:167.14ms
step:597/1390 train_time:98121ms step_avg:167.16ms
step:598/1390 train_time:98300ms step_avg:167.18ms
step:599/1390 train_time:98477ms step_avg:167.19ms
step:600/1390 train_time:98659ms step_avg:167.22ms
step:601/1390 train_time:98836ms step_avg:167.23ms
step:602/1390 train_time:99010ms step_avg:167.25ms
step:603/1390 train_time:99191ms step_avg:167.27ms
step:604/1390 train_time:99375ms step_avg:167.30ms
step:605/1390 train_time:99564ms step_avg:167.33ms
step:606/1390 train_time:99743ms step_avg:167.35ms
step:607/1390 train_time:99934ms step_avg:167.39ms
step:608/1390 train_time:100122ms step_avg:167.43ms
step:609/1390 train_time:100300ms step_avg:167.44ms
step:610/1390 train_time:100473ms step_avg:167.46ms
step:611/1390 train_time:100653ms step_avg:167.48ms
step:612/1390 train_time:100833ms step_avg:167.50ms
step:613/1390 train_time:101014ms step_avg:167.52ms
step:614/1390 train_time:101192ms step_avg:167.54ms
step:615/1390 train_time:101367ms step_avg:167.55ms
step:616/1390 train_time:101542ms step_avg:167.56ms
step:617/1390 train_time:101722ms step_avg:167.58ms
step:618/1390 train_time:101899ms step_avg:167.60ms
step:619/1390 train_time:102078ms step_avg:167.62ms
step:620/1390 train_time:102266ms step_avg:167.65ms
step:621/1390 train_time:102452ms step_avg:167.68ms
step:622/1390 train_time:102639ms step_avg:167.71ms
step:623/1390 train_time:102821ms step_avg:167.73ms
step:624/1390 train_time:103011ms step_avg:167.77ms
step:625/1390 train_time:103192ms step_avg:167.79ms
step:625/1390 val_loss:3.5575 train_time:103294ms step_avg:167.96ms
step:626/1390 train_time:103382ms step_avg:167.83ms
step:627/1390 train_time:103571ms step_avg:167.86ms
step:628/1390 train_time:103749ms step_avg:167.88ms
step:629/1390 train_time:103939ms step_avg:167.91ms
step:630/1390 train_time:104117ms step_avg:167.93ms
step:631/1390 train_time:104303ms step_avg:167.96ms
step:632/1390 train_time:104482ms step_avg:167.98ms
step:633/1390 train_time:104665ms step_avg:168.00ms
step:634/1390 train_time:104851ms step_avg:168.03ms
step:635/1390 train_time:105042ms step_avg:168.07ms
step:636/1390 train_time:105227ms step_avg:168.09ms
step:637/1390 train_time:105410ms step_avg:168.12ms
step:638/1390 train_time:105588ms step_avg:168.13ms
step:639/1390 train_time:105766ms step_avg:168.15ms
step:640/1390 train_time:105951ms step_avg:168.18ms
step:641/1390 train_time:106130ms step_avg:168.19ms
step:642/1390 train_time:106309ms step_avg:168.21ms
step:643/1390 train_time:106491ms step_avg:168.23ms
step:644/1390 train_time:106669ms step_avg:168.25ms
step:645/1390 train_time:106854ms step_avg:168.27ms
step:646/1390 train_time:107033ms step_avg:168.29ms
step:647/1390 train_time:107211ms step_avg:168.31ms
step:648/1390 train_time:107416ms step_avg:168.36ms
step:649/1390 train_time:107597ms step_avg:168.38ms
step:650/1390 train_time:107791ms step_avg:168.42ms
step:651/1390 train_time:107974ms step_avg:168.45ms
step:652/1390 train_time:108160ms step_avg:168.47ms
step:653/1390 train_time:108343ms step_avg:168.50ms
step:654/1390 train_time:108526ms step_avg:168.52ms
step:655/1390 train_time:108705ms step_avg:168.53ms
step:656/1390 train_time:108882ms step_avg:168.55ms
step:657/1390 train_time:109068ms step_avg:168.57ms
step:658/1390 train_time:109252ms step_avg:168.60ms
step:659/1390 train_time:109436ms step_avg:168.62ms
step:660/1390 train_time:109619ms step_avg:168.64ms
step:661/1390 train_time:109808ms step_avg:168.68ms
step:662/1390 train_time:109987ms step_avg:168.69ms
step:663/1390 train_time:110165ms step_avg:168.71ms
step:664/1390 train_time:110351ms step_avg:168.73ms
step:665/1390 train_time:110532ms step_avg:168.75ms
step:666/1390 train_time:110710ms step_avg:168.77ms
step:667/1390 train_time:110901ms step_avg:168.80ms
step:668/1390 train_time:111086ms step_avg:168.82ms
step:669/1390 train_time:111281ms step_avg:168.86ms
step:670/1390 train_time:111460ms step_avg:168.88ms
step:671/1390 train_time:111644ms step_avg:168.90ms
step:672/1390 train_time:111834ms step_avg:168.93ms
step:673/1390 train_time:112016ms step_avg:168.95ms
step:674/1390 train_time:112205ms step_avg:168.98ms
step:675/1390 train_time:112398ms step_avg:169.02ms
step:676/1390 train_time:112583ms step_avg:169.04ms
step:677/1390 train_time:112762ms step_avg:169.06ms
step:678/1390 train_time:112936ms step_avg:169.07ms
step:679/1390 train_time:113125ms step_avg:169.10ms
step:680/1390 train_time:113321ms step_avg:169.14ms
step:681/1390 train_time:113511ms step_avg:169.17ms
step:682/1390 train_time:113693ms step_avg:169.19ms
step:683/1390 train_time:113874ms step_avg:169.20ms
step:684/1390 train_time:114064ms step_avg:169.23ms
step:685/1390 train_time:114252ms step_avg:169.26ms
step:686/1390 train_time:114438ms step_avg:169.29ms
step:687/1390 train_time:114613ms step_avg:169.30ms
step:688/1390 train_time:114804ms step_avg:169.33ms
step:689/1390 train_time:114993ms step_avg:169.36ms
step:690/1390 train_time:115192ms step_avg:169.40ms
step:691/1390 train_time:115370ms step_avg:169.41ms
step:692/1390 train_time:115548ms step_avg:169.43ms
step:693/1390 train_time:115726ms step_avg:169.44ms
step:694/1390 train_time:115906ms step_avg:169.45ms
step:695/1390 train_time:116083ms step_avg:169.46ms
step:696/1390 train_time:116265ms step_avg:169.48ms
step:697/1390 train_time:116450ms step_avg:169.51ms
step:698/1390 train_time:116631ms step_avg:169.52ms
step:699/1390 train_time:116814ms step_avg:169.54ms
step:700/1390 train_time:117001ms step_avg:169.57ms
step:701/1390 train_time:117180ms step_avg:169.58ms
step:702/1390 train_time:117368ms step_avg:169.61ms
step:703/1390 train_time:117546ms step_avg:169.62ms
step:704/1390 train_time:117727ms step_avg:169.64ms
step:705/1390 train_time:117910ms step_avg:169.65ms
step:706/1390 train_time:118118ms step_avg:169.71ms
step:707/1390 train_time:118298ms step_avg:169.72ms
step:708/1390 train_time:118492ms step_avg:169.76ms
step:709/1390 train_time:118679ms step_avg:169.78ms
step:710/1390 train_time:118870ms step_avg:169.81ms
step:711/1390 train_time:119055ms step_avg:169.84ms
step:712/1390 train_time:119253ms step_avg:169.88ms
step:713/1390 train_time:119435ms step_avg:169.89ms
step:714/1390 train_time:119614ms step_avg:169.91ms
step:715/1390 train_time:119809ms step_avg:169.94ms
step:716/1390 train_time:120001ms step_avg:169.97ms
step:717/1390 train_time:120186ms step_avg:169.99ms
step:718/1390 train_time:120367ms step_avg:170.01ms
step:719/1390 train_time:120541ms step_avg:170.02ms
step:720/1390 train_time:120730ms step_avg:170.04ms
step:721/1390 train_time:120914ms step_avg:170.06ms
step:722/1390 train_time:121101ms step_avg:170.09ms
step:723/1390 train_time:121281ms step_avg:170.10ms
step:724/1390 train_time:121469ms step_avg:170.12ms
step:725/1390 train_time:121669ms step_avg:170.17ms
step:726/1390 train_time:121868ms step_avg:170.21ms
step:727/1390 train_time:122070ms step_avg:170.25ms
step:728/1390 train_time:122250ms step_avg:170.27ms
step:729/1390 train_time:122434ms step_avg:170.28ms
step:730/1390 train_time:122633ms step_avg:170.32ms
step:731/1390 train_time:122817ms step_avg:170.34ms
step:732/1390 train_time:122993ms step_avg:170.35ms
step:733/1390 train_time:123183ms step_avg:170.38ms
step:734/1390 train_time:123363ms step_avg:170.39ms
step:735/1390 train_time:123552ms step_avg:170.42ms
step:736/1390 train_time:123745ms step_avg:170.45ms
step:737/1390 train_time:123940ms step_avg:170.48ms
step:738/1390 train_time:124123ms step_avg:170.50ms
step:739/1390 train_time:124309ms step_avg:170.52ms
step:740/1390 train_time:124502ms step_avg:170.55ms
step:741/1390 train_time:124703ms step_avg:170.59ms
step:742/1390 train_time:124894ms step_avg:170.62ms
step:743/1390 train_time:125085ms step_avg:170.65ms
step:744/1390 train_time:125282ms step_avg:170.68ms
step:745/1390 train_time:125487ms step_avg:170.73ms
step:746/1390 train_time:125667ms step_avg:170.74ms
step:747/1390 train_time:125856ms step_avg:170.77ms
step:748/1390 train_time:126040ms step_avg:170.79ms
step:749/1390 train_time:126236ms step_avg:170.82ms
step:750/1390 train_time:126426ms step_avg:170.85ms
step:750/1390 val_loss:3.5074 train_time:126556ms step_avg:171.02ms
step:751/1390 train_time:126641ms step_avg:170.91ms
step:752/1390 train_time:126833ms step_avg:170.93ms
step:753/1390 train_time:127014ms step_avg:170.95ms
step:754/1390 train_time:127196ms step_avg:170.96ms
step:755/1390 train_time:127381ms step_avg:170.98ms
step:756/1390 train_time:127563ms step_avg:171.00ms
step:757/1390 train_time:127774ms step_avg:171.05ms
step:758/1390 train_time:127967ms step_avg:171.08ms
step:759/1390 train_time:128161ms step_avg:171.11ms
step:760/1390 train_time:128342ms step_avg:171.12ms
step:761/1390 train_time:128578ms step_avg:171.21ms
step:762/1390 train_time:128759ms step_avg:171.22ms
step:763/1390 train_time:128945ms step_avg:171.24ms
step:764/1390 train_time:129138ms step_avg:171.27ms
step:765/1390 train_time:129320ms step_avg:171.28ms
step:766/1390 train_time:129518ms step_avg:171.32ms
step:767/1390 train_time:129703ms step_avg:171.34ms
step:768/1390 train_time:129894ms step_avg:171.36ms
step:769/1390 train_time:130080ms step_avg:171.38ms
step:770/1390 train_time:130263ms step_avg:171.40ms
step:771/1390 train_time:130448ms step_avg:171.42ms
step:772/1390 train_time:130627ms step_avg:171.43ms
step:773/1390 train_time:130825ms step_avg:171.46ms
step:774/1390 train_time:131011ms step_avg:171.48ms
step:775/1390 train_time:131196ms step_avg:171.50ms
step:776/1390 train_time:131386ms step_avg:171.52ms
step:777/1390 train_time:131580ms step_avg:171.55ms
step:778/1390 train_time:131764ms step_avg:171.57ms
step:779/1390 train_time:131941ms step_avg:171.57ms
step:780/1390 train_time:132129ms step_avg:171.60ms
step:781/1390 train_time:132315ms step_avg:171.61ms
step:782/1390 train_time:132501ms step_avg:171.63ms
step:783/1390 train_time:132683ms step_avg:171.65ms
step:784/1390 train_time:132873ms step_avg:171.67ms
step:785/1390 train_time:133054ms step_avg:171.68ms
step:786/1390 train_time:133244ms step_avg:171.71ms
step:787/1390 train_time:133428ms step_avg:171.72ms
step:788/1390 train_time:133614ms step_avg:171.74ms
step:789/1390 train_time:133791ms step_avg:171.75ms
step:790/1390 train_time:133976ms step_avg:171.76ms
step:791/1390 train_time:134159ms step_avg:171.78ms
step:792/1390 train_time:134359ms step_avg:171.82ms
step:793/1390 train_time:134540ms step_avg:171.83ms
step:794/1390 train_time:134725ms step_avg:171.84ms
step:795/1390 train_time:134931ms step_avg:171.89ms
step:796/1390 train_time:135121ms step_avg:171.91ms
step:797/1390 train_time:135309ms step_avg:171.93ms
step:798/1390 train_time:135500ms step_avg:171.95ms
step:799/1390 train_time:135709ms step_avg:172.00ms
step:800/1390 train_time:135891ms step_avg:172.01ms
step:801/1390 train_time:136077ms step_avg:172.03ms
step:802/1390 train_time:136278ms step_avg:172.07ms
step:803/1390 train_time:136458ms step_avg:172.08ms
step:804/1390 train_time:136638ms step_avg:172.09ms
step:805/1390 train_time:136837ms step_avg:172.12ms
step:806/1390 train_time:137022ms step_avg:172.14ms
step:807/1390 train_time:137204ms step_avg:172.15ms
step:808/1390 train_time:137393ms step_avg:172.17ms
step:809/1390 train_time:137577ms step_avg:172.19ms
step:810/1390 train_time:137762ms step_avg:172.20ms
step:811/1390 train_time:137941ms step_avg:172.21ms
step:812/1390 train_time:138132ms step_avg:172.23ms
step:813/1390 train_time:138309ms step_avg:172.24ms
step:814/1390 train_time:138494ms step_avg:172.26ms
step:815/1390 train_time:138677ms step_avg:172.27ms
step:816/1390 train_time:138876ms step_avg:172.30ms
step:817/1390 train_time:139055ms step_avg:172.31ms
step:818/1390 train_time:139234ms step_avg:172.32ms
step:819/1390 train_time:139425ms step_avg:172.34ms
step:820/1390 train_time:139613ms step_avg:172.36ms
step:821/1390 train_time:139791ms step_avg:172.37ms
step:822/1390 train_time:139975ms step_avg:172.38ms
step:823/1390 train_time:140159ms step_avg:172.40ms
step:824/1390 train_time:140338ms step_avg:172.40ms
step:825/1390 train_time:140527ms step_avg:172.43ms
step:826/1390 train_time:140735ms step_avg:172.47ms
step:827/1390 train_time:140923ms step_avg:172.49ms
step:828/1390 train_time:141123ms step_avg:172.52ms
step:829/1390 train_time:141321ms step_avg:172.55ms
step:830/1390 train_time:141510ms step_avg:172.57ms
step:831/1390 train_time:141704ms step_avg:172.60ms
step:832/1390 train_time:141899ms step_avg:172.63ms
step:833/1390 train_time:142088ms step_avg:172.65ms
step:834/1390 train_time:142288ms step_avg:172.68ms
step:835/1390 train_time:142479ms step_avg:172.70ms
step:836/1390 train_time:142681ms step_avg:172.74ms
step:837/1390 train_time:142873ms step_avg:172.76ms
step:838/1390 train_time:143059ms step_avg:172.78ms
step:839/1390 train_time:143239ms step_avg:172.79ms
step:840/1390 train_time:143419ms step_avg:172.79ms
step:841/1390 train_time:143613ms step_avg:172.82ms
step:842/1390 train_time:143799ms step_avg:172.84ms
step:843/1390 train_time:143984ms step_avg:172.85ms
step:844/1390 train_time:144174ms step_avg:172.87ms
step:845/1390 train_time:144361ms step_avg:172.89ms
step:846/1390 train_time:144561ms step_avg:172.92ms
step:847/1390 train_time:144747ms step_avg:172.94ms
step:848/1390 train_time:144928ms step_avg:172.94ms
step:849/1390 train_time:145119ms step_avg:172.97ms
step:850/1390 train_time:145311ms step_avg:172.99ms
step:851/1390 train_time:145518ms step_avg:173.03ms
step:852/1390 train_time:145721ms step_avg:173.07ms
step:853/1390 train_time:145900ms step_avg:173.07ms
step:854/1390 train_time:146083ms step_avg:173.08ms
step:855/1390 train_time:146271ms step_avg:173.10ms
step:856/1390 train_time:146446ms step_avg:173.10ms
step:857/1390 train_time:146640ms step_avg:173.13ms
step:858/1390 train_time:146845ms step_avg:173.17ms
step:859/1390 train_time:147034ms step_avg:173.19ms
step:860/1390 train_time:147218ms step_avg:173.20ms
step:861/1390 train_time:147414ms step_avg:173.22ms
step:862/1390 train_time:147606ms step_avg:173.25ms
step:863/1390 train_time:147818ms step_avg:173.29ms
step:864/1390 train_time:148005ms step_avg:173.31ms
step:865/1390 train_time:148182ms step_avg:173.31ms
step:866/1390 train_time:148421ms step_avg:173.39ms
step:867/1390 train_time:148605ms step_avg:173.40ms
step:868/1390 train_time:148786ms step_avg:173.41ms
step:869/1390 train_time:148974ms step_avg:173.43ms
step:870/1390 train_time:149173ms step_avg:173.46ms
step:871/1390 train_time:149367ms step_avg:173.48ms
step:872/1390 train_time:149550ms step_avg:173.49ms
step:873/1390 train_time:149737ms step_avg:173.51ms
step:874/1390 train_time:149930ms step_avg:173.53ms
step:875/1390 train_time:150130ms step_avg:173.56ms
step:875/1390 val_loss:3.4596 train_time:150238ms step_avg:173.69ms
step:876/1390 train_time:150326ms step_avg:173.59ms
step:877/1390 train_time:150518ms step_avg:173.61ms
step:878/1390 train_time:150706ms step_avg:173.62ms
step:879/1390 train_time:150892ms step_avg:173.64ms
step:880/1390 train_time:151075ms step_avg:173.65ms
step:881/1390 train_time:151257ms step_avg:173.66ms
step:882/1390 train_time:151453ms step_avg:173.68ms
step:883/1390 train_time:151639ms step_avg:173.70ms
step:884/1390 train_time:151830ms step_avg:173.72ms
step:885/1390 train_time:152024ms step_avg:173.74ms
step:886/1390 train_time:152224ms step_avg:173.77ms
step:887/1390 train_time:152412ms step_avg:173.79ms
step:888/1390 train_time:152613ms step_avg:173.82ms
step:889/1390 train_time:152818ms step_avg:173.85ms
step:890/1390 train_time:153000ms step_avg:173.86ms
step:891/1390 train_time:153187ms step_avg:173.88ms
step:892/1390 train_time:153376ms step_avg:173.90ms
step:893/1390 train_time:153557ms step_avg:173.90ms
step:894/1390 train_time:153747ms step_avg:173.92ms
step:895/1390 train_time:153942ms step_avg:173.95ms
step:896/1390 train_time:154126ms step_avg:173.96ms
step:897/1390 train_time:154316ms step_avg:173.98ms
step:898/1390 train_time:154509ms step_avg:174.00ms
step:899/1390 train_time:154706ms step_avg:174.02ms
step:900/1390 train_time:154888ms step_avg:174.03ms
step:901/1390 train_time:155084ms step_avg:174.06ms
step:902/1390 train_time:155257ms step_avg:174.06ms
step:903/1390 train_time:155464ms step_avg:174.09ms
step:904/1390 train_time:155657ms step_avg:174.11ms
step:905/1390 train_time:155838ms step_avg:174.12ms
step:906/1390 train_time:156029ms step_avg:174.14ms
step:907/1390 train_time:156237ms step_avg:174.18ms
step:908/1390 train_time:156420ms step_avg:174.19ms
step:909/1390 train_time:156599ms step_avg:174.19ms
step:910/1390 train_time:156822ms step_avg:174.25ms
step:911/1390 train_time:157007ms step_avg:174.26ms
step:912/1390 train_time:157190ms step_avg:174.27ms
step:913/1390 train_time:157382ms step_avg:174.29ms
step:914/1390 train_time:157573ms step_avg:174.31ms
step:915/1390 train_time:157767ms step_avg:174.33ms
step:916/1390 train_time:157964ms step_avg:174.35ms
step:917/1390 train_time:158154ms step_avg:174.37ms
step:918/1390 train_time:158350ms step_avg:174.39ms
step:919/1390 train_time:158567ms step_avg:174.44ms
step:920/1390 train_time:158755ms step_avg:174.46ms
step:921/1390 train_time:158946ms step_avg:174.47ms
step:922/1390 train_time:159149ms step_avg:174.51ms
step:923/1390 train_time:159325ms step_avg:174.51ms
step:924/1390 train_time:159517ms step_avg:174.53ms
step:925/1390 train_time:159711ms step_avg:174.55ms
step:926/1390 train_time:159898ms step_avg:174.56ms
step:927/1390 train_time:160087ms step_avg:174.58ms
step:928/1390 train_time:160276ms step_avg:174.59ms
step:929/1390 train_time:160473ms step_avg:174.62ms
step:930/1390 train_time:160663ms step_avg:174.63ms
step:931/1390 train_time:160846ms step_avg:174.64ms
step:932/1390 train_time:161035ms step_avg:174.66ms
step:933/1390 train_time:161240ms step_avg:174.69ms
step:934/1390 train_time:161429ms step_avg:174.71ms
step:935/1390 train_time:161644ms step_avg:174.75ms
step:936/1390 train_time:161838ms step_avg:174.77ms
step:937/1390 train_time:162070ms step_avg:174.83ms
step:938/1390 train_time:162271ms step_avg:174.86ms
step:939/1390 train_time:162473ms step_avg:174.89ms
step:940/1390 train_time:162675ms step_avg:174.92ms
step:941/1390 train_time:162858ms step_avg:174.93ms
step:942/1390 train_time:163048ms step_avg:174.94ms
step:943/1390 train_time:163262ms step_avg:174.99ms
step:944/1390 train_time:163491ms step_avg:175.04ms
step:945/1390 train_time:163688ms step_avg:175.07ms
step:946/1390 train_time:163896ms step_avg:175.10ms
step:947/1390 train_time:164098ms step_avg:175.13ms
step:948/1390 train_time:164295ms step_avg:175.15ms
step:949/1390 train_time:164488ms step_avg:175.17ms
step:950/1390 train_time:164672ms step_avg:175.18ms
step:951/1390 train_time:164924ms step_avg:175.27ms
step:952/1390 train_time:165105ms step_avg:175.27ms
step:953/1390 train_time:165301ms step_avg:175.29ms
step:954/1390 train_time:165483ms step_avg:175.30ms
step:955/1390 train_time:165678ms step_avg:175.32ms
step:956/1390 train_time:165897ms step_avg:175.37ms
step:957/1390 train_time:166087ms step_avg:175.38ms
step:958/1390 train_time:166293ms step_avg:175.41ms
step:959/1390 train_time:166516ms step_avg:175.47ms
step:960/1390 train_time:166712ms step_avg:175.49ms
step:961/1390 train_time:166896ms step_avg:175.50ms
step:962/1390 train_time:167101ms step_avg:175.53ms
step:963/1390 train_time:167334ms step_avg:175.59ms
step:964/1390 train_time:167531ms step_avg:175.61ms
step:965/1390 train_time:167718ms step_avg:175.62ms
step:966/1390 train_time:167907ms step_avg:175.63ms
step:967/1390 train_time:168105ms step_avg:175.66ms
step:968/1390 train_time:168284ms step_avg:175.66ms
step:969/1390 train_time:168485ms step_avg:175.69ms
step:970/1390 train_time:168667ms step_avg:175.69ms
step:971/1390 train_time:168860ms step_avg:175.71ms
step:972/1390 train_time:169043ms step_avg:175.72ms
step:973/1390 train_time:169234ms step_avg:175.74ms
step:974/1390 train_time:169433ms step_avg:175.76ms
step:975/1390 train_time:169619ms step_avg:175.77ms
step:976/1390 train_time:169806ms step_avg:175.78ms
step:977/1390 train_time:169987ms step_avg:175.79ms
step:978/1390 train_time:170172ms step_avg:175.80ms
step:979/1390 train_time:170359ms step_avg:175.81ms
step:980/1390 train_time:170547ms step_avg:175.82ms
step:981/1390 train_time:170721ms step_avg:175.82ms
step:982/1390 train_time:170900ms step_avg:175.82ms
step:983/1390 train_time:171080ms step_avg:175.83ms
step:984/1390 train_time:171263ms step_avg:175.84ms
step:985/1390 train_time:171466ms step_avg:175.86ms
step:986/1390 train_time:171693ms step_avg:175.91ms
step:987/1390 train_time:171876ms step_avg:175.92ms
step:988/1390 train_time:172072ms step_avg:175.94ms
step:989/1390 train_time:172262ms step_avg:175.96ms
step:990/1390 train_time:172473ms step_avg:175.99ms
step:991/1390 train_time:172663ms step_avg:176.01ms
step:992/1390 train_time:172879ms step_avg:176.05ms
step:993/1390 train_time:173122ms step_avg:176.12ms
step:994/1390 train_time:173304ms step_avg:176.12ms
step:995/1390 train_time:173484ms step_avg:176.13ms
step:996/1390 train_time:173665ms step_avg:176.13ms
step:997/1390 train_time:173844ms step_avg:176.13ms
step:998/1390 train_time:174026ms step_avg:176.14ms
step:999/1390 train_time:174208ms step_avg:176.15ms
step:1000/1390 train_time:174391ms step_avg:176.15ms
step:1000/1390 val_loss:3.3977 train_time:174502ms step_avg:176.26ms
step:1001/1390 train_time:174589ms step_avg:176.17ms
step:1002/1390 train_time:174783ms step_avg:176.19ms
step:1003/1390 train_time:174998ms step_avg:176.23ms
step:1004/1390 train_time:175194ms step_avg:176.25ms
step:1005/1390 train_time:175392ms step_avg:176.27ms
step:1006/1390 train_time:175580ms step_avg:176.29ms
step:1007/1390 train_time:175772ms step_avg:176.30ms
step:1008/1390 train_time:175963ms step_avg:176.32ms
step:1009/1390 train_time:176187ms step_avg:176.36ms
step:1010/1390 train_time:176371ms step_avg:176.37ms
step:1011/1390 train_time:176569ms step_avg:176.39ms
step:1012/1390 train_time:176754ms step_avg:176.40ms
step:1013/1390 train_time:176957ms step_avg:176.43ms
step:1014/1390 train_time:177144ms step_avg:176.44ms
step:1015/1390 train_time:177338ms step_avg:176.46ms
step:1016/1390 train_time:177528ms step_avg:176.47ms
step:1017/1390 train_time:177730ms step_avg:176.49ms
step:1018/1390 train_time:177925ms step_avg:176.51ms
step:1019/1390 train_time:178124ms step_avg:176.54ms
step:1020/1390 train_time:178332ms step_avg:176.57ms
step:1021/1390 train_time:178513ms step_avg:176.57ms
step:1022/1390 train_time:178697ms step_avg:176.58ms
step:1023/1390 train_time:178899ms step_avg:176.60ms
step:1024/1390 train_time:179093ms step_avg:176.62ms
step:1025/1390 train_time:179293ms step_avg:176.64ms
step:1026/1390 train_time:179487ms step_avg:176.66ms
step:1027/1390 train_time:179679ms step_avg:176.68ms
step:1028/1390 train_time:179892ms step_avg:176.71ms
step:1029/1390 train_time:180107ms step_avg:176.75ms
step:1030/1390 train_time:180303ms step_avg:176.77ms
step:1031/1390 train_time:180483ms step_avg:176.77ms
step:1032/1390 train_time:180672ms step_avg:176.78ms
step:1033/1390 train_time:180855ms step_avg:176.79ms
step:1034/1390 train_time:181053ms step_avg:176.81ms
step:1035/1390 train_time:181255ms step_avg:176.83ms
step:1036/1390 train_time:181440ms step_avg:176.84ms
step:1037/1390 train_time:181657ms step_avg:176.88ms
step:1038/1390 train_time:181861ms step_avg:176.91ms
step:1039/1390 train_time:182044ms step_avg:176.91ms
step:1040/1390 train_time:182230ms step_avg:176.92ms
step:1041/1390 train_time:182428ms step_avg:176.94ms
step:1042/1390 train_time:182612ms step_avg:176.95ms
step:1043/1390 train_time:182806ms step_avg:176.97ms
step:1044/1390 train_time:183025ms step_avg:177.01ms
step:1045/1390 train_time:183228ms step_avg:177.03ms
step:1046/1390 train_time:183424ms step_avg:177.05ms
step:1047/1390 train_time:183613ms step_avg:177.06ms
step:1048/1390 train_time:183805ms step_avg:177.08ms
step:1049/1390 train_time:183991ms step_avg:177.08ms
step:1050/1390 train_time:184188ms step_avg:177.10ms
step:1051/1390 train_time:184394ms step_avg:177.13ms
step:1052/1390 train_time:184582ms step_avg:177.14ms
step:1053/1390 train_time:184766ms step_avg:177.15ms
step:1054/1390 train_time:184957ms step_avg:177.16ms
step:1055/1390 train_time:185146ms step_avg:177.17ms
step:1056/1390 train_time:185339ms step_avg:177.19ms
step:1057/1390 train_time:185533ms step_avg:177.20ms
step:1058/1390 train_time:185742ms step_avg:177.23ms
step:1059/1390 train_time:185954ms step_avg:177.27ms
step:1060/1390 train_time:186155ms step_avg:177.29ms
step:1061/1390 train_time:186334ms step_avg:177.29ms
step:1062/1390 train_time:186529ms step_avg:177.31ms
step:1063/1390 train_time:186714ms step_avg:177.32ms
step:1064/1390 train_time:186895ms step_avg:177.32ms
step:1065/1390 train_time:187090ms step_avg:177.34ms
step:1066/1390 train_time:187297ms step_avg:177.36ms
step:1067/1390 train_time:187499ms step_avg:177.39ms
step:1068/1390 train_time:187687ms step_avg:177.40ms
step:1069/1390 train_time:187908ms step_avg:177.44ms
step:1070/1390 train_time:188093ms step_avg:177.45ms
step:1071/1390 train_time:188316ms step_avg:177.49ms
step:1072/1390 train_time:188507ms step_avg:177.50ms
step:1073/1390 train_time:188685ms step_avg:177.50ms
step:1074/1390 train_time:188868ms step_avg:177.51ms
step:1075/1390 train_time:189082ms step_avg:177.54ms
step:1076/1390 train_time:189268ms step_avg:177.55ms
step:1077/1390 train_time:189453ms step_avg:177.56ms
step:1078/1390 train_time:189661ms step_avg:177.59ms
step:1079/1390 train_time:189884ms step_avg:177.63ms
step:1080/1390 train_time:190078ms step_avg:177.64ms
step:1081/1390 train_time:190270ms step_avg:177.66ms
step:1082/1390 train_time:190453ms step_avg:177.66ms
step:1083/1390 train_time:190640ms step_avg:177.67ms
step:1084/1390 train_time:190871ms step_avg:177.72ms
step:1085/1390 train_time:191062ms step_avg:177.73ms
step:1086/1390 train_time:191254ms step_avg:177.74ms
step:1087/1390 train_time:191450ms step_avg:177.76ms
step:1088/1390 train_time:191645ms step_avg:177.78ms
step:1089/1390 train_time:191862ms step_avg:177.81ms
step:1090/1390 train_time:192082ms step_avg:177.85ms
step:1091/1390 train_time:192273ms step_avg:177.87ms
step:1092/1390 train_time:192465ms step_avg:177.88ms
step:1093/1390 train_time:192658ms step_avg:177.89ms
step:1094/1390 train_time:192851ms step_avg:177.91ms
step:1095/1390 train_time:193037ms step_avg:177.91ms
step:1096/1390 train_time:193243ms step_avg:177.94ms
step:1097/1390 train_time:193447ms step_avg:177.96ms
step:1098/1390 train_time:193656ms step_avg:177.99ms
step:1099/1390 train_time:193854ms step_avg:178.01ms
step:1100/1390 train_time:194045ms step_avg:178.02ms
step:1101/1390 train_time:194235ms step_avg:178.03ms
step:1102/1390 train_time:194439ms step_avg:178.06ms
step:1103/1390 train_time:194637ms step_avg:178.08ms
step:1104/1390 train_time:194824ms step_avg:178.08ms
step:1105/1390 train_time:195045ms step_avg:178.12ms
step:1106/1390 train_time:195244ms step_avg:178.14ms
step:1107/1390 train_time:195434ms step_avg:178.15ms
step:1108/1390 train_time:195667ms step_avg:178.20ms
step:1109/1390 train_time:195857ms step_avg:178.21ms
step:1110/1390 train_time:196053ms step_avg:178.23ms
step:1111/1390 train_time:196242ms step_avg:178.24ms
step:1112/1390 train_time:196430ms step_avg:178.25ms
step:1113/1390 train_time:196612ms step_avg:178.25ms
step:1114/1390 train_time:196823ms step_avg:178.28ms
step:1115/1390 train_time:197032ms step_avg:178.31ms
step:1116/1390 train_time:197219ms step_avg:178.32ms
step:1117/1390 train_time:197418ms step_avg:178.34ms
step:1118/1390 train_time:197654ms step_avg:178.39ms
step:1119/1390 train_time:197844ms step_avg:178.40ms
step:1120/1390 train_time:198032ms step_avg:178.41ms
step:1121/1390 train_time:198223ms step_avg:178.42ms
step:1122/1390 train_time:198413ms step_avg:178.43ms
step:1123/1390 train_time:198601ms step_avg:178.44ms
step:1124/1390 train_time:198802ms step_avg:178.46ms
step:1125/1390 train_time:198992ms step_avg:178.47ms
step:1125/1390 val_loss:3.3471 train_time:199117ms step_avg:178.58ms
step:1126/1390 train_time:199203ms step_avg:178.50ms
step:1127/1390 train_time:199393ms step_avg:178.51ms
step:1128/1390 train_time:199588ms step_avg:178.52ms
step:1129/1390 train_time:199805ms step_avg:178.56ms
step:1130/1390 train_time:199990ms step_avg:178.56ms
step:1131/1390 train_time:200199ms step_avg:178.59ms
step:1132/1390 train_time:200387ms step_avg:178.60ms
step:1133/1390 train_time:200583ms step_avg:178.61ms
step:1134/1390 train_time:200783ms step_avg:178.63ms
step:1135/1390 train_time:200981ms step_avg:178.65ms
step:1136/1390 train_time:201228ms step_avg:178.71ms
step:1137/1390 train_time:201419ms step_avg:178.72ms
step:1138/1390 train_time:201637ms step_avg:178.76ms
step:1139/1390 train_time:201838ms step_avg:178.78ms
step:1140/1390 train_time:202032ms step_avg:178.79ms
step:1141/1390 train_time:202278ms step_avg:178.85ms
step:1142/1390 train_time:202468ms step_avg:178.86ms
step:1143/1390 train_time:202680ms step_avg:178.89ms
step:1144/1390 train_time:202876ms step_avg:178.90ms
step:1145/1390 train_time:203067ms step_avg:178.91ms
step:1146/1390 train_time:203268ms step_avg:178.93ms
step:1147/1390 train_time:203474ms step_avg:178.96ms
step:1148/1390 train_time:203665ms step_avg:178.97ms
step:1149/1390 train_time:203868ms step_avg:178.99ms
step:1150/1390 train_time:204058ms step_avg:179.00ms
step:1151/1390 train_time:204271ms step_avg:179.03ms
step:1152/1390 train_time:204469ms step_avg:179.04ms
step:1153/1390 train_time:204682ms step_avg:179.07ms
step:1154/1390 train_time:204871ms step_avg:179.08ms
step:1155/1390 train_time:205069ms step_avg:179.10ms
step:1156/1390 train_time:205309ms step_avg:179.15ms
step:1157/1390 train_time:205509ms step_avg:179.17ms
step:1158/1390 train_time:205698ms step_avg:179.18ms
step:1159/1390 train_time:205885ms step_avg:179.19ms
step:1160/1390 train_time:206071ms step_avg:179.19ms
step:1161/1390 train_time:206269ms step_avg:179.21ms
step:1162/1390 train_time:206471ms step_avg:179.23ms
step:1163/1390 train_time:206671ms step_avg:179.25ms
step:1164/1390 train_time:206871ms step_avg:179.26ms
step:1165/1390 train_time:207061ms step_avg:179.27ms
step:1166/1390 train_time:207261ms step_avg:179.29ms
step:1167/1390 train_time:207447ms step_avg:179.30ms
step:1168/1390 train_time:207657ms step_avg:179.32ms
step:1169/1390 train_time:207843ms step_avg:179.33ms
step:1170/1390 train_time:208034ms step_avg:179.34ms
step:1171/1390 train_time:208228ms step_avg:179.35ms
step:1172/1390 train_time:208436ms step_avg:179.38ms
step:1173/1390 train_time:208633ms step_avg:179.39ms
step:1174/1390 train_time:208890ms step_avg:179.46ms
step:1175/1390 train_time:209089ms step_avg:179.48ms
step:1176/1390 train_time:209294ms step_avg:179.50ms
step:1177/1390 train_time:209534ms step_avg:179.55ms
step:1178/1390 train_time:209727ms step_avg:179.56ms
step:1179/1390 train_time:209909ms step_avg:179.56ms
step:1180/1390 train_time:210143ms step_avg:179.61ms
step:1181/1390 train_time:210341ms step_avg:179.63ms
step:1182/1390 train_time:210530ms step_avg:179.63ms
step:1183/1390 train_time:210736ms step_avg:179.66ms
step:1184/1390 train_time:210934ms step_avg:179.67ms
step:1185/1390 train_time:211144ms step_avg:179.70ms
step:1186/1390 train_time:211333ms step_avg:179.70ms
step:1187/1390 train_time:211599ms step_avg:179.78ms
step:1188/1390 train_time:211782ms step_avg:179.78ms
step:1189/1390 train_time:211994ms step_avg:179.81ms
step:1190/1390 train_time:212183ms step_avg:179.82ms
step:1191/1390 train_time:212393ms step_avg:179.84ms
step:1192/1390 train_time:212581ms step_avg:179.85ms
step:1193/1390 train_time:212769ms step_avg:179.86ms
step:1194/1390 train_time:212958ms step_avg:179.86ms
step:1195/1390 train_time:213172ms step_avg:179.89ms
step:1196/1390 train_time:213363ms step_avg:179.90ms
step:1197/1390 train_time:213569ms step_avg:179.92ms
step:1198/1390 train_time:213814ms step_avg:179.98ms
step:1199/1390 train_time:214007ms step_avg:179.99ms
step:1200/1390 train_time:214193ms step_avg:179.99ms
step:1201/1390 train_time:214382ms step_avg:180.00ms
step:1202/1390 train_time:214667ms step_avg:180.09ms
step:1203/1390 train_time:214893ms step_avg:180.13ms
step:1204/1390 train_time:215112ms step_avg:180.16ms
step:1205/1390 train_time:215318ms step_avg:180.18ms
step:1206/1390 train_time:215523ms step_avg:180.20ms
step:1207/1390 train_time:215721ms step_avg:180.22ms
step:1208/1390 train_time:215944ms step_avg:180.25ms
step:1209/1390 train_time:216137ms step_avg:180.26ms
step:1210/1390 train_time:216354ms step_avg:180.30ms
step:1211/1390 train_time:216557ms step_avg:180.31ms
step:1212/1390 train_time:216748ms step_avg:180.32ms
step:1213/1390 train_time:216942ms step_avg:180.33ms
step:1214/1390 train_time:217148ms step_avg:180.36ms
step:1215/1390 train_time:217360ms step_avg:180.38ms
step:1216/1390 train_time:217541ms step_avg:180.38ms
step:1217/1390 train_time:217752ms step_avg:180.41ms
step:1218/1390 train_time:217934ms step_avg:180.41ms
step:1219/1390 train_time:218120ms step_avg:180.41ms
step:1220/1390 train_time:218314ms step_avg:180.43ms
step:1221/1390 train_time:218504ms step_avg:180.43ms
step:1222/1390 train_time:218692ms step_avg:180.44ms
step:1223/1390 train_time:218882ms step_avg:180.45ms
step:1224/1390 train_time:219095ms step_avg:180.47ms
step:1225/1390 train_time:219311ms step_avg:180.50ms
step:1226/1390 train_time:219507ms step_avg:180.52ms
step:1227/1390 train_time:219709ms step_avg:180.53ms
step:1228/1390 train_time:219897ms step_avg:180.54ms
step:1229/1390 train_time:220088ms step_avg:180.55ms
step:1230/1390 train_time:220315ms step_avg:180.59ms
step:1231/1390 train_time:220522ms step_avg:180.61ms
step:1232/1390 train_time:220734ms step_avg:180.63ms
step:1233/1390 train_time:220933ms step_avg:180.65ms
step:1234/1390 train_time:221122ms step_avg:180.66ms
step:1235/1390 train_time:221315ms step_avg:180.67ms
step:1236/1390 train_time:221510ms step_avg:180.68ms
step:1237/1390 train_time:221691ms step_avg:180.68ms
step:1238/1390 train_time:221955ms step_avg:180.75ms
step:1239/1390 train_time:222149ms step_avg:180.76ms
step:1240/1390 train_time:222366ms step_avg:180.79ms
step:1241/1390 train_time:222579ms step_avg:180.81ms
step:1242/1390 train_time:222768ms step_avg:180.82ms
step:1243/1390 train_time:222985ms step_avg:180.85ms
step:1244/1390 train_time:223186ms step_avg:180.86ms
step:1245/1390 train_time:223371ms step_avg:180.87ms
step:1246/1390 train_time:223552ms step_avg:180.87ms
step:1247/1390 train_time:223762ms step_avg:180.89ms
step:1248/1390 train_time:223943ms step_avg:180.89ms
step:1249/1390 train_time:224122ms step_avg:180.89ms
step:1250/1390 train_time:224314ms step_avg:180.90ms
step:1250/1390 val_loss:3.3018 train_time:224430ms step_avg:180.99ms
step:1251/1390 train_time:224523ms step_avg:180.92ms
step:1252/1390 train_time:224725ms step_avg:180.94ms
step:1253/1390 train_time:224906ms step_avg:180.94ms
step:1254/1390 train_time:225093ms step_avg:180.94ms
step:1255/1390 train_time:225367ms step_avg:181.02ms
step:1256/1390 train_time:225564ms step_avg:181.03ms
step:1257/1390 train_time:225753ms step_avg:181.04ms
step:1258/1390 train_time:225970ms step_avg:181.07ms
step:1259/1390 train_time:226172ms step_avg:181.08ms
step:1260/1390 train_time:226351ms step_avg:181.08ms
step:1261/1390 train_time:226546ms step_avg:181.09ms
step:1262/1390 train_time:226758ms step_avg:181.12ms
step:1263/1390 train_time:226958ms step_avg:181.13ms
step:1264/1390 train_time:227150ms step_avg:181.14ms
step:1265/1390 train_time:227337ms step_avg:181.15ms
step:1266/1390 train_time:227535ms step_avg:181.16ms
step:1267/1390 train_time:227739ms step_avg:181.18ms
step:1268/1390 train_time:227934ms step_avg:181.19ms
step:1269/1390 train_time:228164ms step_avg:181.23ms
step:1270/1390 train_time:228355ms step_avg:181.23ms
step:1271/1390 train_time:228554ms step_avg:181.25ms
step:1272/1390 train_time:228737ms step_avg:181.25ms
step:1273/1390 train_time:228923ms step_avg:181.25ms
step:1274/1390 train_time:229109ms step_avg:181.26ms
step:1275/1390 train_time:229313ms step_avg:181.27ms
step:1276/1390 train_time:229496ms step_avg:181.28ms
step:1277/1390 train_time:229689ms step_avg:181.29ms
step:1278/1390 train_time:229884ms step_avg:181.30ms
step:1279/1390 train_time:230079ms step_avg:181.31ms
step:1280/1390 train_time:230319ms step_avg:181.35ms
step:1281/1390 train_time:230518ms step_avg:181.37ms
step:1282/1390 train_time:230700ms step_avg:181.37ms
step:1283/1390 train_time:230893ms step_avg:181.38ms
step:1284/1390 train_time:231098ms step_avg:181.40ms
step:1285/1390 train_time:231292ms step_avg:181.41ms
step:1286/1390 train_time:231490ms step_avg:181.42ms
step:1287/1390 train_time:231691ms step_avg:181.43ms
step:1288/1390 train_time:231874ms step_avg:181.44ms
step:1289/1390 train_time:232126ms step_avg:181.49ms
step:1290/1390 train_time:232351ms step_avg:181.52ms
step:1291/1390 train_time:232571ms step_avg:181.55ms
step:1292/1390 train_time:232768ms step_avg:181.57ms
step:1293/1390 train_time:232996ms step_avg:181.60ms
step:1294/1390 train_time:233195ms step_avg:181.62ms
step:1295/1390 train_time:233393ms step_avg:181.63ms
step:1296/1390 train_time:233607ms step_avg:181.65ms
step:1297/1390 train_time:233807ms step_avg:181.67ms
step:1298/1390 train_time:233993ms step_avg:181.67ms
step:1299/1390 train_time:234186ms step_avg:181.68ms
step:1300/1390 train_time:234371ms step_avg:181.68ms
step:1301/1390 train_time:234554ms step_avg:181.68ms
step:1302/1390 train_time:234753ms step_avg:181.70ms
step:1303/1390 train_time:234970ms step_avg:181.72ms
step:1304/1390 train_time:235180ms step_avg:181.75ms
step:1305/1390 train_time:235370ms step_avg:181.75ms
step:1306/1390 train_time:235574ms step_avg:181.77ms
step:1307/1390 train_time:235770ms step_avg:181.78ms
step:1308/1390 train_time:235990ms step_avg:181.81ms
step:1309/1390 train_time:236199ms step_avg:181.83ms
step:1310/1390 train_time:236385ms step_avg:181.83ms
step:1311/1390 train_time:236574ms step_avg:181.84ms
step:1312/1390 train_time:236758ms step_avg:181.84ms
step:1313/1390 train_time:236959ms step_avg:181.86ms
step:1314/1390 train_time:237155ms step_avg:181.87ms
step:1315/1390 train_time:237359ms step_avg:181.88ms
step:1316/1390 train_time:237548ms step_avg:181.89ms
step:1317/1390 train_time:237747ms step_avg:181.90ms
step:1318/1390 train_time:237989ms step_avg:181.95ms
step:1319/1390 train_time:238195ms step_avg:181.97ms
step:1320/1390 train_time:238388ms step_avg:181.98ms
step:1321/1390 train_time:238580ms step_avg:181.98ms
step:1322/1390 train_time:238821ms step_avg:182.03ms
step:1323/1390 train_time:239011ms step_avg:182.03ms
step:1324/1390 train_time:239214ms step_avg:182.05ms
step:1325/1390 train_time:239429ms step_avg:182.08ms
step:1326/1390 train_time:239647ms step_avg:182.10ms
step:1327/1390 train_time:239834ms step_avg:182.11ms
step:1328/1390 train_time:240014ms step_avg:182.10ms
step:1329/1390 train_time:240298ms step_avg:182.18ms
step:1330/1390 train_time:240513ms step_avg:182.21ms
step:1331/1390 train_time:240792ms step_avg:182.28ms
step:1332/1390 train_time:241034ms step_avg:182.33ms
step:1333/1390 train_time:241242ms step_avg:182.34ms
step:1334/1390 train_time:241433ms step_avg:182.35ms
step:1335/1390 train_time:241613ms step_avg:182.35ms
step:1336/1390 train_time:241860ms step_avg:182.40ms
step:1337/1390 train_time:242067ms step_avg:182.42ms
step:1338/1390 train_time:242261ms step_avg:182.43ms
step:1339/1390 train_time:242459ms step_avg:182.44ms
step:1340/1390 train_time:242683ms step_avg:182.47ms
step:1341/1390 train_time:242885ms step_avg:182.48ms
step:1342/1390 train_time:243083ms step_avg:182.50ms
step:1343/1390 train_time:243285ms step_avg:182.51ms
step:1344/1390 train_time:243481ms step_avg:182.52ms
step:1345/1390 train_time:243687ms step_avg:182.54ms
step:1346/1390 train_time:243881ms step_avg:182.55ms
step:1347/1390 train_time:244085ms step_avg:182.56ms
step:1348/1390 train_time:244272ms step_avg:182.57ms
step:1349/1390 train_time:244464ms step_avg:182.57ms
step:1350/1390 train_time:244647ms step_avg:182.57ms
step:1351/1390 train_time:244840ms step_avg:182.58ms
step:1352/1390 train_time:245089ms step_avg:182.63ms
step:1353/1390 train_time:245301ms step_avg:182.65ms
step:1354/1390 train_time:245504ms step_avg:182.67ms
step:1355/1390 train_time:245694ms step_avg:182.67ms
step:1356/1390 train_time:245882ms step_avg:182.68ms
step:1357/1390 train_time:246094ms step_avg:182.70ms
step:1358/1390 train_time:246315ms step_avg:182.73ms
step:1359/1390 train_time:246506ms step_avg:182.73ms
step:1360/1390 train_time:246718ms step_avg:182.75ms
step:1361/1390 train_time:246930ms step_avg:182.78ms
step:1362/1390 train_time:247142ms step_avg:182.80ms
step:1363/1390 train_time:247367ms step_avg:182.83ms
step:1364/1390 train_time:247563ms step_avg:182.84ms
step:1365/1390 train_time:247748ms step_avg:182.84ms
step:1366/1390 train_time:247945ms step_avg:182.85ms
step:1367/1390 train_time:248144ms step_avg:182.86ms
step:1368/1390 train_time:248349ms step_avg:182.88ms
step:1369/1390 train_time:248578ms step_avg:182.91ms
step:1370/1390 train_time:248810ms step_avg:182.95ms
step:1371/1390 train_time:249014ms step_avg:182.96ms
step:1372/1390 train_time:249253ms step_avg:183.01ms
step:1373/1390 train_time:249447ms step_avg:183.01ms
step:1374/1390 train_time:249674ms step_avg:183.05ms
step:1375/1390 train_time:249869ms step_avg:183.05ms
step:1375/1390 val_loss:3.2744 train_time:249964ms step_avg:183.12ms
step:1376/1390 train_time:250052ms step_avg:183.05ms
step:1377/1390 train_time:250256ms step_avg:183.07ms
step:1378/1390 train_time:250454ms step_avg:183.08ms
step:1379/1390 train_time:250653ms step_avg:183.09ms
step:1380/1390 train_time:250852ms step_avg:183.10ms
step:1381/1390 train_time:251083ms step_avg:183.14ms
step:1382/1390 train_time:251282ms step_avg:183.15ms
step:1383/1390 train_time:251470ms step_avg:183.15ms
step:1384/1390 train_time:251711ms step_avg:183.20ms
step:1385/1390 train_time:251889ms step_avg:183.19ms
step:1386/1390 train_time:252080ms step_avg:183.20ms
step:1387/1390 train_time:252281ms step_avg:183.21ms
step:1388/1390 train_time:252468ms step_avg:183.21ms
step:1389/1390 train_time:252662ms step_avg:183.22ms
step:1390/1390 train_time:252853ms step_avg:183.23ms
step:1390/1390 val_loss:3.2737 train_time:252965ms step_avg:183.31ms
peak memory consumption: 31559 MiB
