{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNousResearch/Hermes-2-Theta-Llama-3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:177\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, mm_processor_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    154\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    155\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    156\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    176\u001b[0m )\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:571\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m engine_config \u001b[38;5;241m=\u001b[39m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py:900\u001b[0m, in \u001b[0;36mEngineArgs.create_engine_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_offload_gb \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU offload space must be non-negative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_offload_gb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    899\u001b[0m device_config \u001b[38;5;241m=\u001b[39m DeviceConfig(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 900\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mis_multimodal_model:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_prefix_caching:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py:837\u001b[0m, in \u001b[0;36mEngineArgs.create_model_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_config\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelConfig:\n\u001b[0;32m--> 837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModelConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_param_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization_param_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce_eager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_context_len_to_capture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_context_len_to_capture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len_to_capture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len_to_capture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_sliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_sliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_tokenizer_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_tokenizer_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserved_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserved_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit_mm_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit_mm_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_async_output_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_async_output_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_neuron_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride_neuron_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/config.py:166\u001b[0m, in \u001b[0;36mModelConfig.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, dtype, seed, revision, code_revision, rope_scaling, rope_theta, tokenizer_revision, max_model_len, spec_target_max_model_len, quantization, quantization_param_path, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, max_logprobs, disable_sliding_window, skip_tokenizer_init, served_model_name, limit_mm_per_prompt, use_async_output_proc, override_neuron_config, config_format, mm_processor_kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_config \u001b[38;5;241m=\u001b[39m get_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, trust_remote_code, revision,\n\u001b[1;32m    163\u001b[0m                             code_revision, rope_scaling, rope_theta,\n\u001b[1;32m    164\u001b[0m                             config_format)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_text_config \u001b[38;5;241m=\u001b[39m get_hf_text_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_config)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_image_processor_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_image_processor_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m _get_and_verify_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_text_config, dtype)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_async_output_proc \u001b[38;5;241m=\u001b[39m use_async_output_proc\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py:269\u001b[0m, in \u001b[0;36mget_hf_image_processor_config\u001b[0;34m(model, revision, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_gguf_file(model):\n\u001b[1;32m    268\u001b[0m     model \u001b[38;5;241m=\u001b[39m Path(model)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_image_processor_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:278\u001b[0m, in \u001b[0;36mget_image_processor_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token` and `use_auth_token` are both specified. Please set only the argument `token`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    276\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m--> 278\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mget_file_from_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mIMAGE_PROCESSOR_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the image processor configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:557\u001b[0m, in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token` and `use_auth_token` are both specified. Please set only the argument `token`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    555\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1295\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1295\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1746\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1746\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1751\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1666\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1663\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1666\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1675\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:364\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 364\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:387\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it, est. speed input: 113.36 toks/s, output: 70.54 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "prompt = \"\"\"\n",
    "On a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\n",
    "\n",
    "They assembled 3 decks of cards, each for a separate type of information composed of the following:\n",
    "\n",
    "Suspect:\n",
    "- Miss Scarlet\n",
    "- Mr. Green\n",
    "- Mrs. White\n",
    "\n",
    "Weapon:\n",
    "- Candlestick\n",
    "- Knife\n",
    "- Lead Pipe\n",
    "\n",
    "Room:\n",
    "- Hall\n",
    "- Lounge\n",
    "- Dining Room\n",
    "\n",
    "After randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\n",
    "\n",
    "- Summer: 2 cards\n",
    "- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\n",
    "- Connor: 2 cards\n",
    "\n",
    "The game proceeded as follows:\n",
    "\n",
    "1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\n",
    "2. The player directed this question to the other players in clockwise order, starting with the player to their left.\n",
    "3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\n",
    "4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\n",
    "5. This continued until either:\n",
    "    a) A player showed a card to the asking player, or\n",
    "    b) All the queried players had stated they didn't have any of the asked-about cards.\n",
    "6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\n",
    "\n",
    "Here is how the game played out:\n",
    "\n",
    "Summer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\n",
    "- Giselle did not have any of the cards\n",
    "- Connor showed Summer a card\n",
    "\n",
    "Giselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\n",
    "- Connor did not have any of the cards\n",
    "- Summer did not have any of the cards\n",
    "\n",
    "Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\n",
    "- Summer did not have any of the cards\n",
    "- Giselle showed Connor 'Miss Scarlet'\n",
    "\n",
    "Summer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\n",
    "- Giselle did not have any of the cards\n",
    "- Connor did not have any of the cards\n",
    "\n",
    "At this point, Giselle was able to correctly infer the solution and win the game.\n",
    "\n",
    "What were the facedown cards in the middle of the table?\n",
    "\"\"\".strip()\n",
    "\n",
    "output = llm.chat([[dict(role=\"user\", content=prompt)]] * 1, sampling_params=SamplingParams(max_tokens=10_000))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm.model_executor.models.llama import LlamaForCausalLM\n",
    "\n",
    "# Get the model\n",
    "vllm_model: LlamaForCausalLM = llm.llm_engine.model_executor.driver_worker.model_runner.model  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 11.01it/s, est. speed input: 6449.47 toks/s, output: 11.02 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.7812,  9.0000, 13.4375,  ..., -2.9688, -2.9688, -2.9688],\n",
      "        [ 6.7812,  9.0000, 13.4375,  ..., -2.9688, -2.9688, -2.9688],\n",
      "        [ 6.1562,  4.0000,  4.1875,  ..., -3.1719, -3.1719, -3.1719],\n",
      "        ...,\n",
      "        [ 2.4688,  5.0000,  5.4688,  ...,  1.9453,  1.9453,  1.9453],\n",
      "        [ 3.0312,  7.2500,  5.5312,  ...,  3.2188,  3.2188,  3.2188],\n",
      "        [ 6.2500,  9.6875,  9.0000,  ..., -1.7109, -1.7109, -1.7109]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compute_logits = vllm_model.compute_logits\n",
    "\n",
    "logprobs = None\n",
    "\n",
    "def hook(*args, **kwargs):\n",
    "    global logprobs\n",
    "    output = compute_logits(*args, **kwargs)\n",
    "    if output.shape[0] > 1:\n",
    "        logprobs = output\n",
    "        print(logprobs)\n",
    "    return output\n",
    "\n",
    "vllm_model.compute_logits = hook\n",
    "\n",
    "vllm_model.train()\n",
    "output = llm.chat([[dict(role=\"user\", content=prompt)]] * 1, sampling_params=SamplingParams(max_tokens=1, prompt_logprobs=True))  # type: ignore\n",
    "\n",
    "vllm_model.compute_logits = compute_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "logprobs.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5938,  7.2812,  4.3750,  ..., -3.0781, -3.0781, -3.0781]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "\n",
    "model = llama3_1_8b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight: (128256, 4096)\n",
      "layers.0.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.0.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.0.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.0.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.0.input_layernorm.weight: (4096,)\n",
      "layers.0.post_attention_layernorm.weight: (4096,)\n",
      "layers.1.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.1.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.1.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.1.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.1.input_layernorm.weight: (4096,)\n",
      "layers.1.post_attention_layernorm.weight: (4096,)\n",
      "layers.2.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.2.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.2.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.2.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.2.input_layernorm.weight: (4096,)\n",
      "layers.2.post_attention_layernorm.weight: (4096,)\n",
      "layers.3.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.3.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.3.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.3.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.3.input_layernorm.weight: (4096,)\n",
      "layers.3.post_attention_layernorm.weight: (4096,)\n",
      "layers.4.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.4.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.4.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.4.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.4.input_layernorm.weight: (4096,)\n",
      "layers.4.post_attention_layernorm.weight: (4096,)\n",
      "layers.5.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.5.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.5.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.5.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.5.input_layernorm.weight: (4096,)\n",
      "layers.5.post_attention_layernorm.weight: (4096,)\n",
      "layers.6.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.6.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.6.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.6.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.6.input_layernorm.weight: (4096,)\n",
      "layers.6.post_attention_layernorm.weight: (4096,)\n",
      "layers.7.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.7.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.7.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.7.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.7.input_layernorm.weight: (4096,)\n",
      "layers.7.post_attention_layernorm.weight: (4096,)\n",
      "layers.8.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.8.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.8.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.8.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.8.input_layernorm.weight: (4096,)\n",
      "layers.8.post_attention_layernorm.weight: (4096,)\n",
      "layers.9.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.9.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.9.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.9.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.9.input_layernorm.weight: (4096,)\n",
      "layers.9.post_attention_layernorm.weight: (4096,)\n",
      "layers.10.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.10.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.10.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.10.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.10.input_layernorm.weight: (4096,)\n",
      "layers.10.post_attention_layernorm.weight: (4096,)\n",
      "layers.11.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.11.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.11.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.11.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.11.input_layernorm.weight: (4096,)\n",
      "layers.11.post_attention_layernorm.weight: (4096,)\n",
      "layers.12.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.12.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.12.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.12.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.12.input_layernorm.weight: (4096,)\n",
      "layers.12.post_attention_layernorm.weight: (4096,)\n",
      "layers.13.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.13.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.13.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.13.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.13.input_layernorm.weight: (4096,)\n",
      "layers.13.post_attention_layernorm.weight: (4096,)\n",
      "layers.14.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.14.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.14.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.14.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.14.input_layernorm.weight: (4096,)\n",
      "layers.14.post_attention_layernorm.weight: (4096,)\n",
      "layers.15.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.15.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.15.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.15.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.15.input_layernorm.weight: (4096,)\n",
      "layers.15.post_attention_layernorm.weight: (4096,)\n",
      "layers.16.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.16.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.16.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.16.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.16.input_layernorm.weight: (4096,)\n",
      "layers.16.post_attention_layernorm.weight: (4096,)\n",
      "layers.17.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.17.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.17.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.17.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.17.input_layernorm.weight: (4096,)\n",
      "layers.17.post_attention_layernorm.weight: (4096,)\n",
      "layers.18.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.18.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.18.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.18.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.18.input_layernorm.weight: (4096,)\n",
      "layers.18.post_attention_layernorm.weight: (4096,)\n",
      "layers.19.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.19.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.19.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.19.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.19.input_layernorm.weight: (4096,)\n",
      "layers.19.post_attention_layernorm.weight: (4096,)\n",
      "layers.20.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.20.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.20.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.20.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.20.input_layernorm.weight: (4096,)\n",
      "layers.20.post_attention_layernorm.weight: (4096,)\n",
      "layers.21.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.21.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.21.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.21.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.21.input_layernorm.weight: (4096,)\n",
      "layers.21.post_attention_layernorm.weight: (4096,)\n",
      "layers.22.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.22.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.22.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.22.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.22.input_layernorm.weight: (4096,)\n",
      "layers.22.post_attention_layernorm.weight: (4096,)\n",
      "layers.23.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.23.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.23.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.23.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.23.input_layernorm.weight: (4096,)\n",
      "layers.23.post_attention_layernorm.weight: (4096,)\n",
      "layers.24.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.24.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.24.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.24.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.24.input_layernorm.weight: (4096,)\n",
      "layers.24.post_attention_layernorm.weight: (4096,)\n",
      "layers.25.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.25.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.25.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.25.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.25.input_layernorm.weight: (4096,)\n",
      "layers.25.post_attention_layernorm.weight: (4096,)\n",
      "layers.26.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.26.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.26.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.26.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.26.input_layernorm.weight: (4096,)\n",
      "layers.26.post_attention_layernorm.weight: (4096,)\n",
      "layers.27.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.27.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.27.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.27.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.27.input_layernorm.weight: (4096,)\n",
      "layers.27.post_attention_layernorm.weight: (4096,)\n",
      "layers.28.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.28.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.28.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.28.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.28.input_layernorm.weight: (4096,)\n",
      "layers.28.post_attention_layernorm.weight: (4096,)\n",
      "layers.29.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.29.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.29.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.29.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.29.input_layernorm.weight: (4096,)\n",
      "layers.29.post_attention_layernorm.weight: (4096,)\n",
      "layers.30.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.30.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.30.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.30.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.30.input_layernorm.weight: (4096,)\n",
      "layers.30.post_attention_layernorm.weight: (4096,)\n",
      "layers.31.self_attn.qkv_proj.weight: (6144, 4096)\n",
      "layers.31.self_attn.o_proj.weight: (4096, 4096)\n",
      "layers.31.mlp.gate_up_proj.weight: (28672, 4096)\n",
      "layers.31.mlp.down_proj.weight: (4096, 14336)\n",
      "layers.31.input_layernorm.weight: (4096,)\n",
      "layers.31.post_attention_layernorm.weight: (4096,)\n",
      "norm.weight: (4096,)\n"
     ]
    }
   ],
   "source": [
    "vllm_state_dict = vllm_model.state_dict()\n",
    "\n",
    "for key in vllm_state_dict:\n",
    "    print(key + \":\", tuple(vllm_state_dict[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight: (128256, 4096)\n",
      "layers.0.attn.q_proj.weight: (4096, 4096)\n",
      "layers.0.attn.k_proj.weight: (1024, 4096)\n",
      "layers.0.attn.v_proj.weight: (1024, 4096)\n",
      "layers.0.attn.output_proj.weight: (4096, 4096)\n",
      "layers.0.mlp.w1.weight: (14336, 4096)\n",
      "layers.0.mlp.w2.weight: (4096, 14336)\n",
      "layers.0.mlp.w3.weight: (14336, 4096)\n",
      "layers.0.sa_norm.scale: (4096,)\n",
      "layers.0.mlp_norm.scale: (4096,)\n",
      "layers.1.attn.q_proj.weight: (4096, 4096)\n",
      "layers.1.attn.k_proj.weight: (1024, 4096)\n",
      "layers.1.attn.v_proj.weight: (1024, 4096)\n",
      "layers.1.attn.output_proj.weight: (4096, 4096)\n",
      "layers.1.mlp.w1.weight: (14336, 4096)\n",
      "layers.1.mlp.w2.weight: (4096, 14336)\n",
      "layers.1.mlp.w3.weight: (14336, 4096)\n",
      "layers.1.sa_norm.scale: (4096,)\n",
      "layers.1.mlp_norm.scale: (4096,)\n",
      "layers.2.attn.q_proj.weight: (4096, 4096)\n",
      "layers.2.attn.k_proj.weight: (1024, 4096)\n",
      "layers.2.attn.v_proj.weight: (1024, 4096)\n",
      "layers.2.attn.output_proj.weight: (4096, 4096)\n",
      "layers.2.mlp.w1.weight: (14336, 4096)\n",
      "layers.2.mlp.w2.weight: (4096, 14336)\n",
      "layers.2.mlp.w3.weight: (14336, 4096)\n",
      "layers.2.sa_norm.scale: (4096,)\n",
      "layers.2.mlp_norm.scale: (4096,)\n",
      "layers.3.attn.q_proj.weight: (4096, 4096)\n",
      "layers.3.attn.k_proj.weight: (1024, 4096)\n",
      "layers.3.attn.v_proj.weight: (1024, 4096)\n",
      "layers.3.attn.output_proj.weight: (4096, 4096)\n",
      "layers.3.mlp.w1.weight: (14336, 4096)\n",
      "layers.3.mlp.w2.weight: (4096, 14336)\n",
      "layers.3.mlp.w3.weight: (14336, 4096)\n",
      "layers.3.sa_norm.scale: (4096,)\n",
      "layers.3.mlp_norm.scale: (4096,)\n",
      "layers.4.attn.q_proj.weight: (4096, 4096)\n",
      "layers.4.attn.k_proj.weight: (1024, 4096)\n",
      "layers.4.attn.v_proj.weight: (1024, 4096)\n",
      "layers.4.attn.output_proj.weight: (4096, 4096)\n",
      "layers.4.mlp.w1.weight: (14336, 4096)\n",
      "layers.4.mlp.w2.weight: (4096, 14336)\n",
      "layers.4.mlp.w3.weight: (14336, 4096)\n",
      "layers.4.sa_norm.scale: (4096,)\n",
      "layers.4.mlp_norm.scale: (4096,)\n",
      "layers.5.attn.q_proj.weight: (4096, 4096)\n",
      "layers.5.attn.k_proj.weight: (1024, 4096)\n",
      "layers.5.attn.v_proj.weight: (1024, 4096)\n",
      "layers.5.attn.output_proj.weight: (4096, 4096)\n",
      "layers.5.mlp.w1.weight: (14336, 4096)\n",
      "layers.5.mlp.w2.weight: (4096, 14336)\n",
      "layers.5.mlp.w3.weight: (14336, 4096)\n",
      "layers.5.sa_norm.scale: (4096,)\n",
      "layers.5.mlp_norm.scale: (4096,)\n",
      "layers.6.attn.q_proj.weight: (4096, 4096)\n",
      "layers.6.attn.k_proj.weight: (1024, 4096)\n",
      "layers.6.attn.v_proj.weight: (1024, 4096)\n",
      "layers.6.attn.output_proj.weight: (4096, 4096)\n",
      "layers.6.mlp.w1.weight: (14336, 4096)\n",
      "layers.6.mlp.w2.weight: (4096, 14336)\n",
      "layers.6.mlp.w3.weight: (14336, 4096)\n",
      "layers.6.sa_norm.scale: (4096,)\n",
      "layers.6.mlp_norm.scale: (4096,)\n",
      "layers.7.attn.q_proj.weight: (4096, 4096)\n",
      "layers.7.attn.k_proj.weight: (1024, 4096)\n",
      "layers.7.attn.v_proj.weight: (1024, 4096)\n",
      "layers.7.attn.output_proj.weight: (4096, 4096)\n",
      "layers.7.mlp.w1.weight: (14336, 4096)\n",
      "layers.7.mlp.w2.weight: (4096, 14336)\n",
      "layers.7.mlp.w3.weight: (14336, 4096)\n",
      "layers.7.sa_norm.scale: (4096,)\n",
      "layers.7.mlp_norm.scale: (4096,)\n",
      "layers.8.attn.q_proj.weight: (4096, 4096)\n",
      "layers.8.attn.k_proj.weight: (1024, 4096)\n",
      "layers.8.attn.v_proj.weight: (1024, 4096)\n",
      "layers.8.attn.output_proj.weight: (4096, 4096)\n",
      "layers.8.mlp.w1.weight: (14336, 4096)\n",
      "layers.8.mlp.w2.weight: (4096, 14336)\n",
      "layers.8.mlp.w3.weight: (14336, 4096)\n",
      "layers.8.sa_norm.scale: (4096,)\n",
      "layers.8.mlp_norm.scale: (4096,)\n",
      "layers.9.attn.q_proj.weight: (4096, 4096)\n",
      "layers.9.attn.k_proj.weight: (1024, 4096)\n",
      "layers.9.attn.v_proj.weight: (1024, 4096)\n",
      "layers.9.attn.output_proj.weight: (4096, 4096)\n",
      "layers.9.mlp.w1.weight: (14336, 4096)\n",
      "layers.9.mlp.w2.weight: (4096, 14336)\n",
      "layers.9.mlp.w3.weight: (14336, 4096)\n",
      "layers.9.sa_norm.scale: (4096,)\n",
      "layers.9.mlp_norm.scale: (4096,)\n",
      "layers.10.attn.q_proj.weight: (4096, 4096)\n",
      "layers.10.attn.k_proj.weight: (1024, 4096)\n",
      "layers.10.attn.v_proj.weight: (1024, 4096)\n",
      "layers.10.attn.output_proj.weight: (4096, 4096)\n",
      "layers.10.mlp.w1.weight: (14336, 4096)\n",
      "layers.10.mlp.w2.weight: (4096, 14336)\n",
      "layers.10.mlp.w3.weight: (14336, 4096)\n",
      "layers.10.sa_norm.scale: (4096,)\n",
      "layers.10.mlp_norm.scale: (4096,)\n",
      "layers.11.attn.q_proj.weight: (4096, 4096)\n",
      "layers.11.attn.k_proj.weight: (1024, 4096)\n",
      "layers.11.attn.v_proj.weight: (1024, 4096)\n",
      "layers.11.attn.output_proj.weight: (4096, 4096)\n",
      "layers.11.mlp.w1.weight: (14336, 4096)\n",
      "layers.11.mlp.w2.weight: (4096, 14336)\n",
      "layers.11.mlp.w3.weight: (14336, 4096)\n",
      "layers.11.sa_norm.scale: (4096,)\n",
      "layers.11.mlp_norm.scale: (4096,)\n",
      "layers.12.attn.q_proj.weight: (4096, 4096)\n",
      "layers.12.attn.k_proj.weight: (1024, 4096)\n",
      "layers.12.attn.v_proj.weight: (1024, 4096)\n",
      "layers.12.attn.output_proj.weight: (4096, 4096)\n",
      "layers.12.mlp.w1.weight: (14336, 4096)\n",
      "layers.12.mlp.w2.weight: (4096, 14336)\n",
      "layers.12.mlp.w3.weight: (14336, 4096)\n",
      "layers.12.sa_norm.scale: (4096,)\n",
      "layers.12.mlp_norm.scale: (4096,)\n",
      "layers.13.attn.q_proj.weight: (4096, 4096)\n",
      "layers.13.attn.k_proj.weight: (1024, 4096)\n",
      "layers.13.attn.v_proj.weight: (1024, 4096)\n",
      "layers.13.attn.output_proj.weight: (4096, 4096)\n",
      "layers.13.mlp.w1.weight: (14336, 4096)\n",
      "layers.13.mlp.w2.weight: (4096, 14336)\n",
      "layers.13.mlp.w3.weight: (14336, 4096)\n",
      "layers.13.sa_norm.scale: (4096,)\n",
      "layers.13.mlp_norm.scale: (4096,)\n",
      "layers.14.attn.q_proj.weight: (4096, 4096)\n",
      "layers.14.attn.k_proj.weight: (1024, 4096)\n",
      "layers.14.attn.v_proj.weight: (1024, 4096)\n",
      "layers.14.attn.output_proj.weight: (4096, 4096)\n",
      "layers.14.mlp.w1.weight: (14336, 4096)\n",
      "layers.14.mlp.w2.weight: (4096, 14336)\n",
      "layers.14.mlp.w3.weight: (14336, 4096)\n",
      "layers.14.sa_norm.scale: (4096,)\n",
      "layers.14.mlp_norm.scale: (4096,)\n",
      "layers.15.attn.q_proj.weight: (4096, 4096)\n",
      "layers.15.attn.k_proj.weight: (1024, 4096)\n",
      "layers.15.attn.v_proj.weight: (1024, 4096)\n",
      "layers.15.attn.output_proj.weight: (4096, 4096)\n",
      "layers.15.mlp.w1.weight: (14336, 4096)\n",
      "layers.15.mlp.w2.weight: (4096, 14336)\n",
      "layers.15.mlp.w3.weight: (14336, 4096)\n",
      "layers.15.sa_norm.scale: (4096,)\n",
      "layers.15.mlp_norm.scale: (4096,)\n",
      "layers.16.attn.q_proj.weight: (4096, 4096)\n",
      "layers.16.attn.k_proj.weight: (1024, 4096)\n",
      "layers.16.attn.v_proj.weight: (1024, 4096)\n",
      "layers.16.attn.output_proj.weight: (4096, 4096)\n",
      "layers.16.mlp.w1.weight: (14336, 4096)\n",
      "layers.16.mlp.w2.weight: (4096, 14336)\n",
      "layers.16.mlp.w3.weight: (14336, 4096)\n",
      "layers.16.sa_norm.scale: (4096,)\n",
      "layers.16.mlp_norm.scale: (4096,)\n",
      "layers.17.attn.q_proj.weight: (4096, 4096)\n",
      "layers.17.attn.k_proj.weight: (1024, 4096)\n",
      "layers.17.attn.v_proj.weight: (1024, 4096)\n",
      "layers.17.attn.output_proj.weight: (4096, 4096)\n",
      "layers.17.mlp.w1.weight: (14336, 4096)\n",
      "layers.17.mlp.w2.weight: (4096, 14336)\n",
      "layers.17.mlp.w3.weight: (14336, 4096)\n",
      "layers.17.sa_norm.scale: (4096,)\n",
      "layers.17.mlp_norm.scale: (4096,)\n",
      "layers.18.attn.q_proj.weight: (4096, 4096)\n",
      "layers.18.attn.k_proj.weight: (1024, 4096)\n",
      "layers.18.attn.v_proj.weight: (1024, 4096)\n",
      "layers.18.attn.output_proj.weight: (4096, 4096)\n",
      "layers.18.mlp.w1.weight: (14336, 4096)\n",
      "layers.18.mlp.w2.weight: (4096, 14336)\n",
      "layers.18.mlp.w3.weight: (14336, 4096)\n",
      "layers.18.sa_norm.scale: (4096,)\n",
      "layers.18.mlp_norm.scale: (4096,)\n",
      "layers.19.attn.q_proj.weight: (4096, 4096)\n",
      "layers.19.attn.k_proj.weight: (1024, 4096)\n",
      "layers.19.attn.v_proj.weight: (1024, 4096)\n",
      "layers.19.attn.output_proj.weight: (4096, 4096)\n",
      "layers.19.mlp.w1.weight: (14336, 4096)\n",
      "layers.19.mlp.w2.weight: (4096, 14336)\n",
      "layers.19.mlp.w3.weight: (14336, 4096)\n",
      "layers.19.sa_norm.scale: (4096,)\n",
      "layers.19.mlp_norm.scale: (4096,)\n",
      "layers.20.attn.q_proj.weight: (4096, 4096)\n",
      "layers.20.attn.k_proj.weight: (1024, 4096)\n",
      "layers.20.attn.v_proj.weight: (1024, 4096)\n",
      "layers.20.attn.output_proj.weight: (4096, 4096)\n",
      "layers.20.mlp.w1.weight: (14336, 4096)\n",
      "layers.20.mlp.w2.weight: (4096, 14336)\n",
      "layers.20.mlp.w3.weight: (14336, 4096)\n",
      "layers.20.sa_norm.scale: (4096,)\n",
      "layers.20.mlp_norm.scale: (4096,)\n",
      "layers.21.attn.q_proj.weight: (4096, 4096)\n",
      "layers.21.attn.k_proj.weight: (1024, 4096)\n",
      "layers.21.attn.v_proj.weight: (1024, 4096)\n",
      "layers.21.attn.output_proj.weight: (4096, 4096)\n",
      "layers.21.mlp.w1.weight: (14336, 4096)\n",
      "layers.21.mlp.w2.weight: (4096, 14336)\n",
      "layers.21.mlp.w3.weight: (14336, 4096)\n",
      "layers.21.sa_norm.scale: (4096,)\n",
      "layers.21.mlp_norm.scale: (4096,)\n",
      "layers.22.attn.q_proj.weight: (4096, 4096)\n",
      "layers.22.attn.k_proj.weight: (1024, 4096)\n",
      "layers.22.attn.v_proj.weight: (1024, 4096)\n",
      "layers.22.attn.output_proj.weight: (4096, 4096)\n",
      "layers.22.mlp.w1.weight: (14336, 4096)\n",
      "layers.22.mlp.w2.weight: (4096, 14336)\n",
      "layers.22.mlp.w3.weight: (14336, 4096)\n",
      "layers.22.sa_norm.scale: (4096,)\n",
      "layers.22.mlp_norm.scale: (4096,)\n",
      "layers.23.attn.q_proj.weight: (4096, 4096)\n",
      "layers.23.attn.k_proj.weight: (1024, 4096)\n",
      "layers.23.attn.v_proj.weight: (1024, 4096)\n",
      "layers.23.attn.output_proj.weight: (4096, 4096)\n",
      "layers.23.mlp.w1.weight: (14336, 4096)\n",
      "layers.23.mlp.w2.weight: (4096, 14336)\n",
      "layers.23.mlp.w3.weight: (14336, 4096)\n",
      "layers.23.sa_norm.scale: (4096,)\n",
      "layers.23.mlp_norm.scale: (4096,)\n",
      "layers.24.attn.q_proj.weight: (4096, 4096)\n",
      "layers.24.attn.k_proj.weight: (1024, 4096)\n",
      "layers.24.attn.v_proj.weight: (1024, 4096)\n",
      "layers.24.attn.output_proj.weight: (4096, 4096)\n",
      "layers.24.mlp.w1.weight: (14336, 4096)\n",
      "layers.24.mlp.w2.weight: (4096, 14336)\n",
      "layers.24.mlp.w3.weight: (14336, 4096)\n",
      "layers.24.sa_norm.scale: (4096,)\n",
      "layers.24.mlp_norm.scale: (4096,)\n",
      "layers.25.attn.q_proj.weight: (4096, 4096)\n",
      "layers.25.attn.k_proj.weight: (1024, 4096)\n",
      "layers.25.attn.v_proj.weight: (1024, 4096)\n",
      "layers.25.attn.output_proj.weight: (4096, 4096)\n",
      "layers.25.mlp.w1.weight: (14336, 4096)\n",
      "layers.25.mlp.w2.weight: (4096, 14336)\n",
      "layers.25.mlp.w3.weight: (14336, 4096)\n",
      "layers.25.sa_norm.scale: (4096,)\n",
      "layers.25.mlp_norm.scale: (4096,)\n",
      "layers.26.attn.q_proj.weight: (4096, 4096)\n",
      "layers.26.attn.k_proj.weight: (1024, 4096)\n",
      "layers.26.attn.v_proj.weight: (1024, 4096)\n",
      "layers.26.attn.output_proj.weight: (4096, 4096)\n",
      "layers.26.mlp.w1.weight: (14336, 4096)\n",
      "layers.26.mlp.w2.weight: (4096, 14336)\n",
      "layers.26.mlp.w3.weight: (14336, 4096)\n",
      "layers.26.sa_norm.scale: (4096,)\n",
      "layers.26.mlp_norm.scale: (4096,)\n",
      "layers.27.attn.q_proj.weight: (4096, 4096)\n",
      "layers.27.attn.k_proj.weight: (1024, 4096)\n",
      "layers.27.attn.v_proj.weight: (1024, 4096)\n",
      "layers.27.attn.output_proj.weight: (4096, 4096)\n",
      "layers.27.mlp.w1.weight: (14336, 4096)\n",
      "layers.27.mlp.w2.weight: (4096, 14336)\n",
      "layers.27.mlp.w3.weight: (14336, 4096)\n",
      "layers.27.sa_norm.scale: (4096,)\n",
      "layers.27.mlp_norm.scale: (4096,)\n",
      "layers.28.attn.q_proj.weight: (4096, 4096)\n",
      "layers.28.attn.k_proj.weight: (1024, 4096)\n",
      "layers.28.attn.v_proj.weight: (1024, 4096)\n",
      "layers.28.attn.output_proj.weight: (4096, 4096)\n",
      "layers.28.mlp.w1.weight: (14336, 4096)\n",
      "layers.28.mlp.w2.weight: (4096, 14336)\n",
      "layers.28.mlp.w3.weight: (14336, 4096)\n",
      "layers.28.sa_norm.scale: (4096,)\n",
      "layers.28.mlp_norm.scale: (4096,)\n",
      "layers.29.attn.q_proj.weight: (4096, 4096)\n",
      "layers.29.attn.k_proj.weight: (1024, 4096)\n",
      "layers.29.attn.v_proj.weight: (1024, 4096)\n",
      "layers.29.attn.output_proj.weight: (4096, 4096)\n",
      "layers.29.mlp.w1.weight: (14336, 4096)\n",
      "layers.29.mlp.w2.weight: (4096, 14336)\n",
      "layers.29.mlp.w3.weight: (14336, 4096)\n",
      "layers.29.sa_norm.scale: (4096,)\n",
      "layers.29.mlp_norm.scale: (4096,)\n",
      "layers.30.attn.q_proj.weight: (4096, 4096)\n",
      "layers.30.attn.k_proj.weight: (1024, 4096)\n",
      "layers.30.attn.v_proj.weight: (1024, 4096)\n",
      "layers.30.attn.output_proj.weight: (4096, 4096)\n",
      "layers.30.mlp.w1.weight: (14336, 4096)\n",
      "layers.30.mlp.w2.weight: (4096, 14336)\n",
      "layers.30.mlp.w3.weight: (14336, 4096)\n",
      "layers.30.sa_norm.scale: (4096,)\n",
      "layers.30.mlp_norm.scale: (4096,)\n",
      "layers.31.attn.q_proj.weight: (4096, 4096)\n",
      "layers.31.attn.k_proj.weight: (1024, 4096)\n",
      "layers.31.attn.v_proj.weight: (1024, 4096)\n",
      "layers.31.attn.output_proj.weight: (4096, 4096)\n",
      "layers.31.mlp.w1.weight: (14336, 4096)\n",
      "layers.31.mlp.w2.weight: (4096, 14336)\n",
      "layers.31.mlp.w3.weight: (14336, 4096)\n",
      "layers.31.sa_norm.scale: (4096,)\n",
      "layers.31.mlp_norm.scale: (4096,)\n",
      "norm.scale: (4096,)\n",
      "output.weight: (128256, 4096)\n"
     ]
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "\n",
    "for key in state_dict:\n",
    "    print(key + \":\", tuple(state_dict[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight: (128256, 4096)\n",
      "layers.0.attn.q_proj.weight: (4096, 4096)\n",
      "layers.0.attn.k_proj.weight: (1024, 4096)\n",
      "layers.0.attn.v_proj.weight: (1024, 4096)\n",
      "layers.0.attn.output_proj.weight: (4096, 4096)\n",
      "layers.0.mlp.w1.weight: (14336, 4096)\n",
      "layers.0.mlp.w3.weight: (14336, 4096)\n",
      "layers.0.mlp.w2.weight: (4096, 14336)\n",
      "layers.0.sa_norm.scale: (4096,)\n",
      "layers.0.mlp_norm.scale: (4096,)\n",
      "layers.1.attn.q_proj.weight: (4096, 4096)\n",
      "layers.1.attn.k_proj.weight: (1024, 4096)\n",
      "layers.1.attn.v_proj.weight: (1024, 4096)\n",
      "layers.1.attn.output_proj.weight: (4096, 4096)\n",
      "layers.1.mlp.w1.weight: (14336, 4096)\n",
      "layers.1.mlp.w3.weight: (14336, 4096)\n",
      "layers.1.mlp.w2.weight: (4096, 14336)\n",
      "layers.1.sa_norm.scale: (4096,)\n",
      "layers.1.mlp_norm.scale: (4096,)\n",
      "layers.2.attn.q_proj.weight: (4096, 4096)\n",
      "layers.2.attn.k_proj.weight: (1024, 4096)\n",
      "layers.2.attn.v_proj.weight: (1024, 4096)\n",
      "layers.2.attn.output_proj.weight: (4096, 4096)\n",
      "layers.2.mlp.w1.weight: (14336, 4096)\n",
      "layers.2.mlp.w3.weight: (14336, 4096)\n",
      "layers.2.mlp.w2.weight: (4096, 14336)\n",
      "layers.2.sa_norm.scale: (4096,)\n",
      "layers.2.mlp_norm.scale: (4096,)\n",
      "layers.3.attn.q_proj.weight: (4096, 4096)\n",
      "layers.3.attn.k_proj.weight: (1024, 4096)\n",
      "layers.3.attn.v_proj.weight: (1024, 4096)\n",
      "layers.3.attn.output_proj.weight: (4096, 4096)\n",
      "layers.3.mlp.w1.weight: (14336, 4096)\n",
      "layers.3.mlp.w3.weight: (14336, 4096)\n",
      "layers.3.mlp.w2.weight: (4096, 14336)\n",
      "layers.3.sa_norm.scale: (4096,)\n",
      "layers.3.mlp_norm.scale: (4096,)\n",
      "layers.4.attn.q_proj.weight: (4096, 4096)\n",
      "layers.4.attn.k_proj.weight: (1024, 4096)\n",
      "layers.4.attn.v_proj.weight: (1024, 4096)\n",
      "layers.4.attn.output_proj.weight: (4096, 4096)\n",
      "layers.4.mlp.w1.weight: (14336, 4096)\n",
      "layers.4.mlp.w3.weight: (14336, 4096)\n",
      "layers.4.mlp.w2.weight: (4096, 14336)\n",
      "layers.4.sa_norm.scale: (4096,)\n",
      "layers.4.mlp_norm.scale: (4096,)\n",
      "layers.5.attn.q_proj.weight: (4096, 4096)\n",
      "layers.5.attn.k_proj.weight: (1024, 4096)\n",
      "layers.5.attn.v_proj.weight: (1024, 4096)\n",
      "layers.5.attn.output_proj.weight: (4096, 4096)\n",
      "layers.5.mlp.w1.weight: (14336, 4096)\n",
      "layers.5.mlp.w3.weight: (14336, 4096)\n",
      "layers.5.mlp.w2.weight: (4096, 14336)\n",
      "layers.5.sa_norm.scale: (4096,)\n",
      "layers.5.mlp_norm.scale: (4096,)\n",
      "layers.6.attn.q_proj.weight: (4096, 4096)\n",
      "layers.6.attn.k_proj.weight: (1024, 4096)\n",
      "layers.6.attn.v_proj.weight: (1024, 4096)\n",
      "layers.6.attn.output_proj.weight: (4096, 4096)\n",
      "layers.6.mlp.w1.weight: (14336, 4096)\n",
      "layers.6.mlp.w3.weight: (14336, 4096)\n",
      "layers.6.mlp.w2.weight: (4096, 14336)\n",
      "layers.6.sa_norm.scale: (4096,)\n",
      "layers.6.mlp_norm.scale: (4096,)\n",
      "layers.7.attn.q_proj.weight: (4096, 4096)\n",
      "layers.7.attn.k_proj.weight: (1024, 4096)\n",
      "layers.7.attn.v_proj.weight: (1024, 4096)\n",
      "layers.7.attn.output_proj.weight: (4096, 4096)\n",
      "layers.7.mlp.w1.weight: (14336, 4096)\n",
      "layers.7.mlp.w3.weight: (14336, 4096)\n",
      "layers.7.mlp.w2.weight: (4096, 14336)\n",
      "layers.7.sa_norm.scale: (4096,)\n",
      "layers.7.mlp_norm.scale: (4096,)\n",
      "layers.8.attn.q_proj.weight: (4096, 4096)\n",
      "layers.8.attn.k_proj.weight: (1024, 4096)\n",
      "layers.8.attn.v_proj.weight: (1024, 4096)\n",
      "layers.8.attn.output_proj.weight: (4096, 4096)\n",
      "layers.8.mlp.w1.weight: (14336, 4096)\n",
      "layers.8.mlp.w3.weight: (14336, 4096)\n",
      "layers.8.mlp.w2.weight: (4096, 14336)\n",
      "layers.8.sa_norm.scale: (4096,)\n",
      "layers.8.mlp_norm.scale: (4096,)\n",
      "layers.9.attn.q_proj.weight: (4096, 4096)\n",
      "layers.9.attn.k_proj.weight: (1024, 4096)\n",
      "layers.9.attn.v_proj.weight: (1024, 4096)\n",
      "layers.9.attn.output_proj.weight: (4096, 4096)\n",
      "layers.9.mlp.w1.weight: (14336, 4096)\n",
      "layers.9.mlp.w3.weight: (14336, 4096)\n",
      "layers.9.mlp.w2.weight: (4096, 14336)\n",
      "layers.9.sa_norm.scale: (4096,)\n",
      "layers.9.mlp_norm.scale: (4096,)\n",
      "layers.10.attn.q_proj.weight: (4096, 4096)\n",
      "layers.10.attn.k_proj.weight: (1024, 4096)\n",
      "layers.10.attn.v_proj.weight: (1024, 4096)\n",
      "layers.10.attn.output_proj.weight: (4096, 4096)\n",
      "layers.10.mlp.w1.weight: (14336, 4096)\n",
      "layers.10.mlp.w3.weight: (14336, 4096)\n",
      "layers.10.mlp.w2.weight: (4096, 14336)\n",
      "layers.10.sa_norm.scale: (4096,)\n",
      "layers.10.mlp_norm.scale: (4096,)\n",
      "layers.11.attn.q_proj.weight: (4096, 4096)\n",
      "layers.11.attn.k_proj.weight: (1024, 4096)\n",
      "layers.11.attn.v_proj.weight: (1024, 4096)\n",
      "layers.11.attn.output_proj.weight: (4096, 4096)\n",
      "layers.11.mlp.w1.weight: (14336, 4096)\n",
      "layers.11.mlp.w3.weight: (14336, 4096)\n",
      "layers.11.mlp.w2.weight: (4096, 14336)\n",
      "layers.11.sa_norm.scale: (4096,)\n",
      "layers.11.mlp_norm.scale: (4096,)\n",
      "layers.12.attn.q_proj.weight: (4096, 4096)\n",
      "layers.12.attn.k_proj.weight: (1024, 4096)\n",
      "layers.12.attn.v_proj.weight: (1024, 4096)\n",
      "layers.12.attn.output_proj.weight: (4096, 4096)\n",
      "layers.12.mlp.w1.weight: (14336, 4096)\n",
      "layers.12.mlp.w3.weight: (14336, 4096)\n",
      "layers.12.mlp.w2.weight: (4096, 14336)\n",
      "layers.12.sa_norm.scale: (4096,)\n",
      "layers.12.mlp_norm.scale: (4096,)\n",
      "layers.13.attn.q_proj.weight: (4096, 4096)\n",
      "layers.13.attn.k_proj.weight: (1024, 4096)\n",
      "layers.13.attn.v_proj.weight: (1024, 4096)\n",
      "layers.13.attn.output_proj.weight: (4096, 4096)\n",
      "layers.13.mlp.w1.weight: (14336, 4096)\n",
      "layers.13.mlp.w3.weight: (14336, 4096)\n",
      "layers.13.mlp.w2.weight: (4096, 14336)\n",
      "layers.13.sa_norm.scale: (4096,)\n",
      "layers.13.mlp_norm.scale: (4096,)\n",
      "layers.14.attn.q_proj.weight: (4096, 4096)\n",
      "layers.14.attn.k_proj.weight: (1024, 4096)\n",
      "layers.14.attn.v_proj.weight: (1024, 4096)\n",
      "layers.14.attn.output_proj.weight: (4096, 4096)\n",
      "layers.14.mlp.w1.weight: (14336, 4096)\n",
      "layers.14.mlp.w3.weight: (14336, 4096)\n",
      "layers.14.mlp.w2.weight: (4096, 14336)\n",
      "layers.14.sa_norm.scale: (4096,)\n",
      "layers.14.mlp_norm.scale: (4096,)\n",
      "layers.15.attn.q_proj.weight: (4096, 4096)\n",
      "layers.15.attn.k_proj.weight: (1024, 4096)\n",
      "layers.15.attn.v_proj.weight: (1024, 4096)\n",
      "layers.15.attn.output_proj.weight: (4096, 4096)\n",
      "layers.15.mlp.w1.weight: (14336, 4096)\n",
      "layers.15.mlp.w3.weight: (14336, 4096)\n",
      "layers.15.mlp.w2.weight: (4096, 14336)\n",
      "layers.15.sa_norm.scale: (4096,)\n",
      "layers.15.mlp_norm.scale: (4096,)\n",
      "layers.16.attn.q_proj.weight: (4096, 4096)\n",
      "layers.16.attn.k_proj.weight: (1024, 4096)\n",
      "layers.16.attn.v_proj.weight: (1024, 4096)\n",
      "layers.16.attn.output_proj.weight: (4096, 4096)\n",
      "layers.16.mlp.w1.weight: (14336, 4096)\n",
      "layers.16.mlp.w3.weight: (14336, 4096)\n",
      "layers.16.mlp.w2.weight: (4096, 14336)\n",
      "layers.16.sa_norm.scale: (4096,)\n",
      "layers.16.mlp_norm.scale: (4096,)\n",
      "layers.17.attn.q_proj.weight: (4096, 4096)\n",
      "layers.17.attn.k_proj.weight: (1024, 4096)\n",
      "layers.17.attn.v_proj.weight: (1024, 4096)\n",
      "layers.17.attn.output_proj.weight: (4096, 4096)\n",
      "layers.17.mlp.w1.weight: (14336, 4096)\n",
      "layers.17.mlp.w3.weight: (14336, 4096)\n",
      "layers.17.mlp.w2.weight: (4096, 14336)\n",
      "layers.17.sa_norm.scale: (4096,)\n",
      "layers.17.mlp_norm.scale: (4096,)\n",
      "layers.18.attn.q_proj.weight: (4096, 4096)\n",
      "layers.18.attn.k_proj.weight: (1024, 4096)\n",
      "layers.18.attn.v_proj.weight: (1024, 4096)\n",
      "layers.18.attn.output_proj.weight: (4096, 4096)\n",
      "layers.18.mlp.w1.weight: (14336, 4096)\n",
      "layers.18.mlp.w3.weight: (14336, 4096)\n",
      "layers.18.mlp.w2.weight: (4096, 14336)\n",
      "layers.18.sa_norm.scale: (4096,)\n",
      "layers.18.mlp_norm.scale: (4096,)\n",
      "layers.19.attn.q_proj.weight: (4096, 4096)\n",
      "layers.19.attn.k_proj.weight: (1024, 4096)\n",
      "layers.19.attn.v_proj.weight: (1024, 4096)\n",
      "layers.19.attn.output_proj.weight: (4096, 4096)\n",
      "layers.19.mlp.w1.weight: (14336, 4096)\n",
      "layers.19.mlp.w3.weight: (14336, 4096)\n",
      "layers.19.mlp.w2.weight: (4096, 14336)\n",
      "layers.19.sa_norm.scale: (4096,)\n",
      "layers.19.mlp_norm.scale: (4096,)\n",
      "layers.20.attn.q_proj.weight: (4096, 4096)\n",
      "layers.20.attn.k_proj.weight: (1024, 4096)\n",
      "layers.20.attn.v_proj.weight: (1024, 4096)\n",
      "layers.20.attn.output_proj.weight: (4096, 4096)\n",
      "layers.20.mlp.w1.weight: (14336, 4096)\n",
      "layers.20.mlp.w3.weight: (14336, 4096)\n",
      "layers.20.mlp.w2.weight: (4096, 14336)\n",
      "layers.20.sa_norm.scale: (4096,)\n",
      "layers.20.mlp_norm.scale: (4096,)\n",
      "layers.21.attn.q_proj.weight: (4096, 4096)\n",
      "layers.21.attn.k_proj.weight: (1024, 4096)\n",
      "layers.21.attn.v_proj.weight: (1024, 4096)\n",
      "layers.21.attn.output_proj.weight: (4096, 4096)\n",
      "layers.21.mlp.w1.weight: (14336, 4096)\n",
      "layers.21.mlp.w3.weight: (14336, 4096)\n",
      "layers.21.mlp.w2.weight: (4096, 14336)\n",
      "layers.21.sa_norm.scale: (4096,)\n",
      "layers.21.mlp_norm.scale: (4096,)\n",
      "layers.22.attn.q_proj.weight: (4096, 4096)\n",
      "layers.22.attn.k_proj.weight: (1024, 4096)\n",
      "layers.22.attn.v_proj.weight: (1024, 4096)\n",
      "layers.22.attn.output_proj.weight: (4096, 4096)\n",
      "layers.22.mlp.w1.weight: (14336, 4096)\n",
      "layers.22.mlp.w3.weight: (14336, 4096)\n",
      "layers.22.mlp.w2.weight: (4096, 14336)\n",
      "layers.22.sa_norm.scale: (4096,)\n",
      "layers.22.mlp_norm.scale: (4096,)\n",
      "layers.23.attn.q_proj.weight: (4096, 4096)\n",
      "layers.23.attn.k_proj.weight: (1024, 4096)\n",
      "layers.23.attn.v_proj.weight: (1024, 4096)\n",
      "layers.23.attn.output_proj.weight: (4096, 4096)\n",
      "layers.23.mlp.w1.weight: (14336, 4096)\n",
      "layers.23.mlp.w3.weight: (14336, 4096)\n",
      "layers.23.mlp.w2.weight: (4096, 14336)\n",
      "layers.23.sa_norm.scale: (4096,)\n",
      "layers.23.mlp_norm.scale: (4096,)\n",
      "layers.24.attn.q_proj.weight: (4096, 4096)\n",
      "layers.24.attn.k_proj.weight: (1024, 4096)\n",
      "layers.24.attn.v_proj.weight: (1024, 4096)\n",
      "layers.24.attn.output_proj.weight: (4096, 4096)\n",
      "layers.24.mlp.w1.weight: (14336, 4096)\n",
      "layers.24.mlp.w3.weight: (14336, 4096)\n",
      "layers.24.mlp.w2.weight: (4096, 14336)\n",
      "layers.24.sa_norm.scale: (4096,)\n",
      "layers.24.mlp_norm.scale: (4096,)\n",
      "layers.25.attn.q_proj.weight: (4096, 4096)\n",
      "layers.25.attn.k_proj.weight: (1024, 4096)\n",
      "layers.25.attn.v_proj.weight: (1024, 4096)\n",
      "layers.25.attn.output_proj.weight: (4096, 4096)\n",
      "layers.25.mlp.w1.weight: (14336, 4096)\n",
      "layers.25.mlp.w3.weight: (14336, 4096)\n",
      "layers.25.mlp.w2.weight: (4096, 14336)\n",
      "layers.25.sa_norm.scale: (4096,)\n",
      "layers.25.mlp_norm.scale: (4096,)\n",
      "layers.26.attn.q_proj.weight: (4096, 4096)\n",
      "layers.26.attn.k_proj.weight: (1024, 4096)\n",
      "layers.26.attn.v_proj.weight: (1024, 4096)\n",
      "layers.26.attn.output_proj.weight: (4096, 4096)\n",
      "layers.26.mlp.w1.weight: (14336, 4096)\n",
      "layers.26.mlp.w3.weight: (14336, 4096)\n",
      "layers.26.mlp.w2.weight: (4096, 14336)\n",
      "layers.26.sa_norm.scale: (4096,)\n",
      "layers.26.mlp_norm.scale: (4096,)\n",
      "layers.27.attn.q_proj.weight: (4096, 4096)\n",
      "layers.27.attn.k_proj.weight: (1024, 4096)\n",
      "layers.27.attn.v_proj.weight: (1024, 4096)\n",
      "layers.27.attn.output_proj.weight: (4096, 4096)\n",
      "layers.27.mlp.w1.weight: (14336, 4096)\n",
      "layers.27.mlp.w3.weight: (14336, 4096)\n",
      "layers.27.mlp.w2.weight: (4096, 14336)\n",
      "layers.27.sa_norm.scale: (4096,)\n",
      "layers.27.mlp_norm.scale: (4096,)\n",
      "layers.28.attn.q_proj.weight: (4096, 4096)\n",
      "layers.28.attn.k_proj.weight: (1024, 4096)\n",
      "layers.28.attn.v_proj.weight: (1024, 4096)\n",
      "layers.28.attn.output_proj.weight: (4096, 4096)\n",
      "layers.28.mlp.w1.weight: (14336, 4096)\n",
      "layers.28.mlp.w3.weight: (14336, 4096)\n",
      "layers.28.mlp.w2.weight: (4096, 14336)\n",
      "layers.28.sa_norm.scale: (4096,)\n",
      "layers.28.mlp_norm.scale: (4096,)\n",
      "layers.29.attn.q_proj.weight: (4096, 4096)\n",
      "layers.29.attn.k_proj.weight: (1024, 4096)\n",
      "layers.29.attn.v_proj.weight: (1024, 4096)\n",
      "layers.29.attn.output_proj.weight: (4096, 4096)\n",
      "layers.29.mlp.w1.weight: (14336, 4096)\n",
      "layers.29.mlp.w3.weight: (14336, 4096)\n",
      "layers.29.mlp.w2.weight: (4096, 14336)\n",
      "layers.29.sa_norm.scale: (4096,)\n",
      "layers.29.mlp_norm.scale: (4096,)\n",
      "layers.30.attn.q_proj.weight: (4096, 4096)\n",
      "layers.30.attn.k_proj.weight: (1024, 4096)\n",
      "layers.30.attn.v_proj.weight: (1024, 4096)\n",
      "layers.30.attn.output_proj.weight: (4096, 4096)\n",
      "layers.30.mlp.w1.weight: (14336, 4096)\n",
      "layers.30.mlp.w3.weight: (14336, 4096)\n",
      "layers.30.mlp.w2.weight: (4096, 14336)\n",
      "layers.30.sa_norm.scale: (4096,)\n",
      "layers.30.mlp_norm.scale: (4096,)\n",
      "layers.31.attn.q_proj.weight: (4096, 4096)\n",
      "layers.31.attn.k_proj.weight: (1024, 4096)\n",
      "layers.31.attn.v_proj.weight: (1024, 4096)\n",
      "layers.31.attn.output_proj.weight: (4096, 4096)\n",
      "layers.31.mlp.w1.weight: (14336, 4096)\n",
      "layers.31.mlp.w3.weight: (14336, 4096)\n",
      "layers.31.mlp.w2.weight: (4096, 14336)\n",
      "layers.31.sa_norm.scale: (4096,)\n",
      "layers.31.mlp_norm.scale: (4096,)\n",
      "norm.scale: (4096,)\n"
     ]
    }
   ],
   "source": [
    "mapped_vllm_state_dict = {}\n",
    "\n",
    "# Map embedding and norm layers directly\n",
    "mapped_vllm_state_dict['tok_embeddings.weight'] = vllm_state_dict['embed_tokens.weight']\n",
    "\n",
    "\n",
    "# Map each transformer layer\n",
    "for i in range(32):\n",
    "    prefix = f'layers.{i}.'\n",
    "    vllm_prefix = f'layers.{i}.'\n",
    "    \n",
    "    # Split QKV projection\n",
    "    qkv = vllm_state_dict[vllm_prefix + 'self_attn.qkv_proj.weight']\n",
    "    q, k, v = torch.split(qkv, [4096, 1024, 1024], dim=0)\n",
    "    mapped_vllm_state_dict[prefix + 'attn.q_proj.weight'] = q\n",
    "    mapped_vllm_state_dict[prefix + 'attn.k_proj.weight'] = k \n",
    "    mapped_vllm_state_dict[prefix + 'attn.v_proj.weight'] = v\n",
    "\n",
    "    # Map attention output projection\n",
    "    mapped_vllm_state_dict[prefix + 'attn.output_proj.weight'] = vllm_state_dict[vllm_prefix + 'self_attn.o_proj.weight']\n",
    "\n",
    "    # Split gate/up projection\n",
    "    gate_up = vllm_state_dict[vllm_prefix + 'mlp.gate_up_proj.weight']\n",
    "    gate, up = torch.split(gate_up, [14336, 14336], dim=0)\n",
    "    mapped_vllm_state_dict[prefix + 'mlp.w1.weight'] = gate\n",
    "    mapped_vllm_state_dict[prefix + 'mlp.w3.weight'] = up\n",
    "\n",
    "    # Map down projection\n",
    "    mapped_vllm_state_dict[prefix + 'mlp.w2.weight'] = vllm_state_dict[vllm_prefix + 'mlp.down_proj.weight']\n",
    "\n",
    "    # Map norms\n",
    "    mapped_vllm_state_dict[prefix + 'sa_norm.scale'] = vllm_state_dict[vllm_prefix + 'input_layernorm.weight']\n",
    "    mapped_vllm_state_dict[prefix + 'mlp_norm.scale'] = vllm_state_dict[vllm_prefix + 'post_attention_layernorm.weight']\n",
    "\n",
    "mapped_vllm_state_dict['norm.scale'] = vllm_state_dict['norm.weight']\n",
    "\n",
    "for key in mapped_vllm_state_dict:\n",
    "    print(key + \":\", tuple(mapped_vllm_state_dict[key].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_vllm_state_dict[\"output.weight\"] = llm.llm_engine.model_executor.driver_worker.model_runner.model.lm_head.state_dict()[\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight: 0.0000\n",
      "layers.0.attn.q_proj.weight: 0.0000\n",
      "layers.0.attn.k_proj.weight: 0.0001\n",
      "layers.0.attn.v_proj.weight: -0.0000\n",
      "layers.0.attn.output_proj.weight: -0.0000\n",
      "layers.0.mlp.w1.weight: 0.0001\n",
      "layers.0.mlp.w2.weight: -0.0000\n",
      "layers.0.mlp.w3.weight: -0.0001\n",
      "layers.0.sa_norm.scale: nan\n",
      "layers.0.mlp_norm.scale: nan\n",
      "layers.1.attn.q_proj.weight: -0.0003\n",
      "layers.1.attn.k_proj.weight: -0.0004\n",
      "layers.1.attn.v_proj.weight: -0.0000\n",
      "layers.1.attn.output_proj.weight: -0.0000\n",
      "layers.1.mlp.w1.weight: -0.0000\n",
      "layers.1.mlp.w2.weight: -0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m mapped_vllm_state_dict:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Stack the tensors into a 2xN matrix before computing correlation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     stacked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m      6\u001b[0m         state_dict[key]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m      7\u001b[0m         mapped_vllm_state_dict[key]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     ])\n\u001b[0;32m----> 9\u001b[0m     correlation \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrcoef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrelation\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Print correlation between state_dict and mapped_vllm_state_dict tensors\n",
    "for key in state_dict:\n",
    "    if key in mapped_vllm_state_dict:\n",
    "        # Stack the tensors into a 2xN matrix before computing correlation\n",
    "        stacked = torch.stack([\n",
    "            state_dict[key].flatten().float(),\n",
    "            mapped_vllm_state_dict[key].flatten().float().to(\"cpu\")\n",
    "        ])\n",
    "        correlation = torch.corrcoef(stacked)[0,1]\n",
    "        print(f\"{key}: {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'vllm.attention.backends.flash_attn.FlashAttentionMetadata'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it, est. speed input: 113.81 toks/s, output: 70.43 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=6, prompt=\"<|begin_of_text|><|im_start|>user\\nOn a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\\n\\nThey assembled 3 decks of cards, each for a separate type of information composed of the following:\\n\\nSuspect:\\n- Miss Scarlet\\n- Mr. Green\\n- Mrs. White\\n\\nWeapon:\\n- Candlestick\\n- Knife\\n- Lead Pipe\\n\\nRoom:\\n- Hall\\n- Lounge\\n- Dining Room\\n\\nAfter randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\\n\\n- Summer: 2 cards\\n- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\\n- Connor: 2 cards\\n\\nThe game proceeded as follows:\\n\\n1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\\n2. The player directed this question to the other players in clockwise order, starting with the player to their left.\\n3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\\n4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\\n5. This continued until either:\\n    a) A player showed a card to the asking player, or\\n    b) All the queried players had stated they didn't have any of the asked-about cards.\\n6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\\n\\nHere is how the game played out:\\n\\nSummer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\\n- Giselle did not have any of the cards\\n- Connor showed Summer a card\\n\\nGiselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\\n- Connor did not have any of the cards\\n- Summer did not have any of the cards\\n\\nConnor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\\n- Summer did not have any of the cards\\n- Giselle showed Connor 'Miss Scarlet'\\n\\nSummer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\\n- Giselle did not have any of the cards\\n- Connor did not have any of the cards\\n\\nAt this point, Giselle was able to correctly infer the solution and win the game.\\n\\nWhat were the facedown cards in the middle of the table?<|im_end|>\\n<|im_start|>assistant\\n\", prompt_token_ids=[128000, 128000, 128002, 882, 198, 1966, 264, 8369, 10683, 1938, 19367, 11, 480, 285, 6853, 323, 58280, 7731, 1523, 311, 1514, 264, 16736, 23347, 1847, 382, 7009, 35105, 220, 18, 30881, 315, 7563, 11, 1855, 369, 264, 8821, 955, 315, 2038, 24306, 315, 279, 2768, 1473, 78524, 1002, 512, 12, 9083, 81818, 198, 12, 4491, 13, 7997, 198, 12, 18083, 13, 5929, 271, 29314, 512, 12, 73997, 30133, 198, 12, 62302, 198, 12, 30982, 28905, 271, 14330, 512, 12, 11166, 198, 12, 50767, 198, 12, 39190, 10637, 271, 6153, 27716, 320, 438, 89447, 8, 19301, 832, 3786, 505, 1855, 1912, 323, 25012, 1124, 304, 279, 6278, 315, 279, 2007, 17011, 785, 11, 814, 75371, 279, 9861, 7563, 323, 27023, 704, 279, 2768, 311, 1855, 2851, 1473, 12, 19367, 25, 220, 17, 7563, 198, 12, 480, 285, 6853, 25, 220, 17, 7563, 4417, 43, 26645, 518, 364, 36412, 81818, 1329, 12, 58280, 25, 220, 17, 7563, 271, 791, 1847, 45374, 439, 11263, 1473, 16, 13, 1952, 872, 2543, 11, 264, 2851, 4691, 922, 264, 743, 315, 7041, 220, 18, 7563, 11, 832, 505, 1855, 315, 279, 1847, 596, 11306, 13, 320, 9290, 25, 25640, 1436, 2610, 922, 904, 7563, 11, 2737, 1884, 304, 872, 1866, 1450, 29275, 17, 13, 578, 2851, 15910, 420, 3488, 311, 279, 1023, 4311, 304, 66770, 2015, 11, 6041, 449, 279, 2851, 311, 872, 2163, 627, 18, 13, 1442, 264, 2851, 1047, 832, 477, 810, 315, 279, 4691, 69205, 7563, 11, 814, 1047, 311, 1501, 832, 315, 1884, 7563, 320, 1073, 872, 5873, 8, 311, 279, 10371, 2851, 38171, 13, 578, 2543, 1243, 9670, 11, 323, 1514, 5946, 311, 279, 1828, 2851, 627, 19, 13, 1442, 264, 2851, 1550, 539, 617, 904, 315, 279, 4691, 69205, 7563, 11, 814, 1071, 779, 11, 323, 279, 3488, 5946, 311, 279, 1828, 2851, 304, 66770, 2015, 627, 20, 13, 1115, 8738, 3156, 3060, 512, 262, 264, 8, 362, 2851, 8710, 264, 3786, 311, 279, 10371, 2851, 11, 477, 198, 262, 293, 8, 2052, 279, 79002, 4311, 1047, 11224, 814, 3287, 956, 617, 904, 315, 279, 4691, 69205, 7563, 627, 21, 13, 4740, 264, 2851, 596, 2543, 9670, 320, 50998, 555, 1694, 6982, 264, 3786, 477, 3515, 682, 79002, 4311, 1522, 705, 1514, 7882, 311, 279, 1828, 2851, 304, 66770, 2015, 382, 8586, 374, 1268, 279, 1847, 6476, 704, 1473, 51787, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 35, 5859, 10637, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 8710, 19367, 264, 3786, 271, 38, 285, 6853, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 43, 26645, 3730, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 271, 57987, 4691, 422, 5606, 1047, 364, 36412, 81818, 6, 477, 364, 34, 3397, 30133, 6, 477, 364, 72945, 3730, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 480, 285, 6853, 8710, 58280, 364, 36412, 81818, 3961, 51787, 4691, 422, 5606, 1047, 364, 12555, 13, 7997, 6, 477, 364, 57505, 6, 477, 364, 72945, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 271, 1688, 420, 1486, 11, 480, 285, 6853, 574, 3025, 311, 12722, 24499, 279, 6425, 323, 3243, 279, 1847, 382, 3923, 1051, 279, 17011, 785, 7563, 304, 279, 6278, 315, 279, 2007, 30, 128003, 198, 128002, 78191, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='We know that Giselle held \\'Lounge\\' and \\'Miss Scarlet\\' cards and that Connor showed Summer a card at some point. Also, Giselle was able to infer the solution at the end.\\n\\nLet\\'s analyze the possible scenarios:\\n\\n1. If \"Knife\" is in the middle: Summer would have shown \"Knife\" during her second turn to Connor since it\\'s the only card Summer knew was in the middle. However, this contradicts the fact that Summer did not show any cards.\\n\\n2. If \"Dining Room\" is in the middle: Summer would have shown \"Dining Room\" during her first turn to Giselle since she knew Giselle held \"Lounge\". However, this also contradicts the fact that Summer did not show any cards.\\n\\n3. If \"Hall\" is in the middle: Summer could not have asked about it during her first turn since she didn\\'t know if Connor held \"Hall\" or not. However, she could have asked about it during her second turn, but Connor did not have the card.\\n\\nSo, we are left with the remaining possibilities. Since Summer did not show any cards, it must be that none of the facedown cards matched with any of the cards Giselle or Connor already had. Hence, it is impossible for Giselle to deduce the correct solution based on the given information.\\n\\nThis conundrum arises because the game\\'s rules allow players to ask about any cards, not limited to those they have in their hand or which are facedown. This discrepancy in the rules creates an anomaly where the game\\'s logical foundation is breached, rendering the game no longer playable according to its intended design. It is therefore impossible to determine the facedown cards with the given information within this flawed game setting.', token_ids=(1687, 1440, 430, 480, 285, 6853, 5762, 364, 43, 26645, 6, 323, 364, 36412, 81818, 6, 7563, 323, 430, 58280, 8710, 19367, 264, 3786, 520, 1063, 1486, 13, 7429, 11, 480, 285, 6853, 574, 3025, 311, 24499, 279, 6425, 520, 279, 842, 382, 10267, 596, 24564, 279, 3284, 26350, 1473, 16, 13, 1442, 330, 57505, 1, 374, 304, 279, 6278, 25, 19367, 1053, 617, 6982, 330, 57505, 1, 2391, 1077, 2132, 2543, 311, 58280, 2533, 433, 596, 279, 1193, 3786, 19367, 7020, 574, 304, 279, 6278, 13, 4452, 11, 420, 23093, 31095, 279, 2144, 430, 19367, 1550, 539, 1501, 904, 7563, 382, 17, 13, 1442, 330, 35, 5859, 10637, 1, 374, 304, 279, 6278, 25, 19367, 1053, 617, 6982, 330, 35, 5859, 10637, 1, 2391, 1077, 1176, 2543, 311, 480, 285, 6853, 2533, 1364, 7020, 480, 285, 6853, 5762, 330, 43, 26645, 3343, 4452, 11, 420, 1101, 23093, 31095, 279, 2144, 430, 19367, 1550, 539, 1501, 904, 7563, 382, 18, 13, 1442, 330, 72945, 1, 374, 304, 279, 6278, 25, 19367, 1436, 539, 617, 4691, 922, 433, 2391, 1077, 1176, 2543, 2533, 1364, 3287, 956, 1440, 422, 58280, 5762, 330, 72945, 1, 477, 539, 13, 4452, 11, 1364, 1436, 617, 4691, 922, 433, 2391, 1077, 2132, 2543, 11, 719, 58280, 1550, 539, 617, 279, 3786, 382, 4516, 11, 584, 527, 2163, 449, 279, 9861, 24525, 13, 8876, 19367, 1550, 539, 1501, 904, 7563, 11, 433, 2011, 387, 430, 7000, 315, 279, 17011, 785, 7563, 18545, 449, 904, 315, 279, 7563, 480, 285, 6853, 477, 58280, 2736, 1047, 13, 32140, 11, 433, 374, 12266, 369, 480, 285, 6853, 311, 7836, 10743, 279, 4495, 6425, 3196, 389, 279, 2728, 2038, 382, 2028, 390, 1263, 10952, 48282, 1606, 279, 1847, 596, 5718, 2187, 4311, 311, 2610, 922, 904, 7563, 11, 539, 7347, 311, 1884, 814, 617, 304, 872, 1450, 477, 902, 527, 17011, 785, 13, 1115, 79105, 304, 279, 5718, 11705, 459, 64048, 1405, 279, 1847, 596, 20406, 16665, 374, 82166, 11, 21568, 279, 1847, 912, 5129, 52135, 4184, 311, 1202, 10825, 2955, 13, 1102, 374, 9093, 12266, 311, 8417, 279, 17011, 785, 7563, 449, 279, 2728, 2038, 2949, 420, 48008, 1847, 6376, 13, 128003), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729721783.637649, last_token_time=1729721783.637649, first_scheduled_time=1729721783.6405513, first_token_time=1729721783.700682, time_in_queue=0.0029022693634033203, finished_time=1729721788.7675114, scheduler_time=0.025968797000587074, model_forward_time=None, model_execute_time=None), lora_request=None)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm.forward_context import get_forward_context\n",
    "\n",
    "compute_logits = vllm_model.forward\n",
    "\n",
    "def hook(*args, **kwargs):\n",
    "    print(type(get_forward_context()))\n",
    "    return compute_logits(*args, **kwargs)\n",
    "\n",
    "vllm_model.forward = hook\n",
    "\n",
    "output = llm.chat([[dict(role=\"user\", content=prompt)]] * 1, sampling_params=SamplingParams(max_tokens=10_000))  # type: ignore\n",
    "\n",
    "vllm_model.forward = compute_logits\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.49s/it, est. speed input: 106.64 toks/s, output: 70.55 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=8, prompt=\"<|begin_of_text|><|im_start|>user\\nOn a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\\n\\nThey assembled 3 decks of cards, each for a separate type of information composed of the following:\\n\\nSuspect:\\n- Miss Scarlet\\n- Mr. Green\\n- Mrs. White\\n\\nWeapon:\\n- Candlestick\\n- Knife\\n- Lead Pipe\\n\\nRoom:\\n- Hall\\n- Lounge\\n- Dining Room\\n\\nAfter randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\\n\\n- Summer: 2 cards\\n- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\\n- Connor: 2 cards\\n\\nThe game proceeded as follows:\\n\\n1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\\n2. The player directed this question to the other players in clockwise order, starting with the player to their left.\\n3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\\n4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\\n5. This continued until either:\\n    a) A player showed a card to the asking player, or\\n    b) All the queried players had stated they didn't have any of the asked-about cards.\\n6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\\n\\nHere is how the game played out:\\n\\nSummer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\\n- Giselle did not have any of the cards\\n- Connor showed Summer a card\\n\\nGiselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\\n- Connor did not have any of the cards\\n- Summer did not have any of the cards\\n\\nConnor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\\n- Summer did not have any of the cards\\n- Giselle showed Connor 'Miss Scarlet'\\n\\nSummer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\\n- Giselle did not have any of the cards\\n- Connor did not have any of the cards\\n\\nAt this point, Giselle was able to correctly infer the solution and win the game.\\n\\nWhat were the facedown cards in the middle of the table?<|im_end|>\\n<|im_start|>assistant\\n\", prompt_token_ids=[128000, 128000, 128002, 882, 198, 1966, 264, 8369, 10683, 1938, 19367, 11, 480, 285, 6853, 323, 58280, 7731, 1523, 311, 1514, 264, 16736, 23347, 1847, 382, 7009, 35105, 220, 18, 30881, 315, 7563, 11, 1855, 369, 264, 8821, 955, 315, 2038, 24306, 315, 279, 2768, 1473, 78524, 1002, 512, 12, 9083, 81818, 198, 12, 4491, 13, 7997, 198, 12, 18083, 13, 5929, 271, 29314, 512, 12, 73997, 30133, 198, 12, 62302, 198, 12, 30982, 28905, 271, 14330, 512, 12, 11166, 198, 12, 50767, 198, 12, 39190, 10637, 271, 6153, 27716, 320, 438, 89447, 8, 19301, 832, 3786, 505, 1855, 1912, 323, 25012, 1124, 304, 279, 6278, 315, 279, 2007, 17011, 785, 11, 814, 75371, 279, 9861, 7563, 323, 27023, 704, 279, 2768, 311, 1855, 2851, 1473, 12, 19367, 25, 220, 17, 7563, 198, 12, 480, 285, 6853, 25, 220, 17, 7563, 4417, 43, 26645, 518, 364, 36412, 81818, 1329, 12, 58280, 25, 220, 17, 7563, 271, 791, 1847, 45374, 439, 11263, 1473, 16, 13, 1952, 872, 2543, 11, 264, 2851, 4691, 922, 264, 743, 315, 7041, 220, 18, 7563, 11, 832, 505, 1855, 315, 279, 1847, 596, 11306, 13, 320, 9290, 25, 25640, 1436, 2610, 922, 904, 7563, 11, 2737, 1884, 304, 872, 1866, 1450, 29275, 17, 13, 578, 2851, 15910, 420, 3488, 311, 279, 1023, 4311, 304, 66770, 2015, 11, 6041, 449, 279, 2851, 311, 872, 2163, 627, 18, 13, 1442, 264, 2851, 1047, 832, 477, 810, 315, 279, 4691, 69205, 7563, 11, 814, 1047, 311, 1501, 832, 315, 1884, 7563, 320, 1073, 872, 5873, 8, 311, 279, 10371, 2851, 38171, 13, 578, 2543, 1243, 9670, 11, 323, 1514, 5946, 311, 279, 1828, 2851, 627, 19, 13, 1442, 264, 2851, 1550, 539, 617, 904, 315, 279, 4691, 69205, 7563, 11, 814, 1071, 779, 11, 323, 279, 3488, 5946, 311, 279, 1828, 2851, 304, 66770, 2015, 627, 20, 13, 1115, 8738, 3156, 3060, 512, 262, 264, 8, 362, 2851, 8710, 264, 3786, 311, 279, 10371, 2851, 11, 477, 198, 262, 293, 8, 2052, 279, 79002, 4311, 1047, 11224, 814, 3287, 956, 617, 904, 315, 279, 4691, 69205, 7563, 627, 21, 13, 4740, 264, 2851, 596, 2543, 9670, 320, 50998, 555, 1694, 6982, 264, 3786, 477, 3515, 682, 79002, 4311, 1522, 705, 1514, 7882, 311, 279, 1828, 2851, 304, 66770, 2015, 382, 8586, 374, 1268, 279, 1847, 6476, 704, 1473, 51787, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 35, 5859, 10637, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 8710, 19367, 264, 3786, 271, 38, 285, 6853, 4691, 422, 5606, 1047, 364, 50329, 13, 5929, 6, 477, 364, 57505, 6, 477, 364, 43, 26645, 3730, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 271, 57987, 4691, 422, 5606, 1047, 364, 36412, 81818, 6, 477, 364, 34, 3397, 30133, 6, 477, 364, 72945, 3730, 12, 19367, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 480, 285, 6853, 8710, 58280, 364, 36412, 81818, 3961, 51787, 4691, 422, 5606, 1047, 364, 12555, 13, 7997, 6, 477, 364, 57505, 6, 477, 364, 72945, 3730, 12, 480, 285, 6853, 1550, 539, 617, 904, 315, 279, 7563, 198, 12, 58280, 1550, 539, 617, 904, 315, 279, 7563, 271, 1688, 420, 1486, 11, 480, 285, 6853, 574, 3025, 311, 12722, 24499, 279, 6425, 323, 3243, 279, 1847, 382, 3923, 1051, 279, 17011, 785, 7563, 304, 279, 6278, 315, 279, 2007, 30, 128003, 198, 128002, 78191, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\"Let's analyze the game:\\n\\nAfter the first turn, Summer sees that Connor has a 'Miss Scarlet', which means the cards in the middle of the table cannot be ('Miss Scarlet', 'Candlestick', and 'Hall'), or else Connor would have shown one of those cards to Summer.\\n\\nAfter Giselle's turn, we know that she had a 'Lounge' and had not seen a 'Mrs. White' or a 'Knife'. This means that Summer's cards cannot be ('Mrs. White', 'Knife', and 'Lounge'), otherwise, Giselle would have seen them in Summer's hand. \\n\\nAfter Connor's turn, we know that he had not seen a 'Miss Scarlet', a 'Candlestick', or a 'Hall', which means he must have two other cards. He showed a card to Summer, which means it was not one of the cards he needed, so it must be ('Mrs. White', 'Knife'), or ('Mr. Green', 'Lead Pipe').\\n\\nNow, let's consider the possibilities:\\n\\n- If Connor showed ('Mrs. White', 'Knife') to Summer, then Summer must have ('Mrs. White', 'Knife') or ('Mr. Green', 'Lead Pipe'). In this case, Giselle would have seen ('Mrs. White', 'Mrs. White') or ('Mrs. White', 'Mr. Green'), which is impossible since she only saw 'Miss Scarlet'.\\n- If Connor showed ('Mr. Green', 'Lead Pipe') to Summer, then Summer must have ('Mrs. White', 'Knife') and ('Miss Scarlet', 'Candlestick'). In this case, Giselle would have seen ('Miss Scarlet', 'Mrs. White'), which is possible, so this seems to be the solution.\\n\\nThe facedown cards in the middle of the table are ('Mrs. White', 'Knife', 'Dining Room').\", token_ids=(10267, 596, 24564, 279, 1847, 1473, 6153, 279, 1176, 2543, 11, 19367, 16008, 430, 58280, 706, 264, 364, 36412, 81818, 518, 902, 3445, 279, 7563, 304, 279, 6278, 315, 279, 2007, 4250, 387, 4417, 36412, 81818, 518, 364, 34, 3397, 30133, 518, 323, 364, 72945, 4670, 477, 775, 58280, 1053, 617, 6982, 832, 315, 1884, 7563, 311, 19367, 382, 6153, 480, 285, 6853, 596, 2543, 11, 584, 1440, 430, 1364, 1047, 264, 364, 43, 26645, 6, 323, 1047, 539, 3970, 264, 364, 50329, 13, 5929, 6, 477, 264, 364, 57505, 4527, 1115, 3445, 430, 19367, 596, 7563, 4250, 387, 4417, 50329, 13, 5929, 518, 364, 57505, 518, 323, 364, 43, 26645, 4670, 6062, 11, 480, 285, 6853, 1053, 617, 3970, 1124, 304, 19367, 596, 1450, 13, 4815, 6153, 58280, 596, 2543, 11, 584, 1440, 430, 568, 1047, 539, 3970, 264, 364, 36412, 81818, 518, 264, 364, 34, 3397, 30133, 518, 477, 264, 364, 72945, 518, 902, 3445, 568, 2011, 617, 1403, 1023, 7563, 13, 1283, 8710, 264, 3786, 311, 19367, 11, 902, 3445, 433, 574, 539, 832, 315, 279, 7563, 568, 4460, 11, 779, 433, 2011, 387, 4417, 50329, 13, 5929, 518, 364, 57505, 4670, 477, 4417, 12555, 13, 7997, 518, 364, 54963, 28905, 873, 382, 7184, 11, 1095, 596, 2980, 279, 24525, 1473, 12, 1442, 58280, 8710, 4417, 50329, 13, 5929, 518, 364, 57505, 873, 311, 19367, 11, 1243, 19367, 2011, 617, 4417, 50329, 13, 5929, 518, 364, 57505, 873, 477, 4417, 12555, 13, 7997, 518, 364, 54963, 28905, 1861, 763, 420, 1162, 11, 480, 285, 6853, 1053, 617, 3970, 4417, 50329, 13, 5929, 518, 364, 50329, 13, 5929, 873, 477, 4417, 50329, 13, 5929, 518, 364, 12555, 13, 7997, 4670, 902, 374, 12266, 2533, 1364, 1193, 5602, 364, 36412, 81818, 24482, 12, 1442, 58280, 8710, 4417, 12555, 13, 7997, 518, 364, 54963, 28905, 873, 311, 19367, 11, 1243, 19367, 2011, 617, 4417, 50329, 13, 5929, 518, 364, 57505, 873, 323, 4417, 36412, 81818, 518, 364, 34, 3397, 30133, 1861, 763, 420, 1162, 11, 480, 285, 6853, 1053, 617, 3970, 4417, 36412, 81818, 518, 364, 50329, 13, 5929, 4670, 902, 374, 3284, 11, 779, 420, 5084, 311, 387, 279, 6425, 382, 791, 17011, 785, 7563, 304, 279, 6278, 315, 279, 2007, 527, 4417, 50329, 13, 5929, 518, 364, 57505, 518, 364, 35, 5859, 10637, 1861, 128003), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729722069.1363103, last_token_time=1729722069.1363103, first_scheduled_time=1729722069.1391888, first_token_time=1729722069.199031, time_in_queue=0.002878427505493164, finished_time=1729722074.6116498, scheduler_time=0.02783600099337491, model_forward_time=None, model_execute_time=None), lora_request=None)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_model.train()\n",
    "llm.chat([[dict(role=\"user\", content=prompt)]] * 1, sampling_params=SamplingParams(max_tokens=10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mvllm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Dummy loss - just use mean of output\u001b[39;00m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:345\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer):\n\u001b[1;32m    344\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 345\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: hidden_states,\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m: residual\n\u001b[1;32m    353\u001b[0m     })\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:257\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(\n\u001b[1;32m    256\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 257\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    263\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    264\u001b[0m     hidden_states, residual)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:187\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    185\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    186\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(positions, q, k)\n\u001b[0;32m--> 187\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/layer.py:100\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, attn_type)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     92\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     attn_type: AttentionType \u001b[38;5;241m=\u001b[39m AttentionType\u001b[38;5;241m.\u001b[39mDECODER,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_k_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_v_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py:586\u001b[0m, in \u001b[0;36mFlashAttentionImpl.forward\u001b[0;34m(self, query, key, value, kv_cache, attn_metadata, k_scale, v_scale, attn_type)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# NOTE(woosuk): FlashAttention does not support FP8 KV cache.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m k_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m v_scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m, (\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey/v_scale is not supported in FlashAttention.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 586\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munified_flash_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malibi_slopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_soft_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[0;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/autograd.py:98\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.autograd_impl\u001b[0;34m(keyset, *args, **keyword_only_args)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mautograd_impl\u001b[39m(keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeyword_only_args):\n\u001b[0;32m---> 98\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mGenerated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_only_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/autograd.py:40\u001b[0m, in \u001b[0;36mmake_autograd_impl.<locals>.forward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m     38\u001b[0m keyset \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyset\n\u001b[1;32m     39\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m metadata\u001b[38;5;241m.\u001b[39mkeyword_only_args\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_autograd_keyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39m_setup_context_fn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# The Dispatcher will remove args that are equal to their default\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# values from (args, kwargs). We're going to add it back so that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# their setup_context (along with the rest of their operator\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# registrations)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mfill_defaults(op\u001b[38;5;241m.\u001b[39m_schema, args, kwargs)\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:672\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self_, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredispatch\u001b[39m(self_, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch_boxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:494\u001b[0m, in \u001b[0;36mCustomOpDef._register_to_dispatcher.<locals>.adinplaceorview_impl\u001b[0;34m(keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m                 autograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mincrement_version(v)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39m_AutoDispatchBelowADInplaceOrView():\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opoverload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_ADInplaceOrView_keyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_ops.py:672\u001b[0m, in \u001b[0;36mOpOverload.redispatch\u001b[0;34m(self_, keyset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mredispatch\u001b[39m(self_, keyset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mredispatch_boxed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/torch/_library/custom_ops.py:236\u001b[0m, in \u001b[0;36mCustomOpDef.register_kernel.<locals>.inner.<locals>.backend_impl\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackend_impl\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Checks the assumption that outputs cannot alias\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# inputs or other outputs.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     storages \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28mid\u001b[39m(tensor\u001b[38;5;241m.\u001b[39muntyped_storage())\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m iter_tensors(args, kwargs)\n\u001b[1;32m    234\u001b[0m     }\n\u001b[0;32m--> 236\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     tuple_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/atreides/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py:626\u001b[0m, in \u001b[0;36munified_flash_attention\u001b[0;34m(query, key, value, num_heads, head_size, num_kv_heads, kv_cache, kv_cache_dtype, k_scale, v_scale, softmax_scale, window_size, alibi_slopes, logits_soft_cap)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mcustom_op(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm::unified_flash_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    607\u001b[0m                          mutates_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkv_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munified_flash_attention\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m     logits_soft_cap: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    625\u001b[0m     current_metadata \u001b[38;5;241m=\u001b[39m get_forward_context()\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m current_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_metadata, FlashAttentionMetadata)\n\u001b[1;32m    628\u001b[0m     attn_metadata: FlashAttentionMetadata \u001b[38;5;241m=\u001b[39m current_metadata\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# I have this from earlier\n",
    "prompt_token_ids: list[int] = output[0].prompt_token_ids  # type: ignore\n",
    "\n",
    "# Create inputs based on your existing prompt_token_ids\n",
    "input_ids = torch.tensor([prompt_token_ids], dtype=torch.long, device=\"cuda:0\")\n",
    "positions = torch.arange(len(prompt_token_ids), device=\"cuda:0\").unsqueeze(0)\n",
    "kv_caches = [\n",
    "    torch.zeros(\n",
    "        1,\n",
    "        vllm_model.layers[0].self_attn.num_heads,\n",
    "        len(prompt_token_ids),\n",
    "        vllm_model.layers[0].self_attn.head_dim,\n",
    "        device=\"cuda:0\",\n",
    "    )\n",
    "    for _ in range(len(vllm_model.layers))\n",
    "]\n",
    "attn_metadata = {\n",
    "    \"attention_mask\": torch.ones(1, len(prompt_token_ids), device=\"cuda:0\"),\n",
    "    \"sequence_lengths\": torch.tensor([len(prompt_token_ids)], device=\"cuda:0\"),\n",
    "    \"max_sequence_length\": len(prompt_token_ids),\n",
    "}\n",
    "\n",
    "# Try training step\n",
    "optimizer = torch.optim.Adam(vllm_model.parameters())\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "# try:\n",
    "output = vllm_model(input_ids, positions, kv_caches, attn_metadata, None)\n",
    "\n",
    "# Dummy loss - just use mean of output\n",
    "loss = output.mean()\n",
    "\n",
    "# Check if backprop works\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Model is trainable! Gradients computed and parameters updated.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Model is not trainable. Error: {type(e)}\")\n",
    "\n",
    "#     # Let's get more info about what might be wrong\n",
    "#     print(\"\\nChecking parameter properties:\")\n",
    "#     for name, param in model.named_parameters():\n",
    "#         print(f\"{name}:\")\n",
    "#         print(f\"  requires_grad: {param.requires_grad}\")\n",
    "#         print(f\"  grad_fn: {param.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "model_dir = subprocess.run(\n",
    "    \"HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    shell=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ").stdout.strip()\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.training.checkpointing import FullModelHFCheckpointer\n",
    "\n",
    "checkpointer = FullModelHFCheckpointer(\n",
    "    checkpoint_dir=model_dir,\n",
    "    checkpoint_files=glob.glob(f\"{model_dir}/*.safetensors\"),\n",
    "    output_dir=model_dir,\n",
    "    model_type='LLAMA3' # type: ignore\n",
    ")\n",
    "state_dict = checkpointer.load_checkpoint()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(mapped_vllm_state_dict, assign=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0100,  0.0172,  0.0035,  ...,  0.0012, -0.0167, -0.0111],\n",
       "        [-0.0067,  0.0117,  0.0114,  ..., -0.0090,  0.0092, -0.0007],\n",
       "        [ 0.0142,  0.0093,  0.0083,  ..., -0.0025, -0.0064, -0.0140],\n",
       "        ...,\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077],\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077],\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0100,  0.0172,  0.0035,  ...,  0.0012, -0.0167, -0.0111],\n",
       "        [-0.0067,  0.0117,  0.0114,  ..., -0.0090,  0.0092, -0.0007],\n",
       "        [ 0.0142,  0.0093,  0.0083,  ..., -0.0025, -0.0064, -0.0140],\n",
       "        ...,\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077],\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077],\n",
       "        [-0.0023, -0.0005,  0.0067,  ...,  0.0008,  0.0066,  0.0077]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_vllm_state_dict[\"output.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "\n",
    "model = llama3_1_8b()\n",
    "model.load_state_dict(state_dict[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128000,\n",
       " 128002,\n",
       " 882,\n",
       " 198,\n",
       " 1966,\n",
       " 264,\n",
       " 8369,\n",
       " 10683,\n",
       " 1938,\n",
       " 19367,\n",
       " 11,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 323,\n",
       " 58280,\n",
       " 7731,\n",
       " 1523,\n",
       " 311,\n",
       " 1514,\n",
       " 264,\n",
       " 16736,\n",
       " 23347,\n",
       " 1847,\n",
       " 382,\n",
       " 7009,\n",
       " 35105,\n",
       " 220,\n",
       " 18,\n",
       " 30881,\n",
       " 315,\n",
       " 7563,\n",
       " 11,\n",
       " 1855,\n",
       " 369,\n",
       " 264,\n",
       " 8821,\n",
       " 955,\n",
       " 315,\n",
       " 2038,\n",
       " 24306,\n",
       " 315,\n",
       " 279,\n",
       " 2768,\n",
       " 1473,\n",
       " 78524,\n",
       " 1002,\n",
       " 512,\n",
       " 12,\n",
       " 9083,\n",
       " 81818,\n",
       " 198,\n",
       " 12,\n",
       " 4491,\n",
       " 13,\n",
       " 7997,\n",
       " 198,\n",
       " 12,\n",
       " 18083,\n",
       " 13,\n",
       " 5929,\n",
       " 271,\n",
       " 29314,\n",
       " 512,\n",
       " 12,\n",
       " 73997,\n",
       " 30133,\n",
       " 198,\n",
       " 12,\n",
       " 62302,\n",
       " 198,\n",
       " 12,\n",
       " 30982,\n",
       " 28905,\n",
       " 271,\n",
       " 14330,\n",
       " 512,\n",
       " 12,\n",
       " 11166,\n",
       " 198,\n",
       " 12,\n",
       " 50767,\n",
       " 198,\n",
       " 12,\n",
       " 39190,\n",
       " 10637,\n",
       " 271,\n",
       " 6153,\n",
       " 27716,\n",
       " 320,\n",
       " 438,\n",
       " 89447,\n",
       " 8,\n",
       " 19301,\n",
       " 832,\n",
       " 3786,\n",
       " 505,\n",
       " 1855,\n",
       " 1912,\n",
       " 323,\n",
       " 25012,\n",
       " 1124,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 17011,\n",
       " 785,\n",
       " 11,\n",
       " 814,\n",
       " 75371,\n",
       " 279,\n",
       " 9861,\n",
       " 7563,\n",
       " 323,\n",
       " 27023,\n",
       " 704,\n",
       " 279,\n",
       " 2768,\n",
       " 311,\n",
       " 1855,\n",
       " 2851,\n",
       " 1473,\n",
       " 12,\n",
       " 19367,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 4417,\n",
       " 43,\n",
       " 26645,\n",
       " 518,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 1329,\n",
       " 12,\n",
       " 58280,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 271,\n",
       " 791,\n",
       " 1847,\n",
       " 45374,\n",
       " 439,\n",
       " 11263,\n",
       " 1473,\n",
       " 16,\n",
       " 13,\n",
       " 1952,\n",
       " 872,\n",
       " 2543,\n",
       " 11,\n",
       " 264,\n",
       " 2851,\n",
       " 4691,\n",
       " 922,\n",
       " 264,\n",
       " 743,\n",
       " 315,\n",
       " 7041,\n",
       " 220,\n",
       " 18,\n",
       " 7563,\n",
       " 11,\n",
       " 832,\n",
       " 505,\n",
       " 1855,\n",
       " 315,\n",
       " 279,\n",
       " 1847,\n",
       " 596,\n",
       " 11306,\n",
       " 13,\n",
       " 320,\n",
       " 9290,\n",
       " 25,\n",
       " 25640,\n",
       " 1436,\n",
       " 2610,\n",
       " 922,\n",
       " 904,\n",
       " 7563,\n",
       " 11,\n",
       " 2737,\n",
       " 1884,\n",
       " 304,\n",
       " 872,\n",
       " 1866,\n",
       " 1450,\n",
       " 29275,\n",
       " 17,\n",
       " 13,\n",
       " 578,\n",
       " 2851,\n",
       " 15910,\n",
       " 420,\n",
       " 3488,\n",
       " 311,\n",
       " 279,\n",
       " 1023,\n",
       " 4311,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 11,\n",
       " 6041,\n",
       " 449,\n",
       " 279,\n",
       " 2851,\n",
       " 311,\n",
       " 872,\n",
       " 2163,\n",
       " 627,\n",
       " 18,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 1047,\n",
       " 832,\n",
       " 477,\n",
       " 810,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1047,\n",
       " 311,\n",
       " 1501,\n",
       " 832,\n",
       " 315,\n",
       " 1884,\n",
       " 7563,\n",
       " 320,\n",
       " 1073,\n",
       " 872,\n",
       " 5873,\n",
       " 8,\n",
       " 311,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 38171,\n",
       " 13,\n",
       " 578,\n",
       " 2543,\n",
       " 1243,\n",
       " 9670,\n",
       " 11,\n",
       " 323,\n",
       " 1514,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 19,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1071,\n",
       " 779,\n",
       " 11,\n",
       " 323,\n",
       " 279,\n",
       " 3488,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 627,\n",
       " 20,\n",
       " 13,\n",
       " 1115,\n",
       " 8738,\n",
       " 3156,\n",
       " 3060,\n",
       " 512,\n",
       " 262,\n",
       " 264,\n",
       " 8,\n",
       " 362,\n",
       " 2851,\n",
       " 8710,\n",
       " 264,\n",
       " 3786,\n",
       " 311,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 11,\n",
       " 477,\n",
       " 198,\n",
       " 262,\n",
       " 293,\n",
       " 8,\n",
       " 2052,\n",
       " 279,\n",
       " 79002,\n",
       " 4311,\n",
       " 1047,\n",
       " 11224,\n",
       " 814,\n",
       " 3287,\n",
       " 956,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 627,\n",
       " 21,\n",
       " 13,\n",
       " 4740,\n",
       " 264,\n",
       " 2851,\n",
       " 596,\n",
       " 2543,\n",
       " 9670,\n",
       " 320,\n",
       " 50998,\n",
       " 555,\n",
       " 1694,\n",
       " 6982,\n",
       " 264,\n",
       " 3786,\n",
       " 477,\n",
       " 3515,\n",
       " 682,\n",
       " 79002,\n",
       " 4311,\n",
       " 1522,\n",
       " 705,\n",
       " 1514,\n",
       " 7882,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 382,\n",
       " 8586,\n",
       " 374,\n",
       " 1268,\n",
       " 279,\n",
       " 1847,\n",
       " 6476,\n",
       " 704,\n",
       " 1473,\n",
       " 51787,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 50329,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 5859,\n",
       " 10637,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 8710,\n",
       " 19367,\n",
       " 264,\n",
       " 3786,\n",
       " 271,\n",
       " 38,\n",
       " 285,\n",
       " 6853,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 50329,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 43,\n",
       " 26645,\n",
       " 3730,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 57987,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 3397,\n",
       " 30133,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 8710,\n",
       " 58280,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 3961,\n",
       " 51787,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 12555,\n",
       " 13,\n",
       " 7997,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 57505,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 1688,\n",
       " 420,\n",
       " 1486,\n",
       " 11,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 574,\n",
       " 3025,\n",
       " 311,\n",
       " 12722,\n",
       " 24499,\n",
       " 279,\n",
       " 6425,\n",
       " 323,\n",
       " 3243,\n",
       " 279,\n",
       " 1847,\n",
       " 382,\n",
       " 3923,\n",
       " 1051,\n",
       " 279,\n",
       " 17011,\n",
       " 785,\n",
       " 7563,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 30,\n",
       " 128003,\n",
       " 198,\n",
       " 128002,\n",
       " 78191,\n",
       " 198]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tokens(messages: list[dict]) -> list[int]:\n",
    "    generate = llm.generate\n",
    "\n",
    "    def get_tokens(prompts: list[dict], *args: object, **kwargs: object) -> list[int]:\n",
    "        return llm.get_tokenizer().encode(prompts[0][\"prompt\"])\n",
    "\n",
    "    llm.generate = get_tokens  # type: ignore\n",
    "    tokens = llm.chat(messages)  # type: ignore\n",
    "    llm.generate = generate  # type: ignore\n",
    "    return tokens  # type: ignore\n",
    "\n",
    "\n",
    "get_tokens([dict(role=\"user\", content=prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "\n",
    "destroy_model_parallel()\n",
    "del llm.llm_engine.model_executor.driver_worker  # type: ignore\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerSelfAttentionLayer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (pos_embeddings): Llama3ScaledRoPE()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (sa_norm): RMSNorm()\n",
       "      (mlp_norm): RMSNorm()\n",
       "      (sa_scale): Identity()\n",
       "      (mlp_scale): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move all model parameters and buffers to CUDA\n",
    "for param in model.parameters():\n",
    "    if param.device.type != 'cuda':\n",
    "        param.data = param.data.to('cuda')\n",
    "        \n",
    "for buffer in model.buffers():\n",
    "    if buffer.device.type != 'cuda':\n",
    "        buffer.data = buffer.data.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtune.generation import generate\n",
    "\n",
    "result = generate(\n",
    "    model,\n",
    "    torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])], device=\"cuda\"),\n",
    "    max_generated_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|im_start|>user\n",
      "On a warm spring day Summer, Giselle and Connor sat down to play a casual mystery game.\n",
      "\n",
      "They assembled 3 decks of cards, each for a separate type of information composed of the following:\n",
      "\n",
      "Suspect:\n",
      "- Miss Scarlet\n",
      "- Mr. Green\n",
      "- Mrs. White\n",
      "\n",
      "Weapon:\n",
      "- Candlestick\n",
      "- Knife\n",
      "- Lead Pipe\n",
      "\n",
      "Room:\n",
      "- Hall\n",
      "- Lounge\n",
      "- Dining Room\n",
      "\n",
      "After randomly (and blindly) choosing one card from each group and placing them in the middle of the table facedown, they shuffled the remaining cards and dealt out the following to each player:\n",
      "\n",
      "- Summer: 2 cards\n",
      "- Giselle: 2 cards ('Lounge', 'Miss Scarlet')\n",
      "- Connor: 2 cards\n",
      "\n",
      "The game proceeded as follows:\n",
      "\n",
      "1. On their turn, a player asked about a set of exactly 3 cards, one from each of the game's categories. (Note: Players could ask about any cards, including those in their own hand.)\n",
      "2. The player directed this question to the other players in clockwise order, starting with the player to their left.\n",
      "3. If a player had one or more of the asked-about cards, they had to show one of those cards (of their choice) to the asking player privately. The turn then ended, and play passed to the next player.\n",
      "4. If a player did not have any of the asked-about cards, they said so, and the question passed to the next player in clockwise order.\n",
      "5. This continued until either:\n",
      "    a) A player showed a card to the asking player, or\n",
      "    b) All the queried players had stated they didn't have any of the asked-about cards.\n",
      "6. After a player's turn ended (either by being shown a card or having all queried players pass), play moved to the next player in clockwise order.\n",
      "\n",
      "Here is how the game played out:\n",
      "\n",
      "Summer asked if anyone had 'Mrs. White' or 'Knife' or 'Dining Room':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor showed Summer a card\n",
      "\n",
      "Giselle asked if anyone had 'Mrs. White' or 'Knife' or 'Lounge':\n",
      "- Connor did not have any of the cards\n",
      "- Summer did not have any of the cards\n",
      "\n",
      "Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'Hall':\n",
      "- Summer did not have any of the cards\n",
      "- Giselle showed Connor 'Miss Scarlet'\n",
      "\n",
      "Summer asked if anyone had 'Mr. Green' or 'Knife' or 'Hall':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor did not have any of the cards\n",
      "\n",
      "At this point, Giselle was able to correctly infer the solution and win the game.\n",
      "\n",
      "What were the facedown cards in the middle of the table?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " - hatch\n",
      " present allowed\n",
      "\n",
      " the - conc or\n",
      " weren-8\n",
      " and\n",
      "                          \n",
      " of today\n",
      " #- every world?\n",
      "\n",
      " As,\n",
      " Tomorrow, optimeter one were. teams: them\n",
      " a\n",
      " understanding-for\n",
      " the\n",
      " the the\n",
      " telephone\n",
      " \"\n",
      "\n",
      "- some \n",
      " had tricks\n",
      " me ( t, regulated[,\n",
      " completed that\n",
      " had left\n",
      " and\n",
      " Membership the:N Teller\n",
      " top( there Every\n",
      " following things\n",
      "\n",
      " 00- d,A intense entity\n",
      "\n",
      " ignored,the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_tokenizer().decode(result[0].squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questiondefee\n",
      "给 this certain sunny day in, aretaelle, Alex are on to have a game game game.\" had a36 different of cards, each containing themselves different game of clues: of  following:\n",
      "\n",
      "Deckpects cards- Mr Scarlet\n",
      "- Colonel. Green\n",
      "- Mrs. White\n",
      "Weapon:\n",
      "- Pistolstick\n",
      "- Rev\n",
      "- Rope Pipe\n",
      "\n",
      "Room:\n",
      "- Kitchen\n",
      "- Kitchen\n",
      "- Kitchen Room\n",
      "\n",
      "Each each shwithout secretly) selecting a card from each deck, putting them face a center of the table,own, G had them cards cards and drew  the following hands each player:Sus : Sus3 cards (- Giselle: 2 cards\n",
      "randomounge' 'Miss Scarlet')\n",
      "- Connor: 2 cards ('Summer rest was with such:\n",
      "\n",
      "1. Summer each turn, a player can a the specific of  two2 cards ( one from each group the three's three ( ForSus that A could only about the  they even those they front own hand.)\n",
      "2. The other then the question to one person two, a order.\n",
      " starting from the player to their right.3. Each a player had one of more of the asked cards cards in they revealed to reveal it of them cards fromof their choice) to the asker player.\n",
      ". \n",
      " other then ended.\n",
      " and playopl to the next player.\n",
      "4. If none player did青年 have any of the asked-about cards, they responded \", rebuilding the asking was to the next player.\n",
      " clockwise order.\n",
      "5. If process until one:\n",
      "    -. A player saw a card that the asking player ( ending\n",
      "    b) All players asked cards had passed they did't have any of the asked-about cards.\n",
      "\n",
      "6. The every card showed turn,,either by showing shown a card or not their players players pass), they passed to the next player in clockwise order.\n",
      "\n",
      "The is the the game proceeded out:\n",
      "\n",
      "Summer started G anyone had theMiss. White' in 'Knife' or 'Dining Room'.- Giselle, not have any of these cards,- Connor had G the ':Giselle asked if anyone had 'Miss. White startDate or 'C' or 'Hallounge':\n",
      "- Summer did not have any of the cards\n",
      "- Summer said not have any of the cards (Connor asked if anyone had 'Miss Scarlet' or 'Candlestick' or 'D':\n",
      "- Summer did not have any of the cards\n",
      "- Giselle said Summer aL Scarlet'\n",
      "\n",
      "G asked if anyone had 'Mr. Green' or 'Lead' or 'Hall':\n",
      "- Giselle did not have any of the cards\n",
      "- Connor showed not have any of the cards\n",
      "\n",
      "G this point, Connoriselle decided sure to ded ded the identities. announced the game.\n",
      "\n",
      "What cards the threeown cards in the middle of the table?\n",
      "\n",
      " \n",
      "user\n",
      "\n",
      "Since\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tokens = torch.tensor([get_tokens([dict(role=\"user\", content=prompt)])])\n",
    "seq_len = input_tokens.size(1)\n",
    "\n",
    "mask = torch.tril(torch.ones((1, seq_len, seq_len), dtype=torch.bool))\n",
    "input_pos = torch.arange(seq_len).unsqueeze(0).expand(1, seq_len)\n",
    "\n",
    "output = model.forward(input_tokens, mask=mask, input_pos=input_pos)\n",
    "\n",
    "# Handle output whether it's a tensor or list of tensors\n",
    "if isinstance(output, list):\n",
    "    output = output[-1]\n",
    "output = output.squeeze(0)\n",
    "\n",
    "# Apply temperature sampling\n",
    "temperature = 0.7\n",
    "logits = output / temperature\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Sample from the probability distribution\n",
    "predicted_tokens = torch.multinomial(probs, num_samples=1).squeeze(-1).tolist()\n",
    "\n",
    "print(llm.get_tokenizer().decode(predicted_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QuestionQuestionee\\nWhat a certain summer day, and aabeelle, I were on to have a game game game.Summer decided a5 boxes of cards, shuffled containing a different game of clue: of  following:\\n\\n1pects cards-  Scarlet\\n- Colonel. Green\\n- Colonel. White\\nWeapon:\\n- Candlestick\\n- Rev\\n- Rev Pipe\\n\\nRoom:\\n- Kitchen\\n- Kitchen\\n- Kitchen Room\\n\\nThe sh shand secretly) selecting a card from each deck, placing them face a center of the table,own, they decided them remaining cards and drew   following:\\n\\n each player:\\n\\nSus : Sus2 Sus from- Giselle: 2 cards\\ncauseounge\\' \\'C Scarlet\\')\\n- Connor: 2 cards (\\'The remaining began with follows:\\n\\n1. Summer their turn, a player could a a specific of cards two2 cards ( one from each group the\\'s three ( ForSus that A could not about the  they including the they the own hand.)\\n2. The player who their question to another person two, the order.\\n starting with the player to their left.\\n3. Each a player knew one of more of the cards cards cards, they revealed to reveal them of them cards toof their choice) to the asker player.\\n.\\n If card then ended.\\n and the passed to the next player.\\n4. If no player did not have any of the asked-about cards, they said \" and and the turn was to the next player.\\n clockwise order.\\n5. If process until all all    -. A player had a card that the asking player, or\\n    b) The players asked cards had passed they did\\'t have any of the asked-about cards.\\n\\n6. If a player showed turn,,either by showing shown a card or by their players players pass), they passed to the next player in clockwise order.\\n\\nSummer\\'s the the game proceeded out:\\n\\nSummer: G anyone had theMiss. White\\', or \\'C\\' or \\'Dining Room\\'.- Giselle said not have any of those asked,- Connor showed Summer the\\'fromGiselle asked if anyone had \\'Miss. White\\' or \\'C\\' or \\'Hallounge\\':\\n- Summer did not have any of the cards\\n- Summer showed not have any of the cards\\n\\nConnor asked if anyone had \\'Miss Scarlet\\' or \\'Candlestick\\' or \\'Hall\\':\\n- Summer did not have any of the cards\\n- Giselle showed Summer aMiss Scarlet\\'\\n\\nG asked if anyone had \\'Mr. Green\\' or \\'Lead\\' or \\'D\\':\\n- Giselle did not have any of the cards\\n- Connor did not have any of the cards\\n\\nG this point, Summeriselle had able to ded ded the remaining. said the game.\\n\\nWhat were the cardsown cards in the middle of the table?\\n\\n \\n\\n\\nuser\\n\\nLet'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14924,\n",
       " 14924,\n",
       " 2176,\n",
       " 198,\n",
       " 3923,\n",
       " 264,\n",
       " 3738,\n",
       " 7474,\n",
       " 1938,\n",
       " 11,\n",
       " 323,\n",
       " 264,\n",
       " 8393,\n",
       " 6853,\n",
       " 11,\n",
       " 358,\n",
       " 1051,\n",
       " 389,\n",
       " 311,\n",
       " 617,\n",
       " 264,\n",
       " 1847,\n",
       " 1847,\n",
       " 1847,\n",
       " 13,\n",
       " 51787,\n",
       " 6773,\n",
       " 264,\n",
       " 20,\n",
       " 15039,\n",
       " 315,\n",
       " 7563,\n",
       " 11,\n",
       " 75371,\n",
       " 8649,\n",
       " 264,\n",
       " 2204,\n",
       " 1847,\n",
       " 315,\n",
       " 31089,\n",
       " 25,\n",
       " 315,\n",
       " 220,\n",
       " 2768,\n",
       " 1473,\n",
       " 16,\n",
       " 8132,\n",
       " 7563,\n",
       " 12,\n",
       " 220,\n",
       " 81818,\n",
       " 198,\n",
       " 12,\n",
       " 52798,\n",
       " 13,\n",
       " 7997,\n",
       " 198,\n",
       " 12,\n",
       " 52798,\n",
       " 13,\n",
       " 5929,\n",
       " 198,\n",
       " 29314,\n",
       " 512,\n",
       " 12,\n",
       " 73997,\n",
       " 30133,\n",
       " 198,\n",
       " 12,\n",
       " 10315,\n",
       " 198,\n",
       " 12,\n",
       " 10315,\n",
       " 28905,\n",
       " 271,\n",
       " 14330,\n",
       " 512,\n",
       " 12,\n",
       " 19915,\n",
       " 198,\n",
       " 12,\n",
       " 19915,\n",
       " 198,\n",
       " 12,\n",
       " 19915,\n",
       " 10637,\n",
       " 271,\n",
       " 791,\n",
       " 559,\n",
       " 559,\n",
       " 438,\n",
       " 42839,\n",
       " 8,\n",
       " 27397,\n",
       " 264,\n",
       " 3786,\n",
       " 505,\n",
       " 1855,\n",
       " 9722,\n",
       " 11,\n",
       " 25012,\n",
       " 1124,\n",
       " 3663,\n",
       " 264,\n",
       " 4219,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 11,\n",
       " 785,\n",
       " 11,\n",
       " 814,\n",
       " 6773,\n",
       " 1124,\n",
       " 9861,\n",
       " 7563,\n",
       " 323,\n",
       " 24465,\n",
       " 220,\n",
       " 220,\n",
       " 2768,\n",
       " 1473,\n",
       " 1855,\n",
       " 2851,\n",
       " 1473,\n",
       " 78524,\n",
       " 220,\n",
       " 25,\n",
       " 16687,\n",
       " 17,\n",
       " 16687,\n",
       " 505,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 198,\n",
       " 1593,\n",
       " 26645,\n",
       " 6,\n",
       " 364,\n",
       " 34,\n",
       " 81818,\n",
       " 1329,\n",
       " 12,\n",
       " 58280,\n",
       " 25,\n",
       " 220,\n",
       " 17,\n",
       " 7563,\n",
       " 4417,\n",
       " 791,\n",
       " 9861,\n",
       " 6137,\n",
       " 449,\n",
       " 11263,\n",
       " 1473,\n",
       " 16,\n",
       " 13,\n",
       " 19367,\n",
       " 872,\n",
       " 2543,\n",
       " 11,\n",
       " 264,\n",
       " 2851,\n",
       " 1436,\n",
       " 264,\n",
       " 264,\n",
       " 3230,\n",
       " 315,\n",
       " 7563,\n",
       " 1403,\n",
       " 17,\n",
       " 7563,\n",
       " 320,\n",
       " 832,\n",
       " 505,\n",
       " 1855,\n",
       " 1912,\n",
       " 279,\n",
       " 220,\n",
       " 596,\n",
       " 2380,\n",
       " 320,\n",
       " 1789,\n",
       " 78524,\n",
       " 430,\n",
       " 362,\n",
       " 1436,\n",
       " 539,\n",
       " 922,\n",
       " 279,\n",
       " 220,\n",
       " 814,\n",
       " 2737,\n",
       " 279,\n",
       " 814,\n",
       " 279,\n",
       " 1866,\n",
       " 1450,\n",
       " 29275,\n",
       " 17,\n",
       " 13,\n",
       " 578,\n",
       " 2851,\n",
       " 889,\n",
       " 872,\n",
       " 3488,\n",
       " 311,\n",
       " 2500,\n",
       " 1732,\n",
       " 1403,\n",
       " 11,\n",
       " 279,\n",
       " 2015,\n",
       " 627,\n",
       " 6041,\n",
       " 449,\n",
       " 279,\n",
       " 2851,\n",
       " 311,\n",
       " 872,\n",
       " 2163,\n",
       " 627,\n",
       " 18,\n",
       " 13,\n",
       " 9062,\n",
       " 264,\n",
       " 2851,\n",
       " 7020,\n",
       " 832,\n",
       " 315,\n",
       " 810,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 7563,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 10675,\n",
       " 311,\n",
       " 16805,\n",
       " 1124,\n",
       " 315,\n",
       " 1124,\n",
       " 7563,\n",
       " 311,\n",
       " 1073,\n",
       " 872,\n",
       " 5873,\n",
       " 8,\n",
       " 311,\n",
       " 279,\n",
       " 113408,\n",
       " 2851,\n",
       " 627,\n",
       " 627,\n",
       " 1442,\n",
       " 3786,\n",
       " 1243,\n",
       " 9670,\n",
       " 627,\n",
       " 323,\n",
       " 279,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 19,\n",
       " 13,\n",
       " 1442,\n",
       " 912,\n",
       " 2851,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 11,\n",
       " 814,\n",
       " 1071,\n",
       " 330,\n",
       " 323,\n",
       " 323,\n",
       " 279,\n",
       " 2543,\n",
       " 574,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 627,\n",
       " 66770,\n",
       " 2015,\n",
       " 627,\n",
       " 20,\n",
       " 13,\n",
       " 1442,\n",
       " 1920,\n",
       " 3156,\n",
       " 682,\n",
       " 682,\n",
       " 262,\n",
       " 482,\n",
       " 13,\n",
       " 362,\n",
       " 2851,\n",
       " 1047,\n",
       " 264,\n",
       " 3786,\n",
       " 430,\n",
       " 279,\n",
       " 10371,\n",
       " 2851,\n",
       " 11,\n",
       " 477,\n",
       " 198,\n",
       " 262,\n",
       " 293,\n",
       " 8,\n",
       " 578,\n",
       " 4311,\n",
       " 4691,\n",
       " 7563,\n",
       " 1047,\n",
       " 5946,\n",
       " 814,\n",
       " 1550,\n",
       " 956,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 4691,\n",
       " 69205,\n",
       " 7563,\n",
       " 382,\n",
       " 21,\n",
       " 13,\n",
       " 1442,\n",
       " 264,\n",
       " 2851,\n",
       " 8710,\n",
       " 2543,\n",
       " 11,\n",
       " 11,\n",
       " 50998,\n",
       " 555,\n",
       " 9204,\n",
       " 6982,\n",
       " 264,\n",
       " 3786,\n",
       " 477,\n",
       " 555,\n",
       " 872,\n",
       " 4311,\n",
       " 4311,\n",
       " 1522,\n",
       " 705,\n",
       " 814,\n",
       " 5946,\n",
       " 311,\n",
       " 279,\n",
       " 1828,\n",
       " 2851,\n",
       " 304,\n",
       " 66770,\n",
       " 2015,\n",
       " 382,\n",
       " 51787,\n",
       " 596,\n",
       " 279,\n",
       " 279,\n",
       " 1847,\n",
       " 45374,\n",
       " 704,\n",
       " 1473,\n",
       " 51787,\n",
       " 25,\n",
       " 480,\n",
       " 5606,\n",
       " 1047,\n",
       " 279,\n",
       " 36412,\n",
       " 13,\n",
       " 5929,\n",
       " 518,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 5859,\n",
       " 10637,\n",
       " 4527,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1071,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 1884,\n",
       " 4691,\n",
       " 11,\n",
       " 12,\n",
       " 58280,\n",
       " 8710,\n",
       " 19367,\n",
       " 279,\n",
       " 364,\n",
       " 505,\n",
       " 38,\n",
       " 285,\n",
       " 6853,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 13,\n",
       " 5929,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 26645,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 19367,\n",
       " 8710,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 57987,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 36412,\n",
       " 81818,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 34,\n",
       " 3397,\n",
       " 30133,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 72945,\n",
       " 3730,\n",
       " 12,\n",
       " 19367,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 8710,\n",
       " 19367,\n",
       " 264,\n",
       " 36412,\n",
       " 81818,\n",
       " 3961,\n",
       " 38,\n",
       " 4691,\n",
       " 422,\n",
       " 5606,\n",
       " 1047,\n",
       " 364,\n",
       " 12555,\n",
       " 13,\n",
       " 7997,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 54963,\n",
       " 6,\n",
       " 477,\n",
       " 364,\n",
       " 35,\n",
       " 3730,\n",
       " 12,\n",
       " 480,\n",
       " 285,\n",
       " 6853,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 198,\n",
       " 12,\n",
       " 58280,\n",
       " 1550,\n",
       " 539,\n",
       " 617,\n",
       " 904,\n",
       " 315,\n",
       " 279,\n",
       " 7563,\n",
       " 271,\n",
       " 38,\n",
       " 420,\n",
       " 1486,\n",
       " 11,\n",
       " 19367,\n",
       " 285,\n",
       " 6853,\n",
       " 1047,\n",
       " 3025,\n",
       " 311,\n",
       " 7836,\n",
       " 7836,\n",
       " 279,\n",
       " 9861,\n",
       " 13,\n",
       " 1071,\n",
       " 279,\n",
       " 1847,\n",
       " 382,\n",
       " 3923,\n",
       " 1051,\n",
       " 279,\n",
       " 7563,\n",
       " 785,\n",
       " 7563,\n",
       " 304,\n",
       " 279,\n",
       " 6278,\n",
       " 315,\n",
       " 279,\n",
       " 2007,\n",
       " 1980,\n",
       " 4815,\n",
       " 198,\n",
       " 882,\n",
       " 198,\n",
       " 198,\n",
       " 10267]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.generation import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tiktoken.load import dump_tiktoken_bpe\n",
    "\n",
    "tokenizer_data = json.load(open(f\"{model_dir}/tokenizer.json\", \"r\"))\n",
    "dump_tiktoken_bpe(\n",
    "    {token.encode(): rank for token, rank in tokenizer_data[\"model\"][\"vocab\"].items()},\n",
    "    f\"{model_dir}/tokenizer.bpe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtune.models.llama3._tokenizer.Llama3Tokenizer at 0x736d0f378500>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.data import ChatMLTemplate\n",
    "from torchtune.models.llama3 import Llama3Tokenizer\n",
    "\n",
    "tokenizer = Llama3Tokenizer(\n",
    "    path=model_dir + \"/tokenizer.bpe\", prompt_template=ChatMLTemplate()  # type: ignore\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_models.llama3.api.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer.tt_model.tt_model = Tokenizer.get_instance().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128006, 882, 128007, 271, 9906, 11, 1917, 0, 128009]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.data import Message\n",
    "\n",
    "tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor([tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|begin_of_text|>': 128000,\n",
       " '<|end_of_text|>': 128001,\n",
       " '<|reserved_special_token_0|>': 128002,\n",
       " '<|reserved_special_token_1|>': 128003,\n",
       " '<|finetune_right_pad_id|>': 128004,\n",
       " '<|step_id|>': 128005,\n",
       " '<|start_header_id|>': 128006,\n",
       " '<|end_header_id|>': 128007,\n",
       " '<|eom_id|>': 128008,\n",
       " '<|eot_id|>': 128009,\n",
       " '<|python_tag|>': 128010,\n",
       " '<|image|>': 128256,\n",
       " '<|video|>': 128012,\n",
       " '<|reserved_special_token_2|>': 128013,\n",
       " '<|reserved_special_token_3|>': 128014,\n",
       " '<|reserved_special_token_4|>': 128015,\n",
       " '<|reserved_special_token_5|>': 128016,\n",
       " '<|reserved_special_token_6|>': 128017,\n",
       " '<|reserved_special_token_7|>': 128018,\n",
       " '<|reserved_special_token_8|>': 128019,\n",
       " '<|reserved_special_token_9|>': 128020,\n",
       " '<|reserved_special_token_10|>': 128021,\n",
       " '<|reserved_special_token_11|>': 128022,\n",
       " '<|reserved_special_token_12|>': 128023,\n",
       " '<|reserved_special_token_13|>': 128024,\n",
       " '<|reserved_special_token_14|>': 128025,\n",
       " '<|reserved_special_token_15|>': 128026,\n",
       " '<|reserved_special_token_16|>': 128027,\n",
       " '<|reserved_special_token_17|>': 128028,\n",
       " '<|reserved_special_token_18|>': 128029,\n",
       " '<|reserved_special_token_19|>': 128030,\n",
       " '<|reserved_special_token_20|>': 128031,\n",
       " '<|reserved_special_token_21|>': 128032,\n",
       " '<|reserved_special_token_22|>': 128033,\n",
       " '<|reserved_special_token_23|>': 128034,\n",
       " '<|reserved_special_token_24|>': 128035,\n",
       " '<|reserved_special_token_25|>': 128036,\n",
       " '<|reserved_special_token_26|>': 128037,\n",
       " '<|reserved_special_token_27|>': 128038,\n",
       " '<|reserved_special_token_28|>': 128039,\n",
       " '<|reserved_special_token_29|>': 128040,\n",
       " '<|reserved_special_token_30|>': 128041,\n",
       " '<|reserved_special_token_31|>': 128042,\n",
       " '<|reserved_special_token_32|>': 128043,\n",
       " '<|reserved_special_token_33|>': 128044,\n",
       " '<|reserved_special_token_34|>': 128045,\n",
       " '<|reserved_special_token_35|>': 128046,\n",
       " '<|reserved_special_token_36|>': 128047,\n",
       " '<|reserved_special_token_37|>': 128048,\n",
       " '<|reserved_special_token_38|>': 128049,\n",
       " '<|reserved_special_token_39|>': 128050,\n",
       " '<|reserved_special_token_40|>': 128051,\n",
       " '<|reserved_special_token_41|>': 128052,\n",
       " '<|reserved_special_token_42|>': 128053,\n",
       " '<|reserved_special_token_43|>': 128054,\n",
       " '<|reserved_special_token_44|>': 128055,\n",
       " '<|reserved_special_token_45|>': 128056,\n",
       " '<|reserved_special_token_46|>': 128057,\n",
       " '<|reserved_special_token_47|>': 128058,\n",
       " '<|reserved_special_token_48|>': 128059,\n",
       " '<|reserved_special_token_49|>': 128060,\n",
       " '<|reserved_special_token_50|>': 128061,\n",
       " '<|reserved_special_token_51|>': 128062,\n",
       " '<|reserved_special_token_52|>': 128063,\n",
       " '<|reserved_special_token_53|>': 128064,\n",
       " '<|reserved_special_token_54|>': 128065,\n",
       " '<|reserved_special_token_55|>': 128066,\n",
       " '<|reserved_special_token_56|>': 128067,\n",
       " '<|reserved_special_token_57|>': 128068,\n",
       " '<|reserved_special_token_58|>': 128069,\n",
       " '<|reserved_special_token_59|>': 128070,\n",
       " '<|reserved_special_token_60|>': 128071,\n",
       " '<|reserved_special_token_61|>': 128072,\n",
       " '<|reserved_special_token_62|>': 128073,\n",
       " '<|reserved_special_token_63|>': 128074,\n",
       " '<|reserved_special_token_64|>': 128075,\n",
       " '<|reserved_special_token_65|>': 128076,\n",
       " '<|reserved_special_token_66|>': 128077,\n",
       " '<|reserved_special_token_67|>': 128078,\n",
       " '<|reserved_special_token_68|>': 128079,\n",
       " '<|reserved_special_token_69|>': 128080,\n",
       " '<|reserved_special_token_70|>': 128081,\n",
       " '<|reserved_special_token_71|>': 128082,\n",
       " '<|reserved_special_token_72|>': 128083,\n",
       " '<|reserved_special_token_73|>': 128084,\n",
       " '<|reserved_special_token_74|>': 128085,\n",
       " '<|reserved_special_token_75|>': 128086,\n",
       " '<|reserved_special_token_76|>': 128087,\n",
       " '<|reserved_special_token_77|>': 128088,\n",
       " '<|reserved_special_token_78|>': 128089,\n",
       " '<|reserved_special_token_79|>': 128090,\n",
       " '<|reserved_special_token_80|>': 128091,\n",
       " '<|reserved_special_token_81|>': 128092,\n",
       " '<|reserved_special_token_82|>': 128093,\n",
       " '<|reserved_special_token_83|>': 128094,\n",
       " '<|reserved_special_token_84|>': 128095,\n",
       " '<|reserved_special_token_85|>': 128096,\n",
       " '<|reserved_special_token_86|>': 128097,\n",
       " '<|reserved_special_token_87|>': 128098,\n",
       " '<|reserved_special_token_88|>': 128099,\n",
       " '<|reserved_special_token_89|>': 128100,\n",
       " '<|reserved_special_token_90|>': 128101,\n",
       " '<|reserved_special_token_91|>': 128102,\n",
       " '<|reserved_special_token_92|>': 128103,\n",
       " '<|reserved_special_token_93|>': 128104,\n",
       " '<|reserved_special_token_94|>': 128105,\n",
       " '<|reserved_special_token_95|>': 128106,\n",
       " '<|reserved_special_token_96|>': 128107,\n",
       " '<|reserved_special_token_97|>': 128108,\n",
       " '<|reserved_special_token_98|>': 128109,\n",
       " '<|reserved_special_token_99|>': 128110,\n",
       " '<|reserved_special_token_100|>': 128111,\n",
       " '<|reserved_special_token_101|>': 128112,\n",
       " '<|reserved_special_token_102|>': 128113,\n",
       " '<|reserved_special_token_103|>': 128114,\n",
       " '<|reserved_special_token_104|>': 128115,\n",
       " '<|reserved_special_token_105|>': 128116,\n",
       " '<|reserved_special_token_106|>': 128117,\n",
       " '<|reserved_special_token_107|>': 128118,\n",
       " '<|reserved_special_token_108|>': 128119,\n",
       " '<|reserved_special_token_109|>': 128120,\n",
       " '<|reserved_special_token_110|>': 128121,\n",
       " '<|reserved_special_token_111|>': 128122,\n",
       " '<|reserved_special_token_112|>': 128123,\n",
       " '<|reserved_special_token_113|>': 128124,\n",
       " '<|reserved_special_token_114|>': 128125,\n",
       " '<|reserved_special_token_115|>': 128126,\n",
       " '<|reserved_special_token_116|>': 128127,\n",
       " '<|reserved_special_token_117|>': 128128,\n",
       " '<|reserved_special_token_118|>': 128129,\n",
       " '<|reserved_special_token_119|>': 128130,\n",
       " '<|reserved_special_token_120|>': 128131,\n",
       " '<|reserved_special_token_121|>': 128132,\n",
       " '<|reserved_special_token_122|>': 128133,\n",
       " '<|reserved_special_token_123|>': 128134,\n",
       " '<|reserved_special_token_124|>': 128135,\n",
       " '<|reserved_special_token_125|>': 128136,\n",
       " '<|reserved_special_token_126|>': 128137,\n",
       " '<|reserved_special_token_127|>': 128138,\n",
       " '<|reserved_special_token_128|>': 128139,\n",
       " '<|reserved_special_token_129|>': 128140,\n",
       " '<|reserved_special_token_130|>': 128141,\n",
       " '<|reserved_special_token_131|>': 128142,\n",
       " '<|reserved_special_token_132|>': 128143,\n",
       " '<|reserved_special_token_133|>': 128144,\n",
       " '<|reserved_special_token_134|>': 128145,\n",
       " '<|reserved_special_token_135|>': 128146,\n",
       " '<|reserved_special_token_136|>': 128147,\n",
       " '<|reserved_special_token_137|>': 128148,\n",
       " '<|reserved_special_token_138|>': 128149,\n",
       " '<|reserved_special_token_139|>': 128150,\n",
       " '<|reserved_special_token_140|>': 128151,\n",
       " '<|reserved_special_token_141|>': 128152,\n",
       " '<|reserved_special_token_142|>': 128153,\n",
       " '<|reserved_special_token_143|>': 128154,\n",
       " '<|reserved_special_token_144|>': 128155,\n",
       " '<|reserved_special_token_145|>': 128156,\n",
       " '<|reserved_special_token_146|>': 128157,\n",
       " '<|reserved_special_token_147|>': 128158,\n",
       " '<|reserved_special_token_148|>': 128159,\n",
       " '<|reserved_special_token_149|>': 128160,\n",
       " '<|reserved_special_token_150|>': 128161,\n",
       " '<|reserved_special_token_151|>': 128162,\n",
       " '<|reserved_special_token_152|>': 128163,\n",
       " '<|reserved_special_token_153|>': 128164,\n",
       " '<|reserved_special_token_154|>': 128165,\n",
       " '<|reserved_special_token_155|>': 128166,\n",
       " '<|reserved_special_token_156|>': 128167,\n",
       " '<|reserved_special_token_157|>': 128168,\n",
       " '<|reserved_special_token_158|>': 128169,\n",
       " '<|reserved_special_token_159|>': 128170,\n",
       " '<|reserved_special_token_160|>': 128171,\n",
       " '<|reserved_special_token_161|>': 128172,\n",
       " '<|reserved_special_token_162|>': 128173,\n",
       " '<|reserved_special_token_163|>': 128174,\n",
       " '<|reserved_special_token_164|>': 128175,\n",
       " '<|reserved_special_token_165|>': 128176,\n",
       " '<|reserved_special_token_166|>': 128177,\n",
       " '<|reserved_special_token_167|>': 128178,\n",
       " '<|reserved_special_token_168|>': 128179,\n",
       " '<|reserved_special_token_169|>': 128180,\n",
       " '<|reserved_special_token_170|>': 128181,\n",
       " '<|reserved_special_token_171|>': 128182,\n",
       " '<|reserved_special_token_172|>': 128183,\n",
       " '<|reserved_special_token_173|>': 128184,\n",
       " '<|reserved_special_token_174|>': 128185,\n",
       " '<|reserved_special_token_175|>': 128186,\n",
       " '<|reserved_special_token_176|>': 128187,\n",
       " '<|reserved_special_token_177|>': 128188,\n",
       " '<|reserved_special_token_178|>': 128189,\n",
       " '<|reserved_special_token_179|>': 128190,\n",
       " '<|reserved_special_token_180|>': 128191,\n",
       " '<|reserved_special_token_181|>': 128192,\n",
       " '<|reserved_special_token_182|>': 128193,\n",
       " '<|reserved_special_token_183|>': 128194,\n",
       " '<|reserved_special_token_184|>': 128195,\n",
       " '<|reserved_special_token_185|>': 128196,\n",
       " '<|reserved_special_token_186|>': 128197,\n",
       " '<|reserved_special_token_187|>': 128198,\n",
       " '<|reserved_special_token_188|>': 128199,\n",
       " '<|reserved_special_token_189|>': 128200,\n",
       " '<|reserved_special_token_190|>': 128201,\n",
       " '<|reserved_special_token_191|>': 128202,\n",
       " '<|reserved_special_token_192|>': 128203,\n",
       " '<|reserved_special_token_193|>': 128204,\n",
       " '<|reserved_special_token_194|>': 128205,\n",
       " '<|reserved_special_token_195|>': 128206,\n",
       " '<|reserved_special_token_196|>': 128207,\n",
       " '<|reserved_special_token_197|>': 128208,\n",
       " '<|reserved_special_token_198|>': 128209,\n",
       " '<|reserved_special_token_199|>': 128210,\n",
       " '<|reserved_special_token_200|>': 128211,\n",
       " '<|reserved_special_token_201|>': 128212,\n",
       " '<|reserved_special_token_202|>': 128213,\n",
       " '<|reserved_special_token_203|>': 128214,\n",
       " '<|reserved_special_token_204|>': 128215,\n",
       " '<|reserved_special_token_205|>': 128216,\n",
       " '<|reserved_special_token_206|>': 128217,\n",
       " '<|reserved_special_token_207|>': 128218,\n",
       " '<|reserved_special_token_208|>': 128219,\n",
       " '<|reserved_special_token_209|>': 128220,\n",
       " '<|reserved_special_token_210|>': 128221,\n",
       " '<|reserved_special_token_211|>': 128222,\n",
       " '<|reserved_special_token_212|>': 128223,\n",
       " '<|reserved_special_token_213|>': 128224,\n",
       " '<|reserved_special_token_214|>': 128225,\n",
       " '<|reserved_special_token_215|>': 128226,\n",
       " '<|reserved_special_token_216|>': 128227,\n",
       " '<|reserved_special_token_217|>': 128228,\n",
       " '<|reserved_special_token_218|>': 128229,\n",
       " '<|reserved_special_token_219|>': 128230,\n",
       " '<|reserved_special_token_220|>': 128231,\n",
       " '<|reserved_special_token_221|>': 128232,\n",
       " '<|reserved_special_token_222|>': 128233,\n",
       " '<|reserved_special_token_223|>': 128234,\n",
       " '<|reserved_special_token_224|>': 128235,\n",
       " '<|reserved_special_token_225|>': 128236,\n",
       " '<|reserved_special_token_226|>': 128237,\n",
       " '<|reserved_special_token_227|>': 128238,\n",
       " '<|reserved_special_token_228|>': 128239,\n",
       " '<|reserved_special_token_229|>': 128240,\n",
       " '<|reserved_special_token_230|>': 128241,\n",
       " '<|reserved_special_token_231|>': 128242,\n",
       " '<|reserved_special_token_232|>': 128243,\n",
       " '<|reserved_special_token_233|>': 128244,\n",
       " '<|reserved_special_token_234|>': 128245,\n",
       " '<|reserved_special_token_235|>': 128246,\n",
       " '<|reserved_special_token_236|>': 128247,\n",
       " '<|reserved_special_token_237|>': 128248,\n",
       " '<|reserved_special_token_238|>': 128249,\n",
       " '<|reserved_special_token_239|>': 128250,\n",
       " '<|reserved_special_token_240|>': 128251,\n",
       " '<|reserved_special_token_241|>': 128252,\n",
       " '<|reserved_special_token_242|>': 128253,\n",
       " '<|reserved_special_token_243|>': 128254,\n",
       " '<|reserved_special_token_244|>': 128255}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128001, 128009, 128008]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.stop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize_messages(\n",
    "                [\n",
    "                    Message(role=\"user\", content=\"Hello, world!\", masked=True),\n",
    "                    Message(role=\"assistant\", content=\"\", eot=False),\n",
    "                ],\n",
    "                add_end_tokens=False,\n",
    "            )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n<|im_start|>userHello, world!<|im_end|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n<|im_start|>assistantHello there, nice to meet you! How can I assist you today?<|im_end|><|reserved_special_token_1|>utterstock<|reserved_special_token_1|>\\n```\\n<|im_start|>userI am looking for a good way to learn Spanish. Can you recommend any apps or websites?<|im_end|>\\n```\\n\\n<|im_start|>assistantCertainly, I can help you with that!<|im_end|>Here are some popular language learning apps and websites that you might find helpful:\\n\\n1. Duolingo'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.generation import generate\n",
    "\n",
    "\n",
    "tokenizer.tokenize_messages\n",
    "\n",
    "tokens, logits = generate(\n",
    "    model=model,\n",
    "    prompt=torch.tensor(\n",
    "        [\n",
    "            tokenizer.tokenize_messages(\n",
    "                [\n",
    "                    Message(role=\"user\", content=\"Hello, world!\", masked=True),\n",
    "                ],\n",
    "                add_end_tokens=False,\n",
    "            )[0]\n",
    "        ]\n",
    "    ).to(\"cuda\"),\n",
    "    max_generated_tokens=100,\n",
    "    stop_tokens=tokenizer.stop_tokens,\n",
    ")\n",
    "tokenizer.decode(list(tokens[0]), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#\\n\\nI! I! Iitle'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.forward(torch.tensor([tokenizer.tokenize_message(Message(role=\"user\", content=\"Hello, world!\"))]))\n",
    "\n",
    "tokenizer.decode(token_ids=response.argmax(dim=-1).squeeze().tolist(), skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
