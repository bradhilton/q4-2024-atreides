{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.langfuse import langfuse\n",
    "langfuse.enabled = False\n",
    "# langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from lib.rl.episode import Episode, EpisodeCompletion\n",
    "import random\n",
    "import re\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class TemporalCluePuzzle(TypedDict):\n",
    "    num_clues: int\n",
    "    prompt: str\n",
    "    solution: dict[str, str]\n",
    "\n",
    "\n",
    "temporal_clue_puzzles: list[TemporalCluePuzzle] = json.load(\n",
    "    open(\"./data/temporal-clue-puzzles.json\")\n",
    ")\n",
    "random.seed(42)\n",
    "random.shuffle(temporal_clue_puzzles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.clue import Clue\n",
    "\n",
    "chain_of_thought_examples: list[dict[str, str]] = json.load(\n",
    "    open(\"./data/chain-of-thought-examples.json\")\n",
    ")\n",
    "chain_of_thought_examples.pop(6)\n",
    "chain_of_thought_examples.pop(3)\n",
    "\n",
    "def get_episode(puzzle: TemporalCluePuzzle) -> Episode:\n",
    "\n",
    "    def validate(completion: EpisodeCompletion) -> None:\n",
    "        ...\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]) -> None:\n",
    "        for completion in completions:\n",
    "            content = completion.last_assistant_message.get(\"content\")\n",
    "            assert isinstance(content, str)\n",
    "            num_correct = 0\n",
    "            for key, value in puzzle[\"solution\"].items():\n",
    "                if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "                    match = matches[-1]\n",
    "                    if match.strip().lower() == value.lower():\n",
    "                        num_correct += 1\n",
    "            completion.commit(reward=num_correct / len(puzzle[\"solution\"]))\n",
    "            \n",
    "    example = random.choices(chain_of_thought_examples, k=1)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": puzzle[\"prompt\"]\n",
    "                .replace(\n",
    "                    \"Fill out your final answers in the following format:\",\n",
    "                    \"After verifiably finding all the correct answers, fill out your final answers in the following format:\",\n",
    "                )\n",
    "                ,\n",
    "            },\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": \"Let's think this through step by step...\",\n",
    "            # },\n",
    "        ],\n",
    "        on_sample=on_sample,\n",
    "        examples=[\n",
    "            {\"role\": \"user\", \"content\": example[0][\"prompt\"]},\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"content\": example[0][\"chain_of_thought\"]\n",
    "                + (example[0][\"answer\"] and f\"\\n\\n---\\n\\n{example[0]['answer']}\"),\n",
    "            },\n",
    "            # {\"role\": \"user\", \"content\": example[1][\"prompt\"]},\n",
    "            # {\n",
    "            #     \"role\": \"assistant\",\n",
    "            #     \"content\": example[1][\"chain_of_thought\"] \n",
    "            #     + (example[1][\"answer\"] and f\"\\n\\n---\\n\\n{example[1]['answer']}\"),\n",
    "            # },\n",
    "        ],\n",
    "        # logprobs_mask=Clue.get_logprobs_mask(),\n",
    "    )\n",
    "\n",
    "\n",
    "temporal_clue_episodes = [get_episode(puzzle) for puzzle in temporal_clue_puzzles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_clue_episodes[64:] = [\n",
    "    get_episode(puzzle)\n",
    "    for puzzle in json.load(open(\"./data/temporal-clue-puzzles-2.json\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "zebra_grid_questions = pl.read_parquet(\n",
    "    \"hf://datasets/allenai/ZebraLogicBench-private/grid_mode/test-00000-of-00001.parquet\"\n",
    ").to_dicts()\n",
    "random.shuffle(zebra_grid_questions)\n",
    "\n",
    "\n",
    "def get_episode(question: dict) -> Episode:\n",
    "    prompt = f\"\"\"{question[\"puzzle\"]}\n",
    "Fill in the grid with the correct values:\n",
    "\n",
    "| {' | '.join(question[\"solution\"][\"header\"])} |\n",
    "| {' | '.join([\"-\" * len(header) for header in question[\"solution\"][\"header\"]])} |\n",
    "\"\"\"\n",
    "\n",
    "    for _ in question[\"solution\"][\"rows\"]:\n",
    "        prompt += f\"| {' | '.join([\" \" * len(header) for header in question[\"solution\"][\"header\"]])} |\\n\"\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"\\| \" + r\"\\|\".join(r\"(.*?)\" for _ in question[\"solution\"][\"header\"]) + r\" \\|\"\n",
    "    )\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]):\n",
    "        for completion in completions:\n",
    "            assert \"content\" in completion.last_assistant_message and isinstance(\n",
    "                completion.last_assistant_message[\"content\"], str\n",
    "            )\n",
    "            num_cells = sum(len(row) for row in question[\"solution\"][\"rows\"])\n",
    "            num_correct = 0\n",
    "            for match, row in zip(\n",
    "                re.findall(pattern, completion.last_assistant_message[\"content\"])[\n",
    "                    -len(question[\"solution\"][\"rows\"]) :\n",
    "                ],\n",
    "                question[\"solution\"][\"rows\"],\n",
    "            ):\n",
    "                for cell, value in zip(match, row):\n",
    "                    if cell.strip().lower() == value.lower():\n",
    "                        num_correct += 1\n",
    "            completion.commit(reward=num_correct / num_cells)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "zebra_grid_episodes = [get_episode(question) for question in zebra_grid_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "math_questions = list(\n",
    "    load_dataset(\"lighteval/MATH\", \"all\")[\"train\"].to_iterable_dataset()  # type: ignore\n",
    ")\n",
    "random.shuffle(math_questions)\n",
    "\n",
    "\n",
    "question_solution = None\n",
    "pattern = re.compile(r\"\\\\boxed{([^}]+)}\")\n",
    "\n",
    "\n",
    "def get_episode(question: dict) -> Episode:\n",
    "    prompt = (\n",
    "        f\"{question['problem']}\\n\\n\"\n",
    "        \"Solve this math problem and show your work. Your final answer MUST be \"\n",
    "        \"formatted in a LaTeX box using \\\\boxed{{}}. For example: \"\n",
    "        \"$1+1=\\\\boxed{{2}}$\\n\\n\"\n",
    "        \"You can submit multiple attempts. Each attempt should end with a boxed \"\n",
    "        \"answer. Your last answer will be weighted the most, but you can get \"\n",
    "        \"partial credit if an earlier answer is correct. If after multiple \"\n",
    "        \"attempts you decide an earlier answer is the correct one, just submit \"\n",
    "        \"it again to get full credit.\"\n",
    "    )\n",
    "\n",
    "    global question_solution\n",
    "    question_solution = question[\"solution\"]\n",
    "    solution = re.search(pattern, question[\"solution\"])\n",
    "    assert solution is not None, question[\"solution\"]\n",
    "    solution = solution.group(1)\n",
    "\n",
    "    def on_sample(completions: list[EpisodeCompletion]):\n",
    "        for completion in completions:\n",
    "            content = completion.last_assistant_message.get(\"content\")\n",
    "            assert isinstance(content, str)\n",
    "            solutions = [\n",
    "                match.group(1) for match in re.finditer(r\"\\\\boxed{([^}]+)}\", content)\n",
    "            ][::-1]\n",
    "            try:\n",
    "                reward = 0.9 ** solutions.index(solution)\n",
    "            except ValueError:\n",
    "                reward = 0\n",
    "            completion.commit(reward=reward)\n",
    "\n",
    "    return Episode(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        on_sample=on_sample,\n",
    "    )\n",
    "\n",
    "\n",
    "math_episodes = [\n",
    "    get_episode(question)\n",
    "    for question in math_questions[:2048]\n",
    "    if re.search(pattern, question[\"solution\"]) is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from lib.rl.completion import SplitMethod\n",
    "from lib.rl.completion_sampler import CompletionSampler, SamplingKwargs\n",
    "from lib.rl.trainer import ExploreImpl, ExploreOptions\n",
    "from lib.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultExploreImpl(ExploreImpl):\n",
    "    explore_options: ExploreOptions\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        def done_callback(task: asyncio.Task[Episode]) -> None:\n",
    "            try:\n",
    "                done_episodes.put_nowait(task.result())\n",
    "            except BaseException as exception:\n",
    "                done_episodes.put_nowait(exception)\n",
    "\n",
    "        priority = 1\n",
    "        while episode := await ready_episodes.get():\n",
    "            asyncio.create_task(\n",
    "                self._explore_episode(\n",
    "                    completion_sampler, tokenizer, episode, update_progress, priority\n",
    "                )\n",
    "            ).add_done_callback(done_callback)\n",
    "            priority += 1\n",
    "\n",
    "    async def _explore_episode(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        episode: Episode,\n",
    "        update_progress: Callable[[float], None],\n",
    "        priority: int,\n",
    "    ) -> Episode:\n",
    "        for _ in range(self.explore_options.iterations):\n",
    "            await episode.sample_completions(\n",
    "                completion_sampler=completion_sampler,\n",
    "                tokenizer=tokenizer,\n",
    "                num_parents=self.explore_options.num_parents,\n",
    "                branch_factor=self.explore_options.branch_factor,\n",
    "                get_recovery_pattern=self.explore_options.get_recovery_pattern,\n",
    "                max_splits_per_completion=self.explore_options.max_split_points\n",
    "                or self.explore_options.num_parents,\n",
    "                priority=priority,\n",
    "                sample_probability_power=self.explore_options.get_sample_probability_power(),\n",
    "                sampling_kwargs=self.explore_options.sampling_kwargs,\n",
    "                split_by=self.explore_options.split_method,\n",
    "                split_separators=self.explore_options.split_separators,\n",
    "            )\n",
    "            update_progress(1 / self.explore_options.iterations)\n",
    "        return episode\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SimpleExploreImpl(ExploreImpl):\n",
    "    num_samples: int\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        while episode := await ready_episodes.get():\n",
    "            task = asyncio.create_task(\n",
    "                episode.sample_completions(\n",
    "                    completion_sampler,\n",
    "                    tokenizer,\n",
    "                    num_parents=1,\n",
    "                    branch_factor=self.num_samples,\n",
    "                    sampling_kwargs=self.sampling_kwargs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            def done_callback(_: asyncio.Task[bool], episode=episode) -> None:\n",
    "                try:\n",
    "                    done_episodes.put_nowait(episode)\n",
    "                    update_progress(1)\n",
    "                except BaseException as e:\n",
    "                    done_episodes.put_nowait(e)\n",
    "\n",
    "            task.add_done_callback(done_callback)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TreeExploreImpl(ExploreImpl):\n",
    "    branch_factor: int\n",
    "    depth: int\n",
    "    num_roots: int | None = None\n",
    "    best_leaf_sampling_temperature: float = 0.01\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "    split_method: SplitMethod = \"count\"\n",
    "    split_separators: set[str] = field(default_factory=set)\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        model = await completion_sampler.get_model()\n",
    "\n",
    "        async def expand(episode: Episode, priority: int) -> None:\n",
    "            num_roots = self.num_roots or self.branch_factor\n",
    "\n",
    "            # If there are existing trajectories, we'll sample one\n",
    "            # of the best ones to stabilize and improve training.\n",
    "            leaves = list(episode.completion.leaves())\n",
    "            if leaves:\n",
    "                best_leaf = random.choices(\n",
    "                    leaves,\n",
    "                    weights=[\n",
    "                        np.exp(\n",
    "                            leaf.cumulative_reward()\n",
    "                            / self.best_leaf_sampling_temperature\n",
    "                        )\n",
    "                        for leaf in leaves\n",
    "                    ],\n",
    "                    k=1,\n",
    "                )[0]\n",
    "                best_leaf = best_leaf.recursive_copy(model=model)\n",
    "                best_leaf.commit()\n",
    "                while best_leaf.parent and best_leaf.parent.parent is None:\n",
    "                    best_leaf = best_leaf.merge()\n",
    "\n",
    "            pending: set[asyncio.Task] = {\n",
    "                asyncio.create_task(\n",
    "                    episode.sample_completions(\n",
    "                        completion_sampler,\n",
    "                        tokenizer,\n",
    "                        num_parents=1,\n",
    "                        branch_factor=num_roots - 1 if best_leaf else num_roots,\n",
    "                        priority=priority,\n",
    "                        sampling_kwargs=self.sampling_kwargs,\n",
    "                        split_by=self.split_method,\n",
    "                        split_separators=self.split_separators,\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "\n",
    "            num_leaves = 0\n",
    "            while pending:\n",
    "                finished, pending = await asyncio.wait(\n",
    "                    pending, return_when=asyncio.FIRST_COMPLETED\n",
    "                )\n",
    "                for task in finished:\n",
    "                    try:\n",
    "                        task.result()\n",
    "                    except BaseException as e:\n",
    "                        await done_episodes.put(e)\n",
    "                        return\n",
    "                _num_leaves = 0\n",
    "                for leaf in episode.completion.leaves(model=model):\n",
    "                    _num_leaves += 1\n",
    "                    num_partitions = self.depth - leaf.depth() + 1\n",
    "                    if num_partitions > 1:\n",
    "                        parents = list(\n",
    "                            leaf.split(\n",
    "                                by=self.split_method,\n",
    "                                at=(\n",
    "                                    split / num_partitions\n",
    "                                    for split in range(1, num_partitions)\n",
    "                                ),\n",
    "                                separators=self.split_separators,\n",
    "                                cache=True,\n",
    "                            )\n",
    "                        )[:-1]\n",
    "                        for parent in parents:\n",
    "                            pending.add(\n",
    "                                asyncio.create_task(\n",
    "                                    episode._sample_completions(\n",
    "                                        parent=parent,\n",
    "                                        model=model,\n",
    "                                        completion_sampler=completion_sampler,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        branch_factor=self.branch_factor,\n",
    "                                        fork_decay=1.0,\n",
    "                                        recovery_pattern=None,\n",
    "                                        split_separators=self.split_separators,\n",
    "                                        sampling_kwargs=self.sampling_kwargs\n",
    "                                        or SamplingKwargs(),\n",
    "                                        priority=priority,\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                update_progress(\n",
    "                    (_num_leaves - num_leaves)\n",
    "                    / (num_roots * (self.branch_factor ** (self.depth - 1)))\n",
    "                )\n",
    "                num_leaves = _num_leaves\n",
    "\n",
    "            await done_episodes.put(episode)\n",
    "\n",
    "        priority = 0\n",
    "        while episode := await ready_episodes.get():\n",
    "            priority += 1\n",
    "            asyncio.create_task(expand(episode, priority))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IterativeVineExploreImpl(ExploreImpl):\n",
    "    branch_factor: int\n",
    "    depth: int\n",
    "    sampling_kwargs: SamplingKwargs | None = None\n",
    "    split_method: SplitMethod = \"count\"\n",
    "    split_separators: set[str] = field(default_factory=set)\n",
    "\n",
    "    async def __call__(\n",
    "        self,\n",
    "        completion_sampler: CompletionSampler,\n",
    "        tokenizer: Tokenizer,\n",
    "        ready_episodes: asyncio.Queue[Episode],\n",
    "        done_episodes: asyncio.Queue[Episode | BaseException],\n",
    "        update_progress: Callable[[float], None],\n",
    "    ) -> None:\n",
    "        model = await completion_sampler.get_model()\n",
    "\n",
    "        async def iterate_vine(episode: Episode, priority: int) -> None:\n",
    "\n",
    "            for depth in range(self.depth):\n",
    "                if depth == 0:\n",
    "                    parent = episode.completion\n",
    "                else:\n",
    "                    parent = max(\n",
    "                        episode.completion.leaves(model=model),\n",
    "                        key=lambda leaf: leaf.reward,\n",
    "                    )\n",
    "                    parent = list(\n",
    "                        parent.split(\n",
    "                            by=self.split_method,\n",
    "                            at=[1 / (self.depth - depth + 1)],\n",
    "                            separators=self.split_separators,\n",
    "                            cache=True,\n",
    "                        )\n",
    "                    )[0]\n",
    "                await episode._sample_completions(\n",
    "                    parent=parent,\n",
    "                    model=model,\n",
    "                    completion_sampler=completion_sampler,\n",
    "                    tokenizer=tokenizer,\n",
    "                    branch_factor=self.branch_factor,\n",
    "                    fork_decay=1.0,\n",
    "                    recovery_pattern=None,\n",
    "                    split_separators=self.split_separators,\n",
    "                    sampling_kwargs=self.sampling_kwargs or SamplingKwargs(),\n",
    "                    priority=priority,\n",
    "                )\n",
    "                update_progress(1 / self.depth)\n",
    "\n",
    "            await done_episodes.put(episode)\n",
    "\n",
    "        priority = 0\n",
    "        while episode := await ready_episodes.get():\n",
    "            priority += 1\n",
    "            asyncio.create_task(iterate_vine(episode, priority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-05 02:43:25 config.py:510] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 01-05 02:43:25 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-05 02:43:25 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n",
      "INFO 01-05 02:43:25 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/atreides/experiments/wandb/run-20250105_024325-rl115</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl115' target=\"_blank\">rl115</a></strong> to <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/atreides-experiments' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/atreides-experiments/runs/rl115' target=\"_blank\">https://wandb.ai/bradhilton/atreides-experiments/runs/rl115</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aioitertools.helpers import maybe_await\n",
    "import asyncio\n",
    "from collections import Counter\n",
    "import itertools as it\n",
    "from lib import clue\n",
    "from lib.rl.episode import Episode\n",
    "from lib.rl.ppo import PPOLoss\n",
    "from lib.rl.recipe import ComponentConfig, TuneRecipeConfig\n",
    "from lib.rl.trainer import Eval, ExploreOptions, Trainer, vLLMConfig\n",
    "import torch\n",
    "from torchtune.models.llama3_1 import llama3_1_8b\n",
    "from typing import AsyncIterable\n",
    "\n",
    "\n",
    "episodes_per_iteration = 16 * torch.cuda.device_count()\n",
    "\n",
    "\n",
    "async def train_episodes(\n",
    "    revisit_frequency: float = 0.0,\n",
    ") -> AsyncIterable[Episode | BaseException]:\n",
    "    pending: set[asyncio.Task[Episode | BaseException]] = set()\n",
    "    episodes = (\n",
    "        maybe_await(episode)\n",
    "        for episodes in zip(\n",
    "            # (clue.sample_random_episode() for _ in it.repeat(0)),\n",
    "            it.cycle(temporal_clue_episodes[64:]),\n",
    "            # it.cycle(zebra_grid_episodes[64:]),\n",
    "            # it.cycle(math_episodes[64:]),\n",
    "        )\n",
    "        for episode in episodes\n",
    "    )\n",
    "    visited_episodes = Counter[Episode]()\n",
    "    while True:\n",
    "        pending.update(\n",
    "            asyncio.create_task(next(episodes))\n",
    "            for _ in range(episodes_per_iteration - len(pending))  # type: ignore\n",
    "        )\n",
    "        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "        if len(visited_episodes) > episodes_per_iteration:\n",
    "            while random.random() < revisit_frequency:\n",
    "                episode = min(visited_episodes, key=lambda e: visited_episodes[e])\n",
    "                visited_episodes[episode] += 1\n",
    "                yield episode\n",
    "        for task in done:\n",
    "            try:\n",
    "                result = task.result()\n",
    "                if isinstance(result, Episode):\n",
    "                    visited_episodes[result] += 1\n",
    "                yield result\n",
    "            except BaseException as e:\n",
    "                yield e\n",
    "\n",
    "\n",
    "async def val_episodes() -> AsyncIterable[Episode | BaseException]:\n",
    "    for fut in asyncio.as_completed(clue.sample_random_episode() for _ in range(64)):\n",
    "        try:\n",
    "            yield await fut\n",
    "        except BaseException as e:\n",
    "            yield e\n",
    "\n",
    "\n",
    "explore_options = ExploreOptions(\n",
    "    iterations=1,\n",
    "    num_parents=6,\n",
    "    branch_factor=2,\n",
    "    patience=60,\n",
    "    advantage_max_weight=0.15,\n",
    "    sample_probability_power=None,\n",
    "    sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "    # split_method=\"prob\",\n",
    "    # split_point_std_deviation=0.5,\n",
    ")\n",
    "\n",
    "model_name = \"rl115\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    output_dir=f\"./models/{model_name}\",\n",
    "    explore_options=explore_options,\n",
    "    # explore_impl=DefaultExploreImpl(explore_options),\n",
    "    # explore_impl=SimpleExploreImpl(\n",
    "    #     num_samples=8, sampling_kwargs={\"max_tokens\": 4096}\n",
    "    # ),\n",
    "    explore_impl=TreeExploreImpl(\n",
    "        branch_factor=2,\n",
    "        depth=5,\n",
    "        num_roots=8,\n",
    "        # best_leaf_sampling_temperature=0.05,\n",
    "        sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "    ),\n",
    "    force_terminate_vllms=True,\n",
    "    train_episodes=train_episodes(revisit_frequency=0.5),\n",
    "    episodes_per_iteration=episodes_per_iteration,\n",
    "    max_mask_sequence_batch_size=1,\n",
    "    evals=[\n",
    "        # Eval(\n",
    "        #     name=\"variable_clue\",\n",
    "        #     episodes=val_episodes(),\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "        Eval(\n",
    "            name=\"temporal_clue\",\n",
    "            episodes=temporal_clue_episodes[:64],\n",
    "            samples_per_episode=3,\n",
    "            sampling_kwargs={\"max_tokens\": 4096, \"stop\": [\"://\", \"<|end_of_text|>\"]},\n",
    "        ),\n",
    "        # Eval(\n",
    "        #     name=\"zebra_grid\",\n",
    "        #     episodes=zebra_grid_episodes[:64],\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "        # Eval(\n",
    "        #     name=\"math\",\n",
    "        #     episodes=math_episodes[:64],\n",
    "        #     samples_per_episode=3,\n",
    "        #     sampling_kwargs={\"max_tokens\": 4096},\n",
    "        # ),\n",
    "    ],\n",
    "    tune_model=llama3_1_8b,\n",
    "    tune_model_type=\"LLAMA3\",\n",
    "    tune_recipe_config=TuneRecipeConfig(\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        num_output_chunks=4,\n",
    "        resume_from_checkpoint=False,\n",
    "        batch_size=1,\n",
    "        epochs=1,\n",
    "        max_steps_per_epoch=32,\n",
    "        optimizer=ComponentConfig(\n",
    "            \"torch.optim.AdamW\",\n",
    "            # \"bitsandbytes.optim.PagedAdamW8bit\",\n",
    "            # \"bitsandbytes.optim.AdamW\",\n",
    "            # params=PLACEHOLDER,\n",
    "            lr=2e-6,\n",
    "            fused=True,\n",
    "        ),\n",
    "        loss=ComponentConfig(\n",
    "            PPOLoss,\n",
    "            policy_coef=0.0,\n",
    "            clip_epsilon=0.2,\n",
    "            unclipped_policy_coef=0.0,\n",
    "            tanh_log_policy_coef=0.8,\n",
    "            tanh_log_policy_beta=2.0,\n",
    "            use_reference_logprobs=True,\n",
    "            advantage_prediction_coef=0.0,\n",
    "            predicted_advantage_weight=0.0,\n",
    "            value_coef=0.0,\n",
    "            entropy_coef=0.0,\n",
    "            entropy_target=0.6,\n",
    "            entropy_target_coef=0.05,\n",
    "            kl_coef=0.05,\n",
    "            weighted_entropy_coef=0.0,\n",
    "            weighted_kl_coef=0.0,\n",
    "            weighted_ce_coef=0.0,\n",
    "            normalize_values=False,\n",
    "            normalize_value_predictions=False,\n",
    "            normalize_advantages=False,\n",
    "        ),\n",
    "        compile=False,\n",
    "        optimizer_in_bwd=False,\n",
    "        gradient_accumulation_steps=1,\n",
    "        enable_activation_checkpointing=True,\n",
    "        enable_activation_offloading=False,\n",
    "        custom_sharded_layers=[\"tok_embeddings\", \"output\"],\n",
    "        log_every_n_steps=1,\n",
    "        log_peak_memory_stats=True,\n",
    "    ),\n",
    "    # tune_run=False,\n",
    "    tune_sequence_length=16384,\n",
    "    vllm_config=vLLMConfig(\n",
    "        env={\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": \"1\"},\n",
    "        kwargs=dict(\n",
    "            block_size=32,\n",
    "            disable_log_requests=True,\n",
    "            enable_chunked_prefill=True,\n",
    "            enable_prefix_caching=True,\n",
    "            enforce_eager=True,\n",
    "            gpu_memory_utilization=0.9,\n",
    "            max_model_len=16384,\n",
    "            max_num_seqs=512,\n",
    "            max_num_batched_tokens=16384,\n",
    "            preemption_mode=\"swap\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            swap_space=100,\n",
    "        ),\n",
    "        max_concurrent_samples=512,\n",
    "        min_time_between_requests=0.0,\n",
    "        timeout=120 + 15 * torch.cuda.device_count(),\n",
    "    ),\n",
    "    wandb_kwargs=dict(\n",
    "        name=model_name,\n",
    "        id=model_name,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2 vLLM servers...\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8011 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000b3d1b69284c9ebcf1461cc816f0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d89067b4c22400e97311d2ca57c9210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 46 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl115/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|23|Loss: 0.0120: 100%|██████████| 23/23 [04:12<00:00, 10.74s/it, advantage=0.0000, advantage_prediction=0.2794, entropy=0.5468, entropy_target=0.0532, exploration=0.4170, kl_div=0.1897, policy=1838595200.0000, reinforce=0.0191, tanh_log_policy=-0.0002, unclipped_policy=-9021022208.0000, value=0.0000, weighted_ce=0.0191, weighted_entropy=-0.0258, weighted_kl_div=-0.0075]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 1 model files to /home/ubuntu/atreides/experiments/models/rl115/0001\n",
      "Starting 2 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0001 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0001 --port=8012 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ecf74a55b446e9999b1065f3d7afce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72bcb55d3054b2a9079db70a83d6a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 49 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl115/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|24|Loss: 0.0201: 100%|██████████| 24/24 [04:23<00:00, 10.72s/it, advantage=0.0000, advantage_prediction=0.6457, entropy=0.6227, entropy_target=0.0227, exploration=0.9370, kl_div=0.3480, policy=402964160.0000, reinforce=0.0076, tanh_log_policy=0.0019, unclipped_policy=-2060111616.0000, value=0.0000, weighted_ce=0.0076, weighted_entropy=-0.0093, weighted_kl_div=-0.0044]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 2 model files to /home/ubuntu/atreides/experiments/models/rl115/0002\n",
      "Starting 2 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0002 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0002 --port=8013 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc151af5fca457d8ca551e25d3be2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6ca090a99c4c72a5709c5b9eef948a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 58 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl115/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|29|Loss: 0.0074: 100%|██████████| 29/29 [05:17<00:00, 10.72s/it, advantage=0.0000, advantage_prediction=2.7164, entropy=0.6366, entropy_target=0.0366, exploration=0.4686, kl_div=0.2028, policy=5342518837248.0000, reinforce=-0.0835, tanh_log_policy=-0.0057, unclipped_policy=5340809658368.0000, value=0.0000, weighted_ce=-0.0835, weighted_entropy=0.0731, weighted_kl_div=0.0266]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 3 model files to /home/ubuntu/atreides/experiments/models/rl115/0003\n",
      "Starting 2 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0003 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0003 --port=8014 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8399a1a8fe384dcb826f169df71be8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ba5ae4925d4f0187d76cc5430dddd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 56 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl115/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|28|Loss: 0.0127: 100%|██████████| 28/28 [05:06<00:00, 10.71s/it, advantage=0.0000, advantage_prediction=0.8513, entropy=0.5809, entropy_target=0.0191, exploration=0.7960, kl_div=0.3106, policy=67462365184.0000, reinforce=-0.0402, tanh_log_policy=-0.0047, unclipped_policy=18982391808.0000, value=0.0000, weighted_ce=-0.0402, weighted_entropy=0.0196, weighted_kl_div=0.0159]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 4 model files to /home/ubuntu/atreides/experiments/models/rl115/0004\n",
      "Starting 2 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0004 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0004 --port=8015 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de22546697a4c0ea389811898c9f550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1ee4ccea764ab0a74220a3558acc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 59 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl115/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|29|Loss: 0.0146: 100%|██████████| 29/29 [05:16<00:00, 10.73s/it, advantage=0.0000, advantage_prediction=0.5995, entropy=0.4606, entropy_target=0.1394, exploration=0.3083, kl_div=0.1560, policy=2371535872.0000, reinforce=0.0007, tanh_log_policy=-0.0003, unclipped_policy=-27041712128.0000, value=0.0000, weighted_ce=0.0007, weighted_entropy=-0.0045, weighted_kl_div=-0.0008]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 5 model files to /home/ubuntu/atreides/experiments/models/rl115/0005\n",
      "Starting 2 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0005 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl115/0005 --port=8016 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbb4e91c5fa43a68962d7587ba7249d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1089213fd26d4f01bb981b38c22e44cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/32 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "await trainer.train(iterations=50, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 56 sequences\n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl111/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 00:49:18.623000 143029 torch/distributed/run.py:793] \n",
      "W0105 00:49:18.623000 143029 torch/distributed/run.py:793] *****************************************\n",
      "W0105 00:49:18.623000 143029 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0105 00:49:18.623000 143029 torch/distributed/run.py:793] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: lib.rl.mlp_head_checkpointer.MLPHeadCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl111/0001\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl111/0001/hf_model_0001.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl111/0001/hf_model_0002.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl111/0001/hf_model_0003.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl111/0001/hf_model_0004.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl111\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl111/tensors\n",
      "  num_sequences: 55\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  advantage_prediction_coef: 0.0\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.6\n",
      "  entropy_target_coef: 0.05\n",
      "  kl_coef: 0.3\n",
      "  normalize_advantages: false\n",
      "  normalize_value_predictions: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  predicted_advantage_weight: 0.0\n",
      "  tanh_log_policy_coef: 0.8\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.0\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: 32\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl111\n",
      "  name: rl111\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl111\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: true\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20250105_004923-rl111\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl111\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl111\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/atreides/experiments/models/rl111/0001/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:Loading MLP head weights from /home/ubuntu/atreides/experiments/models/rl111/0001/mlp_head.pt.ignore\n",
      "INFO:torchtune.utils._logging:Loading MLP head weights from /home/ubuntu/atreides/experiments/models/rl111/0001/mlp_head.pt.ignore\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 7.36 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.95 GiB\n",
      "\tGPU peak memory reserved: 16.12 GiB\n",
      "\tGPU peak memory active: 15.95 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|28|Loss: 0.0606: 100%|██████████| 28/28 [05:06<00:00, 10.70s/it, advantage=0.0000, advantage_prediction=0.5814, entropy=0.6279, entropy_target=0.0279, exploration=0.6466, kl_div=0.2263, policy=-0.0020, reinforce=-0.0584, tanh_log_policy=-0.0108, unclipped_policy=-0.0345, value=0.0000, weighted_ce=-0.0584, weighted_entropy=0.0301, weighted_kl_div=0.0236]  INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 8.42 secs\n",
      "INFO:torchtune.utils._logging:MLP head checkpoint saved to /home/ubuntu/atreides/experiments/models/rl111/mlp_head_0.pt.ignore\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl111/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl111/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl111/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl111/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 11.75 secs\n",
      "1|28|Loss: 0.0606: 100%|██████████| 28/28 [05:26<00:00, 11.67s/it, advantage=0.0000, advantage_prediction=0.5814, entropy=0.6279, entropy_target=0.0279, exploration=0.6466, kl_div=0.2263, policy=-0.0020, reinforce=-0.0584, tanh_log_policy=-0.0108, unclipped_policy=-0.0345, value=0.0000, weighted_ce=-0.0584, weighted_entropy=0.0301, weighted_kl_div=0.0236]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:      advantage_prediction ▃▃▂▁▃▁▂▃█▂▁▄▄▃▁▂▁▅▇▃▁▃▃▃▂▁▂▁\n",
      "wandb:                   entropy ▄▇█▅▅▂▇▄▂▁▂▄▃▆▅▃▇▄▅▆▅▄▄▃▂▅▆▄\n",
      "wandb:            entropy_target ▂▇█▂▃▄▆▁▄▆▄▁▃▄▂▃▆▁▃▅▃▁▂▂▅▃▅▁\n",
      "wandb:               exploration ▂▅▆▃▅▂█▅▃▁▂▆▃▇▅▃█▄▆▇▅▅▅▄▁▆▇▄\n",
      "wandb:               global_step ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
      "wandb:                    kl_div ▂█▇▄▆▂▇▇▄▁▂▇▄█▆▄▆▅▇▇▆▅▆▃▁▇▅▄\n",
      "wandb:                      loss ▁██▃▅▂█▅▂▂▃▅▃▇▅▃▆▃▅▆▅▃▅▁▂▆▅▁\n",
      "wandb:                        lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:        peak_memory_active ▁███████████████████████████\n",
      "wandb:         peak_memory_alloc ▁███████████████████████████\n",
      "wandb:      peak_memory_reserved ▁███████████████████████████\n",
      "wandb:                    policy ▃▄▃▂▄▂▃▃█▄▂▄▇▃▂▃▂▅▇▃▂▂▅▂▃▁▁▂\n",
      "wandb:                 reinforce ▅▃▇▅▄▆▇▅▁▆█▇▆▅▆▅▅▆▂▃▆▅▅▂▅▆▇▃\n",
      "wandb:           tanh_log_policy █▆▇▆▄▇█▄▂▅█▆▆▇▆▆▆▇▅▃▅▆▇▂▆▆▇▁\n",
      "wandb: tokens_per_second_per_gpu ▁▆▆▇▆█▆▇▇▇▆▇█▄▇▇▆█▆█▆▆▇▆▅█▇▇\n",
      "wandb:          unclipped_policy ▃▃▃▂▃▃▃▃█▅▂▄▇▃▂▃▃▅▇▃▂▁▆▁▃▁▁▁\n",
      "wandb:                     value ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:               weighted_ce ▅▃▇▅▄▆▇▅▁▆█▇▆▅▆▅▅▆▂▃▆▅▅▂▅▆▇▃\n",
      "wandb:          weighted_entropy ▄█▂▅▇▃▃▅▇▃▁▁▃▃▃▄▅▄▆▇▄▄▅█▅▃▂▅\n",
      "wandb:           weighted_kl_div ▄▅▂▂▄▃▂▄█▃▁▃▄▂▃▃▃▃█▄▂▄▃▁▃▁▁▅\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:      advantage_prediction 0.58135\n",
      "wandb:                   entropy 0.62787\n",
      "wandb:            entropy_target 0.02787\n",
      "wandb:               exploration 0.64663\n",
      "wandb:               global_step 28\n",
      "wandb:                    kl_div 0.22635\n",
      "wandb:                      loss 0.06063\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 53.40025\n",
      "wandb:         peak_memory_alloc 53.40025\n",
      "wandb:      peak_memory_reserved 65.04102\n",
      "wandb:                    policy -0.00202\n",
      "wandb:                 reinforce -0.05839\n",
      "wandb:           tanh_log_policy -0.01083\n",
      "wandb: tokens_per_second_per_gpu 1267.56689\n",
      "wandb:          unclipped_policy -0.03451\n",
      "wandb:                     value 0\n",
      "wandb:               weighted_ce -0.05839\n",
      "wandb:          weighted_entropy 0.03012\n",
      "wandb:           weighted_kl_div 0.02361\n",
      "wandb: \n",
      "wandb: 🚀 View run rl111 at: https://wandb.ai/bradhilton/torchtune/runs/rl111\n",
      "wandb: ⭐️ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20250105_004923-rl111/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 2 model files to /home/ubuntu/atreides/experiments/models/rl111/0002\n"
     ]
    }
   ],
   "source": [
    "trainer.tune_recipe_config.loss.kl_coef = 0.3\n",
    "await trainer.tune(trainer.explore_results[-1], verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl111/0002 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl111/0002 --port=8013 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52af20f939294b1880eb6a2e10ce2734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc1e5b1183946b0a23c6994b230e087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 62 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl111/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1|31|Loss: 0.0520: 100%|██████████| 31/31 [05:38<00:00, 10.72s/it, advantage=0.0000, advantage_prediction=0.7889, entropy=0.6102, entropy_target=0.0102, exploration=0.3028, kl_div=0.1441, policy=-0.0074, reinforce=0.0150, tanh_log_policy=-0.0077, unclipped_policy=-0.0423, value=0.0000, weighted_ce=0.0150, weighted_entropy=-0.0291, weighted_kl_div=-0.0009]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 3 model files to /home/ubuntu/atreides/experiments/models/rl111/0003\n",
      "Starting 2 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl111/0003 --port=8004 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl111/0003 --port=8014 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4b18b5ffda4cf4b2e2000ee9f6cee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3c4d9b29c84a358701d2bffae470ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.tune_recipe_config.loss.kl_coef = 0.4\n",
    "await trainer.train(iterations=50, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.tune_recipe_config.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 51 sequences\n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl110/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0104 23:57:40.116000 124059 torch/distributed/run.py:793] \n",
      "W0104 23:57:40.116000 124059 torch/distributed/run.py:793] *****************************************\n",
      "W0104 23:57:40.116000 124059 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0104 23:57:40.116000 124059 torch/distributed/run.py:793] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: lib.rl.mlp_head_checkpointer.MLPHeadCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl110/0001\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl110/0001/hf_model_0001.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl110/0001/hf_model_0002.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl110/0001/hf_model_0003.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl110/0001/hf_model_0004.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl110\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl110/tensors\n",
      "  num_sequences: 50\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  advantage_prediction_coef: 0.0\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.6\n",
      "  entropy_target_coef: 0.05\n",
      "  kl_coef: 0.4\n",
      "  normalize_advantages: false\n",
      "  normalize_value_predictions: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  predicted_advantage_weight: 0.0\n",
      "  tanh_log_policy_coef: 0.8\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.0\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: 32\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl110\n",
      "  name: rl110\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl110\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: true\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20250104_235744-rl110\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl110\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl110\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/atreides/experiments/models/rl110/0001/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:Loading MLP head weights from /home/ubuntu/atreides/experiments/models/rl110/0001/mlp_head.pt.ignore\n",
      "INFO:torchtune.utils._logging:Loading MLP head weights from /home/ubuntu/atreides/experiments/models/rl110/0001/mlp_head.pt.ignore\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 7.38 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.95 GiB\n",
      "\tGPU peak memory reserved: 16.12 GiB\n",
      "\tGPU peak memory active: 15.95 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|25|Loss: 0.1052: 100%|██████████| 25/25 [04:33<00:00, 10.70s/it, advantage=0.0000, advantage_prediction=0.2916, entropy=0.5669, entropy_target=0.0331, exploration=0.6689, kl_div=0.2571, policy=-0.0102, reinforce=0.0123, tanh_log_policy=0.0009, unclipped_policy=-0.0244, value=0.0000, weighted_ce=0.0123, weighted_entropy=-0.0120, weighted_kl_div=-0.0056]   INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 8.21 secs\n",
      "INFO:torchtune.utils._logging:MLP head checkpoint saved to /home/ubuntu/atreides/experiments/models/rl110/mlp_head_0.pt.ignore\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl110/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl110/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl110/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl110/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 11.61 secs\n",
      "1|25|Loss: 0.1052: 100%|██████████| 25/25 [04:54<00:00, 11.76s/it, advantage=0.0000, advantage_prediction=0.2916, entropy=0.5669, entropy_target=0.0331, exploration=0.6689, kl_div=0.2571, policy=-0.0102, reinforce=0.0123, tanh_log_policy=0.0009, unclipped_policy=-0.0244, value=0.0000, weighted_ce=0.0123, weighted_entropy=-0.0120, weighted_kl_div=-0.0056]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:      advantage_prediction ▁▃▂▁▂▁▃▁▁█▁▃▂▃▁▁▁▂▁▁▁▁▁▃▁\n",
      "wandb:                   entropy ▆▆▆▂▄▄▂▂█▃▁▅▇▅▁▆▃▄▇▆▇▅▄▅▃\n",
      "wandb:            entropy_target ▆▄▆▂▂▂▂▃█▁▄▃▇▃▄▅▂▂▆▅▆▃▂▃▁\n",
      "wandb:               exploration ▄▅▇▃▅▄▃▂█▃▁▄▆▄▁▅▂▃▅▄▅▄▃▄▃\n",
      "wandb:               global_step ▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██\n",
      "wandb:                    kl_div ▅▇▇▄█▅▅▃▅▅▂▅▅▅▁▇▄▄▇▅▇▅▅▆▄\n",
      "wandb:                      loss ▆▆█▄█▄▄▃▆▅▂▅▆▅▁▇▃▃█▅█▅▅▆▃\n",
      "wandb:                        lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:        peak_memory_active ▁████████████████████████\n",
      "wandb:         peak_memory_alloc ▁████████████████████████\n",
      "wandb:      peak_memory_reserved ▁████████████████████████\n",
      "wandb:                    policy ▁▅▃▃▃▂▅▁▃█▂▅▂▆▁▂▂▃▂▁▁▃▁▆▁\n",
      "wandb:                 reinforce █▁▆▆▆▄▄▆▂▄▆▄▄▄▆▅▆▄▇▆▇▆▇▃▇\n",
      "wandb:           tanh_log_policy █▁▆▆▅▃▄▅▂█▅▅▅▄▅▄▆▁▆▆█▇▅▄▆\n",
      "wandb: tokens_per_second_per_gpu ▁███▄▇▆▇█▃▆▇▆▇██▆▇▆█▆▇█▅▇\n",
      "wandb:          unclipped_policy ▁▅▃▃▂▃▅▁▃█▂▅▂▆▁▂▂▃▂▁▁▃▁▆▁\n",
      "wandb:                     value ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:               weighted_ce █▁▆▆▆▄▄▆▂▄▆▄▄▄▆▅▆▄▇▆▇▆▇▃▇\n",
      "wandb:          weighted_entropy ▁█▃▃▃▅▅▃▆▆▃▅▅▆▃▄▃▆▂▃▂▃▂▇▂\n",
      "wandb:           weighted_kl_div ▁█▃▃▃▄▅▃▅▅▃▆▄▆▂▃▃▃▂▃▁▃▂▇▂\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:      advantage_prediction 0.2916\n",
      "wandb:                   entropy 0.5669\n",
      "wandb:            entropy_target 0.0331\n",
      "wandb:               exploration 0.66893\n",
      "wandb:               global_step 25\n",
      "wandb:                    kl_div 0.25706\n",
      "wandb:                      loss 0.10523\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 53.40027\n",
      "wandb:         peak_memory_alloc 53.40027\n",
      "wandb:      peak_memory_reserved 64.26562\n",
      "wandb:                    policy -0.0102\n",
      "wandb:                 reinforce 0.01232\n",
      "wandb:           tanh_log_policy 0.00093\n",
      "wandb: tokens_per_second_per_gpu 1295.42212\n",
      "wandb:          unclipped_policy -0.02443\n",
      "wandb:                     value 0\n",
      "wandb:               weighted_ce 0.01232\n",
      "wandb:          weighted_entropy -0.01196\n",
      "wandb:           weighted_kl_div -0.00564\n",
      "wandb: \n",
      "wandb: 🚀 View run rl110 at: https://wandb.ai/bradhilton/torchtune/runs/rl110\n",
      "wandb: ⭐️ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20250104_235744-rl110/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 2 model files to /home/ubuntu/atreides/experiments/models/rl110/0002\n"
     ]
    }
   ],
   "source": [
    "# trainer.tune_recipe_config.shuffle = False\n",
    "trainer.tune_recipe_config.loss.kl_coef = 0.4\n",
    "await trainer.tune(trainer.explore_results[-1], verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2 vLLM servers...\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve NousResearch/Hermes-2-Theta-Llama-3-8B --port=8001 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "INFO 01-04 17:58:02 api_server.py:712] vLLM API server version 0.6.6.post1\n",
      "INFO 01-04 17:58:02 api_server.py:713] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=100.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=16384, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode='swap', served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7aeaa537f4c0>)\n",
      "INFO 01-04 17:58:02 api_server.py:712] vLLM API server version 0.6.6.post1\n",
      "INFO 01-04 17:58:02 api_server.py:713] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-2-Theta-Llama-3-8B', config='', host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='NousResearch/Hermes-2-Theta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=16384, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=100.0, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=16384, max_num_seqs=512, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode='swap', served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x78f3567834c0>)\n",
      "INFO 01-04 17:58:02 api_server.py:199] Started engine process with PID 16268\n",
      "INFO 01-04 17:58:02 api_server.py:199] Started engine process with PID 16269\n",
      "WARNING 01-04 17:58:03 config.py:2398] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 01-04 17:58:03 config.py:2398] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 01-04 17:58:06 config.py:2398] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "WARNING 01-04 17:58:06 config.py:2398] User-specified max_model_len (16384) is greater than the derived max_model_len (max_position_embeddings=8192 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. Make sure the value is correct and within the model context size.\n",
      "INFO 01-04 17:58:07 config.py:510] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 01-04 17:58:07 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 01-04 17:58:07 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 01-04 17:58:07 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 01-04 17:58:07 config.py:510] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "INFO 01-04 17:58:07 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 01-04 17:58:07 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 01-04 17:58:07 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 01-04 17:58:10 config.py:510] This model supports multiple tasks: {'embed', 'score', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 01-04 17:58:10 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 01-04 17:58:10 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 01-04 17:58:10 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 01-04 17:58:10 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
      "INFO 01-04 17:58:10 config.py:510] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "INFO 01-04 17:58:10 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 01-04 17:58:10 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 01-04 17:58:10 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 01-04 17:58:10 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
      "INFO 01-04 17:58:11 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-04 17:58:11 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-04 17:58:12 model_runner.py:1094] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 01-04 17:58:12 model_runner.py:1094] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 01-04 17:58:12 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "INFO 01-04 17:58:12 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.47it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.48it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.42it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.12it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.11it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.78it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.74it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.79it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.75it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 17:58:15 model_runner.py:1099] Loading model weights took 14.9595 GB\n",
      "INFO 01-04 17:58:15 model_runner.py:1099] Loading model weights took 14.9595 GB\n",
      "INFO 01-04 17:58:15 worker.py:241] Memory profiling takes 0.75 seconds\n",
      "INFO 01-04 17:58:15 worker.py:241] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.90) = 71.19GiB\n",
      "INFO 01-04 17:58:15 worker.py:241] model weights take 14.96GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.48GiB; the rest of the memory reserved for KV Cache is 53.57GiB.\n",
      "INFO 01-04 17:58:16 gpu_executor.py:76] # GPU blocks: 13714, # CPU blocks: 25600\n",
      "INFO 01-04 17:58:16 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 26.79x\n",
      "INFO 01-04 17:58:16 worker.py:241] Memory profiling takes 0.78 seconds\n",
      "INFO 01-04 17:58:16 worker.py:241] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.90) = 71.19GiB\n",
      "INFO 01-04 17:58:16 worker.py:241] model weights take 14.96GiB; non_torch_memory takes 0.17GiB; PyTorch activation peak memory takes 2.48GiB; the rest of the memory reserved for KV Cache is 53.57GiB.\n",
      "INFO 01-04 17:58:16 gpu_executor.py:76] # GPU blocks: 13714, # CPU blocks: 25600\n",
      "INFO 01-04 17:58:16 gpu_executor.py:80] Maximum concurrency for 16384 tokens per request: 26.79x\n",
      "INFO 01-04 17:59:00 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 45.02 seconds\n",
      "INFO 01-04 17:59:00 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 45.03 seconds\n",
      "INFO 01-04 17:59:00 api_server.py:640] Using supplied chat template:\n",
      "INFO 01-04 17:59:00 api_server.py:640] None\n",
      "INFO 01-04 17:59:00 launcher.py:19] Available routes are:\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /pooling, Methods: POST\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /score, Methods: POST\n",
      "INFO 01-04 17:59:00 launcher.py:27] Route: /v1/score, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [16149]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 17:59:01 api_server.py:640] Using supplied chat template:\n",
      "INFO 01-04 17:59:01 api_server.py:640] None\n",
      "INFO 01-04 17:59:01 launcher.py:19] Available routes are:\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /pooling, Methods: POST\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /score, Methods: POST\n",
      "INFO 01-04 17:59:01 launcher.py:27] Route: /v1/score, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [16147]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-04 17:59:01 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO:     127.0.0.1:35036 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n",
      "INFO 01-04 17:59:02 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO:     127.0.0.1:50284 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b6e5fcea594334836f83d0c33ced84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await trainer.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention.flex_attention import create_block_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention.flex_attention import (\n",
    "        BlockMask,\n",
    "        create_block_mask as create_block_causal_mask_flex,\n",
    "        flex_attention,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Union[torch.Tensor, torch.nn.attention.flex_attention.BlockMask]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.modules.attention_utils import _MaskType\n",
    "_MaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 43 sequences\n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl101/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0104 18:50:54.849000 19603 torch/distributed/run.py:793] \n",
      "W0104 18:50:54.849000 19603 torch/distributed/run.py:793] *****************************************\n",
      "W0104 18:50:54.849000 19603 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0104 18:50:54.849000 19603 torch/distributed/run.py:793] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: lib.rl.mlp_head_checkpointer.MLPHeadCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl101\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: 1.0\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl101/tensors\n",
      "  num_sequences: 43\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  advantage_prediction_coef: 0.0\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.6\n",
      "  entropy_target_coef: 0.05\n",
      "  kl_coef: 0.05\n",
      "  normalize_advantages: false\n",
      "  normalize_value_predictions: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  predicted_advantage_weight: 0.0\n",
      "  tanh_log_policy_coef: 0.4\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.0\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: 32\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl101\n",
      "  name: rl101\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl101\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: true\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20250104_185059-rl101\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl101\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl101\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:No MLP head weights found, will initialize from scratch\n",
      "INFO:torchtune.utils._logging:No MLP head weights found, will initialize from scratch\n",
      "INFO:torchtune.utils._logging:FSDP is enabled. Instantiating model and loading checkpoint on Rank 0 ...\n",
      "INFO:torchtune.utils._logging:Instantiating model and loading checkpoint took 7.08 secs\n",
      "INFO:torchtune.utils._logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.95 GiB\n",
      "\tGPU peak memory reserved: 16.12 GiB\n",
      "\tGPU peak memory active: 15.95 GiB\n",
      "INFO:torchtune.utils._logging:Optimizer is initialized.\n",
      "INFO:torchtune.utils._logging:Loss is initialized.\n",
      "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
      "WARNING:torchtune.utils._logging: Profiling disabled.\n",
      "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|22|Loss: 0.0015: 100%|██████████| 22/22 [04:01<00:00, 10.72s/it, advantage=0.0000, advantage_prediction=0.8766, entropy=0.5677, entropy_target=0.0323, exploration=0.4443, kl_div=0.2010, policy=0.0071, reinforce=-0.0468, tanh_log_policy=-0.0254, unclipped_policy=-0.0767, value=0.0000, weighted_ce=-0.0468, weighted_entropy=0.0108, weighted_kl_div=0.0380] INFO:torchtune.utils._logging:Saving checkpoint. This may take some time. Retrieving full model state dict...\n",
      "INFO:torchtune.utils._logging:Getting full model state dict took 8.32 secs\n",
      "INFO:torchtune.utils._logging:MLP head checkpoint saved to /home/ubuntu/atreides/experiments/models/rl101/mlp_head_0.pt.ignore\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.98 GB saved to /home/ubuntu/atreides/experiments/models/rl101/hf_model_0001_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 5.00 GB saved to /home/ubuntu/atreides/experiments/models/rl101/hf_model_0002_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 4.92 GB saved to /home/ubuntu/atreides/experiments/models/rl101/hf_model_0003_0.pt\n",
      "INFO:torchtune.utils._logging:Model checkpoint of size 1.17 GB saved to /home/ubuntu/atreides/experiments/models/rl101/hf_model_0004_0.pt\n",
      "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
      "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "INFO:torchtune.utils._logging:Saving checkpoint took 11.57 secs\n",
      "1|22|Loss: 0.0015: 100%|██████████| 22/22 [04:22<00:00, 11.91s/it, advantage=0.0000, advantage_prediction=0.8766, entropy=0.5677, entropy_target=0.0323, exploration=0.4443, kl_div=0.2010, policy=0.0071, reinforce=-0.0468, tanh_log_policy=-0.0254, unclipped_policy=-0.0767, value=0.0000, weighted_ce=-0.0468, weighted_entropy=0.0108, weighted_kl_div=0.0380]\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:      advantage_prediction ▅▃▃▂▄▆▂▃▅▆▄▃▇█▄▂▄▃▃▁▅▃\n",
      "wandb:                   entropy ▄▃▂▄█▃▅█▄▂▆▄▁▄▅▄▄▄▄▃▅▄\n",
      "wandb:            entropy_target ▃▅▇▂▆▄▂▆▃▆▃▃█▂▁▂▃▃▂▅▁▂\n",
      "wandb:               exploration ▆▅▂▃▇▂▄█▃▁▆▄▁▄▅▄▃▄▄▂▄▃\n",
      "wandb:               global_step ▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██\n",
      "wandb:                 grad_norm ▆▅▄▄█▆▆▅▄▄▃▂▃▅▃▁▃▃▃▂▂▁\n",
      "wandb:                    kl_div ▆▄▂▄█▃▅▅▄▁▆▃▁▄▅▄▃▁▄▂▅▃\n",
      "wandb:                      loss ▅▇▇▅█▅▅▇▅▆▆▅▇▄▅▅▅▄▅▆▄▁\n",
      "wandb:                        lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:        peak_memory_active ▁█████████████████████\n",
      "wandb:         peak_memory_alloc ▁█████████████████████\n",
      "wandb:      peak_memory_reserved ▁█████████████████████\n",
      "wandb:                    policy ▅▂▃▃▂▄▃▂▁▃▇▄▁█▄▁▅▃▃▁▅▁\n",
      "wandb:                 reinforce ▂▇▅▄▃▃▄█▄▂▄▂▅▂▅▄▂▇▄▅▁▁\n",
      "wandb:           tanh_log_policy ▆█▇█▇▇██▇▇▇▇▇▆█▇▇▇▇▇▇▁\n",
      "wandb: tokens_per_second_per_gpu ▁▆▇▇▇▇▇▆▅█▆▆▇▇▆▇▇▇▇▆▆▆\n",
      "wandb:          unclipped_policy ▆▄▅▅▄▆▅▄▃▅▇▅▄█▅▃▆▅▅▄▆▁\n",
      "wandb:                     value ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:               weighted_ce ▂▇▅▄▃▃▄█▄▂▄▂▅▂▅▄▂▇▄▅▁▁\n",
      "wandb:          weighted_entropy ▇▂▄▅▄▆▄▁▄▅▄▆▃▇▂▄▆▂▅▃█▄\n",
      "wandb:           weighted_kl_div ▄▂▂▃▄▄▃▁▂▅▃▅▃▅▄▃▄▂▄▃▅█\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:      advantage_prediction 0.87656\n",
      "wandb:                   entropy 0.5677\n",
      "wandb:            entropy_target 0.0323\n",
      "wandb:               exploration 0.44427\n",
      "wandb:               global_step 22\n",
      "wandb:                 grad_norm 0.20605\n",
      "wandb:                    kl_div 0.20099\n",
      "wandb:                      loss 0.0015\n",
      "wandb:                        lr 0.0\n",
      "wandb:        peak_memory_active 53.40022\n",
      "wandb:         peak_memory_alloc 53.40022\n",
      "wandb:      peak_memory_reserved 65.24023\n",
      "wandb:                    policy 0.00708\n",
      "wandb:                 reinforce -0.04682\n",
      "wandb:           tanh_log_policy -0.0254\n",
      "wandb: tokens_per_second_per_gpu 1196.11389\n",
      "wandb:          unclipped_policy -0.07667\n",
      "wandb:                     value 0\n",
      "wandb:               weighted_ce -0.04682\n",
      "wandb:          weighted_entropy 0.01075\n",
      "wandb:           weighted_kl_div 0.03803\n",
      "wandb: \n",
      "wandb: 🚀 View run rl101 at: https://wandb.ai/bradhilton/torchtune/runs/rl101\n",
      "wandb: ⭐️ View project at: https://wandb.ai/bradhilton/torchtune\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20250104_185059-rl101/logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved iteration 1 model files to /home/ubuntu/atreides/experiments/models/rl101/0001\n"
     ]
    }
   ],
   "source": [
    "trainer.tune_recipe_config.clip_grad_norm = 1.0\n",
    "await trainer.tune(result, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 2 vLLM servers...\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl101/0001 --port=8000 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "$ vllm serve /home/ubuntu/atreides/experiments/models/rl101/0001 --port=8002 --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.9 --max-model-len=16384 --max-num-seqs=512 --max-num-batched-tokens=16384 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=100 --api-key=default\n",
      "vLLM servers started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09379b3d25214b1895c36efdb29a1bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "temporal_clue:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36a547692e24235a57811be31cab7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "explore:   0%|          | 0/64 [00:00<?, ?episode/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 28 sequences\n",
      "Experienced the following exception while stopping vLLM servers: <class 'TimeoutError'> \n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl101/config.yaml\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No model checkpoint files found to save in output directory /home/ubuntu/atreides/experiments/models/rl101",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtrain(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:300\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, iterations, verbosity)\u001b[0m\n\u001b[1;32m    298\u001b[0m     explore_result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(explore_result, ExploreResult)\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune(explore_result, verbosity\u001b[38;5;241m=\u001b[39mverbosity)\n\u001b[1;32m    301\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m     )\n\u001b[1;32m    311\u001b[0m )\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:786\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, result, verbosity)\u001b[0m\n\u001b[1;32m    784\u001b[0m     cleanup_before_training()\n\u001b[1;32m    785\u001b[0m     recipe_main(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune_recipe_config)\n\u001b[0;32m--> 786\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:926\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, base_checkpoint_dir)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_checkpoint_dir: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# Find the latest epoch number from model checkpoint files\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    914\u001b[0m         (\n\u001b[1;32m    915\u001b[0m             \u001b[38;5;28mint\u001b[39m(result\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    922\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 926\u001b[0m         epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model checkpoint files found to save in output directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# Find the next iteration number by looking at existing subdirectories\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    932\u001b[0m             (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    941\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: No model checkpoint files found to save in output directory /home/ubuntu/atreides/experiments/models/rl101"
     ]
    }
   ],
   "source": [
    "await trainer.train(iterations=50, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning model on 28 sequences\n",
      "$ tune run --nnodes=1 --nproc-per-node=2 lib.rl.recipe.TuneRecipe --config /home/ubuntu/atreides/experiments/models/rl101/config.yaml\n",
      "Running with torchrun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0104 19:02:08.771000 21853 torch/distributed/run.py:793] \n",
      "W0104 19:02:08.771000 21853 torch/distributed/run.py:793] *****************************************\n",
      "W0104 19:02:08.771000 21853 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0104 19:02:08.771000 21853 torch/distributed/run.py:793] *****************************************\n",
      "INFO:torchtune.utils._logging:Running FullFinetuneRecipe with resolved config:\n",
      "\n",
      "batch_size: 1\n",
      "checkpointer:\n",
      "  _component_: lib.rl.mlp_head_checkpointer.MLPHeadCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/atreides/experiments/models/rl101/0001\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/atreides/experiments/models/rl101/0001/hf_model_0001.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl101/0001/hf_model_0002.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl101/0001/hf_model_0003.pt\n",
      "  - /home/ubuntu/atreides/experiments/models/rl101/0001/hf_model_0004.pt\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl101\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: 1.0\n",
      "compile: false\n",
      "custom_sharded_layers:\n",
      "- tok_embeddings\n",
      "- output\n",
      "dataset:\n",
      "  _component_: lib.rl.pack.PackedDataset\n",
      "  dir: /home/ubuntu/atreides/experiments/models/rl101/tensors\n",
      "  num_sequences: 28\n",
      "  sequence_length: 16384\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 1\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: lib.rl.ppo.PPOLoss\n",
      "  advantage_prediction_coef: 0.0\n",
      "  clip_epsilon: 0.2\n",
      "  entropy_coef: 0.0\n",
      "  entropy_target: 0.6\n",
      "  entropy_target_coef: 0.05\n",
      "  kl_coef: 0.05\n",
      "  normalize_advantages: false\n",
      "  normalize_value_predictions: false\n",
      "  normalize_values: false\n",
      "  policy_coef: 0.0\n",
      "  predicted_advantage_weight: 0.0\n",
      "  tanh_log_policy_coef: 0.4\n",
      "  unclipped_policy_coef: 0.0\n",
      "  value_coef: 0.0\n",
      "  weighted_ce_coef: 0.0\n",
      "  weighted_entropy_coef: 0.0\n",
      "  weighted_kl_coef: 0.0\n",
      "max_steps_per_epoch: 32\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  id: rl101\n",
      "  name: rl101\n",
      "  resume: allow\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1._model_builders.llama3_1_8b\n",
      "num_output_chunks: 4\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 4.0e-06\n",
      "optimizer_in_bwd: false\n",
      "reference_checkpointer:\n",
      "  _component_: torchtune.training.checkpointing._checkpointer.FullModelHFCheckpointer\n",
      "  checkpoint_dir: /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725\n",
      "  checkpoint_files:\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00001-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00003-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00004-of-00004.safetensors\n",
      "  - /home/ubuntu/.cache/huggingface/hub/models--NousResearch--Hermes-2-Theta-Llama-3-8B/snapshots/57a73110702e7b05ba3f39fef36297454c680725/model-00002-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: /home/ubuntu/atreides/experiments/models/rl101\n",
      "  recipe_checkpoint: null\n",
      "resume_from_checkpoint: false\n",
      "seed: 42\n",
      "shuffle: true\n",
      "\n",
      "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 42. Local seed is seed + rank = 42 + 0\n",
      "wandb: Currently logged in as: bradhilton. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.18.3\n",
      "wandb: Run data is saved locally in /home/ubuntu/atreides/experiments/wandb/run-20250104_190213-rl101\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Resuming run rl101\n",
      "wandb: ⭐️ View project at https://wandb.ai/bradhilton/torchtune\n",
      "wandb: 🚀 View run at https://wandb.ai/bradhilton/torchtune/runs/rl101\n",
      "INFO:torchtune.utils._logging:Logging /home/ubuntu/atreides/experiments/models/rl101/0001/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils._logging:Loading MLP head weights from /home/ubuntu/atreides/experiments/models/rl101/0001/mlp_head.pt.ignore\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/training/checkpointing/_utils.py\", line 203, in safe_torch_load\n",
      "[rank1]:     state_dict = torch.load(\n",
      "[rank1]:                  ^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 1359, in load\n",
      "[rank1]:     raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
      "[rank1]: _pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "[rank1]: \t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "[rank1]: \t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "[rank1]: \tWeightsUnpickler error: Unsupported global: GLOBAL torch.distributed.tensor.DTensor was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DTensor])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "[rank1]: Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "[rank1]:   File \"<frozen runpy>\", line 88, in _run_code\n",
      "[rank1]:   File \"/home/ubuntu/atreides/experiments/lib/rl/recipe.py\", line 1324, in <module>\n",
      "[rank1]:     sys.exit(config.parse(recipe_main)())  # type: ignore\n",
      "[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "[rank1]:     sys.exit(recipe_main(conf))\n",
      "[rank1]:              ^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/ubuntu/atreides/experiments/lib/rl/recipe.py\", line 1318, in recipe_main\n",
      "[rank1]:     recipe.setup(cfg=cfg)\n",
      "[rank1]:   File \"/home/ubuntu/atreides/experiments/lib/rl/recipe.py\", line 464, in setup\n",
      "[rank1]:     checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)\n",
      "[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/ubuntu/atreides/experiments/lib/rl/recipe.py\", line 407, in load_checkpoint\n",
      "[rank1]:     checkpoint_dict = self._checkpointer.load_checkpoint()\n",
      "[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/ubuntu/atreides/experiments/lib/rl/mlp_head_checkpointer.py\", line 33, in load_checkpoint\n",
      "[rank1]:     converted_state_dict[MLP_HEAD_KEY] = safe_torch_load(mlp_head_path)\n",
      "[rank1]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/training/checkpointing/_utils.py\", line 210, in safe_torch_load\n",
      "[rank1]:     raise ValueError(f\"Unable to load checkpoint from {checkpoint_path}. \") from e\n",
      "[rank1]: ValueError: Unable to load checkpoint from /home/ubuntu/atreides/experiments/models/rl101/0001/mlp_head.pt.ignore. \n",
      "INFO:torchtune.utils._logging:Loading MLP head weights from /home/ubuntu/atreides/experiments/models/rl101/0001/mlp_head.pt.ignore\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/training/checkpointing/_utils.py\", line 203, in safe_torch_load\n",
      "[rank0]:     state_dict = torch.load(\n",
      "[rank0]:                  ^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/serialization.py\", line 1359, in load\n",
      "[rank0]:     raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
      "[rank0]: _pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "[rank0]: \t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "[rank0]: \t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "[rank0]: \tWeightsUnpickler error: Unsupported global: GLOBAL torch.distributed.tensor.DTensor was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DTensor])` to allowlist this global if you trust this class/function.\n",
      "\n",
      "[rank0]: Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "[rank0]:   File \"<frozen runpy>\", line 88, in _run_code\n",
      "[rank0]:   File \"/home/ubuntu/atreides/experiments/lib/rl/recipe.py\", line 1324, in <module>\n",
      "[rank0]:     sys.exit(config.parse(recipe_main)())  # type: ignore\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "[rank0]:     sys.exit(recipe_main(conf))\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/ubuntu/atreides/experiments/lib/rl/recipe.py\", line 1318, in recipe_main\n",
      "[rank0]:     recipe.setup(cfg=cfg)\n",
      "[rank0]:   File \"/home/ubuntu/atreides/experiments/lib/rl/recipe.py\", line 464, in setup\n",
      "[rank0]:     checkpoint_dict = self.load_checkpoint(cfg_checkpointer=cfg.checkpointer)\n",
      "[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/ubuntu/atreides/experiments/lib/rl/recipe.py\", line 407, in load_checkpoint\n",
      "[rank0]:     checkpoint_dict = self._checkpointer.load_checkpoint()\n",
      "[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/ubuntu/atreides/experiments/lib/rl/mlp_head_checkpointer.py\", line 33, in load_checkpoint\n",
      "[rank0]:     converted_state_dict[MLP_HEAD_KEY] = safe_torch_load(mlp_head_path)\n",
      "[rank0]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/training/checkpointing/_utils.py\", line 210, in safe_torch_load\n",
      "[rank0]:     raise ValueError(f\"Unable to load checkpoint from {checkpoint_path}. \") from e\n",
      "[rank0]: ValueError: Unable to load checkpoint from /home/ubuntu/atreides/experiments/models/rl101/0001/mlp_head.pt.ignore. \n",
      "W0104 19:02:15.107000 21853 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 21918 closing signal SIGTERM\n",
      "E0104 19:02:15.271000 21853 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 21919) of binary: /home/ubuntu/atreides/.venv/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/atreides/.venv/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 49, in main\n",
      "    parser.run(args)\n",
      "  File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/_cli/tune.py\", line 43, in run\n",
      "    args.func(args)\n",
      "  File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 206, in _run_cmd\n",
      "    self._run_distributed(args, is_builtin=is_builtin)\n",
      "  File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torchtune/_cli/run.py\", line 95, in _run_distributed\n",
      "    run(args)\n",
      "  File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/atreides/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "lib.rl.recipe FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-01-04_19:02:15\n",
      "  host      : 192-222-53-59\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 21919)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No model checkpoint files found to save in output directory /home/ubuntu/atreides/experiments/models/rl101",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtune(trainer\u001b[38;5;241m.\u001b[39mexplore_results[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:786\u001b[0m, in \u001b[0;36mTrainer.tune\u001b[0;34m(self, result, verbosity)\u001b[0m\n\u001b[1;32m    784\u001b[0m     cleanup_before_training()\n\u001b[1;32m    785\u001b[0m     recipe_main(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune_recipe_config)\n\u001b[0;32m--> 786\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/atreides/experiments/lib/rl/trainer.py:926\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, base_checkpoint_dir)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save\u001b[39m(\u001b[38;5;28mself\u001b[39m, base_checkpoint_dir: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m# Find the latest epoch number from model checkpoint files\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    914\u001b[0m         (\n\u001b[1;32m    915\u001b[0m             \u001b[38;5;28mint\u001b[39m(result\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    922\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 926\u001b[0m         epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model checkpoint files found to save in output directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# Find the next iteration number by looking at existing subdirectories\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    932\u001b[0m             (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    941\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: No model checkpoint files found to save in output directory /home/ubuntu/atreides/experiments/models/rl101"
     ]
    }
   ],
   "source": [
    "await trainer.tune(trainer.explore_results[-1], verbosity=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
