{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Generic, Optional, TypeVar\n",
    "\n",
    "A = TypeVar(\"A\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Step(Generic[A]):\n",
    "    parent: Optional[\"Step[A]\"]\n",
    "    action: A\n",
    "    reward: float\n",
    "    children: list[\"Step[A]\"] = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def state(self) -> list[A]:\n",
    "        \"\"\"\n",
    "        S - The state (the history of actions) before taking this action\n",
    "        \"\"\"\n",
    "        if not self.parent:\n",
    "            return []\n",
    "        return self.parent.next_state\n",
    "\n",
    "    @property\n",
    "    def next_state(self) -> list[A]:\n",
    "        \"\"\"\n",
    "        S' - The next state (the history of actions) after taking this action\n",
    "        \"\"\"\n",
    "        return self.state + [self.action]\n",
    "\n",
    "    def state_value(self, gamma: float = 0.9) -> float:\n",
    "        \"\"\"\n",
    "        V(s) - The expected future reward before taking this action\n",
    "        Calculated as sum of Q-values for all possible actions from the parent state\n",
    "        Same as Q(s,a) if this is the root node\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.parent.next_state_value(gamma)\n",
    "            if self.parent\n",
    "            else self.state_action_value(gamma)\n",
    "        )\n",
    "\n",
    "    def next_state_value(self, gamma: float = 0.9) -> float:\n",
    "        \"\"\"\n",
    "        V(s') - The expected future reward after taking this action\n",
    "        Calculated as sum of Q-values for all possible actions\n",
    "        \"\"\"\n",
    "        if not self.children:\n",
    "            return 0.0\n",
    "        return sum(child.state_action_value(gamma) for child in self.children) / len(\n",
    "            self.children\n",
    "        )\n",
    "\n",
    "    def state_action_value(self, gamma: float = 0.9) -> float:\n",
    "        \"\"\"\n",
    "        Q(s,a) - The expected future reward from taking this action in the parent state\n",
    "        Calculated as immediate reward plus discounted future state value\n",
    "        \"\"\"\n",
    "        return self.reward + gamma * self.next_state_value(gamma)\n",
    "\n",
    "    def advantage(self, gamma: float = 0.9) -> float:\n",
    "        \"\"\"\n",
    "        A(s, a) - The advantage of taking this action in the current state\n",
    "        Calculated as the difference between Q(s,a) and V(s)\n",
    "        \"\"\"\n",
    "        return self.state_action_value(gamma) - self.state_value(gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
